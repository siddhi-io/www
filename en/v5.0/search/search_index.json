{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"download/","text":"Siddhi 5.0 Download Select the appropriate Siddhi distribution for your use case. Siddhi Distribution Siddhi 5.0 ( Distribution 0.1.0 ) Siddhi Tooling Siddhi Runner Refer the user guide to use Siddhi as a Local Microservice Siddhi Docker Siddhi 5.0 (based on Distribution 0.1.0) Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu Refer the user guide to use Siddhi as a Docker Microservice Siddhi Kubernetes Siddhi 5.0 (based on Distribution 0.1.1) Siddhi CRD Refer the user guide to use Siddhi as Kubernetes Microservice Siddhi Libs Siddhi 5.0.x Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Refer the user guide to use Siddhi as a Java library For other Siddhi Versions refer the Download Archives","title":"Download"},{"location":"download/#siddhi-50-download","text":"Select the appropriate Siddhi distribution for your use case.","title":"Siddhi 5.0 Download"},{"location":"download/#siddhi-distribution","text":"Siddhi 5.0 ( Distribution 0.1.0 ) Siddhi Tooling Siddhi Runner Refer the user guide to use Siddhi as a Local Microservice","title":"Siddhi Distribution"},{"location":"download/#siddhi-docker","text":"Siddhi 5.0 (based on Distribution 0.1.0) Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu Refer the user guide to use Siddhi as a Docker Microservice","title":"Siddhi Docker"},{"location":"download/#siddhi-kubernetes","text":"Siddhi 5.0 (based on Distribution 0.1.1) Siddhi CRD Refer the user guide to use Siddhi as Kubernetes Microservice","title":"Siddhi Kubernetes"},{"location":"download/#siddhi-libs","text":"Siddhi 5.0.x Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Refer the user guide to use Siddhi as a Java library For other Siddhi Versions refer the Download Archives","title":"Siddhi Libs"},{"location":"release-notes/","text":"Release Notes WIP","title":"Release Notes"},{"location":"release-notes/#release-notes","text":"WIP","title":"Release Notes"},{"location":"development/","text":"Siddhi 5.0 Development Guide Obtaining and Building Project Source code Find the project source code here and the instruction to building the project repos here . Getting Involved in Project Development Siddhi design-related discussions are carried out in the Siddhi-Dev Google Group , you can subscribe to it to get notifications on the discussions and please feel free to get involved by contributing and sharing your thoughts and ideas. You can also propose changes or improvements by starting a thread in the Siddhi-Dev Google Group, and also by reporting issues in the Siddhi GitHub repository with the label type/improvement or type/new-feature . Project Architecture Find out about the architecture of Siddhi for the Siddhi Architecture documentation.","title":"Introduction"},{"location":"development/#siddhi-50-development-guide","text":"","title":"Siddhi 5.0 Development Guide"},{"location":"development/#obtaining-and-building-project-source-code","text":"Find the project source code here and the instruction to building the project repos here .","title":"Obtaining and Building Project Source code"},{"location":"development/#getting-involved-in-project-development","text":"Siddhi design-related discussions are carried out in the Siddhi-Dev Google Group , you can subscribe to it to get notifications on the discussions and please feel free to get involved by contributing and sharing your thoughts and ideas. You can also propose changes or improvements by starting a thread in the Siddhi-Dev Google Group, and also by reporting issues in the Siddhi GitHub repository with the label type/improvement or type/new-feature .","title":"Getting Involved in Project Development"},{"location":"development/#project-architecture","text":"Find out about the architecture of Siddhi for the Siddhi Architecture documentation.","title":"Project Architecture"},{"location":"development/architecture/","text":"Siddhi 5.0 Architecture Siddhi is an open source, cloud-native, stream processing and complex event processing engine. It can be utilized in any of the following ways: Run as a server on its own Run as a micro service on bare metal, VM, Docker and natively in Kubernetes Embedded into any Java or Python based application Run on an Android application Siddhi provides streaming data integration and data analytical operators. It connects multiple disparate live data sources, orchestrates data flows, calculates analytics, and also detects complex event patterns. This allows developers to build applications that collect data, perform data transformation and analytics, and publish the results to data sinks in real time. This section illustrates the architecture of the Siddhi Engine and guides you through its key functionality. We hope this article helps developers to understand Siddhi and its codebase better, and also help them to contribute and improve Siddhi. Main Design Decisions Event-by-event processing of real-time streaming data to achieve low latency. Ease of use with Streaming SQL providing an intuitive way to express stream processing logic and complex event processing constructs such as Patterns. Achieve high performance by processing events in-memory and using data stores for long term data storage. Optimize performance by enforcing a strict event stream schema and by pre-compiling the queries. Optimize memory consumption by having only the absolutely necessary information in-memory and dropping the rest as soon as possible. Supporting multiple extension points to accommodate a diverse set of functionality such as supporting multiple sources, sinks, functions, aggregation operations, windows, etc. High-Level Architecture At a high level, Siddhi consumes events from various events sources, processes them according to the defined Siddhi application, and produces results to the subscribed event sinks. Siddhi can store and consume events from in-memory tables or from external data stores such as RDBMS , MongoDB , Hazelcast in-memory grid, etc. (i.e., when configured to do so). Siddhi also allows applications and users to query Siddhi via its Store Query API to interactively retrieve data from in-memory and other stores. Main Modules in Siddhi Engine Siddhi Engine comprises four main modules, they are: Siddhi Query API : This allows users to define the execution logic of the Siddhi application as queries and definitions using POJOs (Plain Old Java Objects). Internally, Siddhi uses these objects to identify the logic that it is expected to perform. Siddhi Query Compiler : This allows users to define the Siddhi application using the Siddhi Streaming SQL, and it compiles the Streaming SQL script to Siddhi Query API POJOs so that Siddhi can execute them. Siddhi Core : This builds the execution runtime based on the defined Siddhi Application POJOs and processes the incoming events as and when they arrive. Siddhi Annotation : This is a helper module that allows all extensions to be annotated so that they can be picked by Siddhi Core for processing. This also helps Siddhi to generate the extension documentation. Siddhi Component Architecture The following diagram illustrates the main components of Siddhi and how they work together. Here the Siddhi Core module maintains the execution logic. It also interacts with the external environment and systems for consuming, processing and publishing events. It uses the following components to achieve its tasks: SiddhiManager : This is a key component of Siddhi Core that manages Siddhi Application Runtimes and facilitates their functionality via Siddhi Context with periodic state persistence, statistics reporting and extension loading. It is recommended to use one Siddhi Manager for a single JVM. SiddhiAppRuntime : Siddhi Application Runtime can be generated for each Siddhi Application through the Siddhi Manager. Siddhi Application Runtimes provide an isolated execution environment for each defined Siddhi Application. These Siddhi Application Runtimes can have their own lifecycle and they execute based on the logic defined in their Siddhi Application. SiddhiContext : This is a shared object across all the Siddhi Application Runtimes within the same Siddhi manager. It contains references to the persistence store for periodic persistence, statistics manager to report performance statistics of Siddhi Application Runtimes, and extension holders for loading Siddhi extensions. Siddhi Application Creation Execution logic of the Siddhi Engine is composed as a Siddhi Application, and this is usually passed as a string to SiddhiManager to create the SiddhiAppRuntime for execution. When a Siddhi Application is passed to the SiddhiManager.createSiddhiAppRuntime() , it is processed internally with the SiddhiCompiler . Here, the SiddhiApp String is compiled to SiddhiApp object model by the SiddhiQLBaseVisitorImpl class. This validates the syntax of the given Siddhi Application. The model is then passed to the SiddhiAppParser to create the SiddhiAppRuntime . During this phase, the semantics of the Siddhi Application is validated and the execution logic of the Siddhi Application is optimized. Siddhi App Execution Flow Following diagram depicts the execution flow within a Siddhi App Runtime. The path taken by events within Siddhi Engine is indicated in blue. The components that are involved in handling the events are the following: StreamJunction This routes events of a particular stream to various components within the Siddhi App Runtime. A stream junction is generated for each defined or inferred Stream in the Siddhi Application. A stream junction by default uses the incoming event's thread and passes all the events to its subscribed components as soon as they arrive, but this behaviour can be altered by configuring @Async annotation to buffer the events at the and stream junction and to use another one or more threads to collect the events from the buffer and process the subsequent executions. InputHandler Input handler is used to push Event and Event[] objects into stream junctions from defined event sources, and from Java/Python programmes. StreamCallback This receives Event[] s from stream junction and passes them to event sinks to publish to external endpoints, and/or passes them to subscribed Java/Python programmes for further processing. Queries Partitions These components process events by filtering, transforming, aggregating, joining, pattern matching, etc. They consume events from one or more stream junctions, process them and publish the processed events into a set of stream junctions based on the defined queries or partitions. Source Sources consume events from external sources in various data formats, convert them into Siddhi events using SourceMapper s and pass them to corresponding stream junction via their associated input handlers. A source is generated for each @Source annotation defined above a stream definition. SourceMapper A source mapper is a sub-component of source, and it needs to be configured for each source in order to convert the incoming event into Siddhi event. The source mapper type can be configured using the @Map annotation within the @Source annotation. When the @Map annotation is not defined, Siddhi uses the PassThroughSourceMapper , where it assumes that the incoming message is already in the Siddhi Event format (i.e Event or Event[] ), and therefore makes no changes to the incoming event format. Sink Sinks consumes events from its associated stream junction, convert them to various data formats via SinkMapper and publish them to external endpoints as defined in the @Sink annotation. A sink is generated for each @Sink annotation defined above a stream definition. SinkMapper A sink mapper is a sub-component of sink. and its need to be configured for each sink in order to map the Siddhi events to the specified data format so that they can be published via the sink. The sink mapper type can be configured using the @Map annotation within the @Sink annotation. When the @Map annotation is not defined, Siddhi uses PassThroughSinkMapper , where it passes the Siddhi Event (i.e Event or Event[] ) without any formatting to the Sink. Table Tables are used to store events. When tables are defined by default, Siddhi uses the InMemoryTable implementation to store events in-memory. When @Store annotation is used on top of the table definition, it loads the associated external data store connector based on the defined store type. Most table implementations are extended from either AbstractRecordTable or AbstractQueryableRecordTable abstract classes the former provides the functionality to query external data store based on a given filtering condition, and the latter queries external data store by providing projection, limits, and ordering parameters in addition to data filter condition. Window Windows store events as and when they arrive and automatically expire/clean them based on the given window constraint. Multiple types of windows are can be implemented by extending the WindowProcessor abstract class. IncrementalAggregation Long running time series aggregates defined via the aggregation definition is calculated in an incremental manner using the Incremental Aggregation Processor for the defined time periods. Incremental aggregation functions can be implemented by extending IncrementalAttributeAggregator . By default, incremental aggregations aggregate all the values in-memory, but when it is associated with a store by adding @store annotation it uses in-memory to aggregate partial results and uses data stores to persist those increments. When requested for aggregate results it retrieves data from data stores and (if needed from) in-memory, computes combined aggregate results and provides as the output. Trigger A trigger triggers events at a given interval as given in the trigger definition. The triggered events are pushed to a stream junction having the same name as the trigger. QueryCallback A query callback taps into the events that are emitted by a particular query. It notifies the event occurrence timestamp and classifies the output events into currentEvents , and expiredEvents . Siddhi Query Execution Siddhi QueryRuntimes can be categorized into three main types: SingleInputStream : Queries that consist of query types such as filters and windows. JoinInputStream : Queries that consist of joins. StateInputStream : Queries that consist of patterns and sequences. The following section explains the internals of each query type. SingleInputStream Query Runtime (Filter Windows) A single input stream query runtime is generated for filter and window queries. They consume events from a stream junction or a window and convert the incoming events according to the expected output stream format at the ProcessStreamReceiver by dropping all the unrelated incoming stream attributes. Then the converted events are passed through a few Processors such as FilterProcessor , StreamProcessor , StreamFunctionProcessor , WindowProcessor , and QuerySelector . Here, the StreamProcessor , StreamFunctionProcessor , and WindowProcessor can be extended with various stream processing capabilities. The last processor of the chain of processors must always be a QuerySelector and it can't appear anywhere else. When the query runtime consumes events from a stream, its processor chain can maximum contain one WindowProcessor , and when query runtime consumes events from a window, its chain of processors cannot contain any WindowProcessor . The FilterProcessor is implemented using expressions that return a boolean value. ExpressionExecutor is used to process conditions, mathematical operations, unary operations, constant values, variables, and functions. Expressions have a tree structure, and they are processed based using the Depth First search algorithm. To achieve high performance, Siddhi currently depends on the user to formulate the least successful case in the leftmost side of the condition, thereby increasing the chance of early false detection. The condition expression price = 100 and ( Symbol == 'IBM' or Symbol == 'MSFT' ) is represented as shown below. These expressions also support the execution of user-defined functions (UDFs), and they can be implemented by extending the FunctionExecutor class. After getting processed by all the processors, events reach the QuerySelector for transformation. At the QuerySelector , events are transformed based on the select clause of the query. The select clause produces one AttributeProcessor for each output stream attribute, and these AttributeProcessor s contain expressions defining data transformation including constant values, variables, user-defined functions, etc. They can also contain AttributeAggregatorExecutor s to process aggregation operations such as sum , count , etc. If there is a Group By clause defined, then the GroupByKeyGenerator is used to identify the composite group-by key, and then for each key, an AttributeAggregatorExecutor state is generated to maintain per group-by key aggregations. When each time AttributeProcessor is executed the AttributeAggregatorExecutor calculates per group-by aggregation results and output the values. When AttributeAggregatorExecutor group-by states become obsolete, they are destroyed and automatically cleaned. After an event is transformed to the output format through the above process, it is evaluated against the having condition executor if a having clause is provided. The succeeding events are then ordered, and limited based on order by , limit and offset clauses before they pushed to the OutputRateLimiter . At OutputRateLimiter , the event output is controlled before sending the events to the stream junction or to the query callback. When the output clause is not defined, the PassThroughOutputRateLimiter is used by passing all the events without any rate limiting. Temporal Processing with Windows The temporal event processing aspect is achieved via Window and AttributeAggregators To achieve temporal processing, Siddhi uses the following four type of events: Current Events : Events that are newly arriving to the query from streams. Expired Events : Events that have expired from a window. Timer Events : Events that inform the query about an update of execution time. These events are usually generated by schedulers. Reset Events : Events that resets the Siddhi query states. In Siddhi, when an event comes into a WindowProcessor , it creates an appropriate expired event corresponding to the incoming current event with the expiring timestamp, and stores that event in the window. At the same time, WindowProcessor also forwards the current event to the next processor for further processing. It uses a scheduler or some other counting approach to determine when to emit the events that are stored in in-memory. When the expired events meet the condition for expiry based on the window contains, it emits the expired events to the next processor. At times like in window.timeBatch() there can be cases that need emitting all the events in-memory at once and the output does not need individual expired events values, in this cases the window emits a single reset event instead of sending one expired event for each event it has stored, so that it can reset the states in one go. For the QuerySelector aggregations to work correctly the window must emit a corresponding expired event for each current event it has emitted or it must send a reset event . In the QuerySelector , the arrived current events increase the aggregation values, expired events decrease the values, and reset events reset the aggregation calculation to produce correct query output. For example, the sliding TimeWindow ( window.time() ) creates a corresponding expired event for each current event that arrives, adds the expired event s to the window, adds an entry to the scheduler to notify when that event need to be expired, and finally sends the current event to the next processor for subsequent processing. The scheduler notifies the window by sending a timer event , and when the window receives an indication that the expected expiry time has come for the oldest event in the window via a timer event or by other means, it removes the expired event from the window and passes that to the next processor. JoinInputStream Query Runtime (Join) Join input stream query runtime is generated for join queries. This can consume events from two stream junctions and perform a join operation as depicted above. It can also perform a join by consuming events from one stream junction and join against itself, or it can also join against a table, window or an aggregation. When a join is performed with a table, window or aggregation, the WindowProcessor in the above image is replaced with the corresponding table, window or aggregation and no basic processors are used on their side. The joining operation is triggered by the events that arrive from the stream junction. Here, when an event from one stream reaches the pre JoinProcessor , it matches against all the available events of the other stream's WindowProcessor . When a match is found, those matched events are sent to the QuerySelector as current events , and at the same time, the original event is added to the WindowProcessor where it remains until it expires. Similarly, when an event expires from the WindowProcessor , it matches against all the available events of the other stream's WindowProcessor , and when a match is found, those matched events are sent to the QuerySelector as expired events . Note Despite the optimizations, a join query is quite expensive when it comes to performance. This is because the WindowProcessor is locked during the matching process to avoid race conditions and to achieve accuracy while joining. Therefore, when possible avoid matching large (time or length) windows in high volume streams. StateInputStream Query Runtime (Pattern Sequence) The state input stream query runtime is generated for pattern and sequence queries. This consumes events from one or more stream junctions via ProcessStreamReceiver s and checks whether the events match each pattern or sequence condition by processing the set of basic processors associated with each ProcessStreamReceiver . The PreStateProcessor s usually contains lists of state events that are already matched by previous conditions, and if its the first condition then it will have an empty state event in its list. When ProcessStreamReceiver consumes an event, it passes the event to the PreStateProcessor which updates the list of state events it has with the incoming event and executes the condition by passing the events to the basic processors. The state events that match the conditions reach the PostStateProcessor which will then stores the events to the state event list of the following PreStateProcessor . If it is the final condition's PostStateProcessor , then it will pass the state event to the QuerySelector to generate and emit the output. Siddhi Partition Execution A partition is a wrapper around one or more Siddhi queries and inner streams that connect them. A partition is implemented in Siddhi as a PartitionRuntime which contains multiple QueryRuntime s and inner stream junctions. Each partitioned stream entering the partition goes through a designated PartitionStreamReceiver . The PartitionExecutor of PartitionStreamReceiver evaluates the incoming events to identify their associated partition-key using either RangePartitionExecutor or ValuePartitionExecutor . The identified partition-key is then set as thread local variable and the event is passed to the QueryRuntime s of processing. The QueryRuntime s process events by maintaining separate states for each partition-key such that producing separate output per partition. When a partition query consumes a non-partitioned global stream, the QueryRuntime s are executed for each available partition-key in the system such that allowing all partitions to receive the same event. When the partitions are obsolete PartitionRuntime deletes all the partition states from its QueryRuntime s. Siddhi Aggregation Siddhi supports long duration time series aggregations via its aggregation definition. AggregationRuntime implements this by the use of streaming lambda architecture , where it processes part of the data in-memory and gets part of the data from data stores. AggregationRuntime creates an in-memory table or external store for each time granularity (i.e seconds, minutes, days, etc) it has to process the events, and when events enter it calculates the aggregations in-memory for its least granularity (usually seconds) using the IncrementalExecutor and maintains the running aggregation values in its BaseIncrementalValueStore . At each clock end time of the granularity (end of each second) IncrementalExecutor stores the summarized values to the associated granularity table and also passes the summarized values to the IncrementalExecutor of the next granularity level, which also follows the same methodology in processing the events. Through this approach each time granularities, the current time duration will be in-memory and all the historical time durations will be in stored in the tables. The aggregations results are calculated by IncrementalAttributeAggregator s and stored in such a way that allows proper data composition upon retrial, for example, avg() is stored as sum and count . This allows data composition across various granularity time durations when retrieving, for example, results for avg() composed by returning sum of sum s divided by the sum of count s. Aggregation can also work in a distributed manner and across system restarts. This is done by storing node specific IDs and granularity time duration information in the tables. To make sure tables do not go out of memory IncrementalDataPurging is used to purge old data. When aggregation is queried through join or store query for a given time granularity it reads the data from the in-memory BaseIncrementalValueStore and from the tables computes the composite results as described, and presents the results. Siddhi Event Formats Siddhi has three event formats. Event This is the format exposed to external systems when they send events via Input Handler and consume events via Stream Callback or Query Callback. This consists of a timestamp and an Object[] that contains all the values in accordance to the corresponding stream. StreamEvent (Subtype of ComplexEvent ) This is used within queries. This contains a timestamp and the following three Object[] s: beforeWindowData : This contains values that are only used in processors that are executed before the WindowProcessor . onAfterWindowData : This contains values that are only used by the WindowProcessor and the other processors that follow it, but not sent as output. outputData : This contains the values that are sent via the output stream of the query. In order to optimize the amount of data that is stored in the in-memory at windows, the content in beforeWindowData is cleared before the event enters the WindowProcessor . StreamEvents can also be chained by linking each other via the next property in them. StateEvent (Subtype of ComplexEvent ) This is used in joins, patterns and sequences queries when we need to associate events of multiple streams, tables, windows or aggregations together. This contains a timestamp , a collection of StreamEvent s representing different streams, tables, etc, that are used in the query, and an Object[] to contain outputData values that are needed for query output. The StreamEvent s within the StateEvent and the StateEvent themselves can be chained by linking each other with the next property in them. Event Chunks Event Chunks provide an easier way of manipulating the chain of StreamEvent s and StateEvent s so that they are be easily iterated, inserted and removed. Summary This article focuses on describing the architecture of Siddhi and rationalizing some of the architectural decisions made when implementing the system. It also explains the key features of Siddhi. We hope this will be a good starting point for new developers to understand Siddhi and to start contributing to it.","title":"Architecture"},{"location":"development/architecture/#siddhi-50-architecture","text":"Siddhi is an open source, cloud-native, stream processing and complex event processing engine. It can be utilized in any of the following ways: Run as a server on its own Run as a micro service on bare metal, VM, Docker and natively in Kubernetes Embedded into any Java or Python based application Run on an Android application Siddhi provides streaming data integration and data analytical operators. It connects multiple disparate live data sources, orchestrates data flows, calculates analytics, and also detects complex event patterns. This allows developers to build applications that collect data, perform data transformation and analytics, and publish the results to data sinks in real time. This section illustrates the architecture of the Siddhi Engine and guides you through its key functionality. We hope this article helps developers to understand Siddhi and its codebase better, and also help them to contribute and improve Siddhi.","title":"Siddhi 5.0 Architecture"},{"location":"development/architecture/#main-design-decisions","text":"Event-by-event processing of real-time streaming data to achieve low latency. Ease of use with Streaming SQL providing an intuitive way to express stream processing logic and complex event processing constructs such as Patterns. Achieve high performance by processing events in-memory and using data stores for long term data storage. Optimize performance by enforcing a strict event stream schema and by pre-compiling the queries. Optimize memory consumption by having only the absolutely necessary information in-memory and dropping the rest as soon as possible. Supporting multiple extension points to accommodate a diverse set of functionality such as supporting multiple sources, sinks, functions, aggregation operations, windows, etc.","title":"Main Design Decisions"},{"location":"development/architecture/#high-level-architecture","text":"At a high level, Siddhi consumes events from various events sources, processes them according to the defined Siddhi application, and produces results to the subscribed event sinks. Siddhi can store and consume events from in-memory tables or from external data stores such as RDBMS , MongoDB , Hazelcast in-memory grid, etc. (i.e., when configured to do so). Siddhi also allows applications and users to query Siddhi via its Store Query API to interactively retrieve data from in-memory and other stores.","title":"High-Level Architecture"},{"location":"development/architecture/#main-modules-in-siddhi-engine","text":"Siddhi Engine comprises four main modules, they are: Siddhi Query API : This allows users to define the execution logic of the Siddhi application as queries and definitions using POJOs (Plain Old Java Objects). Internally, Siddhi uses these objects to identify the logic that it is expected to perform. Siddhi Query Compiler : This allows users to define the Siddhi application using the Siddhi Streaming SQL, and it compiles the Streaming SQL script to Siddhi Query API POJOs so that Siddhi can execute them. Siddhi Core : This builds the execution runtime based on the defined Siddhi Application POJOs and processes the incoming events as and when they arrive. Siddhi Annotation : This is a helper module that allows all extensions to be annotated so that they can be picked by Siddhi Core for processing. This also helps Siddhi to generate the extension documentation.","title":"Main Modules in Siddhi Engine"},{"location":"development/architecture/#siddhi-component-architecture","text":"The following diagram illustrates the main components of Siddhi and how they work together. Here the Siddhi Core module maintains the execution logic. It also interacts with the external environment and systems for consuming, processing and publishing events. It uses the following components to achieve its tasks: SiddhiManager : This is a key component of Siddhi Core that manages Siddhi Application Runtimes and facilitates their functionality via Siddhi Context with periodic state persistence, statistics reporting and extension loading. It is recommended to use one Siddhi Manager for a single JVM. SiddhiAppRuntime : Siddhi Application Runtime can be generated for each Siddhi Application through the Siddhi Manager. Siddhi Application Runtimes provide an isolated execution environment for each defined Siddhi Application. These Siddhi Application Runtimes can have their own lifecycle and they execute based on the logic defined in their Siddhi Application. SiddhiContext : This is a shared object across all the Siddhi Application Runtimes within the same Siddhi manager. It contains references to the persistence store for periodic persistence, statistics manager to report performance statistics of Siddhi Application Runtimes, and extension holders for loading Siddhi extensions.","title":"Siddhi Component Architecture"},{"location":"development/architecture/#siddhi-application-creation","text":"Execution logic of the Siddhi Engine is composed as a Siddhi Application, and this is usually passed as a string to SiddhiManager to create the SiddhiAppRuntime for execution. When a Siddhi Application is passed to the SiddhiManager.createSiddhiAppRuntime() , it is processed internally with the SiddhiCompiler . Here, the SiddhiApp String is compiled to SiddhiApp object model by the SiddhiQLBaseVisitorImpl class. This validates the syntax of the given Siddhi Application. The model is then passed to the SiddhiAppParser to create the SiddhiAppRuntime . During this phase, the semantics of the Siddhi Application is validated and the execution logic of the Siddhi Application is optimized.","title":"Siddhi Application Creation"},{"location":"development/architecture/#siddhi-app-execution-flow","text":"Following diagram depicts the execution flow within a Siddhi App Runtime. The path taken by events within Siddhi Engine is indicated in blue. The components that are involved in handling the events are the following: StreamJunction This routes events of a particular stream to various components within the Siddhi App Runtime. A stream junction is generated for each defined or inferred Stream in the Siddhi Application. A stream junction by default uses the incoming event's thread and passes all the events to its subscribed components as soon as they arrive, but this behaviour can be altered by configuring @Async annotation to buffer the events at the and stream junction and to use another one or more threads to collect the events from the buffer and process the subsequent executions. InputHandler Input handler is used to push Event and Event[] objects into stream junctions from defined event sources, and from Java/Python programmes. StreamCallback This receives Event[] s from stream junction and passes them to event sinks to publish to external endpoints, and/or passes them to subscribed Java/Python programmes for further processing. Queries Partitions These components process events by filtering, transforming, aggregating, joining, pattern matching, etc. They consume events from one or more stream junctions, process them and publish the processed events into a set of stream junctions based on the defined queries or partitions. Source Sources consume events from external sources in various data formats, convert them into Siddhi events using SourceMapper s and pass them to corresponding stream junction via their associated input handlers. A source is generated for each @Source annotation defined above a stream definition. SourceMapper A source mapper is a sub-component of source, and it needs to be configured for each source in order to convert the incoming event into Siddhi event. The source mapper type can be configured using the @Map annotation within the @Source annotation. When the @Map annotation is not defined, Siddhi uses the PassThroughSourceMapper , where it assumes that the incoming message is already in the Siddhi Event format (i.e Event or Event[] ), and therefore makes no changes to the incoming event format. Sink Sinks consumes events from its associated stream junction, convert them to various data formats via SinkMapper and publish them to external endpoints as defined in the @Sink annotation. A sink is generated for each @Sink annotation defined above a stream definition. SinkMapper A sink mapper is a sub-component of sink. and its need to be configured for each sink in order to map the Siddhi events to the specified data format so that they can be published via the sink. The sink mapper type can be configured using the @Map annotation within the @Sink annotation. When the @Map annotation is not defined, Siddhi uses PassThroughSinkMapper , where it passes the Siddhi Event (i.e Event or Event[] ) without any formatting to the Sink. Table Tables are used to store events. When tables are defined by default, Siddhi uses the InMemoryTable implementation to store events in-memory. When @Store annotation is used on top of the table definition, it loads the associated external data store connector based on the defined store type. Most table implementations are extended from either AbstractRecordTable or AbstractQueryableRecordTable abstract classes the former provides the functionality to query external data store based on a given filtering condition, and the latter queries external data store by providing projection, limits, and ordering parameters in addition to data filter condition. Window Windows store events as and when they arrive and automatically expire/clean them based on the given window constraint. Multiple types of windows are can be implemented by extending the WindowProcessor abstract class. IncrementalAggregation Long running time series aggregates defined via the aggregation definition is calculated in an incremental manner using the Incremental Aggregation Processor for the defined time periods. Incremental aggregation functions can be implemented by extending IncrementalAttributeAggregator . By default, incremental aggregations aggregate all the values in-memory, but when it is associated with a store by adding @store annotation it uses in-memory to aggregate partial results and uses data stores to persist those increments. When requested for aggregate results it retrieves data from data stores and (if needed from) in-memory, computes combined aggregate results and provides as the output. Trigger A trigger triggers events at a given interval as given in the trigger definition. The triggered events are pushed to a stream junction having the same name as the trigger. QueryCallback A query callback taps into the events that are emitted by a particular query. It notifies the event occurrence timestamp and classifies the output events into currentEvents , and expiredEvents .","title":"Siddhi App Execution Flow"},{"location":"development/architecture/#siddhi-query-execution","text":"Siddhi QueryRuntimes can be categorized into three main types: SingleInputStream : Queries that consist of query types such as filters and windows. JoinInputStream : Queries that consist of joins. StateInputStream : Queries that consist of patterns and sequences. The following section explains the internals of each query type.","title":"Siddhi Query Execution"},{"location":"development/architecture/#singleinputstream-query-runtime-filter-windows","text":"A single input stream query runtime is generated for filter and window queries. They consume events from a stream junction or a window and convert the incoming events according to the expected output stream format at the ProcessStreamReceiver by dropping all the unrelated incoming stream attributes. Then the converted events are passed through a few Processors such as FilterProcessor , StreamProcessor , StreamFunctionProcessor , WindowProcessor , and QuerySelector . Here, the StreamProcessor , StreamFunctionProcessor , and WindowProcessor can be extended with various stream processing capabilities. The last processor of the chain of processors must always be a QuerySelector and it can't appear anywhere else. When the query runtime consumes events from a stream, its processor chain can maximum contain one WindowProcessor , and when query runtime consumes events from a window, its chain of processors cannot contain any WindowProcessor . The FilterProcessor is implemented using expressions that return a boolean value. ExpressionExecutor is used to process conditions, mathematical operations, unary operations, constant values, variables, and functions. Expressions have a tree structure, and they are processed based using the Depth First search algorithm. To achieve high performance, Siddhi currently depends on the user to formulate the least successful case in the leftmost side of the condition, thereby increasing the chance of early false detection. The condition expression price = 100 and ( Symbol == 'IBM' or Symbol == 'MSFT' ) is represented as shown below. These expressions also support the execution of user-defined functions (UDFs), and they can be implemented by extending the FunctionExecutor class. After getting processed by all the processors, events reach the QuerySelector for transformation. At the QuerySelector , events are transformed based on the select clause of the query. The select clause produces one AttributeProcessor for each output stream attribute, and these AttributeProcessor s contain expressions defining data transformation including constant values, variables, user-defined functions, etc. They can also contain AttributeAggregatorExecutor s to process aggregation operations such as sum , count , etc. If there is a Group By clause defined, then the GroupByKeyGenerator is used to identify the composite group-by key, and then for each key, an AttributeAggregatorExecutor state is generated to maintain per group-by key aggregations. When each time AttributeProcessor is executed the AttributeAggregatorExecutor calculates per group-by aggregation results and output the values. When AttributeAggregatorExecutor group-by states become obsolete, they are destroyed and automatically cleaned. After an event is transformed to the output format through the above process, it is evaluated against the having condition executor if a having clause is provided. The succeeding events are then ordered, and limited based on order by , limit and offset clauses before they pushed to the OutputRateLimiter . At OutputRateLimiter , the event output is controlled before sending the events to the stream junction or to the query callback. When the output clause is not defined, the PassThroughOutputRateLimiter is used by passing all the events without any rate limiting.","title":"SingleInputStream Query Runtime (Filter &amp; Windows)"},{"location":"development/architecture/#temporal-processing-with-windows","text":"The temporal event processing aspect is achieved via Window and AttributeAggregators To achieve temporal processing, Siddhi uses the following four type of events: Current Events : Events that are newly arriving to the query from streams. Expired Events : Events that have expired from a window. Timer Events : Events that inform the query about an update of execution time. These events are usually generated by schedulers. Reset Events : Events that resets the Siddhi query states. In Siddhi, when an event comes into a WindowProcessor , it creates an appropriate expired event corresponding to the incoming current event with the expiring timestamp, and stores that event in the window. At the same time, WindowProcessor also forwards the current event to the next processor for further processing. It uses a scheduler or some other counting approach to determine when to emit the events that are stored in in-memory. When the expired events meet the condition for expiry based on the window contains, it emits the expired events to the next processor. At times like in window.timeBatch() there can be cases that need emitting all the events in-memory at once and the output does not need individual expired events values, in this cases the window emits a single reset event instead of sending one expired event for each event it has stored, so that it can reset the states in one go. For the QuerySelector aggregations to work correctly the window must emit a corresponding expired event for each current event it has emitted or it must send a reset event . In the QuerySelector , the arrived current events increase the aggregation values, expired events decrease the values, and reset events reset the aggregation calculation to produce correct query output. For example, the sliding TimeWindow ( window.time() ) creates a corresponding expired event for each current event that arrives, adds the expired event s to the window, adds an entry to the scheduler to notify when that event need to be expired, and finally sends the current event to the next processor for subsequent processing. The scheduler notifies the window by sending a timer event , and when the window receives an indication that the expected expiry time has come for the oldest event in the window via a timer event or by other means, it removes the expired event from the window and passes that to the next processor.","title":"Temporal Processing with Windows"},{"location":"development/architecture/#joininputstream-query-runtime-join","text":"Join input stream query runtime is generated for join queries. This can consume events from two stream junctions and perform a join operation as depicted above. It can also perform a join by consuming events from one stream junction and join against itself, or it can also join against a table, window or an aggregation. When a join is performed with a table, window or aggregation, the WindowProcessor in the above image is replaced with the corresponding table, window or aggregation and no basic processors are used on their side. The joining operation is triggered by the events that arrive from the stream junction. Here, when an event from one stream reaches the pre JoinProcessor , it matches against all the available events of the other stream's WindowProcessor . When a match is found, those matched events are sent to the QuerySelector as current events , and at the same time, the original event is added to the WindowProcessor where it remains until it expires. Similarly, when an event expires from the WindowProcessor , it matches against all the available events of the other stream's WindowProcessor , and when a match is found, those matched events are sent to the QuerySelector as expired events . Note Despite the optimizations, a join query is quite expensive when it comes to performance. This is because the WindowProcessor is locked during the matching process to avoid race conditions and to achieve accuracy while joining. Therefore, when possible avoid matching large (time or length) windows in high volume streams.","title":"JoinInputStream Query Runtime (Join)"},{"location":"development/architecture/#stateinputstream-query-runtime-pattern-sequence","text":"The state input stream query runtime is generated for pattern and sequence queries. This consumes events from one or more stream junctions via ProcessStreamReceiver s and checks whether the events match each pattern or sequence condition by processing the set of basic processors associated with each ProcessStreamReceiver . The PreStateProcessor s usually contains lists of state events that are already matched by previous conditions, and if its the first condition then it will have an empty state event in its list. When ProcessStreamReceiver consumes an event, it passes the event to the PreStateProcessor which updates the list of state events it has with the incoming event and executes the condition by passing the events to the basic processors. The state events that match the conditions reach the PostStateProcessor which will then stores the events to the state event list of the following PreStateProcessor . If it is the final condition's PostStateProcessor , then it will pass the state event to the QuerySelector to generate and emit the output.","title":"StateInputStream Query Runtime (Pattern &amp; Sequence)"},{"location":"development/architecture/#siddhi-partition-execution","text":"A partition is a wrapper around one or more Siddhi queries and inner streams that connect them. A partition is implemented in Siddhi as a PartitionRuntime which contains multiple QueryRuntime s and inner stream junctions. Each partitioned stream entering the partition goes through a designated PartitionStreamReceiver . The PartitionExecutor of PartitionStreamReceiver evaluates the incoming events to identify their associated partition-key using either RangePartitionExecutor or ValuePartitionExecutor . The identified partition-key is then set as thread local variable and the event is passed to the QueryRuntime s of processing. The QueryRuntime s process events by maintaining separate states for each partition-key such that producing separate output per partition. When a partition query consumes a non-partitioned global stream, the QueryRuntime s are executed for each available partition-key in the system such that allowing all partitions to receive the same event. When the partitions are obsolete PartitionRuntime deletes all the partition states from its QueryRuntime s.","title":"Siddhi Partition Execution"},{"location":"development/architecture/#siddhi-aggregation","text":"Siddhi supports long duration time series aggregations via its aggregation definition. AggregationRuntime implements this by the use of streaming lambda architecture , where it processes part of the data in-memory and gets part of the data from data stores. AggregationRuntime creates an in-memory table or external store for each time granularity (i.e seconds, minutes, days, etc) it has to process the events, and when events enter it calculates the aggregations in-memory for its least granularity (usually seconds) using the IncrementalExecutor and maintains the running aggregation values in its BaseIncrementalValueStore . At each clock end time of the granularity (end of each second) IncrementalExecutor stores the summarized values to the associated granularity table and also passes the summarized values to the IncrementalExecutor of the next granularity level, which also follows the same methodology in processing the events. Through this approach each time granularities, the current time duration will be in-memory and all the historical time durations will be in stored in the tables. The aggregations results are calculated by IncrementalAttributeAggregator s and stored in such a way that allows proper data composition upon retrial, for example, avg() is stored as sum and count . This allows data composition across various granularity time durations when retrieving, for example, results for avg() composed by returning sum of sum s divided by the sum of count s. Aggregation can also work in a distributed manner and across system restarts. This is done by storing node specific IDs and granularity time duration information in the tables. To make sure tables do not go out of memory IncrementalDataPurging is used to purge old data. When aggregation is queried through join or store query for a given time granularity it reads the data from the in-memory BaseIncrementalValueStore and from the tables computes the composite results as described, and presents the results.","title":"Siddhi Aggregation"},{"location":"development/architecture/#siddhi-event-formats","text":"Siddhi has three event formats. Event This is the format exposed to external systems when they send events via Input Handler and consume events via Stream Callback or Query Callback. This consists of a timestamp and an Object[] that contains all the values in accordance to the corresponding stream. StreamEvent (Subtype of ComplexEvent ) This is used within queries. This contains a timestamp and the following three Object[] s: beforeWindowData : This contains values that are only used in processors that are executed before the WindowProcessor . onAfterWindowData : This contains values that are only used by the WindowProcessor and the other processors that follow it, but not sent as output. outputData : This contains the values that are sent via the output stream of the query. In order to optimize the amount of data that is stored in the in-memory at windows, the content in beforeWindowData is cleared before the event enters the WindowProcessor . StreamEvents can also be chained by linking each other via the next property in them. StateEvent (Subtype of ComplexEvent ) This is used in joins, patterns and sequences queries when we need to associate events of multiple streams, tables, windows or aggregations together. This contains a timestamp , a collection of StreamEvent s representing different streams, tables, etc, that are used in the query, and an Object[] to contain outputData values that are needed for query output. The StreamEvent s within the StateEvent and the StateEvent themselves can be chained by linking each other with the next property in them. Event Chunks Event Chunks provide an easier way of manipulating the chain of StreamEvent s and StateEvent s so that they are be easily iterated, inserted and removed.","title":"Siddhi Event Formats"},{"location":"development/architecture/#summary","text":"This article focuses on describing the architecture of Siddhi and rationalizing some of the architectural decisions made when implementing the system. It also explains the key features of Siddhi. We hope this will be a good starting point for new developers to understand Siddhi and to start contributing to it.","title":"Summary"},{"location":"development/build/","text":"Building Siddhi 5.0 Repos Building Java Repos Prerequisites Oracle JDK 8 , OpenJDK 8 , or JDK 11 (Java 8 should be used for building in order to support both Java 8 and Java 11 at runtime) Maven 3.5.x or later version Steps to Build Get a clone or download source from Github repo, E.g. git clone https://github.com/siddhi-io/siddhi.git Run the Maven command mvn clean install from the root directory Command Description mvn clean install Build and install the artifacts into the local repository. mvn clean install -Dmaven.test.skip=true Build and install the artifacts into the local repository, without running any of the unit tests.","title":"Build"},{"location":"development/build/#building-siddhi-50-repos","text":"","title":"Building Siddhi 5.0 Repos"},{"location":"development/build/#building-java-repos","text":"","title":"Building Java Repos"},{"location":"development/build/#prerequisites","text":"Oracle JDK 8 , OpenJDK 8 , or JDK 11 (Java 8 should be used for building in order to support both Java 8 and Java 11 at runtime) Maven 3.5.x or later version","title":"Prerequisites"},{"location":"development/build/#steps-to-build","text":"Get a clone or download source from Github repo, E.g. git clone https://github.com/siddhi-io/siddhi.git Run the Maven command mvn clean install from the root directory Command Description mvn clean install Build and install the artifacts into the local repository. mvn clean install -Dmaven.test.skip=true Build and install the artifacts into the local repository, without running any of the unit tests.","title":"Steps to Build"},{"location":"development/source/","text":"Siddhi 5.0 Source Code Project Source Code Siddhi Core Java Library https://github.com/siddhi-io/siddhi (Java) Siddhi repo, containing the core Java libraries of Siddhi. PySiddhi https://github.com/siddhi-io/pysiddhi (Python) The Python wrapper for Siddhi core Java libraries. This depends on the siddhi-io/siddhi repo. Siddhi Local Microservice Distribution https://github.com/siddhi-io/distribution (Java) The Microservice distribution of the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi repo. Siddhi Docker Microservice Distribution https://github.com/siddhi-io/docker-siddhi (Docker) The Docker wrapper for the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi and siddhi-io/distribution repos. Siddhi Kubernetes Operator https://github.com/siddhi-io/siddhi-operator (Go) The Siddhi Kubernetes CRD repo deploying Siddhi on Kubernetes. This depends on the siddhi-io/siddhi , siddhi-io/distribution and siddhi-io/docker-siddhi repos. Siddhi Extensions Find the supported Siddhi extensions and source here","title":"Source"},{"location":"development/source/#siddhi-50-source-code","text":"","title":"Siddhi 5.0 Source Code"},{"location":"development/source/#project-source-code","text":"","title":"Project Source Code"},{"location":"development/source/#siddhi-core-java-library","text":"https://github.com/siddhi-io/siddhi (Java) Siddhi repo, containing the core Java libraries of Siddhi.","title":"Siddhi Core Java Library"},{"location":"development/source/#pysiddhi","text":"https://github.com/siddhi-io/pysiddhi (Python) The Python wrapper for Siddhi core Java libraries. This depends on the siddhi-io/siddhi repo.","title":"PySiddhi"},{"location":"development/source/#siddhi-local-microservice-distribution","text":"https://github.com/siddhi-io/distribution (Java) The Microservice distribution of the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi repo.","title":"Siddhi Local Microservice Distribution"},{"location":"development/source/#siddhi-docker-microservice-distribution","text":"https://github.com/siddhi-io/docker-siddhi (Docker) The Docker wrapper for the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi and siddhi-io/distribution repos.","title":"Siddhi Docker Microservice Distribution"},{"location":"development/source/#siddhi-kubernetes-operator","text":"https://github.com/siddhi-io/siddhi-operator (Go) The Siddhi Kubernetes CRD repo deploying Siddhi on Kubernetes. This depends on the siddhi-io/siddhi , siddhi-io/distribution and siddhi-io/docker-siddhi repos.","title":"Siddhi Kubernetes Operator"},{"location":"development/source/#siddhi-extensions","text":"Find the supported Siddhi extensions and source here","title":"Siddhi Extensions"},{"location":"docs/","text":"Siddhi 5.0 User Guide This section provides information on using and running Siddhi. Checkout the Siddhi features to get and idea on what it can do in brief. Writing Siddhi Applications Writing steam processing logic in Siddhi is all about building Siddhi Applications. A Siddhi Application is a script with .siddhi file extension having self-contained stream processing logic. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logics that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, users can configure common topic using In-Memory Sink and In-Memory Source in order to communicate between them. To write Siddhi Applications using Siddhi Streaming SQL refer Siddhi Query Guide for details. For specific API information on Siddhi functions and features refer Siddhi API Guide . Find out about the supported Siddhi extensions and their versions here . Executing Siddhi Applications Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP) Siddhi Configurations Refer the Siddhi Config Guide for information on advance Siddhi execution configurations. System Requirements For all Siddhi execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"Introduction"},{"location":"docs/#siddhi-50-user-guide","text":"This section provides information on using and running Siddhi. Checkout the Siddhi features to get and idea on what it can do in brief.","title":"Siddhi 5.0 User Guide"},{"location":"docs/#writing-siddhi-applications","text":"Writing steam processing logic in Siddhi is all about building Siddhi Applications. A Siddhi Application is a script with .siddhi file extension having self-contained stream processing logic. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logics that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, users can configure common topic using In-Memory Sink and In-Memory Source in order to communicate between them. To write Siddhi Applications using Siddhi Streaming SQL refer Siddhi Query Guide for details. For specific API information on Siddhi functions and features refer Siddhi API Guide . Find out about the supported Siddhi extensions and their versions here .","title":"Writing Siddhi Applications"},{"location":"docs/#executing-siddhi-applications","text":"Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP)","title":"Executing Siddhi Applications"},{"location":"docs/#siddhi-configurations","text":"Refer the Siddhi Config Guide for information on advance Siddhi execution configurations.","title":"Siddhi Configurations"},{"location":"docs/#system-requirements","text":"For all Siddhi execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"System Requirements"},{"location":"docs/config-guide/","text":"Siddhi 5.0 Config Guide This section covers the following. Configuring Databases Configuring Periodic State Persistence Configuring Siddhi Sources, Sinks, Stores and Extensions Configuring Authentication Adding Extensions and Third Party Dependencies Configuring Statistics Converting Jars to OSGi Bundles Configuring Databases Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. It is recommended to configure RDBMS databases as datasources under wso2.datasources section of Siddhi configuration yaml, and pass it during startup, this will allow database to reuse connections across multiple Siddhi Apps. By default Siddhi stores product-specific data in predefined embedded H2 database located in SIDDHI_RUNNER_HOME /wso2/runner/database directory. Here, the default H2 database is only suitable for development, testing, and some production environments which do not store data. However, for most production environments we recommend using industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MSSQL. In this case users are expected to add the relevant database drivers to Siddhi's class-path. Including database drivers. The database driver corresponding to the database should be an OSGi bundle and it need to be added to SIDDHI_RUNNER_HOME /lib/ directory. If the driver is a jar then this should be converted to an OSGi bundle before adding . Converting Non OSGi drivers. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. The necessary table schemas are self generated by the features themselves, other than the tables needed for statistics reporting via databases . Below are the sample datasource configuration for each supported database types: MySQL Oracle There are two ways to configure Oracle. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE PostgreSQL MSSQL Configuring Periodic State Persistence Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. This explains how to periodically persisting the state of Siddhi either into a database system or file system, in order to prevent data losses that can result from a system failure. Persistence on Database To perform periodic state persistence on a database, the database should be configured as a datasource and the relevant jdbc drivers should be added to Siddhi's class-path. Refer Database Configuration section for more information. To configure database based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.DBPersistenceStore config datasource The datasource to be used in persisting the state. The datasource should be defined in the Siddhi configuration yaml. For detailed instructions of how to configure a datasource, see Database Configuration . SIDDHI_PERSISTENCE_DB (A datasource that is defined in wso2.datasources in Siddhi configuration yaml) config table The table that should be created and used for persisting states. PERSISTENCE_TABLE The following is a sample configuration for database based state persistence. Persistence on File System To configure file system based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample configuration for file system based state persistence. Configuring Siddhi Elements Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. You can configure some of there environment specific configurations in the Siddhi Configuration yaml rather than configuring in-line, such that your Siddhi Application can become potable between environments. Configuring Sources, Sinks and Stores Multiple sources, sinks, and stores could be defined in Siddhi Configuration yaml as ref , and referred by several Siddhi Applications as described below. The following is the syntax for the configuration. siddhi: refs: - ref: name: ' name ' type: ' type ' properties: property1 : value1 property2 : value2 For each separate refs you want to configure, add a sub-section named ref under the refs subsection. The ref configured in Siddhi Configuration yaml can be referred from a Siddhi Application Source as follows. @Source(ref=' name ', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Similarly Sinks and Store Tables can also be configured and referred from Siddhi Apps. For each separate refs you want to configure, add a sub-section named ref under the refs subsection. Example : Configuring http source using ref Following configuration defines the url and details about basic.auth , in the Siddhi Configuration yaml. siddhi: refs: - ref: name: 'http-passthrough' type: 'http' properties: receiver.url: 'http://0.0.0.0:8008/sweet-production' basic.auth.enabled: false This can be referred in the Siddhi Applications as follows. @Source(ref='http-passthrough', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Configuring Extensions Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. The following is the syntax for the configuration. siddhi: extensions: - extension: name: extension name namespace: extension namespace properties: key : value For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. Following are some examples on overriding default system properties via Siddhi Configuration yaml Example 1 : Defining service host and port for the TCP source siddhi: extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2 : Overwriting the default RDBMS extension configuration siddhi: extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: \"CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}})\" mysql.recordDeleteQuery: \"DELETE FROM {{TABLE_NAME}} {{CONDITION}}\" mysql.recordExistsQuery: \"SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1\" Configuring Authentication Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. By default, Siddhi is configured with user name admin , and password admin . This can be updated by adding related user management configuration as auth.configs to the Siddhi Configuration yaml, and pass it at startup. A sample auth.configs is as follows. # Authentication configuration auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin Adding Extensions and Third Party Dependencies Applicable for all modes. For certain use-cases, Siddhi might require extensions and/or third party dependencies to fulfill some characteristics that it does not provide by default. This section provides details on how to add or update extension and/or third party dependencies that is needed by Siddhi. Adding to Siddhi Java Program When running Siddhi as a Java library, the extension jars and/or third-party dependencies needed for Siddhi can be simply added to Siddhi class-path. When Maven is used as the build tool add them to the pom.xml file along with the other mandatory jars needed by Siddhi as given is Using Siddhi as a library guide. A sample on adding siddhi-io-http extension to the Maven pom.xml is as follows. !--HTTP extension-- dependency groupId org.wso2.extension.siddhi.io.http /groupId artifactId siddhi-io-http /artifactId version ${siddhi.io.http.version} /version /dependency Refer guide for more details on using Siddhi as a Java Library. Adding to Siddhi Local Microservice The most used Siddhi extensions are packed by default with the Siddhi Local Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, adding or replacing the relevant OSGi JARs in SIDDHI_RUNNER_HOME /lib directory. Since Local Microservice is OSGi-based, when adding libraries/drivers they need to be checked if they are OSGi bundles, and if not convert to OSGi before adding them to the SIDDHI_RUNNER_HOME /lib directory. Converting Jars to OSGi Bundles.. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Local Microservice. Adding to Siddhi Docker Microservice The most used Siddhi extensions are packed by default with the Siddhi Docker Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, a new docker image has to be built from either siddhi-runner-base-ubuntu or siddhi-runner-base-alpine images. These images contain Linux OS, JDK and the Siddhi distribution. Sample docker file using siddhi-runner-base-alpine is as follows. Find the necessary artifacts to build the docker from docker-siddhi repository. The necessary OSGi jars and extensions that need to be added to the Siddhi Docker Microservice should be placed at ${BUNDLE_JAR_DIR} ( ./files/lib ) folder as defined in the above docker file, such that they would be bundled during the docker build phase. Converting Jars to OSGi Bundles If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Docker Microservice. Adding to Siddhi Kubernetes Microservice To add or update Siddhi extensions and/or third-party dependencies, a custom docker image has to be created using the steps described in Adding to Siddhi Docker Microservice documentation including the necessary extensions and dependencies. The created image can be then referenced in the sepc.pod subsection in the SiddhiProcess Kubernetes artifact created to deploy Siddhi in Kubernetes. For details on creating the Kubernetes artifacts refer Using Siddhi as Kubernetes Microservice documentation. Configuring Statistics Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi uses dropwizard metrics library to calculate Siddhi and JVM statistics, and it can report the results via JMX Mbeans, console or database. To enable statistics, the relevant configuration should be added to the Siddhi Configuration yaml as follows, and at the same time the statistics collection should be enabled in the Siddhi Application which is being monitored. Refer Siddhi Application Statistics documentation for enabling Siddhi Application level statistics. To enable statistics the relevant matrics related configurations should be added under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. Configuring Metrics reporting level. To modify the statistics reporting, relevant metric names can be added under the wso2.metrics.levels subsection in the Siddhi Configurations yaml, along with the matrics level (i.e., OFF, INFO, DEBUG, TRACE, or ALL) as given bellow. wso2.metrics: # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO The available metrics reporting options are as follows. Reporting via JMX Mbeans JMX Mbeams is the default statistics reporting option of Siddhi. To enable stats with the default configuration add the metric-related properties under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. A sample configuration is as follows. wso2.metrics: enabled: true This will report JMX Mbeans in the name of org.wso2.carbon.metrics . However, in this default configuration the JVM metrics will not be measured. A detail JMX configuration along with the metrics reporting level is as follows. wso2.metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO Reporting via Console To enable statistics by periodically printing the metrics on console add the following configuration to the the Siddhi Configurations yaml file, and pass that during startup. # This is the main configuration for metrics wso2.metrics: # Enable Metrics enabled: false reporting: console: - # The name for the Console Reporter name: Console # Enable Console Reporter enabled: false # Polling Period in seconds. # This is the period for polling metrics from the metric registry and printing in the console pollingPeriod: 5 Reporting via Database To enable JDBC reporting and to periodically clean up the outdated statistics from the database, first a datasource should be created with the relevant database configurations and then the related metrics properties as given bellow should be added to in the Siddhi Configurations yaml file, and pass that during startup. The bellow sample is referring to the datasource with JNDI name jdbc/SiddhiMetricsDB , hence the datasource configuration in yaml should have jndiConfig.name as jdbc/SiddhiMetricsDB . For detailed instructions on configuring a datasource, refer Configuring Databases . . The scripts to create these tables are provided in the SIDDHI_RUNNER_HOME /wso2/runner/dbscripts directory. Sample configuration of reporting via database. wso2.metrics: # Enable Metrics enabled: true jdbc: # Data Source Configurations for JDBC Reporters dataSource: # Default Data Source Configuration - JDBC01 # JNDI name of the data source to be used by the JDBC Reporter. # This data source should be defined under wso2.datasources. dataSourceName: java:comp/env/jdbc/SiddhiMetricsDB # Schedule regular deletion of metrics data older than a set number of days. # It is recommended that you enable this job to ensure your metrics tables do not get extremely large. # Deleting data older than seven days should be sufficient. scheduledCleanup: # Enable scheduled cleanup to delete Metrics data in the database. enabled: false # The scheduled job will cleanup all data older than the specified days daysToKeep: 7 # This is the period for each cleanup operation in seconds. scheduledCleanupPeriod: 86400 reporting: jdbc: - # The name for the JDBC Reporter name: JDBC # Enable JDBC Reporter enabled: true # Source of Metrics, which will be used to identify each metric in database -- # Commented to use the hostname by default # source: Siddhi # Alias referring to the Data Source configuration dataSource: *JDBC01 # Polling Period in seconds. # This is the period for polling metrics from the metric registry and updating the database with the values pollingPeriod: 60 Metrics history and reporting interval If the wso2.metrics.reporting.jdbc subsection is not enabled, the information relating to metrics history will not be persisted for future references. Also note the that the reporting will only start to update the database after the given pollingPeriod time has elapsed. Information about the parameters configured under the jdbc.dataSource subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description dataSourceName java:comp/env/jdbc/SiddhiMetricsDB java:comp/env/ datasource JNDI name . The JNDI name of the datasource used to store metric data. scheduledCleanup.enabled false If this is set to true, metrics data stored in the database is cleared periodically based on scheduled time interval. scheduledCleanup.daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. scheduledCleanup.scheduledCleanupPeriod 86400 The parameter specifies the time interval in seconds at which metric data should be cleaned. Converting Jars to OSGi Bundles To convert jar files to OSGi bundles, first download and save the non-OSGi jar it in a preferred directory in your machine. Then from the CLI, navigate to the SIDDHI_RUNNER_HOME /bin directory, and issue the following command. ./jartobundle.sh path to non OSGi jar ../lib This converts the Jar to OSGi bundles and place it in SIDDHI_RUNNER_HOME /lib directory.","title":"Configuration Guide"},{"location":"docs/config-guide/#siddhi-50-config-guide","text":"This section covers the following. Configuring Databases Configuring Periodic State Persistence Configuring Siddhi Sources, Sinks, Stores and Extensions Configuring Authentication Adding Extensions and Third Party Dependencies Configuring Statistics Converting Jars to OSGi Bundles","title":"Siddhi 5.0 Config Guide"},{"location":"docs/config-guide/#configuring-databases","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. It is recommended to configure RDBMS databases as datasources under wso2.datasources section of Siddhi configuration yaml, and pass it during startup, this will allow database to reuse connections across multiple Siddhi Apps. By default Siddhi stores product-specific data in predefined embedded H2 database located in SIDDHI_RUNNER_HOME /wso2/runner/database directory. Here, the default H2 database is only suitable for development, testing, and some production environments which do not store data. However, for most production environments we recommend using industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MSSQL. In this case users are expected to add the relevant database drivers to Siddhi's class-path. Including database drivers. The database driver corresponding to the database should be an OSGi bundle and it need to be added to SIDDHI_RUNNER_HOME /lib/ directory. If the driver is a jar then this should be converted to an OSGi bundle before adding . Converting Non OSGi drivers. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. The necessary table schemas are self generated by the features themselves, other than the tables needed for statistics reporting via databases . Below are the sample datasource configuration for each supported database types: MySQL Oracle There are two ways to configure Oracle. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE PostgreSQL MSSQL","title":"Configuring Databases"},{"location":"docs/config-guide/#configuring-periodic-state-persistence","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. This explains how to periodically persisting the state of Siddhi either into a database system or file system, in order to prevent data losses that can result from a system failure.","title":"Configuring Periodic State Persistence"},{"location":"docs/config-guide/#persistence-on-database","text":"To perform periodic state persistence on a database, the database should be configured as a datasource and the relevant jdbc drivers should be added to Siddhi's class-path. Refer Database Configuration section for more information. To configure database based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.DBPersistenceStore config datasource The datasource to be used in persisting the state. The datasource should be defined in the Siddhi configuration yaml. For detailed instructions of how to configure a datasource, see Database Configuration . SIDDHI_PERSISTENCE_DB (A datasource that is defined in wso2.datasources in Siddhi configuration yaml) config table The table that should be created and used for persisting states. PERSISTENCE_TABLE The following is a sample configuration for database based state persistence.","title":"Persistence on Database"},{"location":"docs/config-guide/#persistence-on-file-system","text":"To configure file system based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample configuration for file system based state persistence.","title":"Persistence on File System"},{"location":"docs/config-guide/#configuring-siddhi-elements","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. You can configure some of there environment specific configurations in the Siddhi Configuration yaml rather than configuring in-line, such that your Siddhi Application can become potable between environments.","title":"Configuring Siddhi Elements"},{"location":"docs/config-guide/#configuring-sources-sinks-and-stores","text":"Multiple sources, sinks, and stores could be defined in Siddhi Configuration yaml as ref , and referred by several Siddhi Applications as described below. The following is the syntax for the configuration. siddhi: refs: - ref: name: ' name ' type: ' type ' properties: property1 : value1 property2 : value2 For each separate refs you want to configure, add a sub-section named ref under the refs subsection. The ref configured in Siddhi Configuration yaml can be referred from a Siddhi Application Source as follows. @Source(ref=' name ', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Similarly Sinks and Store Tables can also be configured and referred from Siddhi Apps. For each separate refs you want to configure, add a sub-section named ref under the refs subsection. Example : Configuring http source using ref Following configuration defines the url and details about basic.auth , in the Siddhi Configuration yaml. siddhi: refs: - ref: name: 'http-passthrough' type: 'http' properties: receiver.url: 'http://0.0.0.0:8008/sweet-production' basic.auth.enabled: false This can be referred in the Siddhi Applications as follows. @Source(ref='http-passthrough', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double);","title":"Configuring Sources, Sinks and Stores"},{"location":"docs/config-guide/#configuring-extensions","text":"Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. The following is the syntax for the configuration. siddhi: extensions: - extension: name: extension name namespace: extension namespace properties: key : value For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. Following are some examples on overriding default system properties via Siddhi Configuration yaml Example 1 : Defining service host and port for the TCP source siddhi: extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2 : Overwriting the default RDBMS extension configuration siddhi: extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: \"CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}})\" mysql.recordDeleteQuery: \"DELETE FROM {{TABLE_NAME}} {{CONDITION}}\" mysql.recordExistsQuery: \"SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1\"","title":"Configuring Extensions"},{"location":"docs/config-guide/#configuring-authentication","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. By default, Siddhi is configured with user name admin , and password admin . This can be updated by adding related user management configuration as auth.configs to the Siddhi Configuration yaml, and pass it at startup. A sample auth.configs is as follows. # Authentication configuration auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin","title":"Configuring Authentication"},{"location":"docs/config-guide/#adding-extensions-and-third-party-dependencies","text":"Applicable for all modes. For certain use-cases, Siddhi might require extensions and/or third party dependencies to fulfill some characteristics that it does not provide by default. This section provides details on how to add or update extension and/or third party dependencies that is needed by Siddhi.","title":"Adding Extensions and Third Party Dependencies"},{"location":"docs/config-guide/#adding-to-siddhi-java-program","text":"When running Siddhi as a Java library, the extension jars and/or third-party dependencies needed for Siddhi can be simply added to Siddhi class-path. When Maven is used as the build tool add them to the pom.xml file along with the other mandatory jars needed by Siddhi as given is Using Siddhi as a library guide. A sample on adding siddhi-io-http extension to the Maven pom.xml is as follows. !--HTTP extension-- dependency groupId org.wso2.extension.siddhi.io.http /groupId artifactId siddhi-io-http /artifactId version ${siddhi.io.http.version} /version /dependency Refer guide for more details on using Siddhi as a Java Library.","title":"Adding to Siddhi Java Program"},{"location":"docs/config-guide/#adding-to-siddhi-local-microservice","text":"The most used Siddhi extensions are packed by default with the Siddhi Local Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, adding or replacing the relevant OSGi JARs in SIDDHI_RUNNER_HOME /lib directory. Since Local Microservice is OSGi-based, when adding libraries/drivers they need to be checked if they are OSGi bundles, and if not convert to OSGi before adding them to the SIDDHI_RUNNER_HOME /lib directory. Converting Jars to OSGi Bundles.. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Local Microservice.","title":"Adding to Siddhi Local Microservice"},{"location":"docs/config-guide/#adding-to-siddhi-docker-microservice","text":"The most used Siddhi extensions are packed by default with the Siddhi Docker Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, a new docker image has to be built from either siddhi-runner-base-ubuntu or siddhi-runner-base-alpine images. These images contain Linux OS, JDK and the Siddhi distribution. Sample docker file using siddhi-runner-base-alpine is as follows. Find the necessary artifacts to build the docker from docker-siddhi repository. The necessary OSGi jars and extensions that need to be added to the Siddhi Docker Microservice should be placed at ${BUNDLE_JAR_DIR} ( ./files/lib ) folder as defined in the above docker file, such that they would be bundled during the docker build phase. Converting Jars to OSGi Bundles If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Docker Microservice.","title":"Adding to Siddhi Docker Microservice"},{"location":"docs/config-guide/#adding-to-siddhi-kubernetes-microservice","text":"To add or update Siddhi extensions and/or third-party dependencies, a custom docker image has to be created using the steps described in Adding to Siddhi Docker Microservice documentation including the necessary extensions and dependencies. The created image can be then referenced in the sepc.pod subsection in the SiddhiProcess Kubernetes artifact created to deploy Siddhi in Kubernetes. For details on creating the Kubernetes artifacts refer Using Siddhi as Kubernetes Microservice documentation.","title":"Adding to Siddhi Kubernetes Microservice"},{"location":"docs/config-guide/#configuring-statistics","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi uses dropwizard metrics library to calculate Siddhi and JVM statistics, and it can report the results via JMX Mbeans, console or database. To enable statistics, the relevant configuration should be added to the Siddhi Configuration yaml as follows, and at the same time the statistics collection should be enabled in the Siddhi Application which is being monitored. Refer Siddhi Application Statistics documentation for enabling Siddhi Application level statistics. To enable statistics the relevant matrics related configurations should be added under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. Configuring Metrics reporting level. To modify the statistics reporting, relevant metric names can be added under the wso2.metrics.levels subsection in the Siddhi Configurations yaml, along with the matrics level (i.e., OFF, INFO, DEBUG, TRACE, or ALL) as given bellow. wso2.metrics: # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO The available metrics reporting options are as follows.","title":"Configuring Statistics"},{"location":"docs/config-guide/#reporting-via-jmx-mbeans","text":"JMX Mbeams is the default statistics reporting option of Siddhi. To enable stats with the default configuration add the metric-related properties under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. A sample configuration is as follows. wso2.metrics: enabled: true This will report JMX Mbeans in the name of org.wso2.carbon.metrics . However, in this default configuration the JVM metrics will not be measured. A detail JMX configuration along with the metrics reporting level is as follows. wso2.metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO","title":"Reporting via JMX Mbeans"},{"location":"docs/config-guide/#reporting-via-console","text":"To enable statistics by periodically printing the metrics on console add the following configuration to the the Siddhi Configurations yaml file, and pass that during startup. # This is the main configuration for metrics wso2.metrics: # Enable Metrics enabled: false reporting: console: - # The name for the Console Reporter name: Console # Enable Console Reporter enabled: false # Polling Period in seconds. # This is the period for polling metrics from the metric registry and printing in the console pollingPeriod: 5","title":"Reporting via Console"},{"location":"docs/config-guide/#reporting-via-database","text":"To enable JDBC reporting and to periodically clean up the outdated statistics from the database, first a datasource should be created with the relevant database configurations and then the related metrics properties as given bellow should be added to in the Siddhi Configurations yaml file, and pass that during startup. The bellow sample is referring to the datasource with JNDI name jdbc/SiddhiMetricsDB , hence the datasource configuration in yaml should have jndiConfig.name as jdbc/SiddhiMetricsDB . For detailed instructions on configuring a datasource, refer Configuring Databases . . The scripts to create these tables are provided in the SIDDHI_RUNNER_HOME /wso2/runner/dbscripts directory. Sample configuration of reporting via database. wso2.metrics: # Enable Metrics enabled: true jdbc: # Data Source Configurations for JDBC Reporters dataSource: # Default Data Source Configuration - JDBC01 # JNDI name of the data source to be used by the JDBC Reporter. # This data source should be defined under wso2.datasources. dataSourceName: java:comp/env/jdbc/SiddhiMetricsDB # Schedule regular deletion of metrics data older than a set number of days. # It is recommended that you enable this job to ensure your metrics tables do not get extremely large. # Deleting data older than seven days should be sufficient. scheduledCleanup: # Enable scheduled cleanup to delete Metrics data in the database. enabled: false # The scheduled job will cleanup all data older than the specified days daysToKeep: 7 # This is the period for each cleanup operation in seconds. scheduledCleanupPeriod: 86400 reporting: jdbc: - # The name for the JDBC Reporter name: JDBC # Enable JDBC Reporter enabled: true # Source of Metrics, which will be used to identify each metric in database -- # Commented to use the hostname by default # source: Siddhi # Alias referring to the Data Source configuration dataSource: *JDBC01 # Polling Period in seconds. # This is the period for polling metrics from the metric registry and updating the database with the values pollingPeriod: 60 Metrics history and reporting interval If the wso2.metrics.reporting.jdbc subsection is not enabled, the information relating to metrics history will not be persisted for future references. Also note the that the reporting will only start to update the database after the given pollingPeriod time has elapsed. Information about the parameters configured under the jdbc.dataSource subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description dataSourceName java:comp/env/jdbc/SiddhiMetricsDB java:comp/env/ datasource JNDI name . The JNDI name of the datasource used to store metric data. scheduledCleanup.enabled false If this is set to true, metrics data stored in the database is cleared periodically based on scheduled time interval. scheduledCleanup.daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. scheduledCleanup.scheduledCleanupPeriod 86400 The parameter specifies the time interval in seconds at which metric data should be cleaned.","title":"Reporting via Database"},{"location":"docs/config-guide/#converting-jars-to-osgi-bundles","text":"To convert jar files to OSGi bundles, first download and save the non-OSGi jar it in a preferred directory in your machine. Then from the CLI, navigate to the SIDDHI_RUNNER_HOME /bin directory, and issue the following command. ./jartobundle.sh path to non OSGi jar ../lib This converts the Jar to OSGi bundles and place it in SIDDHI_RUNNER_HOME /lib directory.","title":"Converting Jars to OSGi Bundles"},{"location":"docs/extensions/","text":"Siddhi Extensions Following are some supported Siddhi extensions; Extensions released under Apache 2.0 License Execution Extensions Name Description Latest Tested Version execution-string Provides basic string handling capabilities such as concat, length, replace all, etc. 5.0.3 execution-regex Provides basic RegEx execution capabilities. 5.0.3 execution-math Provides useful mathematical functions. 5.0.2 execution-time Provides time related functionality such as getting current time, current date, manipulating/formatting dates, etc. 5.0.2 execution-map Provides the capability to generate and manipulate map data objects. 5.0.2 execution-json Provides the capability to retrieve, insert, and modify JSON elements. 2.0.1 execution-unitconversion Converts various units such as length, mass, time and volume. 2.0.1 execution-reorder Orders out-of-order event arrivals using algorithms such as K-Slack and alpha K-Stack. 5.0.0 execution-unique Retains and process unique events based on the given parameters. 5.0.0 execution-streamingml Performs streaming machine learning (clustering, classification and regression) on event streams. 2.0.1 Input/Output Extensions Name Description Latest Tested Version io-http Receives and publishes events via http and https transports, calls external services, and serves incoming requests and provide synchronous responses. 2.0.6 io-nats Receives and publishes events from/to NATS. 2.0.3 io-kafka Receives and publishes events from/to Kafka. 5.0.2 io-email Receives and publishes events via email using smtp , pop3 and imap protocols. 2.0.2 io-cdc Captures change data from databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 2.0.1 io-tcp Receives and publishes events through TCP transport. 3.0.2 io-googlepubsub Receives and publishes events through Google Pub/Sub. 2.0.1 io-file Receives and publishes event data from/to files. 2.0.1 io-jms Receives and publishes events via Java Message Service (JMS), supporting Message brokers such as ActiveMQ 2.0.2 io-prometheus Consumes and expose Prometheus metrics from/to Prometheus server. 2.0.1 Data Mapping Extensions Name Description Latest Tested Version map-json Converts JSON messages to/from Siddhi events. 5.0.3 map-xml Converts XML messages to/from Siddhi events. 5.0.3 map-text Converts text messages to/from Siddhi events. 2.0.2 map-avro Converts AVRO messages to/from Siddhi events. 2.0.2 map-keyvalue Converts events having Key-Value maps to/from Siddhi events. 2.0.2 map-csv Converts messages with CSV format to/from Siddhi events. 2.0.2 map-binary Converts binary events that adheres to Siddhi format to/from Siddhi events. 2.0.2 Store Extensions Name Description Latest Tested Version store-rdbms Persist and retrieve events to/from RDBMS databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 6.0.1 store-mongodb Persist and retrieve events to/from MongoDB. 2.0.1 store-elasticsearch Persist and retrieve events to/from Elasticsearch. 3.0.0 Script Extensions Name Description Latest Tested Version script-js Allows writing user defined JavaScript functions within Siddhi Applications to process events. 5.0.1","title":"Extensions"},{"location":"docs/extensions/#siddhi-extensions","text":"Following are some supported Siddhi extensions;","title":"Siddhi Extensions"},{"location":"docs/extensions/#extensions-released-under-apache-20-license","text":"","title":"Extensions released under Apache 2.0 License"},{"location":"docs/extensions/#execution-extensions","text":"Name Description Latest Tested Version execution-string Provides basic string handling capabilities such as concat, length, replace all, etc. 5.0.3 execution-regex Provides basic RegEx execution capabilities. 5.0.3 execution-math Provides useful mathematical functions. 5.0.2 execution-time Provides time related functionality such as getting current time, current date, manipulating/formatting dates, etc. 5.0.2 execution-map Provides the capability to generate and manipulate map data objects. 5.0.2 execution-json Provides the capability to retrieve, insert, and modify JSON elements. 2.0.1 execution-unitconversion Converts various units such as length, mass, time and volume. 2.0.1 execution-reorder Orders out-of-order event arrivals using algorithms such as K-Slack and alpha K-Stack. 5.0.0 execution-unique Retains and process unique events based on the given parameters. 5.0.0 execution-streamingml Performs streaming machine learning (clustering, classification and regression) on event streams. 2.0.1","title":"Execution Extensions"},{"location":"docs/extensions/#inputoutput-extensions","text":"Name Description Latest Tested Version io-http Receives and publishes events via http and https transports, calls external services, and serves incoming requests and provide synchronous responses. 2.0.6 io-nats Receives and publishes events from/to NATS. 2.0.3 io-kafka Receives and publishes events from/to Kafka. 5.0.2 io-email Receives and publishes events via email using smtp , pop3 and imap protocols. 2.0.2 io-cdc Captures change data from databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 2.0.1 io-tcp Receives and publishes events through TCP transport. 3.0.2 io-googlepubsub Receives and publishes events through Google Pub/Sub. 2.0.1 io-file Receives and publishes event data from/to files. 2.0.1 io-jms Receives and publishes events via Java Message Service (JMS), supporting Message brokers such as ActiveMQ 2.0.2 io-prometheus Consumes and expose Prometheus metrics from/to Prometheus server. 2.0.1","title":"Input/Output Extensions"},{"location":"docs/extensions/#data-mapping-extensions","text":"Name Description Latest Tested Version map-json Converts JSON messages to/from Siddhi events. 5.0.3 map-xml Converts XML messages to/from Siddhi events. 5.0.3 map-text Converts text messages to/from Siddhi events. 2.0.2 map-avro Converts AVRO messages to/from Siddhi events. 2.0.2 map-keyvalue Converts events having Key-Value maps to/from Siddhi events. 2.0.2 map-csv Converts messages with CSV format to/from Siddhi events. 2.0.2 map-binary Converts binary events that adheres to Siddhi format to/from Siddhi events. 2.0.2","title":"Data Mapping Extensions"},{"location":"docs/extensions/#store-extensions","text":"Name Description Latest Tested Version store-rdbms Persist and retrieve events to/from RDBMS databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 6.0.1 store-mongodb Persist and retrieve events to/from MongoDB. 2.0.1 store-elasticsearch Persist and retrieve events to/from Elasticsearch. 3.0.0","title":"Store Extensions"},{"location":"docs/extensions/#script-extensions","text":"Name Description Latest Tested Version script-js Allows writing user defined JavaScript functions within Siddhi Applications to process events. 5.0.1","title":"Script Extensions"},{"location":"docs/features/","text":"Siddhi 5.0 Features Retrieving Events From various data sources supporting multiple message formats Mapping Events Mapping events with various data formats to Stream for processing Mapping streams to multiple data formats for publishing Processing Streams Filter Filtering stream based on conditions Window Support for sliding and batch (tumbling) and many other type of windows Aggregation Supporting Avg , Sum , Min , Max , etc For long running aggregations and aggregation over windows Ability to perform aggregate processing with Group by and filter aggregated data with Having conditions Incremental Aggregation Support for processing and retrieving long running Aggregation Supports data processing in seconds, minutes, hours, days, months, and years granularity Table and Stores For storing events for future processing and retrieving them on demand Supporting storage in in-memory, RDBMs, Solr, mongoDb, etc Join Joining two streams, two windows based on conditions Joining stream/window with table or incremental aggregation based on conditions Supports inner joins, and left, right full outer joins Pattern Identifies event occurrence patterns among streams over time Identify non occurrence of events Supports repetitive matches of event pattern occurrences with logical conditions and time bound Sequence processing Identifies continuous sequence of events from streams Supports zero to many, one to many, and zero to one event matching conditions Partitions Grouping queries and based on keywords or value ranges for isolated parallel processing Scripting Support writing scripts like JavaScript, Scala and R within Siddhi Queries Process Based on event time Whole execution driven by the event time Publishing Events To various data sources with various message formats Supporting load balancing and failover data publishing Error handling Support errors and exceptions through error streams Automatic backoff retries to external data stores, sources and sinks. Parallel processing Support parallel processing through asynchronous multithreading at streams Snapshot and restore Support for periodic state persistence and restore capabilities to allow state restore during failures","title":"Features"},{"location":"docs/features/#siddhi-50-features","text":"Retrieving Events From various data sources supporting multiple message formats Mapping Events Mapping events with various data formats to Stream for processing Mapping streams to multiple data formats for publishing Processing Streams Filter Filtering stream based on conditions Window Support for sliding and batch (tumbling) and many other type of windows Aggregation Supporting Avg , Sum , Min , Max , etc For long running aggregations and aggregation over windows Ability to perform aggregate processing with Group by and filter aggregated data with Having conditions Incremental Aggregation Support for processing and retrieving long running Aggregation Supports data processing in seconds, minutes, hours, days, months, and years granularity Table and Stores For storing events for future processing and retrieving them on demand Supporting storage in in-memory, RDBMs, Solr, mongoDb, etc Join Joining two streams, two windows based on conditions Joining stream/window with table or incremental aggregation based on conditions Supports inner joins, and left, right full outer joins Pattern Identifies event occurrence patterns among streams over time Identify non occurrence of events Supports repetitive matches of event pattern occurrences with logical conditions and time bound Sequence processing Identifies continuous sequence of events from streams Supports zero to many, one to many, and zero to one event matching conditions Partitions Grouping queries and based on keywords or value ranges for isolated parallel processing Scripting Support writing scripts like JavaScript, Scala and R within Siddhi Queries Process Based on event time Whole execution driven by the event time Publishing Events To various data sources with various message formats Supporting load balancing and failover data publishing Error handling Support errors and exceptions through error streams Automatic backoff retries to external data stores, sources and sinks. Parallel processing Support parallel processing through asynchronous multithreading at streams Snapshot and restore Support for periodic state persistence and restore capabilities to allow state restore during failures","title":"Siddhi 5.0 Features"},{"location":"docs/query-guide/","text":"Siddhi 5.0 Streaming SQL Guide Introduction Siddhi Streaming SQL is designed to process streams of events. It can be used to implement streaming data integration, streaming analytics, rule based and adaptive decision making use cases. It is an evolution of Complex Event Processing (CEP) and Stream Processing systems, hence it can also be used to process stateful computations, detecting of complex event patterns, and sending notifications in real-time. Siddhi Streaming SQL uses SQL like syntax, and annotations to consume events from diverse event sources with various data formats, process then using stateful and stateless operators and send outputs to multiple endpoints according to their accepted event formats. It also supports exposing rule based and adaptive decision making as service endpoints such that external programs and systems can synchronously get decision support form Siddhi. The following sections explains how to write processing logic using Siddhi Streaming SQL. Siddhi Application The processing logic for your program can be written using the Streaming SQL and put together as a single file with .siddhi extension. This file is called as the Siddhi Application or the SiddhiApp . SiddhiApps are named by adding @app:name(' name ') annotation on the top of the SiddhiApp file. When the annotation is not added Siddhi assigns a random UUID as the name of the SiddhiApp. Purpose SiddhiApp provides an isolated execution environment for your processing logic that allows you to deploy and execute processing logic independent of other SiddhiApp in the system. Therefore it's always recommended to have a processing logic related to single use case in a single SiddhiApp. This will help you to group processing logic and easily manage addition and removal of various use cases. The following diagram depicts some of the key Siddhi Streaming SQL elements of Siddhi Application and how event flows through the elements. Below table provides brief description of a few key elements in the Siddhi Streaming SQL Language. Elements Description Stream A logical series of events ordered in time with a uniquely identifiable name, and a defined set of typed attributes defining its schema. Event An event is a single event object associated with a stream. All events of a stream contains a timestamp and an identical set of typed attributes based on the schema of the stream they belong to. Table A structured representation of data stored with a defined schema. Stored data can be backed by In-Memory , or external data stores such as RDBMS , MongoDB , etc. The tables can be accessed and manipulated at runtime. Named Window A structured representation of data stored with a defined schema and eviction policy. Window data is stored In-Memory and automatically cleared by the named window constrain. Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Named Aggregation A structured representation of data that's incrementally aggregated and stored with a defined schema and aggregation granularity such as seconds, minutes, hours, etc. Aggregation data is stored both In-Memory and in external data stores such as RDBMS . Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Query A logical construct that processes events in streaming manner by by consuming data from one or more streams, tables, windows and aggregations, and publishes output events into a stream, table or a window. Source A construct that consumes data from external sources (such as TCP , Kafka , HTTP , etc) with various event formats such as XML , JSON , binary , etc, convert then to Siddhi events, and passes into streams for processing. Sink A construct that consumes events arriving at a stream, maps them to a predefined data format (such as XML , JSON , binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Input Handler A mechanism to programmatically inject events into streams. Stream/Query Callback A mechanism to programmatically consume output events from streams or queries. Partition A logical container that isolates the processing of queries based on the partition keys derived from the events. Inner Stream A positionable stream that connects portioned queries with each other within the partition. Grammar SiddhiApp is a collection of Siddhi Streaming SQL elements composed together as a script. Here each Siddhi element must be separated by a semicolon ; . Hight level syntax of SiddhiApp is as follows. siddhi app : app annotation * ( stream definition | table definition | ... ) + ( query | partition ) + ; Example Siddhi Application with name Temperature-Analytics defined with a stream named TempStream and a query named 5minAvgQuery . @app:name('Temperature-Analytics') define stream TempStream (deviceID long, roomNo int, temp double); @name('5minAvgQuery') from TempStream#window.time(5 min) select roomNo, avg(temp) as avgTemp group by roomNo insert into OutputStream; Stream A stream is a logical series of events ordered in time. Its schema is defined via the stream definition . A stream definition contains the stream name and a set of attributes with specific types and uniquely identifiable names within the stream. All events associated to the stream will have the same schema (i.e., have the same attributes in the same order). Purpose Stream groups common types of events together with a schema. This helps in various ways such as, processing all events together in queries and performing data format transformations together when they are consumed and published via sources and sinks. Syntax The syntax for defining a new stream is as follows. define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure a stream definition. Parameter Description stream name The name of the stream created. (It is recommended to define a stream name in PascalCase .) attribute name Uniquely identifiable name of the stream attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer stream and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . To make the stream process events in multi-threading and asynchronous way use the @Async annotation as shown in Multi-threading and Asynchronous Processing configuration section. Example define stream TempStream (deviceID long, roomNo int, temp double); The above creates a stream with name TempStream having the following attributes. deviceID of type long roomNo of type int temp of type double Source Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. A source configuration allows to define a mapping in order to convert each incoming event from its native data format to a Siddhi event. When customizations to such mappings are not provided, Siddhi assumes that the arriving event adheres to the predefined format based on the stream definition and the configured message mapping type. Purpose Source provides a way to consume events from external systems and convert them to be processed by the associated stream. Syntax To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as follows: @source(type=' source type ', static.key =' value ', static.key =' value ', @map(type=' map type ', static.key =' value ', static.key =' value ', @attributes( attribute1 =' attribute mapping ', attributeN =' attribute mapping ') ) ) define stream stream name ( attribute1 type , attributeN type ); This syntax includes the following annotations. Source The type parameter of @source annotation defines the source type that receives events. The other parameters of @source annotation depends upon the selected source type, and here some of its parameters can be optional. For detailed information about the supported parameters see the documentation of the relevant source. The following is the list of source types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to consume events from other SiddhiApps running on the same JVM. HTTP Expose an HTTP service to consume messages. Kafka Subscribe to Kafka topic to consume events. TCP Expose a TCP service to consume messages. Email Consume emails via POP3 and IMAP protocols. JMS Subscribe to JMS topic or queue to consume events. File Reads files by tailing or as a whole to extract events out of them. CDC Perform change data capture on databases. Prometheus Consume data from Prometheus agent. In-memory is the only source inbuilt in Siddhi, and all other source types are implemented as extensions. Source Mapper Each @source configuration can have a mapping denoted by the @map annotation that defines how to convert the incoming event format to Siddhi events. The type parameter of the @map defines the map type to be used in converting the incoming events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional. For detailed information about the parameters see the documentation of the relevant mapper. Map Attributes @attributes is an optional annotation used with @map to define custom mapping. When @attributes is not provided, each mapper assumes that the incoming events adheres to its own default message format and attempt to convert the events from that format. By adding the @attributes annotation, users can selectively extract data from the incoming message and assign them to the attributes. There are two ways to configure @attributes . Define attribute names as keys, and mapping configurations as values: @attributes( attribute1 =' mapping ', attributeN =' mapping ') Define the mapping configurations in the same order as the attributes defined in stream definition: @attributes( ' mapping for attribute1 ', ' mapping for attributeN ') Supported Source Mapping Types The following is the list of source mapping types supported by Siddhi: Source mapping type Description PassThrough Omits data conversion on Siddhi events. JSON Converts JSON messages to Siddhi events. XML Converts XML messages to Siddhi events. TEXT Converts plain text messages to Siddhi events. Avro Converts Avro events to Siddhi events. Binary Converts Siddhi specific binary events to Siddhi events. Key Value Converts key-value HashMaps to Siddhi events. CSV Converts CSV like delimiter separated events to Siddhi events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the consumed Siddhi events directly to the streams without any data conversion. PassThrough is the only source mapper inbuilt in Siddhi, and all other source mappers are implemented as extensions. Example 1 Receive JSON messages by exposing an HTTP service, and direct them to InputStream stream for processing. Here the HTTP service will be secured with basic authentication, receives events on all network interfaces on port 8080 and context /foo . The service expects the JSON messages to be on the default data format that's supported by the JSON mapper as follows. { \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } The configuration of the HTTP source and JSON source mapper to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json')) define stream InputStream (name string, age int, country string); Example 2 Receive JSON messages by exposing an HTTP service, and direct them to StockStream stream for processing. Here the incoming JSON , as given bellow, do not adhere to the default data format that's supported by the JSON mapper. { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"FB\" }, \"price\":55.6 } } } The configuration of the HTTP source and the custom JSON source mapping to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); The same can also be configured by omitting the attribute names as bellow. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(\"stock.company.symbol\", \"stock.price\", \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); Sink Sinks consumes events from streams and publish them via multiple transports to external endpoints in various data formats. A sink configuration allows users to define a mapping to convert the Siddhi events in to the required output data format (such as JSON , TEXT , XML , etc.) and publish the events to the configured endpoints. When customizations to such mappings are not provided, Siddhi converts events to the predefined event format based on the stream definition and the configured message mapper type before publishing the events. Purpose Sink provides a way to publish Siddhi events of a stream to external systems by converting events to their supported format. Syntax To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as follows: @sink(type=' sink type ', static.key =' value ', dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) ) define stream stream name ( attribute1 type , attributeN type ); Dynamic Properties The sink and sink mapper properties that are categorized as dynamic have the ability to absorb attribute values dynamically from the Siddhi events of their associated streams. This can be configured by enclosing the relevant attribute names in double curly braces as {{...}} , and using it within the property values. Some valid dynamic properties values are: '{{attribute1}}' 'This is {{attribute1}}' {{attribute1}} {{attributeN}} Here the attribute names in the double curly braces will be replaced with the values from the events before they are published. This syntax includes the following annotations. Sink The type parameter of the @sink annotation defines the sink type that publishes the events. The other parameters of the @sink annotation depends upon the selected sink type, and here some of its parameters can be optional and/or dynamic. For detailed information about the supported parameters see documentation of the relevant sink. The following is a list of sink types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to publish events to other SiddhiApps running on the same JVM. Log Logs the events appearing on the streams. HTTP Publish events to an HTTP endpoint. Kafka Publish events to Kafka topic. TCP Publish events to a TCP service. Email Send emails via SMTP protocols. JMS Publish events to JMS topics or queues. File Writes events to files. Prometheus Expose data through Prometheus agent. Distributed Sink Distributed Sinks publish events from a defined stream to multiple endpoints using load balancing or partitioning strategies. Any sink can be used as a distributed sink. A distributed sink configuration allows users to define a common mapping to convert and send the Siddhi events for all its destination endpoints. Purpose Distributed sink provides a way to publish Siddhi events to multiple endpoints in the configured event format. Syntax To configure distributed sink add the sink configuration to a stream definition by adding the @sink annotation and add the configuration parameters that are common of all the destination endpoints inside it, along with the common parameters also add the @distribution annotation specifying the distribution strategy (i.e. roundRobin or partitioned ) and @destination annotations providing each endpoint specific configurations. The distributed sink syntax is as follows: RoundRobin Distributed Sink Publishes events to defined destinations in a round robin manner. @sink(type=' sink type ', common.static.key =' value ', common.dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) @distribution(strategy='roundRobin', @destination( destination.specific.key =' value '), @destination( destination.specific.key =' value '))) ) define stream stream name ( attribute1 type , attributeN type ); Partitioned Distributed Sink Publishes events to defined destinations by partitioning them based on the partitioning key. @sink(type=' sink type ', common.static.key =' value ', common.dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) @distribution(strategy='partitioned', partitionKey=' partition key ', @destination( destination.specific.key =' value '), @destination( destination.specific.key =' value '))) ) define stream stream name ( attribute1 type , attributeN type ); Sink Mapper Each @sink configuration can have a mapping denoted by the @map annotation that defines how to convert Siddhi events to outgoing messages with the defined format. The type parameter of the @map defines the map type to be used in converting the outgoing events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional and/or dynamic. For detailed information about the parameters see the documentation of the relevant mapper. Map Payload @payload is an optional annotation used with @map to define custom mapping. When the @payload annotation is not provided, each mapper maps the outgoing events to its own default event format. The @payload annotation allow users to configure mappers to produce the output payload of their choice, and by using dynamic properties within the payload they can selectively extract and add data from the published Siddhi events. There are two ways you to configure @payload annotation. Some mappers such as XML , JSON , and Test only accept one output payload: @payload( 'This is a test message from {{user}}.') Some mappers such key-value accept series of mapping values: @payload( key1='mapping_1', 'key2'='user : {{user}}') Here, the keys of payload mapping can be defined using the dot notation as a.b.c , or using any constant string value as '$abc' . Supported Sink Mapping Types The following is a list of sink mapping types supported by Siddhi: Sink mapping type Description PassThrough Omits data conversion on outgoing Siddhi events. JSON Converts Siddhi events to JSON messages. XML Converts Siddhi events to XML messages. TEXT Converts Siddhi events to plain text messages. Avro Converts Siddhi events to Avro Events. Binary Converts Siddhi events to Siddhi specific binary events. Key Value Converts Siddhi events to key-value HashMaps. CSV Converts Siddhi events to CSV like delimiter separated events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the outgoing Siddhi events directly to the sinks without any data conversion. PassThrough is the only sink mapper inbuilt in Siddhi, and all other sink mappers are implemented as extensions. Example 1 Publishes OutputStream events by converting them to JSON messages with the default format, and by sending to an HTTP endpoint http://localhost:8005/endpoint1 , using POST method, Accept header, and basic authentication having admin is both username and password. The configuration of the HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/endpoint', method='POST', headers='Accept-Date:20/02/2017', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json')) define stream OutputStream (name string, age int, country string); This will publish a JSON message on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } } Example 2 Publishes StockStream events by converting them to user defined JSON messages, and by sending to an HTTP endpoint http://localhost:8005/stocks . The configuration of the HTTP sink and custom JSON sink mapping to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/stocks', @map(type='json', validate.json='true', enclosing.element='$.Portfolio', @payload(\"\"\"{\"StockData\":{ \"Symbol\":\"{{symbol}}\", \"Price\":{{price}} }}\"\"\"))) define stream StockStream (symbol string, price float, volume long); This will publish a single event as the JSON message on the following format: { \"Portfolio\":{ \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } } } This can also publish multiple events together as a JSON message on the following format: { \"Portfolio\":[ { \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } }, { \"StockData\":{ \"Symbol\":\"FB\", \"Price\":57.0 } } ] } Example 3 Publishes events from the OutputStream stream to multiple the HTTP endpoints using a partitioning strategy. Here the events are sent to either http://localhost:8005/endpoint1 or http://localhost:8006/endpoint2 based on the partitioning key country . It uses default JSON mapping, POST method, and used admin as both the username and the password when publishing to both the endpoints. The configuration of the distributed HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', method='POST', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json'), @distribution(strategy='partitioned', partitionKey='country', @destination(publisher.url='http://localhost:8005/endpoint1'), @destination(publisher.url='http://localhost:8006/endpoint2'))) define stream OutputStream (name string, age int, country string); This will partition the outgoing events and publish all events with the same country attribute value to the same endpoint. The JSON message published will be on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } } Error Handling Errors in Siddhi can be handled at the Streams and in Sinks. Error Handling at Stream When errors are thrown by Siddhi elements subscribed to the stream, the error gets propagated up to the stream that delivered the event to those Siddhi elements. By default the error is logged and dropped at the stream, but this behavior can be altered by by adding @OnError annotation to the corresponding stream definition. @OnError annotation can help users to capture the error and the associated event, and handle them gracefully by sending them to a fault stream. The @OnError annotation and the required action to be specified as bellow. @OnError(action='on error action') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The action parameter of the @OnError annotation defines the action to be executed during failure scenarios. The following actions can be specified to @OnError annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when @OnError annotation is not defined. STREAM : Creates a fault stream and redirects the event and the error to it. The created fault stream will have all the attributes defined in the base stream to capture the error causing event, and in addition it also contains _error attribute of type object to containing the error information. The fault stream can be referred by adding ! in front of the base stream name as ! stream name . Example Handle errors in TempStream by redirecting the errors to a fault stream. The configuration of TempStream stream and @OnError annotation is as follows. @OnError(name='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); Siddhi will infer and automatically defines the fault stream of TempStream as given bellow. define stream !TempStream (deviceID long, roomNo int, temp double, _error object); The SiddhiApp extending the above the use-case by adding failure generation and error handling with the use of queries is as follows. Note: Details on writing processing logics via queries will be explained in later sections. -- Define fault stream to handle error occurred at TempStream subscribers @OnError(name='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); -- Error generation through a custom function `createError()` @name('error-generation') from TempStream#custom:createError() insert into IgnoreStream1; -- Handling error by simply logging the event and error. @name('handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream2; Error Handling at Sink There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. The on.error parameter of the @sink annotation can be specified as bellow. @sink(type=' sink type ', on.error=' on error action ', key =' value ', ...) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following actions can be specified to on.error parameter of @sink annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when on.error parameter is not defined on the @sink annotation. WAIT : Publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publishes to it. STREAM : Pushes the failed events with the corresponding error to the associated fault stream the sink belongs to. Example 1 Introduce back pressure on the threads who bring events via TempStream when the system cannot connect to Kafka. The configuration of TempStream stream and @sink Kafka annotation with on.error property is as follows. @sink(type='kafka', on.error='WAIT', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); Example 2 Send events to the fault stream of TempStream when the system cannot connect to Kafka. The configuration of TempStream stream with associated fault stream, @sink Kafka annotation with on.error property and a queries to handle the error is as follows. Note: Details on writing processing logics via queries will be explained in later sections. @OnError(name='STREAM') @sink(type='kafka', on.error='STREAM', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); -- Handling error by simply logging the event and error. @name('handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream; Query Query defines the processing logic in Siddhi. It consumes events from one or more streams, named-windows , tables , and/or named-aggregations , process the events in a streaming manner, and generate output events into a stream , named-window , or table . Purpose A query provides a way to process the events in the order they arrive and produce output using both stateful and stateless complex event processing and stream processing operations. Syntax The high level query syntax for defining processing logics is as follows: @name(' query name ') from input projection output action The following parameters are used to configure a stream definition. Parameter Description query name The name of the query. Since naming the query (i.e the @name(' query name ') annotation) is optional, when the name is not provided Siddhi assign a system generated name for the query. input Defines the means of event consumption via streams , named-windows , tables , and/or named-aggregations , and defines the processing logic using filters , windows , stream-functions , joins , patterns and sequences . projection Generates output event attributes using select , functions , aggregation-functions , and group by operations, and filters the generated the output using having , limit offset , order by , and output rate limiting operations before sending them out. Here the projection is optional and when it is omitted all the input events will be sent to the output as it is. output action Defines output action (such as insert into , update , delete , etc) that needs to be performed by the generated events on a stream , named-window , or table Example A query consumes events from the TempStream stream and output only the roomNo and temp attributes to the RoomTempStream stream, from which another query consumes the events and sends all its attributes to AnotherRoomTempStream stream. Inferred Stream Here, the RoomTempStream and AnotherRoomTempStream streams are an inferred streams, which means their stream definitions are inferred from the queries and they can be used same as any other defined stream without any restrictions. Value Values are typed data, that can be manipulated, transferred and stored. Values can be referred by the attributes defined in definitions such as streams, and tables. Siddhi supports values of type STRING , INT (Integer), LONG , DOUBLE , FLOAT , BOOL (Boolean) and OBJECT . The syntax of each type and their example use as a constant value is as follows, Attribute Type Format Example int + 123 , -75 , +95 long +L 123000L , -750l , +154L float ( +)?('.' *)? (E(-|+)? +)?F 123.0f , -75.0e-10F , +95.789f double ( +)?('.' *)? (E(-|+)? +)?D? 123.0 , 123.0D , -75.0e-10D , +95.789d bool (true|false) true , false , TRUE , FALSE string '( char * !('|\"|\"\"\"| line ))' or \"( char * !(\"|\"\"\"| line ))\" or \"\"\"( char * !(\"\"\"))\"\"\" 'Any text.' , \"Text with 'single' quotes.\" , \"\"\" Text with 'single' quotes, \"double\" quotes, and new lines. \"\"\" Time Time is a special type of LONG value that denotes time using digits and their unit in the format ( digit + unit )+ . At execution, the time gets converted into milliseconds and returns a LONG value. Unit Syntax Year year | years Month month | months Week week | weeks Day day | days Hour hour | hours Minutes minute | minutes | min Seconds second | seconds | sec Milliseconds millisecond | milliseconds Example 1 hour and 25 minutes can by written as 1 hour and 25 minutes which is equal to the LONG value 5100000 . Select The select clause in Siddhi query defines the output event attributes of the query. Following are some basic query projection operations supported by select. Action Description Select specific attributes for projection Only select some of the input attributes as query output attributes. E.g., Select and output only roomNo and temp attributes from the TempStream stream. from TempStream select roomNo, temp insert into RoomTempStream; Select all attributes for projection Select all input attributes as query output attributes. This can be done by using asterisk ( * ) or by omitting the select clause itself. E.g., Both following queries select all attributes of TempStream input stream and output all attributes to NewTempStream stream. from TempStream select * insert into NewTempStream; or from TempStream insert into NewTempStream; Name attribute Provide a unique name for each output attribute generated by the query. This can help to rename the selected input attributes or assign an attribute name to a projection operation such as function, aggregate-function, mathematical operation, etc, using as keyword. E.g., Query that renames input attribute temp to temperature and function currentTimeMillis() as time . from TempStream select roomNo, temp as temperature, currentTimeMillis() as time insert into RoomTempStream; Constant values as attributes Creates output attributes with a constant value. Any constant value of type STRING , INT , LONG , DOUBLE , FLOAT , BOOL , and time as given in the values section can be defined. E.g., Query specifying 'C' as the constant value for the scale attribute. from TempStream select roomNo, temp, 'C' as scale insert into RoomTempStream; Mathematical and logical expressions in attributes Defines the mathematical and logical operations that need to be performed to generating output attribute values. These expressions are executed in the precedence order given below. Operator precedence Operator Distribution Example () Scope (cost + tax) * 0.05 IS NULL Null check deviceID is null NOT Logical NOT not (price > 10) * , / , % Multiplication, division, modulus temp * 9/5 + 32 + , - Addition, subtraction temp * 9/5 - 32 < , < = , > , >= Comparators: less-than, greater-than-equal, greater-than, less-than-equal totalCost >= price * quantity == , != Comparisons: equal, not equal totalCost != price * quantity IN Checks if value exist in the table roomNo in ServerRoomsTable AND Logical AND temp < 40 and humidity < 40 OR Logical OR humidity < 40 or humidity >= 60 E.g., Query converts temperature from Celsius to Fahrenheit, and identifies rooms with room number between 10 and 15 as server rooms. from TempStream select roomNo, temp * 9/5 + 32 as temp, 'F' as scale, roomNo > 10 and roomNo < 15 as isServerRoom insert into RoomTempStream; Function Function are pre-configured operations that can consumes zero, or more parameters and always produce a single value as result. It can be used anywhere an attribute can be used. Purpose Functions encapsulate pre-configured reusable execution logic allowing users to execute the logic anywhere just by calling the function. This also make writing SiddhiApps simple and easy to understand. Syntax The syntax of function is as follows, function name ( parameter * ) Here function name uniquely identifies the function. The parameter defined input parameters the function can accept. The input parameters can be attributes, constant values, results of other functions, results of mathematical or logical expressions, or time values. The number and type of parameters a function accepts depend on the function itself. Note Functions, mathematical expressions, and logical expressions can be used in a nested manner. Example 1 Function name add accepting two input parameters, is called with an attribute named input and a constant value 75 . add(input, 75) Example 2 Function name alertAfter accepting two input parameters, is called with a time value of 1 hour and 25 minutes and a mathematical addition operation of startTime + 56 . add(1 hour and 25 minutes, startTime + 56) Inbuilt functions Following are some inbuilt Siddhi functions, for more functions refer execution extensions . Inbuilt function Description eventTimestamp Returns event's timestamp. currentTimeMillis Returns current time of SiddhiApp runtime. default Returns a default value if the parameter is null. ifThenElse Returns parameters based on a conditional parameter. UUID Generates a UUID. cast Casts parameter type. convert Converts parameter type. coalesce Returns first not null input parameter. maximum Returns the maximum value of all parameters. minimum Returns the minimum value of all parameters. instanceOfBoolean Checks if the parameter is an instance of Boolean. instanceOfDouble Checks if the parameter is an instance of Double. instanceOfFloat Checks if the parameter is an instance of Float. instanceOfInteger Checks if the parameter is an instance of Integer. instanceOfLong Checks if the parameter is an instance of Long. instanceOfString Checks if the parameter is an instance of String. createSet Creates HashSet with given input parameters. sizeOfSet Returns number of items in the HashSet, that's passed as a parameter. Example Query that converts the roomNo to string using convert function, finds the maximum temperature reading with maximum function, and adds a unique messageID using the UUID function. from TempStream select convert(roomNo, 'string') as roomNo, maximum(tempReading1, tempReading2) as temp, UUID() as messageID insert into RoomTempStream; Filter Filters provide a way of filtering input stream events based on a specified condition. It accepts any type of condition including a combination of functions and/or attributes that produces a Boolean result. Filters allow events to pass through if the condition results in true , and drops if it results in a false . Purpose Filter helps to select the events that are relevant for the processing and omit the ones that are not needed. Syntax Filter conditions should be defined in square brackets next to the input stream as shown below. from input stream [ filter condition ] select attribute name , attribute name , ... insert into output stream Example Query to filter TempStream stream events, having roomNo within the range of 100-210 and temperature greater than 40 degrees, and insert them into HighTempStream stream. from TempStream[(roomNo = 100 and roomNo 210) and temp 40] select roomNo, temp insert into HighTempStream; Window Windows allow you to capture a subset of events based on a specific criterion from an input stream for calculation. Each input stream can only have a maximum of one window. Purpose To create subsets of events within a stream based on time duration, number of events, etc for processing. A window can operate in a sliding or tumbling (batch) manner. Syntax The #window prefix should be inserted next to the relevant stream in order to use a window. Note Filter condition can be applied both before and/or after the window Example If you want to identify the maximum temperature out of the last 10 events, you need to define a length window of 10 events. This window operates in a sliding mode where the following 3 subsets are calculated when a list of 12 events are received in a sequential order. Subset Event Range 1 1-10 2 2-11 3 3-12 The following query finds the maximum temperature out of last 10 events from the TempStream stream, and inserts the results into the MaxTempStream stream. from TempStream#window.length(10) select max(temp) as maxTemp insert into MaxTempStream; If you define the maximum temperature reading out of every 10 events, you need to define a lengthBatch window of 10 events. This window operates as a batch/tumbling mode where the following 3 subsets are calculated when a list of 30 events are received in a sequential order. Subset Event Range 1 1-10 2 11-20 3 21-30 The following query finds the maximum temperature out of every 10 events from the TempStream stream, and inserts the results into the MaxTempStream stream. from TempStream#window.lengthBatch(10) select max(temp) as maxTemp insert into MaxTempStream; Note Similar operations can be done based on time via time windows and timeBatch windows and for others. Code segments such as #window.time(10 min) considers events that arrive during the last 10 minutes in a sliding manner, and the #window.timeBatch(2 min) considers events that arrive every 2 minutes in a tumbling manner. Following are some inbuilt windows shipped with Siddhi. For more window types, see execution extensions . time timeBatch batch timeLength length lengthBatch sort frequent lossyFrequent session cron externalTime externalTimeBatch delay Output event types Projection of the query depends on the output event types such as, current and expired event types. By default all queries produce current events and only queries with windows produce expired events when events expire from the window. You can specify whether the output of a query should be only current events, only expired events or both current and expired events. Note! Controlling the output event types does not alter the execution within the query, and it does not affect the accuracy of the query execution. The following keywords can be used with the output stream to manipulate output. Output event types Description current events Outputs events when incoming events arrive to be processed by the query. This is default when no specific output event type is specified. expired events Outputs events when events expires from the window. all events Outputs events when incoming events arrive to be processed by the query as well as when events expire from the window. The output event type keyword can be used between insert and into as shown in the following example. Example This query delays all events in a stream by 1 minute. from TempStream#window.time(1 min) select * insert expired events into DelayedTempStream Aggregate function Aggregate functions perform aggregate calculations in the query. When a window is defined the aggregation is restricted within that window. If no window is provided aggregation is performed from the start of the Siddhi application. Syntax from input stream #window. window name ( parameter , parameter , ... ) select aggregate function ( parameter , parameter , ... ) as attribute name , attribute2 name , ... insert into output stream ; Aggregate Parameters Aggregate parameters can be attributes, constant values, results of other functions or aggregates, results of mathematical or logical expressions, or time parameters. Aggregate parameters configured in a query depends on the aggregate function being called. Example The following query calculates the average value for the temp attribute of the TempStream stream. This calculation is done for the last 10 minutes in a sliding manner, and the result is output as avgTemp to the AvgTempStream output stream. from TempStream#window.time(10 min) select avg(temp) as avgTemp, roomNo, deviceID insert into AvgTempStream; Following are some inbuilt aggregation functions shipped with Siddhi, for more aggregation functions, see execution extensions . avg sum max min count distinctCount maxForever minForever stdDev Group By Group By allows you to group the aggregate based on specified attributes. Syntax The syntax for the Group By aggregate function is as follows: from input stream #window. window name (...) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... insert into output stream ; Example The following query calculates the average temperature per roomNo and deviceID combination, for events that arrive at the TempStream stream for a sliding time window of 10 minutes. from TempStream#window.time(10 min) select avg(temp) as avgTemp, roomNo, deviceID group by roomNo, deviceID insert into AvgTempStream; Having Having allows you to filter events after processing the select statement. Purpose This allows you to filter the aggregation output. Syntax The syntax for the Having clause is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition insert into output stream ; Example The following query calculates the average temperature per room for the last 10 minutes, and alerts if it exceeds 30 degrees. from TempStream#window.time(10 min) select avg(temp) as avgTemp, roomNo group by roomNo having avgTemp 30 insert into AlertStream; Order By Order By allows you to order the aggregated result in ascending and/or descending order based on specified attributes. By default ordering will be done in ascending manner. User can use 'desc' keyword to order in descending manner. Syntax The syntax for the Order By clause is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name (asc | desc)?, attribute2 name ( ascend/descend )?, ... insert into output stream ; Example The following query calculates the average temperature per roomNo and deviceID combination for every 10 minutes, and generate output events by ordering them in the ascending order of the room's avgTemp and then by the descending order of roomNo. from TempStream#window.timeBatch(10 min) select avg(temp) as avgTemp, roomNo, deviceID group by roomNo, deviceID order by avgTemp, roomNo desc insert into AvgTempStream; Limit Offset When events are emitted as a batch, offset allows you to offset beginning of the output event batch and limit allows you to limit the number of events in the batch from the defined offset. With this users can specify which set of events need be emitted. Syntax The syntax for the Limit Offset clause is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name (asc | desc)?, attribute2 name ( ascend/descend )?, ... limit positive interger ? offset positive interger ? insert into output stream ; Here both limit and offset are optional where limit by default output all the events and offset by default set to 0 . Example The following query calculates the average temperature per roomNo and deviceID combination, for events that arrive at the TempStream stream for every 10 minutes and emits two events with highest average temperature. from TempStream#window.timeBatch(10 min) select avg(temp) as avgTemp, roomNo, deviceID group by roomNo, deviceID order by avgTemp desc limit 2 insert into HighestAvgTempStream; The following query calculates the average temperature per roomNo and deviceID combination, for events that arrive at the TempStream stream for every 10 minutes and emits third, forth and fifth events when sorted in descending order based on their average temperature. from TempStream#window.timeBatch(10 min) select avg(temp) as avgTemp, roomNo, deviceID group by roomNo, deviceID order by avgTemp desc limit 3 offset 2 insert into HighestAvgTempStream; Join (Stream) Joins allow you to get a combined result from two streams in real-time based on a specified condition. Purpose Streams are stateless. Therefore, in order to join two streams, they need to be connected to a window so that there is a pool of events that can be used for joining. Joins also accept conditions to join the appropriate events from each stream. During the joining process each incoming event of each stream is matched against all the events in the other stream's window based on the given condition, and the output events are generated for all the matching event pairs. Note Join can also be performed with stored data , aggregation or externally named windows . Syntax The syntax for a join is as follows: from input stream #window. window name ( parameter , ... ) {unidirectional} {as reference } join input stream #window. window name ( parameter , ... ) {unidirectional} {as reference } on join condition select attribute name , attribute name , ... insert into output stream Here, the join condition allows you to match the attributes from both the streams. Unidirectional join operation By default, events arriving at either stream can trigger the joining process. However, if you want to control the join execution, you can add the unidirectional keyword next to a stream in the join definition as depicted in the syntax in order to enable that stream to trigger the join operation. Here, events arriving at other stream only update the window of that stream, and this stream does not trigger the join operation. Note The unidirectional keyword cannot be applied to both the input streams because the default behaviour already allows both streams to trigger the join operation. Example Assuming that the temperature of regulators are updated every minute. Following is a Siddhi App that controls the temperature regulators if they are not already on for all the rooms with a room temperature greater than 30 degrees. define stream TempStream(deviceID long, roomNo int, temp double); define stream RegulatorStream(deviceID long, roomNo int, isOn bool); from TempStream[temp 30.0]#window.time(1 min) as T join RegulatorStream[isOn == false]#window.length(1) as R on T.roomNo == R.roomNo select T.roomNo, R.deviceID, 'start' as action insert into RegulatorActionStream; Supported join types Following are the supported operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join both the streams. The output is generated only if there is a matching event in both the streams. Left outer join The left outer join operation allows you to join two streams to be merged based on a condition. left outer join is used as the keyword to join both the streams. Here, it returns all the events of left stream even if there are no matching events in the right stream by having null values for the attributes of the right stream. Example The following query generates output events for all events from the StockStream stream regardless of whether a matching symbol exists in the TwitterStream stream or not. from StockStream#window.time(1 min) as S left outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ; Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join both the streams. It returns all the events of the right stream even if there are no matching events in the left stream. Full outer join The full outer join combines the results of left outer join and right outer join. full outer join is used as the keyword to join both the streams. Here, output event are generated for each incoming event even if there are no matching events in the other stream. Example The following query generates output events for all the incoming events of each stream regardless of whether there is a match for the symbol attribute in the other stream or not. from StockStream#window.time(1 min) as S full outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ; Pattern This is a state machine implementation that allows you to detect patterns in the events that arrive over time. This can correlate events within a single stream or between multiple streams. Purpose Patterns allow you to identify trends in events over a time period. Syntax The following is the syntax for a pattern query: from (every)? event reference = input stream [ filter condition ] - (every)? event reference = input stream [ filter condition ] - ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream | Items| Description | |-------------------|-------------| | - | This is used to indicate an event that should be following another event. The subsequent event does not necessarily have to occur immediately after the preceding event. The condition to be met by the preceding event should be added before the sign, and the condition to be met by the subsequent event should be added after the sign. | | event reference | This allows you to add a reference to the the matching event so that it can be accessed later for further processing. | | (within time gap )? | The within clause is optional. It defines the time duration within which all the matching events should occur. | | every | every is an optional keyword. This defines whether the event matching should be triggered for every event arrival in the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. | Siddhi also supports pattern matching with counting events and matching events in a logical order such as ( and , or , and not ). These are described in detail further below in this guide. Example This query sends an alert if the temperature of a room increases by 5 degrees within 10 min. from every( e1=TempStream ) - e2=TempStream[ e1.roomNo == roomNo and (e1.temp + 5) = temp ] within 10 min select e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Here, the matching process begins for each event in the TempStream stream (because every is used with e1=TempStream ), and if another event arrives within 10 minutes with a value for the temp attribute that is greater than or equal to e1.temp + 5 of the event e1, an output is generated via the AlertStream . Counting Pattern Counting patterns allow you to match multiple events that may have been received for the same matching condition. The number of events matched per condition can be limited via condition postfixes. Syntax Each matching condition can contain a collection of events with the minimum and maximum number of events to be matched as shown in the syntax below. from (every)? event reference = input stream [ filter condition ] ( min count : max count )? - ... (within time gap )? select event reference ([event index])?. attribute name , ... insert into output stream Postfix Description Example n1:n2 This matches n1 to n2 events (including n1 and not more than n2 ). 1:4 matches 1 to 4 events. n: This matches n or more events (including n ). 2: matches 2 or more events. :n This matches up to n events (excluding n ). :5 matches up to 5 events. n This matches exactly n events. 5 matches exactly 5 events. Specific occurrences of the event in a collection can be retrieved by using an event index with its reference. Square brackets can be used to indicate the event index where 1 can be used as the index of the first event and last can be used as the index for the last available event in the event collection. If you provide an index greater then the last event index, the system returns null . The following are some valid examples. e1[3] refers to the 3 rd event. e1[last] refers to the last event. e1[last - 1] refers to the event before the last event. Example The following Siddhi App calculates the temperature difference between two regulator events. define stream TempStream (deviceID long, roomNo int, temp double); define stream RegulatorStream (deviceID long, roomNo int, tempSet double, isOn bool); from every( e1=RegulatorStream) - e2=TempStream[e1.roomNo==roomNo] 1: - e3=RegulatorStream[e1.roomNo==roomNo] select e1.roomNo, e2[0].temp - e2[last].temp as tempDiff insert into TempDiffStream; Logical Patterns Logical patterns match events that arrive in temporal order and correlate them with logical relationships such as and , or and not . Syntax from (every)? (not)? event reference = input stream [ filter condition ] ((and|or) event reference = input stream [ filter condition ])? (within time gap )? - ... select event reference ([event index])?. attribute name , ... insert into output stream Keywords such as and , or , or not can be used to illustrate the logical relationship. Key Word Description and This allows both conditions of and to be matched by two events in any order. or The state succeeds if either condition of or is satisfied. Here the event reference of the other condition is null . not condition1 and condition2 When not is included with and , it identifies the events that match arriving before any event that match . not condition for time period When not is included with for , it allows you to identify a situation where no event that matches condition1 arrives during the specified time period . e.g., from not TemperatureStream[temp 60] for 5 sec . Here the not pattern can be followed by either an and clause or the effective period of not can be concluded after a given time period . Further in Siddhi more than two streams cannot be matched with logical conditions using and , or , or not clauses at this point. Detecting Non-occurring Events Siddhi allows you to detect non-occurring events via multiple combinations of the key words specified above as shown in the table below. In the patterns listed, P* can be either a regular event pattern, an absent event pattern or a logical pattern. Pattern Detected Scenario not A for time period The non-occurrence of event A within time period after system start up. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, to indicate that the passenger might be in danger. not A for time period and B After system start up, event A does not occur within time period , but event B occurs at some point in time. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, and the passenger marked that he/she is in danger at some point in time. not A for time period 1 and not B for time period 2 After system start up, event A doess not occur within time period 1 , and event B also does not occur within time period 2 . e.g., Generating an alert if the driver of a taxi has not reached the destination within 30 minutes, and the passenger has not marked himself/herself to be in danger within that same time period. not A for time period or B After system start up, either event A does not occur within time period , or event B occurs at some point in time. e.g., Generating an alert if the taxi has not reached its destination within 30 minutes, or if the passenger has marked that he/she is in danger at some point in time. not A for time period 1 or not B for time period 2 After system start up, either event A does not occur within time period 1 , or event B occurs within time period 2 . e.g., Generating an alert to indicate that the driver is not on an expected route if the taxi has not reached destination A within 20 minutes, or reached destination B within 30 minutes. A \u2192 not B for time period Event B does not occur within time period after the occurrence of event A. e.g., Generating an alert if the taxi has reached its destination, but this was not followed by a payment record. P* \u2192 not A for time period and B After the occurrence of P*, event A does not occur within time period , and event B occurs at some point in time. P* \u2192 not A for time period 1 and not B for time period 2 After the occurrence of P*, event A does not occur within time period 1 , and event B does not occur within time period 2 . P* \u2192 not A for time period or B After the occurrence of P*, either event A does not occur within time period , or event B occurs at some point in time. P* \u2192 not A for time period 1 or not B for time period 2 After the occurrence of P*, either event A does not occur within time period 1 , or event B does not occur within time period 2 . not A for time period \u2192 B Event A does occur within time period after the system start up, but event B occurs after that time period has elapsed. not A for time period and B \u2192 P* Event A does not occur within time period , and event B occurs at some point in time. Then P* occurs after the time period has elapsed, and after B has occurred. not A for time period 1 and not B for time period 2 \u2192 P* After system start up, event A does not occur within time period 1 , and event B does not occur within time period 2 . However, P* occurs after both A and B. not A for time period or B \u2192 P* After system start up, event A does not occur within time period or event B occurs at some point in time. The P* occurs after time period has elapsed, or after B has occurred. not A for time period 1 or not B for time period 2 \u2192 P* After system start up, either event A does not occur within time period 1 , or event B does not occur within time period 2 . Then P* occurs after both time period 1 and time period 2 have elapsed. not A and B Event A does not occur before event B. A and not B Event B does not occur before event A. Example Following Siddhi App, sends the stop control action to the regulator when the key is removed from the hotel room. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream RoomKeyStream(deviceID long, roomNo int, action string); from every( e1=RegulatorStateChangeStream[ action == 'on' ] ) - e2=RoomKeyStream[ e1.roomNo == roomNo and action == 'removed' ] or e3=RegulatorStateChangeStream[ e1.roomNo == roomNo and action == 'off'] select e1.roomNo, ifThenElse( e2 is null, 'none', 'stop' ) as action having action != 'none' insert into RegulatorActionStream; This Siddhi Application generates an alert if we have switch off the regulator before the temperature reaches 12 degrees. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] - not TempStream[e1.roomNo == roomNo and temp 12] and e2=RegulatorStateChangeStream[action == 'off'] select e1.roomNo as roomNo insert into AlertStream; This Siddhi Application generates an alert if the temperature does not reduce to 12 degrees within 5 minutes of switching on the regulator. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] - not TempStream[e1.roomNo == roomNo and temp 12] for '5 min' select e1.roomNo as roomNo insert into AlertStream; Sequence Sequence is a state machine implementation that allows you to detect the sequence of event occurrences over time. Here all matching events need to arrive consecutively to match the sequence condition, and there cannot be any non-matching events arriving within a matching sequence of events. This can correlate events within a single stream or between multiple streams. Purpose This allows you to detect a specified event sequence over a specified time period. Syntax The syntax for a sequence query is as follows: from (every)? event reference = input stream [ filter condition ], event reference = input stream [ filter condition ], ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description , This represents the immediate next event i.e., when an event that matches the first condition arrives, the event that arrives immediately after it should match the second condition. event reference This allows you to add a reference to the the matching event so that it can be accessed later for further processing. (within time gap )? The within clause is optional. It defines the time duration within which all the matching events should occur. every every is an optional keyword. This defines whether the matching event should be triggered for every event that arrives at the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. Example This query generates an alert if the increase in the temperature between two consecutive temperature events exceeds one degree. from every e1=TempStream, e2=TempStream[e1.temp + 1 temp] select e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Counting Sequence Counting sequences allow you to match multiple events for the same matching condition. The number of events matched per condition can be limited via condition postfixes such as Counting Patterns , or by using the * , + , and ? operators. The matching events can also be retrieved using event indexes, similar to how it is done in Counting Patterns . Syntax Each matching condition in a sequence can contain a collection of events as shown below. from (every)? event reference = input stream [ filter condition ](+|*|?)?, event reference = input stream [ filter condition ](+|*|?)?, ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Postfix symbol Required/Optional Description + Optional This matches one or more events to the given condition. * Optional This matches zero or more events to the given condition. ? Optional This matches zero or one events to the given condition. Example This Siddhi application identifies temperature peeks. define stream TempStream(deviceID long, roomNo int, temp double); from every e1=TempStream, e2=TempStream[e1.temp = temp]+, e3=TempStream[e2[last].temp temp] select e1.temp as initialTemp, e2[last].temp as peakTemp insert into PeekTempStream; Logical Sequence Logical sequences identify logical relationships using and , or and not on consecutively arriving events. Syntax The syntax for a logical sequence is as follows: from (every)? (not)? event reference = input stream [ filter condition ] ((and|or) event reference = input stream [ filter condition ])? (within time gap )?, ... select event reference ([event index])?. attribute name , ... insert into output stream Keywords such as and , or , or not can be used to illustrate the logical relationship, similar to how it is done in Logical Patterns . Example This Siddhi application notifies the state when a regulator event is immediately followed by both temperature and humidity events. define stream TempStream(deviceID long, temp double); define stream HumidStream(deviceID long, humid double); define stream RegulatorStream(deviceID long, isOn bool); from every e1=RegulatorStream, e2=TempStream and e3=HumidStream select e2.temp, e3.humid insert into StateNotificationStream; Output rate limiting Output rate limiting allows queries to output events periodically based on a specified condition. Purpose This allows you to limit the output to avoid overloading the subsequent executions, and to remove unnecessary information. Syntax The syntax of an output rate limiting configuration is as follows: from input stream ... select attribute name , attribute name , ... output rate limiting configuration insert into output stream Siddhi supports three types of output rate limiting configurations as explained in the following table: Rate limiting configuration Syntax Description Based on time output event every time interval This outputs output event every time interval time interval. Based on number of events output event every event interval events This outputs output event for every event interval number of events. Snapshot based output snapshot every time interval This outputs all events in the window (or the last event if no window is defined in the query) for every given time interval time interval. Here the output event specifies the event(s) that should be returned as the output of the query. The possible values are as follows: * first : Only the first event processed by the query during the specified time interval/sliding window is emitted. * last : Only the last event processed by the query during the specified time interval/sliding window is emitted. * all : All the events processed by the query during the specified time interval/sliding window are emitted. When no output event is defined, all is used by default. Examples Returning events based on the number of events Here, events are emitted every time the specified number of events arrive. You can also specify whether to emit only the first event/last event, or all the events out of the events that arrived. In this example, the last temperature per sensor is emitted for every 10 events. from TempStreamselect select temp, deviceID group by deviceID output last every 10 events insert into LowRateTempStream; Returning events based on time Here events are emitted for every predefined time interval. You can also specify whether to emit only the first event, last event, or all events out of the events that arrived during the specified time interval. In this example, emits all temperature events every 10 seconds from TempStreamoutput output every 10 sec insert into LowRateTempStream; Returning a periodic snapshot of events This method works best with windows. When an input stream is connected to a window, snapshot rate limiting emits all the current events that have arrived and do not have corresponding expired events for every predefined time interval. If the input stream is not connected to a window, only the last current event for each predefined time interval is emitted. This query emits a snapshot of the events in a time window of 5 seconds every 1 second. from TempStream#window.time(5 sec) output snapshot every 1 sec insert into SnapshotTempStream; Partition Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. A partition can contain one or more queries and there can be multiple instances where the same queries and streams are replicated for each partition. Each partition is tagged with a partition key. Those partitions only process the events that match the corresponding partition key. Purpose Partitions allow you to process the events groups in isolation so that event processing can be performed using the same set of queries for each group. Partition key generation A partition key can be generated in the following two methods: Partition by value This is created by generating unique values using input stream attributes. Syntax partition with ( expression of stream name , expression of stream name , ... ) begin query query ... end; Example This query calculates the maximum temperature recorded within the last 10 events per deviceID . partition with ( deviceID of TempStream ) begin from TempStream#window.length(10) select roomNo, deviceID, max(temp) as maxTemp insert into DeviceTempStream; end; Partition by range This is created by mapping each partition key to a range condition of the input streams numerical attribute. Syntax partition with ( condition as partition key or condition as partition key or ... of stream name , ... ) begin query query ... end; Example This query calculates the average temperature for the last 10 minutes per office area. partition with ( roomNo = 1030 as 'serverRoom' or roomNo 1030 and roomNo = 330 as 'officeRoom' or roomNo 330 as 'lobby' of TempStream) begin from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp insert into AreaTempStream end; Inner Stream Queries inside a partition block can use inner streams to communicate with each other while preserving partition isolation. Inner streams are denoted by a \"#\" placed before the stream name, and these streams cannot be accessed outside a partition block. Purpose Inner streams allow you to connect queries within the partition block so that the output of a query can be used as an input only by another query within the same partition. Therefore, you do not need to repartition the streams if they are communicating within the partition. Example This partition calculates the average temperature of every 10 events for each sensor, and sends an output to the DeviceTempIncreasingStream stream if the consecutive average temperature values increase by more than 5 degrees. partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 < avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end; Purge Partition Based on the partition key used for the partition, multiple instances of streams and queries will be generated. When an extremely large number of unique partition keys are used there is a possibility of very high instances of streams and queries getting generated and eventually system going out of memory. In order to overcome this, users can define a purge interval to clean partitions that will not be used anymore. Purpose @purge allows you to clean the partition instances that will not be used anymore. Syntax The syntax of partition purge configuration is as follows: @purge(enable='true', interval=' purge interval ', idle.period=' idle period of partition instance ') partition with ( partition key of input stream ) begin from input stream ... select attribute name , attribute name , ... insert into output stream end; Partition purge configuration Description Purge interval The periodic time interval to purge the purgeable partition instances. Idle period of partition instance The period, a particular partition instance (for a given partition key) needs to be idle before it becomes purgeable. Examples Mark partition instances eligible for purging, if there are no events from a particular deviceID for 15 seconds, and periodically purge those partition instances every 1 second. @purge(enable='true', interval='1 sec', idle.period='15 sec') partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end; Table A table is a stored version of an stream or a table of events. Its schema is defined via the table definition that is similar to a stream definition. These events are by default stored in-memory , but Siddhi also provides store extensions to work with data/events stored in various data stores through the table abstraction. Purpose Tables allow Siddhi to work with stored events. By defining a schema for tables Siddhi enables them to be processed by queries using their defined attributes with the streaming data. You can also interactively query the state of the stored events in the table. Syntax The syntax for a new table definition is as follows: define table table name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are configured in a table definition: Parameter Description table name The name of the table defined. ( PascalCase is used for table name as a convention.) attribute name The schema of the table is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . Example The following defines a table named RoomTypeTable with roomNo and type attributes of data types int and string respectively. define table RoomTypeTable ( roomNo int, type string ); Primary Keys Tables can be configured with primary keys to avoid the duplication of data. Primary keys are configured by including the @PrimaryKey( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have only one @PrimaryKey annotation. The number of attributes supported differ based on the table implementations. When more than one attribute is used for the primary key, the uniqueness of the events stored in the table is determined based on the combination of values for those attributes. Examples This query creates an event table with the symbol attribute as the primary key. Therefore each entry in this table must have a unique value for symbol attribute. @PrimaryKey('symbol') define table StockTable (symbol string, price float, volume long); Indexes Indexes allow tables to be searched/modified much faster. Indexes are configured by including the @Index( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have 0-1 @Index annotations. Support for the @Index annotation and the number of attributes supported differ based on the table implementations. When more then one attribute is used for index, each one of them is used to index the table for fast access of the data. Indexes can be configured together with primary keys. Examples This query creates an indexed event table named RoomTypeTable with the roomNo attribute as the index key. @Index('roomNo') define table RoomTypeTable (roomNo int, type string); Store Store is a table that refers to data/events stored in data stores outside of Siddhi such as RDBMS, Cassandra, etc. Store is defined via the @store annotation, and the store schema is defined via a table definition associated with it. Purpose Store allows Siddhi to search, retrieve and manipulate data stored in external data stores through Siddhi queries. Syntax The syntax for a defining store and it's associated table definition is as follows: @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN') define table TableName (attribute1 Type1, attributeN TypeN); Example The following defines a RDBMS data store pointing to a MySQL database with name hotel hosted in loacalhost:3306 having a table RoomTypeTable with columns roomNo of INTEGER and type of VARCHAR(255) mapped to Siddhi data types int and string respectively. @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/hotel\", username=\"siddhi\", password=\"123\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table RoomTypeTable ( roomNo int, type string ); Supported Store Types The following is a list of currently supported store types: RDBMS (MySQL, Oracle, SQL Server, PostgreSQL, DB2, H2) MongoDB Operators on Table (and Store) The following operators can be performed on tables (and stores). Insert This allows events to be inserted into tables. This is similar to inserting events into streams. Warning If the table is defined with primary keys, and if you insert duplicate data, primary key constrain violations can occur. In such cases use the update or insert into operation. Syntax from input stream select attribute name , attribute name , ... insert into table Similar to streams, you need to use the current events , expired events or the all events keyword between insert and into keywords in order to insert only the specific output event types. For more information, see output event type Example This query inserts all the events from the TempStream stream to the TempTable table. from TempStream select * insert into TempTable; Join (Table) This allows a stream to retrieve information from a table in a streaming manner. Note Joins can also be performed with two streams , aggregation or against externally named windows . Syntax from input stream join table on condition select ( input stream | table ). attribute name , ( input stream | table ). attribute name , ... insert into output stream Note A table can only be joint with a stream. Two tables cannot be joint because there must be at least one active entity to trigger the join operation. Example This Siddhi App performs a join to retrieve the room type from RoomTypeTable table based on the room number, so that it can filter the events related to server-room s. define table RoomTypeTable (roomNo int, type string); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream join RoomTypeTable on RoomTypeTable.roomNo == TempStream.roomNo select deviceID, RoomTypeTable.type as roomType, type, temp having roomType == 'server-room' insert into ServerRoomTempStream; Supported join types Table join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the table. The output is generated only if there is a matching event in both the stream and the table. Left outer join The left outer join operation allows you to join a stream on left side with a table on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right table by having null values for the attributes of the right table. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a table on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left table. Delete To delete selected events that are stored in a table. Syntax from input stream select attribute name , attribute name , ... delete table (for output event type )? on condition The condition element specifies the basis on which events are selected to be deleted. When specifying the condition, table attributes should be referred to with the table name. To execute delete for specific output event types, use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see output event type Note Table attributes must be always referred to with the table name as follows: table name . attibute name Example In this example, the script deletes a record in the RoomTypeTable table if it has a value for the roomNo attribute that matches the value for the roomNumber attribute of an event in the DeleteStream stream. define table RoomTypeTable (roomNo int, type string); define stream DeleteStream (roomNumber int); from DeleteStream delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; Update This operator updates selected event attributes stored in a table based on a condition. Syntax from input stream select attribute name , attribute name , ... update table (for output event type )? set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition The condition element specifies the basis on which events are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. To execute an update for specific output event types use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see output event type . Note Table attributes must be always referred to with the table name as shown below: table name . attibute name . Example This Siddhi application updates the room occupancy in the RoomOccupancyTable table for each room number based on new arrivals and exits from the UpdateStream stream. define table RoomOccupancyTable (roomNo int, people int); define stream UpdateStream (roomNumber int, arrival int, exit int); from UpdateStream select * update RoomOccupancyTable set RoomOccupancyTable.people = RoomOccupancyTable.people + arrival - exit on RoomOccupancyTable.roomNo == roomNumber; Update or Insert This allows you update if the event attributes already exist in the table based on a condition, or else insert the entry as a new attribute. Syntax from input stream select attribute name , attribute name , ... update or insert into table (for output event type )? set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which events are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note When the attribute to the right is a table attribute, the operations supported differ based on the database type. To execute update upon specific output event types use the current events , expired events or the all events keyword with for as shown in the syntax. To understand more see output event type . Note Table attributes should be always referred to with the table name as table name . attibute name . Example The following query update for events in the UpdateTable event table that have room numbers that match the same in the UpdateStream stream. When such events are found in the event table, they are updated. When a room number available in the stream is not found in the event table, it is inserted from the stream. define table RoomAssigneeTable (roomNo int, type string, assignee string); define stream RoomAssigneeStream (roomNumber int, type string, assignee string); from RoomAssigneeStream select roomNumber as roomNo, type, assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo; In This allows the stream to check whether the expected value exists in the table as a part of a conditional operation. Syntax from input stream [ condition in table ] select attribute name , attribute name , ... insert into output stream The condition element specifies the basis on which events are selected to be compared. When constructing the condition , the table attribute must be always referred to with the table name as shown below: table . attibute name . Example This Siddhi application filters only room numbers that are listed in the ServerRoomTable table. define table ServerRoomTable (roomNo int); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream[ServerRoomTable.roomNo == roomNo in ServerRoomTable] insert into ServerRoomTempStream; Named Aggregation Named aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition . Purpose Named aggregation allows you to retrieve the aggregate values for different time durations. That is, it allows you to obtain aggregates such as sum , count , avg , min , max , count and distinctCount of stream attributes for durations such as sec , min , hour , etc. This is of considerable importance in many Analytics scenarios because aggregate values are often needed for several time periods. Furthermore, this ensures that the aggregations are not lost due to unexpected system failures because aggregates can be stored in different persistence stores . Syntax @store(type=\" store type \", ...) @purge(enable=\" true or false \",interval= purging interval ,@retentionPeriod( granularity = retention period , ...) ) define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; The above syntax includes the following: Item Description @store This annotation is used to refer to the data store where the calculated aggregate results are stored. This annotation is optional. When no annotation is provided, the data is stored in the in-memory store. @purge This annotation is used to configure purging in aggregation granularities. If this annotation is not provided, the default purging mentioned above is applied. If you want to disable automatic data purging, you can use this annotation as follows: '@purge(enable=false) /You should disable data purging if the aggregation query in included in the Siddhi application for read-only purposes. @retentionPeriod This annotation is used to specify the length of time the data needs to be retained when carrying out data purging. If this annotation is not provided, the default retention period is applied. aggregator name This specifies a unique name for the aggregation so that it can be referred when accessing aggregate results. input stream The stream that feeds the aggregation. Note! this stream should be already defined. group by attribute name The group by clause is optional. If it is included in a Siddhi application, aggregate values are calculated per each group by attribute. If it is not used, all the events are aggregated together. by timestamp attribute This clause is optional. This defines the attribute that should be used as the timestamp. If this clause is not used, the event time is used by default. The timestamp could be given as either a string or a long value. If it is a long value, the unix timestamp in milliseconds is expected (e.g. 1496289950000 ). If it is a string value, the supported formats are yyyy - MM - dd HH : mm : ss (if time is in GMT) and yyyy - MM - dd HH : mm : ss Z (if time is not in GMT), here the ISO 8601 UTC offset must be provided for Z . (e.g., +05:30 , -11:00 ). time periods Time periods can be specified as a range where the minimum and the maximum value are separated by three dots, or as comma-separated values. e.g., A range can be specified as sec...year where aggregation is done per second, minute, hour, day, month and year. Comma-separated values can be specified as min, hour. Skipping time durations (e.g., min, day where the hour duration is skipped) when specifying comma-separated values is supported only from v4.1.1 onwards Aggregation's granularity data holders are automatically purged every 15 minutes. When carrying out data purging, the retention period you have specified for each granularity in the named aggregation query is taken into account. The retention period defined for a granularity needs to be greater than or equal to its minimum retention period as specified in the table below. If no valid retention period is defined for a granularity, the default retention period (as specified in the table below) is applied. Granularity Default retention Minimum retention second 120 seconds 120 seconds minute 24 hours 120 minutes hour 30 days 25 hours day 1 year 32 days month All 13 month year All none Note Aggregation is carried out at calendar start times for each granularity with the GMT timezone Note The same aggregation can be defined in multiple Siddhi apps for joining, however, only one siddhi app should carry out the processing (i.e. the aggregation input stream should only feed events to one aggregation definition). Example This Siddhi Application defines an aggregation named TradeAggregation to calculate the average and sum for the price attribute of events arriving at the TradeStream stream. These aggregates are calculated per every time granularity in the second-year range. define stream TradeStream (symbol string, price double, volume long, timestamp long); @purge(enable='true', interval='10 sec',@retentionPeriod(sec='120 sec',min='24 hours',hours='30 days',days='1 year',months='all',years='all')) define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; Distributed Aggregation Distributed Aggregation allows you to partially process aggregations in different shards. This allows Siddhi app in one shard to be responsible only for processing a part of the aggregation. However for this, all aggregations must be based on a common physical database(@store). Syntax @store(type=\" store type \", ...) @PartitionById define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; Following table includes the annotation to be used to enable distributed aggregation, Item Description @PartitionById If the annotation is given, then the distributed aggregation is enabled. Further this can be disabled by using enable element, @PartitionById(enable='false') . Further, following system properties are also available, System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yesio false Note ShardIds should not be changed after the first configuration in order to keep data consistency. Join (Aggregation) This allows a stream to retrieve calculated aggregate values from the aggregation. Note A join can also be performed with two streams , with a table and a stream, or with a stream against externally named windows . Syntax A join with aggregation is similer to the join with table , but with additional within and per clauses. from input stream join aggrigation on join condition within time range per time granularity select attribute name , attribute name , ... insert into output stream ; Apart from constructs of table join this includes the following. Please note that the 'on' condition is optional : Item Description within time range This allows you to specify the time interval for which the aggregate values need to be retrieved. This can be specified by providing the start and end time separated by a comma as string or long values, or by using the wildcard string specifying the data range. For details refer examples. per time granularity This specifies the time granularity by which the aggregate values must be grouped and returned. e.g., If you specify days , the retrieved aggregate values are grouped for each day within the selected time interval. within and per clauses also accept attribute values from the stream. The timestamp of the aggregations can be accessed through the AGG_TIMESTAMP attribute. Example Following aggregation definition will be used for the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select AGG_TIMESTAMP, symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) define stream StockStream (symbol string, value int); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; This query retrieves hourly aggregations within the day 2014-02-15 . define stream StockStream (symbol string, value int); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within \"2014-02-15 **:**:** +05:30\" per \"hours\" select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; This query retrieves all aggregations per perValue stream attribute within the time period between timestamps 1496200000000 and 1596434876000 . define stream StockStream (symbol string, value int, perValue string); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within 1496200000000L, 1596434876000L per S.perValue select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; Supported join types Aggregation join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the aggregation. The output is generated only if there is a matching event in the stream and the aggregation. Left outer join The left outer join operation allows you to join a stream on left side with a aggregation on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right aggregation by having null values for the attributes of the right aggregation. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a aggregation on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left aggregation. Named Window A named window is a window that can be shared across multiple queries. Events can be inserted to a named window from one or more queries and it can produce output events based on the named window type. Syntax The syntax for a named window is as follows: define window window name ( attribute name attribute type , attribute name attribute type , ... ) window type ( parameter , parameter , \u2026) output event type ; The following parameters are configured in a table definition: Parameter Description window name The name of the window defined. ( PascalCase is used for window names as a convention.) attribute name The schema of the window is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . window type ( parameter , ...) The window type associated with the window and its parameters. output output event type This is optional. Keywords such as current events , expired events and all events (the default) can be used to specify when the window output should be exposed. For more information, see output event type . Examples Returning all output when events arrive and when events expire from the window. In this query, the output event type is not specified. Therefore, it returns both current and expired events as the output. define window SensorWindow (name string, value float, roomNo int, deviceID string) timeBatch(1 second); + Returning an output only when events expire from the window. In this query, the output event type of the window is `expired events`. Therefore, it only returns the events that have expired from the window as the output. define window SensorWindow (name string, value float, roomNo int, deviceID string) timeBatch(1 second) output expired events; Operators on Named Windows The following operators can be performed on named windows. Insert This allows events to be inserted into windows. This is similar to inserting events into streams. Syntax from input stream select attribute name , attribute name , ... insert into window To insert only events of a specific output event type, add the current events , expired events or the all events keyword between insert and into keywords (similar to how it is done for streams). For more information, see output event type . Example This query inserts all events from the TempStream stream to the OneMinTempWindow window. define stream TempStream(tempId string, temp double); define window OneMinTempWindow(tempId string, temp double) time(1 min); from TempStream select * insert into OneMinTempWindow; Join (Window) To allow a stream to retrieve information from a window based on a condition. Note A join can also be performed with two streams , aggregation or with tables tables . Syntax from input stream join window on condition select ( input stream | window ). attribute name , ( input stream | window ). attribute name , ... insert into output stream Example This Siddhi Application performs a join count the number of temperature events having more then 40 degrees within the last 2 minutes. define window TwoMinTempWindow (roomNo int, temp double) time(2 min); define stream CheckStream (requestId string); from CheckStream as C join TwoMinTempWindow as T on T.temp 40 select requestId, count(T.temp) as count insert into HighTempCountStream; Supported join types Window join supports following operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join two windows or a stream with a window. The output is generated only if there is a matching event in both stream/window. Left outer join The left outer join operation allows you to join two windows or a stream with a window to be merged based on a condition. Here, it returns all the events of left stream/window even if there are no matching events in the right stream/window by having null values for the attributes of the right stream/window. Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join two windows or a stream with a window. It returns all the events of the right stream/window even if there are no matching events in the left stream/window. Full outer join The full outer join combines the results of left outer join and right outer join . full outer join is used as the keyword to join two windows or a stream with a window. Here, output event are generated for each incoming event even if there are no matching events in the other stream/window. From A window can be an input to a query, similar to streams. Note !!! When window is used as an input to a query, another window cannot be applied on top of this. Syntax from window select attribute name , attribute name , ... insert into output stream Example This Siddhi Application calculates the maximum temperature within the last 5 minutes. define window FiveMinTempWindow (roomNo int, temp double) time(5 min); from FiveMinTempWindow select max(temp) as maxValue, roomNo insert into MaxSensorReadingStream; Trigger Triggers allow events to be periodically generated. Trigger definition can be used to define a trigger. A trigger also works like a stream with a predefined schema. Purpose For some use cases the system should be able to periodically generate events based on a specified time interval to perform some periodic executions. A trigger can be performed for a 'start' operation, for a given time interval , or for a given ' cron expression ' . Syntax The syntax for a trigger definition is as follows. define trigger trigger name at ('start'| every time interval | ' cron expression '); Similar to streams, triggers can be used as inputs. They adhere to the following stream definition and produce the triggered_time attribute of the long type. define stream trigger name (triggered_time long); The following types of triggeres are currently supported: Trigger type Description 'start' An event is triggered when Siddhi is started. every time interval An event is triggered periodically at the given time interval. ' cron expression ' An event is triggered periodically based on the given cron expression. For configuration details, see quartz-scheduler . Examples Triggering events regularly at specific time intervals The following query triggers events every 5 minutes. define trigger FiveMinTriggerStream at every 5 min; Triggering events at a specific time on specified days The following query triggers an event at 10.15 AM on every weekdays. define trigger FiveMinTriggerStream at '0 15 10 ? * MON-FRI'; Script Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Function definitions can be used to define these scripts. Function parameters are passed into the function logic as Object[] and with the name data . Purpose Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Syntax The syntax for a Script definition is as follows. define function function name [ language name ] return return type { operation of the function }; The following parameters are configured when defining a script. Parameter Description function name The name of the function ( camelCase is used for the function name) as a convention. language name The name of the programming language used to define the script, such as javascript , r and scala . return type The attribute type of the function\u2019s return. This can be int , long , float , double , string , bool or object . Here the function implementer should be responsible for returning the output attribute on the defined return type for proper functionality. operation of the function Here, the execution logic of the function is added. This logic should be written in the language specified under the language name , and it should return the output in the data type specified via the return type parameter. Examples This query performs concatenation using JavaScript, and returns the output as a string. define function concatFn[javascript] return string { var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var responce = str1 + str2 + str3; return responce; }; define stream TempStream(deviceID long, roomNo int, temp double); from TempStream select concatFn(roomNo,'-',deviceID) as id, temp insert into DeviceTempStream; Store Query Siddhi store queries are a set of on-demand queries that can be used to perform operations on Siddhi tables, windows, and aggregators. Purpose Store queries allow you to execute the following operations on Siddhi tables, windows, and aggregators without the intervention of streams. Queries supported for tables: SELECT INSERT DELETE UPDATE UPDATE OR INSERT Queries supported for windows and aggregators: SELECT This is be done by submitting the store query to the Siddhi application runtime using its query() method. In order to execute store queries, the Siddhi application of the Siddhi application runtime you are using, should have a store defined, which contains the table that needs to be queried. Example If you need to query the table named RoomTypeTable the it should have been defined in the Siddhi application. In order to execute a store query on RoomTypeTable , you need to submit the store query using query() method of SiddhiAppRuntime instance as below. siddhiAppRuntime.query( store query ); (Table/Window) Select The SELECT store query retrieves records from the specified table or window, based on the given condition. Syntax from table/window on condition ? select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example This query retrieves room numbers and types of the rooms starting from room no 10. from roomTypeTable on roomNo = 10; select roomNo, type (Aggregation) Select The SELECT store query retrieves records from the specified aggregation, based on the given condition, time range, and granularity. Syntax from aggregation on condition ? within time range per time granularity select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example Following aggregation definition will be used for the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) from TradeAggregation within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select symbol, total, avgPrice ; This query retrieves hourly aggregations of \"FB\" symbol within the day 2014-02-15 . from TradeAggregation on symbol == \"FB\" within \"2014-02-15 **:**:** +05:30\" per \"hours\" select symbol, total, avgPrice; Insert This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax select attribute name , attribute name , ... insert into table ; Example This store query inserts a new record to the table RoomOccupancyTable , with the specified attribute values. select 10 as roomNo, 2 as people insert into RoomOccupancyTable Delete The DELETE store query deletes selected records from a specified table. Syntax select ? delete table on conditional expresssion The condition element specifies the basis on which records are selected to be deleted. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example In this example, query deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection which has 10 as the actual value. select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; delete RoomTypeTable on RoomTypeTable.roomNo == 10; Update The UPDATE store query updates selected attributes stored in a specific table, based on a given condition. Syntax select attribute name , attribute name , ...? update table set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition The condition element specifies the basis on which records are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query updates the room occupancy by increasing the value of people by 1, in the RoomOccupancyTable table for each room number greater than 10. select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber; update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + 1 on RoomTypeTable.roomNo == 10; Update or Insert This allows you to update selected attributes if a record that meets the given conditions already exists in the specified table. If a matching record does not exist, the entry is inserted as a new record. Syntax select attribute name , attribute name , ... update or insert into table set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which records are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query tries to update the records in the RoomAssigneeTable table that have room numbers that match the same in the selection. If such records are not found, it inserts a new record based on the values provided in the selection. select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo; Extensions Siddhi supports an extension architecture to enhance its functionality by incorporating other libraries in a seamless manner. Purpose Extensions are supported because, Siddhi core cannot have all the functionality that's needed for all the use cases, mostly use cases require different type of functionality, and for some cases there can be gaps and you need to write the functionality by yourself. All extensions have a namespace. This is used to identify the relevant extensions together, and to let you specifically call the extension. Syntax Extensions follow the following syntax; namespace : function name ( parameter , parameter , ... ) The following parameters are configured when referring a script function. Parameter Description namespace Allows Siddhi to identify the extension without conflict function name The name of the function referred. parameter The function input parameter for function execution. Extension Types Siddhi supports following extension types: Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute. This can be used to manipulate existing event attributes to generate new attributes like any Function operation. This is implemented by extending io.siddhi.core.executor.function.FunctionExecutor . Example : math:sin(x) Here, the sin function of math extension returns the sin value for the x parameter. Aggregate Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute with aggregated results. This can be used in conjunction with a window in order to find the aggregated results based on the given window like any Aggregate Function operation. This is implemented by extending io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor . Example : custom:std(x) Here, the std aggregate function of custom extension returns the standard deviation of the x value based on its assigned window query. Window This allows events to be collected, generated, dropped and expired anytime without altering the event format based on the given input parameters, similar to any other Window operator. This is implemented by extending io.siddhi.core.query.processor.stream.window.WindowProcessor . Example : custom:unique(key) Here, the unique window of the custom extension retains one event for each unique key parameter. Stream Function This allows events to be generated or dropped only during event arrival and altered by adding one or more attributes to it. This is implemented by extending io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor . Example : custom:pol2cart(theta,rho) Here, the pol2cart function of the custom extension returns all the events by calculating the cartesian coordinates x y and adding them as new attributes to the events. Stream Processor This allows events to be collected, generated, dropped and expired anytime by altering the event format by adding one or more attributes to it based on the given input parameters. Implemented by extending io.siddhi.core.query.processor.stream.StreamProcessor . Example : custom:perMinResults( parameter , parameter , ...) Here, the perMinResults function of the custom extension returns all events by adding one or more attributes to the events based on the conversion logic. Altered events are output every minute regardless of event arrivals. Sink Sinks provide a way to publish Siddhi events to external systems in the preferred data format. Sinks publish events from the streams via multiple transports to external endpoints in various data formats. Implemented by extending io.siddhi.core.stream.output.sink.Sink . Example : @sink(type='sink_type', static_option_key1='static_option_value1') To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as above Source Source allows Siddhi to consume events from external systems , and map the events to adhere to the associated stream. Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. Implemented by extending io.siddhi.core.stream.input.source.Source . Example : @source(type='source_type', static.option.key1='static_option_value1') To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as above Store You can use Store extension type to work with data/events stored in various data stores through the table abstraction . You can find more information about these extension types under the heading 'Extension types' in this document. Implemented by extending io.siddhi.core.table.record.AbstractRecordTable . Script Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Implemented by extending io.siddhi.core.function.Script . Source Mapper Each @source configuration has a mapping denoted by the @map annotation that converts the incoming messages format to Siddhi events .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SourceMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Sink Mapper Each @sink configuration has a mapping denoted by the @map annotation that converts the outgoing Siddhi events to configured messages format .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SinkMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Example A window extension created with namespace foo and function name unique can be referred as follows: from StockExchangeStream[price = 20]#window.foo:unique(symbol) select symbol, price insert into StockQuote Available Extensions Siddhi currently has several pre written extensions that are available here We value your contribution on improving Siddhi and its extensions further. Writing Custom Extensions Custom extensions can be written in order to cater use case specific logic that are not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension types and the related maven archetypes are given below. You can use these archetypes to generate Maven projects for each extension type. Follow the procedure for the required archetype, based on your project: Note When using the generated archetype please make sure you complete the @Extension annotation with proper values. This annotation will be used to identify and document the extension, hence your extension will not work without @Extension annotation. siddhi-execution Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Extension Types . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DgroupId=io.siddhi.extension.execution -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfFunction Name of the custom function to be created Y - _nameSpaceOfFunction Namespace of the function, used to grouped similar custom functions Y - groupIdPostfix Namespace of the function is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-execution- classNameOfAggregateFunction Class name of the Aggregate Function N $ classNameOfFunction Class name of the Function N $ classNameOfStreamFunction Class name of the Stream Function N $ classNameOfStreamProcessor Class name of the Stream Processor N $ classNameOfWindow Class name of the Window N $ To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-io Siddhi-io provides following extension types: Sink Source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: * The Source extension type gets inputs to your Siddhi application. * The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DgroupId=io.siddhi.extension.io -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _IOType Type of IO for which Siddhi-io extension is written Y - groupIdPostfix Type of the IO is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-io- classNameOfSink Class name of the Sink N classNameOfSource Class name of the Source N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-map Siddhi-map provides following extension types, Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints (such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DgroupId=io.siddhi.extension.map -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _mapType Type of Mapper for which Siddhi-map extension is written Y - groupIdPostfix Type of the Map is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-map- classNameOfSinkMapper Class name of the Sink Mapper N classNameOfSourceMapper Class name of the Source Mapper N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-script Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Extension Types . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DgroupId=io.siddhi.extension.script -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfScript Name of Custom Script for which Siddhi-script extension is written Y - groupIdPostfix Name of the Script is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-script- classNameOfScript Class name of the Script N Eval To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-store Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Extension Types . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DgroupId=io.siddhi.extension.store -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _storeType Type of Store for which Siddhi-store extension is written Y - groupIdPostfix Type of the Store is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-store- className Class name of the Store N To confirm that all property values are correct, type Y in the console. If not, press N . Configuring and Monitoring Siddhi Applications Multi-threading and Asynchronous Processing When @Async annotation is added to the Streams it enable the Streams to introduce asynchronous and multi-threading behaviour. @Async(buffer.size='256', workers='2', batch.size.max='5') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following elements are configured with this annotation. Annotation Description Default Value buffer.size The size of the event buffer that will be used to handover the execution to other threads. - workers Number of worker threads that will be be used to process the buffered events. 1 batch.size.max The maximum number of events that will be processed together by a worker thread at a given time. buffer.size Statistics Use @app:statistics app level annotation to evaluate the performance of an application, you can enable the statistics of a Siddhi application to be published. This is done via the @app:statistics annotation that can be added to a Siddhi application as shown in the following example. @app:statistics(reporter = 'console') The following elements are configured with this annotation. Annotation Description Default Value reporter The interface in which statistics for the Siddhi application are published. Possible values are as follows: console jmx console interval The time interval (in seconds) at which the statistics for the Siddhi application are reported. 60 include If this parameter is added, only the types of metrics you specify are included in the reporting. The required metric types can be specified as a comma-separated list. It is also possible to use wild cards All ( . ) The metrics are reported in the following format. io.siddhi.SiddhiApps. SiddhiAppName .Siddhi. Component Type . Component Name . Metrics name The following table lists the types of metrics supported for different Siddhi application component types. Component Type Metrics Type Stream Throughput The size of the buffer if parallel processing is enabled via the @async annotation. Trigger Throughput (Trigger and Stream) Source Throughput Sink Throughput Mapper Latency Input/output throughput Table Memory Throughput (For all operations) Throughput (For all operations) Query Memory Latency Window Throughput (For all operations) Latency (For all operation) Partition Throughput (For all operations) Latency (For all operation) e.g., the following is a Siddhi application that includes the @app annotation to report performance statistics. @App:name('TestMetrics') @App:Statistics(reporter = 'console') define stream TestStream (message string); @info(name='logQuery') from TestSream#log(\"Message:\") insert into TempSream; Statistics are reported for this Siddhi application as shown in the extract below. Click to view the extract 11/26/17 8:01:20 PM ============================================================ -- Gauges ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.memory value = 5760 io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.size value = 0 -- Meters ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Sources.TestStream.http.throughput count = 0 mean rate = 0.00 events/second 1-minute rate = 0.00 events/second 5-minute rate = 0.00 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TempSream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second -- Timers ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.latency count = 2 mean rate = 0.11 calls/second 1-minute rate = 0.34 calls/second 5-minute rate = 0.39 calls/second 15-minute rate = 0.40 calls/second min = 0.61 milliseconds max = 1.08 milliseconds mean = 0.84 milliseconds stddev = 0.23 milliseconds median = 0.61 milliseconds 75% < = 1.08 milliseconds 95% < = 1.08 milliseconds 98% < = 1.08 milliseconds 99% < = 1.08 milliseconds 99.9% < = 1.08 milliseconds Event Playback When @app:playback annotation is added to the app, the timestamp of the event (specified via an attribute) is treated as the current time. This results in events being processed faster. The following elements are configured with this annotation. Annotation Description idle.time If no events are received during a time interval specified (in milliseconds) via this element, the Siddhi system time is incremented by a number of seconds specified via the increment element. increment The number of seconds by which the Siddhi system time must be incremented if no events are received during the time interval specified via the idle.time element. e.g., In the following example, the Siddhi system time is incremented by two seconds if no events arrive for a time interval of 100 milliseconds. @app:playback(idle.time = '100 millisecond', increment = '2 sec')","title":"Query Guide"},{"location":"docs/query-guide/#siddhi-50-streaming-sql-guide","text":"","title":"Siddhi 5.0 Streaming SQL Guide"},{"location":"docs/query-guide/#introduction","text":"Siddhi Streaming SQL is designed to process streams of events. It can be used to implement streaming data integration, streaming analytics, rule based and adaptive decision making use cases. It is an evolution of Complex Event Processing (CEP) and Stream Processing systems, hence it can also be used to process stateful computations, detecting of complex event patterns, and sending notifications in real-time. Siddhi Streaming SQL uses SQL like syntax, and annotations to consume events from diverse event sources with various data formats, process then using stateful and stateless operators and send outputs to multiple endpoints according to their accepted event formats. It also supports exposing rule based and adaptive decision making as service endpoints such that external programs and systems can synchronously get decision support form Siddhi. The following sections explains how to write processing logic using Siddhi Streaming SQL.","title":"Introduction"},{"location":"docs/query-guide/#siddhi-application","text":"The processing logic for your program can be written using the Streaming SQL and put together as a single file with .siddhi extension. This file is called as the Siddhi Application or the SiddhiApp . SiddhiApps are named by adding @app:name(' name ') annotation on the top of the SiddhiApp file. When the annotation is not added Siddhi assigns a random UUID as the name of the SiddhiApp. Purpose SiddhiApp provides an isolated execution environment for your processing logic that allows you to deploy and execute processing logic independent of other SiddhiApp in the system. Therefore it's always recommended to have a processing logic related to single use case in a single SiddhiApp. This will help you to group processing logic and easily manage addition and removal of various use cases. The following diagram depicts some of the key Siddhi Streaming SQL elements of Siddhi Application and how event flows through the elements. Below table provides brief description of a few key elements in the Siddhi Streaming SQL Language. Elements Description Stream A logical series of events ordered in time with a uniquely identifiable name, and a defined set of typed attributes defining its schema. Event An event is a single event object associated with a stream. All events of a stream contains a timestamp and an identical set of typed attributes based on the schema of the stream they belong to. Table A structured representation of data stored with a defined schema. Stored data can be backed by In-Memory , or external data stores such as RDBMS , MongoDB , etc. The tables can be accessed and manipulated at runtime. Named Window A structured representation of data stored with a defined schema and eviction policy. Window data is stored In-Memory and automatically cleared by the named window constrain. Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Named Aggregation A structured representation of data that's incrementally aggregated and stored with a defined schema and aggregation granularity such as seconds, minutes, hours, etc. Aggregation data is stored both In-Memory and in external data stores such as RDBMS . Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Query A logical construct that processes events in streaming manner by by consuming data from one or more streams, tables, windows and aggregations, and publishes output events into a stream, table or a window. Source A construct that consumes data from external sources (such as TCP , Kafka , HTTP , etc) with various event formats such as XML , JSON , binary , etc, convert then to Siddhi events, and passes into streams for processing. Sink A construct that consumes events arriving at a stream, maps them to a predefined data format (such as XML , JSON , binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Input Handler A mechanism to programmatically inject events into streams. Stream/Query Callback A mechanism to programmatically consume output events from streams or queries. Partition A logical container that isolates the processing of queries based on the partition keys derived from the events. Inner Stream A positionable stream that connects portioned queries with each other within the partition. Grammar SiddhiApp is a collection of Siddhi Streaming SQL elements composed together as a script. Here each Siddhi element must be separated by a semicolon ; . Hight level syntax of SiddhiApp is as follows. siddhi app : app annotation * ( stream definition | table definition | ... ) + ( query | partition ) + ; Example Siddhi Application with name Temperature-Analytics defined with a stream named TempStream and a query named 5minAvgQuery . @app:name('Temperature-Analytics') define stream TempStream (deviceID long, roomNo int, temp double); @name('5minAvgQuery') from TempStream#window.time(5 min) select roomNo, avg(temp) as avgTemp group by roomNo insert into OutputStream;","title":"Siddhi Application"},{"location":"docs/query-guide/#stream","text":"A stream is a logical series of events ordered in time. Its schema is defined via the stream definition . A stream definition contains the stream name and a set of attributes with specific types and uniquely identifiable names within the stream. All events associated to the stream will have the same schema (i.e., have the same attributes in the same order). Purpose Stream groups common types of events together with a schema. This helps in various ways such as, processing all events together in queries and performing data format transformations together when they are consumed and published via sources and sinks. Syntax The syntax for defining a new stream is as follows. define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure a stream definition. Parameter Description stream name The name of the stream created. (It is recommended to define a stream name in PascalCase .) attribute name Uniquely identifiable name of the stream attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer stream and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . To make the stream process events in multi-threading and asynchronous way use the @Async annotation as shown in Multi-threading and Asynchronous Processing configuration section. Example define stream TempStream (deviceID long, roomNo int, temp double); The above creates a stream with name TempStream having the following attributes. deviceID of type long roomNo of type int temp of type double","title":"Stream"},{"location":"docs/query-guide/#source","text":"Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. A source configuration allows to define a mapping in order to convert each incoming event from its native data format to a Siddhi event. When customizations to such mappings are not provided, Siddhi assumes that the arriving event adheres to the predefined format based on the stream definition and the configured message mapping type. Purpose Source provides a way to consume events from external systems and convert them to be processed by the associated stream. Syntax To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as follows: @source(type=' source type ', static.key =' value ', static.key =' value ', @map(type=' map type ', static.key =' value ', static.key =' value ', @attributes( attribute1 =' attribute mapping ', attributeN =' attribute mapping ') ) ) define stream stream name ( attribute1 type , attributeN type ); This syntax includes the following annotations. Source The type parameter of @source annotation defines the source type that receives events. The other parameters of @source annotation depends upon the selected source type, and here some of its parameters can be optional. For detailed information about the supported parameters see the documentation of the relevant source. The following is the list of source types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to consume events from other SiddhiApps running on the same JVM. HTTP Expose an HTTP service to consume messages. Kafka Subscribe to Kafka topic to consume events. TCP Expose a TCP service to consume messages. Email Consume emails via POP3 and IMAP protocols. JMS Subscribe to JMS topic or queue to consume events. File Reads files by tailing or as a whole to extract events out of them. CDC Perform change data capture on databases. Prometheus Consume data from Prometheus agent. In-memory is the only source inbuilt in Siddhi, and all other source types are implemented as extensions.","title":"Source"},{"location":"docs/query-guide/#source-mapper","text":"Each @source configuration can have a mapping denoted by the @map annotation that defines how to convert the incoming event format to Siddhi events. The type parameter of the @map defines the map type to be used in converting the incoming events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional. For detailed information about the parameters see the documentation of the relevant mapper. Map Attributes @attributes is an optional annotation used with @map to define custom mapping. When @attributes is not provided, each mapper assumes that the incoming events adheres to its own default message format and attempt to convert the events from that format. By adding the @attributes annotation, users can selectively extract data from the incoming message and assign them to the attributes. There are two ways to configure @attributes . Define attribute names as keys, and mapping configurations as values: @attributes( attribute1 =' mapping ', attributeN =' mapping ') Define the mapping configurations in the same order as the attributes defined in stream definition: @attributes( ' mapping for attribute1 ', ' mapping for attributeN ') Supported Source Mapping Types The following is the list of source mapping types supported by Siddhi: Source mapping type Description PassThrough Omits data conversion on Siddhi events. JSON Converts JSON messages to Siddhi events. XML Converts XML messages to Siddhi events. TEXT Converts plain text messages to Siddhi events. Avro Converts Avro events to Siddhi events. Binary Converts Siddhi specific binary events to Siddhi events. Key Value Converts key-value HashMaps to Siddhi events. CSV Converts CSV like delimiter separated events to Siddhi events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the consumed Siddhi events directly to the streams without any data conversion. PassThrough is the only source mapper inbuilt in Siddhi, and all other source mappers are implemented as extensions. Example 1 Receive JSON messages by exposing an HTTP service, and direct them to InputStream stream for processing. Here the HTTP service will be secured with basic authentication, receives events on all network interfaces on port 8080 and context /foo . The service expects the JSON messages to be on the default data format that's supported by the JSON mapper as follows. { \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } The configuration of the HTTP source and JSON source mapper to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json')) define stream InputStream (name string, age int, country string); Example 2 Receive JSON messages by exposing an HTTP service, and direct them to StockStream stream for processing. Here the incoming JSON , as given bellow, do not adhere to the default data format that's supported by the JSON mapper. { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"FB\" }, \"price\":55.6 } } } The configuration of the HTTP source and the custom JSON source mapping to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); The same can also be configured by omitting the attribute names as bellow. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(\"stock.company.symbol\", \"stock.price\", \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long);","title":"Source Mapper"},{"location":"docs/query-guide/#sink","text":"Sinks consumes events from streams and publish them via multiple transports to external endpoints in various data formats. A sink configuration allows users to define a mapping to convert the Siddhi events in to the required output data format (such as JSON , TEXT , XML , etc.) and publish the events to the configured endpoints. When customizations to such mappings are not provided, Siddhi converts events to the predefined event format based on the stream definition and the configured message mapper type before publishing the events. Purpose Sink provides a way to publish Siddhi events of a stream to external systems by converting events to their supported format. Syntax To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as follows: @sink(type=' sink type ', static.key =' value ', dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) ) define stream stream name ( attribute1 type , attributeN type ); Dynamic Properties The sink and sink mapper properties that are categorized as dynamic have the ability to absorb attribute values dynamically from the Siddhi events of their associated streams. This can be configured by enclosing the relevant attribute names in double curly braces as {{...}} , and using it within the property values. Some valid dynamic properties values are: '{{attribute1}}' 'This is {{attribute1}}' {{attribute1}} {{attributeN}} Here the attribute names in the double curly braces will be replaced with the values from the events before they are published. This syntax includes the following annotations. Sink The type parameter of the @sink annotation defines the sink type that publishes the events. The other parameters of the @sink annotation depends upon the selected sink type, and here some of its parameters can be optional and/or dynamic. For detailed information about the supported parameters see documentation of the relevant sink. The following is a list of sink types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to publish events to other SiddhiApps running on the same JVM. Log Logs the events appearing on the streams. HTTP Publish events to an HTTP endpoint. Kafka Publish events to Kafka topic. TCP Publish events to a TCP service. Email Send emails via SMTP protocols. JMS Publish events to JMS topics or queues. File Writes events to files. Prometheus Expose data through Prometheus agent.","title":"Sink"},{"location":"docs/query-guide/#distributed-sink","text":"Distributed Sinks publish events from a defined stream to multiple endpoints using load balancing or partitioning strategies. Any sink can be used as a distributed sink. A distributed sink configuration allows users to define a common mapping to convert and send the Siddhi events for all its destination endpoints. Purpose Distributed sink provides a way to publish Siddhi events to multiple endpoints in the configured event format. Syntax To configure distributed sink add the sink configuration to a stream definition by adding the @sink annotation and add the configuration parameters that are common of all the destination endpoints inside it, along with the common parameters also add the @distribution annotation specifying the distribution strategy (i.e. roundRobin or partitioned ) and @destination annotations providing each endpoint specific configurations. The distributed sink syntax is as follows: RoundRobin Distributed Sink Publishes events to defined destinations in a round robin manner. @sink(type=' sink type ', common.static.key =' value ', common.dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) @distribution(strategy='roundRobin', @destination( destination.specific.key =' value '), @destination( destination.specific.key =' value '))) ) define stream stream name ( attribute1 type , attributeN type ); Partitioned Distributed Sink Publishes events to defined destinations by partitioning them based on the partitioning key. @sink(type=' sink type ', common.static.key =' value ', common.dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) @distribution(strategy='partitioned', partitionKey=' partition key ', @destination( destination.specific.key =' value '), @destination( destination.specific.key =' value '))) ) define stream stream name ( attribute1 type , attributeN type );","title":"Distributed Sink"},{"location":"docs/query-guide/#sink-mapper","text":"Each @sink configuration can have a mapping denoted by the @map annotation that defines how to convert Siddhi events to outgoing messages with the defined format. The type parameter of the @map defines the map type to be used in converting the outgoing events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional and/or dynamic. For detailed information about the parameters see the documentation of the relevant mapper. Map Payload @payload is an optional annotation used with @map to define custom mapping. When the @payload annotation is not provided, each mapper maps the outgoing events to its own default event format. The @payload annotation allow users to configure mappers to produce the output payload of their choice, and by using dynamic properties within the payload they can selectively extract and add data from the published Siddhi events. There are two ways you to configure @payload annotation. Some mappers such as XML , JSON , and Test only accept one output payload: @payload( 'This is a test message from {{user}}.') Some mappers such key-value accept series of mapping values: @payload( key1='mapping_1', 'key2'='user : {{user}}') Here, the keys of payload mapping can be defined using the dot notation as a.b.c , or using any constant string value as '$abc' . Supported Sink Mapping Types The following is a list of sink mapping types supported by Siddhi: Sink mapping type Description PassThrough Omits data conversion on outgoing Siddhi events. JSON Converts Siddhi events to JSON messages. XML Converts Siddhi events to XML messages. TEXT Converts Siddhi events to plain text messages. Avro Converts Siddhi events to Avro Events. Binary Converts Siddhi events to Siddhi specific binary events. Key Value Converts Siddhi events to key-value HashMaps. CSV Converts Siddhi events to CSV like delimiter separated events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the outgoing Siddhi events directly to the sinks without any data conversion. PassThrough is the only sink mapper inbuilt in Siddhi, and all other sink mappers are implemented as extensions. Example 1 Publishes OutputStream events by converting them to JSON messages with the default format, and by sending to an HTTP endpoint http://localhost:8005/endpoint1 , using POST method, Accept header, and basic authentication having admin is both username and password. The configuration of the HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/endpoint', method='POST', headers='Accept-Date:20/02/2017', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json')) define stream OutputStream (name string, age int, country string); This will publish a JSON message on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } } Example 2 Publishes StockStream events by converting them to user defined JSON messages, and by sending to an HTTP endpoint http://localhost:8005/stocks . The configuration of the HTTP sink and custom JSON sink mapping to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/stocks', @map(type='json', validate.json='true', enclosing.element='$.Portfolio', @payload(\"\"\"{\"StockData\":{ \"Symbol\":\"{{symbol}}\", \"Price\":{{price}} }}\"\"\"))) define stream StockStream (symbol string, price float, volume long); This will publish a single event as the JSON message on the following format: { \"Portfolio\":{ \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } } } This can also publish multiple events together as a JSON message on the following format: { \"Portfolio\":[ { \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } }, { \"StockData\":{ \"Symbol\":\"FB\", \"Price\":57.0 } } ] } Example 3 Publishes events from the OutputStream stream to multiple the HTTP endpoints using a partitioning strategy. Here the events are sent to either http://localhost:8005/endpoint1 or http://localhost:8006/endpoint2 based on the partitioning key country . It uses default JSON mapping, POST method, and used admin as both the username and the password when publishing to both the endpoints. The configuration of the distributed HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', method='POST', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json'), @distribution(strategy='partitioned', partitionKey='country', @destination(publisher.url='http://localhost:8005/endpoint1'), @destination(publisher.url='http://localhost:8006/endpoint2'))) define stream OutputStream (name string, age int, country string); This will partition the outgoing events and publish all events with the same country attribute value to the same endpoint. The JSON message published will be on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } }","title":"Sink Mapper"},{"location":"docs/query-guide/#error-handling","text":"Errors in Siddhi can be handled at the Streams and in Sinks. Error Handling at Stream When errors are thrown by Siddhi elements subscribed to the stream, the error gets propagated up to the stream that delivered the event to those Siddhi elements. By default the error is logged and dropped at the stream, but this behavior can be altered by by adding @OnError annotation to the corresponding stream definition. @OnError annotation can help users to capture the error and the associated event, and handle them gracefully by sending them to a fault stream. The @OnError annotation and the required action to be specified as bellow. @OnError(action='on error action') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The action parameter of the @OnError annotation defines the action to be executed during failure scenarios. The following actions can be specified to @OnError annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when @OnError annotation is not defined. STREAM : Creates a fault stream and redirects the event and the error to it. The created fault stream will have all the attributes defined in the base stream to capture the error causing event, and in addition it also contains _error attribute of type object to containing the error information. The fault stream can be referred by adding ! in front of the base stream name as ! stream name . Example Handle errors in TempStream by redirecting the errors to a fault stream. The configuration of TempStream stream and @OnError annotation is as follows. @OnError(name='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); Siddhi will infer and automatically defines the fault stream of TempStream as given bellow. define stream !TempStream (deviceID long, roomNo int, temp double, _error object); The SiddhiApp extending the above the use-case by adding failure generation and error handling with the use of queries is as follows. Note: Details on writing processing logics via queries will be explained in later sections. -- Define fault stream to handle error occurred at TempStream subscribers @OnError(name='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); -- Error generation through a custom function `createError()` @name('error-generation') from TempStream#custom:createError() insert into IgnoreStream1; -- Handling error by simply logging the event and error. @name('handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream2; Error Handling at Sink There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. The on.error parameter of the @sink annotation can be specified as bellow. @sink(type=' sink type ', on.error=' on error action ', key =' value ', ...) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following actions can be specified to on.error parameter of @sink annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when on.error parameter is not defined on the @sink annotation. WAIT : Publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publishes to it. STREAM : Pushes the failed events with the corresponding error to the associated fault stream the sink belongs to. Example 1 Introduce back pressure on the threads who bring events via TempStream when the system cannot connect to Kafka. The configuration of TempStream stream and @sink Kafka annotation with on.error property is as follows. @sink(type='kafka', on.error='WAIT', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); Example 2 Send events to the fault stream of TempStream when the system cannot connect to Kafka. The configuration of TempStream stream with associated fault stream, @sink Kafka annotation with on.error property and a queries to handle the error is as follows. Note: Details on writing processing logics via queries will be explained in later sections. @OnError(name='STREAM') @sink(type='kafka', on.error='STREAM', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); -- Handling error by simply logging the event and error. @name('handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream;","title":"Error Handling"},{"location":"docs/query-guide/#query","text":"Query defines the processing logic in Siddhi. It consumes events from one or more streams, named-windows , tables , and/or named-aggregations , process the events in a streaming manner, and generate output events into a stream , named-window , or table . Purpose A query provides a way to process the events in the order they arrive and produce output using both stateful and stateless complex event processing and stream processing operations. Syntax The high level query syntax for defining processing logics is as follows: @name(' query name ') from input projection output action The following parameters are used to configure a stream definition. Parameter Description query name The name of the query. Since naming the query (i.e the @name(' query name ') annotation) is optional, when the name is not provided Siddhi assign a system generated name for the query. input Defines the means of event consumption via streams , named-windows , tables , and/or named-aggregations , and defines the processing logic using filters , windows , stream-functions , joins , patterns and sequences . projection Generates output event attributes using select , functions , aggregation-functions , and group by operations, and filters the generated the output using having , limit offset , order by , and output rate limiting operations before sending them out. Here the projection is optional and when it is omitted all the input events will be sent to the output as it is. output action Defines output action (such as insert into , update , delete , etc) that needs to be performed by the generated events on a stream , named-window , or table Example A query consumes events from the TempStream stream and output only the roomNo and temp attributes to the RoomTempStream stream, from which another query consumes the events and sends all its attributes to AnotherRoomTempStream stream. Inferred Stream Here, the RoomTempStream and AnotherRoomTempStream streams are an inferred streams, which means their stream definitions are inferred from the queries and they can be used same as any other defined stream without any restrictions.","title":"Query"},{"location":"docs/query-guide/#value","text":"Values are typed data, that can be manipulated, transferred and stored. Values can be referred by the attributes defined in definitions such as streams, and tables. Siddhi supports values of type STRING , INT (Integer), LONG , DOUBLE , FLOAT , BOOL (Boolean) and OBJECT . The syntax of each type and their example use as a constant value is as follows, Attribute Type Format Example int + 123 , -75 , +95 long +L 123000L , -750l , +154L float ( +)?('.' *)? (E(-|+)? +)?F 123.0f , -75.0e-10F , +95.789f double ( +)?('.' *)? (E(-|+)? +)?D? 123.0 , 123.0D , -75.0e-10D , +95.789d bool (true|false) true , false , TRUE , FALSE string '( char * !('|\"|\"\"\"| line ))' or \"( char * !(\"|\"\"\"| line ))\" or \"\"\"( char * !(\"\"\"))\"\"\" 'Any text.' , \"Text with 'single' quotes.\" , \"\"\" Text with 'single' quotes, \"double\" quotes, and new lines. \"\"\" Time Time is a special type of LONG value that denotes time using digits and their unit in the format ( digit + unit )+ . At execution, the time gets converted into milliseconds and returns a LONG value. Unit Syntax Year year | years Month month | months Week week | weeks Day day | days Hour hour | hours Minutes minute | minutes | min Seconds second | seconds | sec Milliseconds millisecond | milliseconds Example 1 hour and 25 minutes can by written as 1 hour and 25 minutes which is equal to the LONG value 5100000 .","title":"Value"},{"location":"docs/query-guide/#select","text":"The select clause in Siddhi query defines the output event attributes of the query. Following are some basic query projection operations supported by select. Action Description Select specific attributes for projection Only select some of the input attributes as query output attributes. E.g., Select and output only roomNo and temp attributes from the TempStream stream. from TempStream select roomNo, temp insert into RoomTempStream; Select all attributes for projection Select all input attributes as query output attributes. This can be done by using asterisk ( * ) or by omitting the select clause itself. E.g., Both following queries select all attributes of TempStream input stream and output all attributes to NewTempStream stream. from TempStream select * insert into NewTempStream; or from TempStream insert into NewTempStream; Name attribute Provide a unique name for each output attribute generated by the query. This can help to rename the selected input attributes or assign an attribute name to a projection operation such as function, aggregate-function, mathematical operation, etc, using as keyword. E.g., Query that renames input attribute temp to temperature and function currentTimeMillis() as time . from TempStream select roomNo, temp as temperature, currentTimeMillis() as time insert into RoomTempStream; Constant values as attributes Creates output attributes with a constant value. Any constant value of type STRING , INT , LONG , DOUBLE , FLOAT , BOOL , and time as given in the values section can be defined. E.g., Query specifying 'C' as the constant value for the scale attribute. from TempStream select roomNo, temp, 'C' as scale insert into RoomTempStream; Mathematical and logical expressions in attributes Defines the mathematical and logical operations that need to be performed to generating output attribute values. These expressions are executed in the precedence order given below. Operator precedence Operator Distribution Example () Scope (cost + tax) * 0.05 IS NULL Null check deviceID is null NOT Logical NOT not (price > 10) * , / , % Multiplication, division, modulus temp * 9/5 + 32 + , - Addition, subtraction temp * 9/5 - 32 < , < = , > , >= Comparators: less-than, greater-than-equal, greater-than, less-than-equal totalCost >= price * quantity == , != Comparisons: equal, not equal totalCost != price * quantity IN Checks if value exist in the table roomNo in ServerRoomsTable AND Logical AND temp < 40 and humidity < 40 OR Logical OR humidity < 40 or humidity >= 60 E.g., Query converts temperature from Celsius to Fahrenheit, and identifies rooms with room number between 10 and 15 as server rooms. from TempStream select roomNo, temp * 9/5 + 32 as temp, 'F' as scale, roomNo > 10 and roomNo < 15 as isServerRoom insert into RoomTempStream;","title":"Select"},{"location":"docs/query-guide/#function","text":"Function are pre-configured operations that can consumes zero, or more parameters and always produce a single value as result. It can be used anywhere an attribute can be used. Purpose Functions encapsulate pre-configured reusable execution logic allowing users to execute the logic anywhere just by calling the function. This also make writing SiddhiApps simple and easy to understand. Syntax The syntax of function is as follows, function name ( parameter * ) Here function name uniquely identifies the function. The parameter defined input parameters the function can accept. The input parameters can be attributes, constant values, results of other functions, results of mathematical or logical expressions, or time values. The number and type of parameters a function accepts depend on the function itself. Note Functions, mathematical expressions, and logical expressions can be used in a nested manner. Example 1 Function name add accepting two input parameters, is called with an attribute named input and a constant value 75 . add(input, 75) Example 2 Function name alertAfter accepting two input parameters, is called with a time value of 1 hour and 25 minutes and a mathematical addition operation of startTime + 56 . add(1 hour and 25 minutes, startTime + 56) Inbuilt functions Following are some inbuilt Siddhi functions, for more functions refer execution extensions . Inbuilt function Description eventTimestamp Returns event's timestamp. currentTimeMillis Returns current time of SiddhiApp runtime. default Returns a default value if the parameter is null. ifThenElse Returns parameters based on a conditional parameter. UUID Generates a UUID. cast Casts parameter type. convert Converts parameter type. coalesce Returns first not null input parameter. maximum Returns the maximum value of all parameters. minimum Returns the minimum value of all parameters. instanceOfBoolean Checks if the parameter is an instance of Boolean. instanceOfDouble Checks if the parameter is an instance of Double. instanceOfFloat Checks if the parameter is an instance of Float. instanceOfInteger Checks if the parameter is an instance of Integer. instanceOfLong Checks if the parameter is an instance of Long. instanceOfString Checks if the parameter is an instance of String. createSet Creates HashSet with given input parameters. sizeOfSet Returns number of items in the HashSet, that's passed as a parameter. Example Query that converts the roomNo to string using convert function, finds the maximum temperature reading with maximum function, and adds a unique messageID using the UUID function. from TempStream select convert(roomNo, 'string') as roomNo, maximum(tempReading1, tempReading2) as temp, UUID() as messageID insert into RoomTempStream;","title":"Function"},{"location":"docs/query-guide/#filter","text":"Filters provide a way of filtering input stream events based on a specified condition. It accepts any type of condition including a combination of functions and/or attributes that produces a Boolean result. Filters allow events to pass through if the condition results in true , and drops if it results in a false . Purpose Filter helps to select the events that are relevant for the processing and omit the ones that are not needed. Syntax Filter conditions should be defined in square brackets next to the input stream as shown below. from input stream [ filter condition ] select attribute name , attribute name , ... insert into output stream Example Query to filter TempStream stream events, having roomNo within the range of 100-210 and temperature greater than 40 degrees, and insert them into HighTempStream stream. from TempStream[(roomNo = 100 and roomNo 210) and temp 40] select roomNo, temp insert into HighTempStream;","title":"Filter"},{"location":"docs/query-guide/#window","text":"Windows allow you to capture a subset of events based on a specific criterion from an input stream for calculation. Each input stream can only have a maximum of one window. Purpose To create subsets of events within a stream based on time duration, number of events, etc for processing. A window can operate in a sliding or tumbling (batch) manner. Syntax The #window prefix should be inserted next to the relevant stream in order to use a window. Note Filter condition can be applied both before and/or after the window Example If you want to identify the maximum temperature out of the last 10 events, you need to define a length window of 10 events. This window operates in a sliding mode where the following 3 subsets are calculated when a list of 12 events are received in a sequential order. Subset Event Range 1 1-10 2 2-11 3 3-12 The following query finds the maximum temperature out of last 10 events from the TempStream stream, and inserts the results into the MaxTempStream stream. from TempStream#window.length(10) select max(temp) as maxTemp insert into MaxTempStream; If you define the maximum temperature reading out of every 10 events, you need to define a lengthBatch window of 10 events. This window operates as a batch/tumbling mode where the following 3 subsets are calculated when a list of 30 events are received in a sequential order. Subset Event Range 1 1-10 2 11-20 3 21-30 The following query finds the maximum temperature out of every 10 events from the TempStream stream, and inserts the results into the MaxTempStream stream. from TempStream#window.lengthBatch(10) select max(temp) as maxTemp insert into MaxTempStream; Note Similar operations can be done based on time via time windows and timeBatch windows and for others. Code segments such as #window.time(10 min) considers events that arrive during the last 10 minutes in a sliding manner, and the #window.timeBatch(2 min) considers events that arrive every 2 minutes in a tumbling manner. Following are some inbuilt windows shipped with Siddhi. For more window types, see execution extensions . time timeBatch batch timeLength length lengthBatch sort frequent lossyFrequent session cron externalTime externalTimeBatch delay Output event types Projection of the query depends on the output event types such as, current and expired event types. By default all queries produce current events and only queries with windows produce expired events when events expire from the window. You can specify whether the output of a query should be only current events, only expired events or both current and expired events. Note! Controlling the output event types does not alter the execution within the query, and it does not affect the accuracy of the query execution. The following keywords can be used with the output stream to manipulate output. Output event types Description current events Outputs events when incoming events arrive to be processed by the query. This is default when no specific output event type is specified. expired events Outputs events when events expires from the window. all events Outputs events when incoming events arrive to be processed by the query as well as when events expire from the window. The output event type keyword can be used between insert and into as shown in the following example. Example This query delays all events in a stream by 1 minute. from TempStream#window.time(1 min) select * insert expired events into DelayedTempStream","title":"Window"},{"location":"docs/query-guide/#aggregate-function","text":"Aggregate functions perform aggregate calculations in the query. When a window is defined the aggregation is restricted within that window. If no window is provided aggregation is performed from the start of the Siddhi application. Syntax from input stream #window. window name ( parameter , parameter , ... ) select aggregate function ( parameter , parameter , ... ) as attribute name , attribute2 name , ... insert into output stream ; Aggregate Parameters Aggregate parameters can be attributes, constant values, results of other functions or aggregates, results of mathematical or logical expressions, or time parameters. Aggregate parameters configured in a query depends on the aggregate function being called. Example The following query calculates the average value for the temp attribute of the TempStream stream. This calculation is done for the last 10 minutes in a sliding manner, and the result is output as avgTemp to the AvgTempStream output stream. from TempStream#window.time(10 min) select avg(temp) as avgTemp, roomNo, deviceID insert into AvgTempStream; Following are some inbuilt aggregation functions shipped with Siddhi, for more aggregation functions, see execution extensions . avg sum max min count distinctCount maxForever minForever stdDev","title":"Aggregate function"},{"location":"docs/query-guide/#group-by","text":"Group By allows you to group the aggregate based on specified attributes. Syntax The syntax for the Group By aggregate function is as follows: from input stream #window. window name (...) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... insert into output stream ; Example The following query calculates the average temperature per roomNo and deviceID combination, for events that arrive at the TempStream stream for a sliding time window of 10 minutes. from TempStream#window.time(10 min) select avg(temp) as avgTemp, roomNo, deviceID group by roomNo, deviceID insert into AvgTempStream;","title":"Group By"},{"location":"docs/query-guide/#having","text":"Having allows you to filter events after processing the select statement. Purpose This allows you to filter the aggregation output. Syntax The syntax for the Having clause is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition insert into output stream ; Example The following query calculates the average temperature per room for the last 10 minutes, and alerts if it exceeds 30 degrees. from TempStream#window.time(10 min) select avg(temp) as avgTemp, roomNo group by roomNo having avgTemp 30 insert into AlertStream;","title":"Having"},{"location":"docs/query-guide/#order-by","text":"Order By allows you to order the aggregated result in ascending and/or descending order based on specified attributes. By default ordering will be done in ascending manner. User can use 'desc' keyword to order in descending manner. Syntax The syntax for the Order By clause is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name (asc | desc)?, attribute2 name ( ascend/descend )?, ... insert into output stream ; Example The following query calculates the average temperature per roomNo and deviceID combination for every 10 minutes, and generate output events by ordering them in the ascending order of the room's avgTemp and then by the descending order of roomNo. from TempStream#window.timeBatch(10 min) select avg(temp) as avgTemp, roomNo, deviceID group by roomNo, deviceID order by avgTemp, roomNo desc insert into AvgTempStream;","title":"Order By"},{"location":"docs/query-guide/#limit-offset","text":"When events are emitted as a batch, offset allows you to offset beginning of the output event batch and limit allows you to limit the number of events in the batch from the defined offset. With this users can specify which set of events need be emitted. Syntax The syntax for the Limit Offset clause is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name (asc | desc)?, attribute2 name ( ascend/descend )?, ... limit positive interger ? offset positive interger ? insert into output stream ; Here both limit and offset are optional where limit by default output all the events and offset by default set to 0 . Example The following query calculates the average temperature per roomNo and deviceID combination, for events that arrive at the TempStream stream for every 10 minutes and emits two events with highest average temperature. from TempStream#window.timeBatch(10 min) select avg(temp) as avgTemp, roomNo, deviceID group by roomNo, deviceID order by avgTemp desc limit 2 insert into HighestAvgTempStream; The following query calculates the average temperature per roomNo and deviceID combination, for events that arrive at the TempStream stream for every 10 minutes and emits third, forth and fifth events when sorted in descending order based on their average temperature. from TempStream#window.timeBatch(10 min) select avg(temp) as avgTemp, roomNo, deviceID group by roomNo, deviceID order by avgTemp desc limit 3 offset 2 insert into HighestAvgTempStream;","title":"Limit &amp; Offset"},{"location":"docs/query-guide/#join-stream","text":"Joins allow you to get a combined result from two streams in real-time based on a specified condition. Purpose Streams are stateless. Therefore, in order to join two streams, they need to be connected to a window so that there is a pool of events that can be used for joining. Joins also accept conditions to join the appropriate events from each stream. During the joining process each incoming event of each stream is matched against all the events in the other stream's window based on the given condition, and the output events are generated for all the matching event pairs. Note Join can also be performed with stored data , aggregation or externally named windows . Syntax The syntax for a join is as follows: from input stream #window. window name ( parameter , ... ) {unidirectional} {as reference } join input stream #window. window name ( parameter , ... ) {unidirectional} {as reference } on join condition select attribute name , attribute name , ... insert into output stream Here, the join condition allows you to match the attributes from both the streams. Unidirectional join operation By default, events arriving at either stream can trigger the joining process. However, if you want to control the join execution, you can add the unidirectional keyword next to a stream in the join definition as depicted in the syntax in order to enable that stream to trigger the join operation. Here, events arriving at other stream only update the window of that stream, and this stream does not trigger the join operation. Note The unidirectional keyword cannot be applied to both the input streams because the default behaviour already allows both streams to trigger the join operation. Example Assuming that the temperature of regulators are updated every minute. Following is a Siddhi App that controls the temperature regulators if they are not already on for all the rooms with a room temperature greater than 30 degrees. define stream TempStream(deviceID long, roomNo int, temp double); define stream RegulatorStream(deviceID long, roomNo int, isOn bool); from TempStream[temp 30.0]#window.time(1 min) as T join RegulatorStream[isOn == false]#window.length(1) as R on T.roomNo == R.roomNo select T.roomNo, R.deviceID, 'start' as action insert into RegulatorActionStream; Supported join types Following are the supported operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join both the streams. The output is generated only if there is a matching event in both the streams. Left outer join The left outer join operation allows you to join two streams to be merged based on a condition. left outer join is used as the keyword to join both the streams. Here, it returns all the events of left stream even if there are no matching events in the right stream by having null values for the attributes of the right stream. Example The following query generates output events for all events from the StockStream stream regardless of whether a matching symbol exists in the TwitterStream stream or not. from StockStream#window.time(1 min) as S left outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ; Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join both the streams. It returns all the events of the right stream even if there are no matching events in the left stream. Full outer join The full outer join combines the results of left outer join and right outer join. full outer join is used as the keyword to join both the streams. Here, output event are generated for each incoming event even if there are no matching events in the other stream. Example The following query generates output events for all the incoming events of each stream regardless of whether there is a match for the symbol attribute in the other stream or not. from StockStream#window.time(1 min) as S full outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ;","title":"Join (Stream)"},{"location":"docs/query-guide/#pattern","text":"This is a state machine implementation that allows you to detect patterns in the events that arrive over time. This can correlate events within a single stream or between multiple streams. Purpose Patterns allow you to identify trends in events over a time period. Syntax The following is the syntax for a pattern query: from (every)? event reference = input stream [ filter condition ] - (every)? event reference = input stream [ filter condition ] - ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream | Items| Description | |-------------------|-------------| | - | This is used to indicate an event that should be following another event. The subsequent event does not necessarily have to occur immediately after the preceding event. The condition to be met by the preceding event should be added before the sign, and the condition to be met by the subsequent event should be added after the sign. | | event reference | This allows you to add a reference to the the matching event so that it can be accessed later for further processing. | | (within time gap )? | The within clause is optional. It defines the time duration within which all the matching events should occur. | | every | every is an optional keyword. This defines whether the event matching should be triggered for every event arrival in the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. | Siddhi also supports pattern matching with counting events and matching events in a logical order such as ( and , or , and not ). These are described in detail further below in this guide. Example This query sends an alert if the temperature of a room increases by 5 degrees within 10 min. from every( e1=TempStream ) - e2=TempStream[ e1.roomNo == roomNo and (e1.temp + 5) = temp ] within 10 min select e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Here, the matching process begins for each event in the TempStream stream (because every is used with e1=TempStream ), and if another event arrives within 10 minutes with a value for the temp attribute that is greater than or equal to e1.temp + 5 of the event e1, an output is generated via the AlertStream .","title":"Pattern"},{"location":"docs/query-guide/#counting-pattern","text":"Counting patterns allow you to match multiple events that may have been received for the same matching condition. The number of events matched per condition can be limited via condition postfixes. Syntax Each matching condition can contain a collection of events with the minimum and maximum number of events to be matched as shown in the syntax below. from (every)? event reference = input stream [ filter condition ] ( min count : max count )? - ... (within time gap )? select event reference ([event index])?. attribute name , ... insert into output stream Postfix Description Example n1:n2 This matches n1 to n2 events (including n1 and not more than n2 ). 1:4 matches 1 to 4 events. n: This matches n or more events (including n ). 2: matches 2 or more events. :n This matches up to n events (excluding n ). :5 matches up to 5 events. n This matches exactly n events. 5 matches exactly 5 events. Specific occurrences of the event in a collection can be retrieved by using an event index with its reference. Square brackets can be used to indicate the event index where 1 can be used as the index of the first event and last can be used as the index for the last available event in the event collection. If you provide an index greater then the last event index, the system returns null . The following are some valid examples. e1[3] refers to the 3 rd event. e1[last] refers to the last event. e1[last - 1] refers to the event before the last event. Example The following Siddhi App calculates the temperature difference between two regulator events. define stream TempStream (deviceID long, roomNo int, temp double); define stream RegulatorStream (deviceID long, roomNo int, tempSet double, isOn bool); from every( e1=RegulatorStream) - e2=TempStream[e1.roomNo==roomNo] 1: - e3=RegulatorStream[e1.roomNo==roomNo] select e1.roomNo, e2[0].temp - e2[last].temp as tempDiff insert into TempDiffStream;","title":"Counting Pattern"},{"location":"docs/query-guide/#logical-patterns","text":"Logical patterns match events that arrive in temporal order and correlate them with logical relationships such as and , or and not . Syntax from (every)? (not)? event reference = input stream [ filter condition ] ((and|or) event reference = input stream [ filter condition ])? (within time gap )? - ... select event reference ([event index])?. attribute name , ... insert into output stream Keywords such as and , or , or not can be used to illustrate the logical relationship. Key Word Description and This allows both conditions of and to be matched by two events in any order. or The state succeeds if either condition of or is satisfied. Here the event reference of the other condition is null . not condition1 and condition2 When not is included with and , it identifies the events that match arriving before any event that match . not condition for time period When not is included with for , it allows you to identify a situation where no event that matches condition1 arrives during the specified time period . e.g., from not TemperatureStream[temp 60] for 5 sec . Here the not pattern can be followed by either an and clause or the effective period of not can be concluded after a given time period . Further in Siddhi more than two streams cannot be matched with logical conditions using and , or , or not clauses at this point.","title":"Logical Patterns"},{"location":"docs/query-guide/#detecting-non-occurring-events","text":"Siddhi allows you to detect non-occurring events via multiple combinations of the key words specified above as shown in the table below. In the patterns listed, P* can be either a regular event pattern, an absent event pattern or a logical pattern. Pattern Detected Scenario not A for time period The non-occurrence of event A within time period after system start up. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, to indicate that the passenger might be in danger. not A for time period and B After system start up, event A does not occur within time period , but event B occurs at some point in time. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, and the passenger marked that he/she is in danger at some point in time. not A for time period 1 and not B for time period 2 After system start up, event A doess not occur within time period 1 , and event B also does not occur within time period 2 . e.g., Generating an alert if the driver of a taxi has not reached the destination within 30 minutes, and the passenger has not marked himself/herself to be in danger within that same time period. not A for time period or B After system start up, either event A does not occur within time period , or event B occurs at some point in time. e.g., Generating an alert if the taxi has not reached its destination within 30 minutes, or if the passenger has marked that he/she is in danger at some point in time. not A for time period 1 or not B for time period 2 After system start up, either event A does not occur within time period 1 , or event B occurs within time period 2 . e.g., Generating an alert to indicate that the driver is not on an expected route if the taxi has not reached destination A within 20 minutes, or reached destination B within 30 minutes. A \u2192 not B for time period Event B does not occur within time period after the occurrence of event A. e.g., Generating an alert if the taxi has reached its destination, but this was not followed by a payment record. P* \u2192 not A for time period and B After the occurrence of P*, event A does not occur within time period , and event B occurs at some point in time. P* \u2192 not A for time period 1 and not B for time period 2 After the occurrence of P*, event A does not occur within time period 1 , and event B does not occur within time period 2 . P* \u2192 not A for time period or B After the occurrence of P*, either event A does not occur within time period , or event B occurs at some point in time. P* \u2192 not A for time period 1 or not B for time period 2 After the occurrence of P*, either event A does not occur within time period 1 , or event B does not occur within time period 2 . not A for time period \u2192 B Event A does occur within time period after the system start up, but event B occurs after that time period has elapsed. not A for time period and B \u2192 P* Event A does not occur within time period , and event B occurs at some point in time. Then P* occurs after the time period has elapsed, and after B has occurred. not A for time period 1 and not B for time period 2 \u2192 P* After system start up, event A does not occur within time period 1 , and event B does not occur within time period 2 . However, P* occurs after both A and B. not A for time period or B \u2192 P* After system start up, event A does not occur within time period or event B occurs at some point in time. The P* occurs after time period has elapsed, or after B has occurred. not A for time period 1 or not B for time period 2 \u2192 P* After system start up, either event A does not occur within time period 1 , or event B does not occur within time period 2 . Then P* occurs after both time period 1 and time period 2 have elapsed. not A and B Event A does not occur before event B. A and not B Event B does not occur before event A. Example Following Siddhi App, sends the stop control action to the regulator when the key is removed from the hotel room. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream RoomKeyStream(deviceID long, roomNo int, action string); from every( e1=RegulatorStateChangeStream[ action == 'on' ] ) - e2=RoomKeyStream[ e1.roomNo == roomNo and action == 'removed' ] or e3=RegulatorStateChangeStream[ e1.roomNo == roomNo and action == 'off'] select e1.roomNo, ifThenElse( e2 is null, 'none', 'stop' ) as action having action != 'none' insert into RegulatorActionStream; This Siddhi Application generates an alert if we have switch off the regulator before the temperature reaches 12 degrees. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] - not TempStream[e1.roomNo == roomNo and temp 12] and e2=RegulatorStateChangeStream[action == 'off'] select e1.roomNo as roomNo insert into AlertStream; This Siddhi Application generates an alert if the temperature does not reduce to 12 degrees within 5 minutes of switching on the regulator. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] - not TempStream[e1.roomNo == roomNo and temp 12] for '5 min' select e1.roomNo as roomNo insert into AlertStream;","title":"Detecting Non-occurring Events"},{"location":"docs/query-guide/#sequence","text":"Sequence is a state machine implementation that allows you to detect the sequence of event occurrences over time. Here all matching events need to arrive consecutively to match the sequence condition, and there cannot be any non-matching events arriving within a matching sequence of events. This can correlate events within a single stream or between multiple streams. Purpose This allows you to detect a specified event sequence over a specified time period. Syntax The syntax for a sequence query is as follows: from (every)? event reference = input stream [ filter condition ], event reference = input stream [ filter condition ], ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description , This represents the immediate next event i.e., when an event that matches the first condition arrives, the event that arrives immediately after it should match the second condition. event reference This allows you to add a reference to the the matching event so that it can be accessed later for further processing. (within time gap )? The within clause is optional. It defines the time duration within which all the matching events should occur. every every is an optional keyword. This defines whether the matching event should be triggered for every event that arrives at the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. Example This query generates an alert if the increase in the temperature between two consecutive temperature events exceeds one degree. from every e1=TempStream, e2=TempStream[e1.temp + 1 temp] select e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Counting Sequence Counting sequences allow you to match multiple events for the same matching condition. The number of events matched per condition can be limited via condition postfixes such as Counting Patterns , or by using the * , + , and ? operators. The matching events can also be retrieved using event indexes, similar to how it is done in Counting Patterns . Syntax Each matching condition in a sequence can contain a collection of events as shown below. from (every)? event reference = input stream [ filter condition ](+|*|?)?, event reference = input stream [ filter condition ](+|*|?)?, ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Postfix symbol Required/Optional Description + Optional This matches one or more events to the given condition. * Optional This matches zero or more events to the given condition. ? Optional This matches zero or one events to the given condition. Example This Siddhi application identifies temperature peeks. define stream TempStream(deviceID long, roomNo int, temp double); from every e1=TempStream, e2=TempStream[e1.temp = temp]+, e3=TempStream[e2[last].temp temp] select e1.temp as initialTemp, e2[last].temp as peakTemp insert into PeekTempStream; Logical Sequence Logical sequences identify logical relationships using and , or and not on consecutively arriving events. Syntax The syntax for a logical sequence is as follows: from (every)? (not)? event reference = input stream [ filter condition ] ((and|or) event reference = input stream [ filter condition ])? (within time gap )?, ... select event reference ([event index])?. attribute name , ... insert into output stream Keywords such as and , or , or not can be used to illustrate the logical relationship, similar to how it is done in Logical Patterns . Example This Siddhi application notifies the state when a regulator event is immediately followed by both temperature and humidity events. define stream TempStream(deviceID long, temp double); define stream HumidStream(deviceID long, humid double); define stream RegulatorStream(deviceID long, isOn bool); from every e1=RegulatorStream, e2=TempStream and e3=HumidStream select e2.temp, e3.humid insert into StateNotificationStream;","title":"Sequence"},{"location":"docs/query-guide/#output-rate-limiting","text":"Output rate limiting allows queries to output events periodically based on a specified condition. Purpose This allows you to limit the output to avoid overloading the subsequent executions, and to remove unnecessary information. Syntax The syntax of an output rate limiting configuration is as follows: from input stream ... select attribute name , attribute name , ... output rate limiting configuration insert into output stream Siddhi supports three types of output rate limiting configurations as explained in the following table: Rate limiting configuration Syntax Description Based on time output event every time interval This outputs output event every time interval time interval. Based on number of events output event every event interval events This outputs output event for every event interval number of events. Snapshot based output snapshot every time interval This outputs all events in the window (or the last event if no window is defined in the query) for every given time interval time interval. Here the output event specifies the event(s) that should be returned as the output of the query. The possible values are as follows: * first : Only the first event processed by the query during the specified time interval/sliding window is emitted. * last : Only the last event processed by the query during the specified time interval/sliding window is emitted. * all : All the events processed by the query during the specified time interval/sliding window are emitted. When no output event is defined, all is used by default. Examples Returning events based on the number of events Here, events are emitted every time the specified number of events arrive. You can also specify whether to emit only the first event/last event, or all the events out of the events that arrived. In this example, the last temperature per sensor is emitted for every 10 events. from TempStreamselect select temp, deviceID group by deviceID output last every 10 events insert into LowRateTempStream; Returning events based on time Here events are emitted for every predefined time interval. You can also specify whether to emit only the first event, last event, or all events out of the events that arrived during the specified time interval. In this example, emits all temperature events every 10 seconds from TempStreamoutput output every 10 sec insert into LowRateTempStream; Returning a periodic snapshot of events This method works best with windows. When an input stream is connected to a window, snapshot rate limiting emits all the current events that have arrived and do not have corresponding expired events for every predefined time interval. If the input stream is not connected to a window, only the last current event for each predefined time interval is emitted. This query emits a snapshot of the events in a time window of 5 seconds every 1 second. from TempStream#window.time(5 sec) output snapshot every 1 sec insert into SnapshotTempStream;","title":"Output rate limiting"},{"location":"docs/query-guide/#partition","text":"Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. A partition can contain one or more queries and there can be multiple instances where the same queries and streams are replicated for each partition. Each partition is tagged with a partition key. Those partitions only process the events that match the corresponding partition key. Purpose Partitions allow you to process the events groups in isolation so that event processing can be performed using the same set of queries for each group. Partition key generation A partition key can be generated in the following two methods: Partition by value This is created by generating unique values using input stream attributes. Syntax partition with ( expression of stream name , expression of stream name , ... ) begin query query ... end; Example This query calculates the maximum temperature recorded within the last 10 events per deviceID . partition with ( deviceID of TempStream ) begin from TempStream#window.length(10) select roomNo, deviceID, max(temp) as maxTemp insert into DeviceTempStream; end; Partition by range This is created by mapping each partition key to a range condition of the input streams numerical attribute. Syntax partition with ( condition as partition key or condition as partition key or ... of stream name , ... ) begin query query ... end; Example This query calculates the average temperature for the last 10 minutes per office area. partition with ( roomNo = 1030 as 'serverRoom' or roomNo 1030 and roomNo = 330 as 'officeRoom' or roomNo 330 as 'lobby' of TempStream) begin from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp insert into AreaTempStream end;","title":"Partition"},{"location":"docs/query-guide/#inner-stream","text":"Queries inside a partition block can use inner streams to communicate with each other while preserving partition isolation. Inner streams are denoted by a \"#\" placed before the stream name, and these streams cannot be accessed outside a partition block. Purpose Inner streams allow you to connect queries within the partition block so that the output of a query can be used as an input only by another query within the same partition. Therefore, you do not need to repartition the streams if they are communicating within the partition. Example This partition calculates the average temperature of every 10 events for each sensor, and sends an output to the DeviceTempIncreasingStream stream if the consecutive average temperature values increase by more than 5 degrees. partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 < avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end;","title":"Inner Stream"},{"location":"docs/query-guide/#purge-partition","text":"Based on the partition key used for the partition, multiple instances of streams and queries will be generated. When an extremely large number of unique partition keys are used there is a possibility of very high instances of streams and queries getting generated and eventually system going out of memory. In order to overcome this, users can define a purge interval to clean partitions that will not be used anymore. Purpose @purge allows you to clean the partition instances that will not be used anymore. Syntax The syntax of partition purge configuration is as follows: @purge(enable='true', interval=' purge interval ', idle.period=' idle period of partition instance ') partition with ( partition key of input stream ) begin from input stream ... select attribute name , attribute name , ... insert into output stream end; Partition purge configuration Description Purge interval The periodic time interval to purge the purgeable partition instances. Idle period of partition instance The period, a particular partition instance (for a given partition key) needs to be idle before it becomes purgeable. Examples Mark partition instances eligible for purging, if there are no events from a particular deviceID for 15 seconds, and periodically purge those partition instances every 1 second. @purge(enable='true', interval='1 sec', idle.period='15 sec') partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end;","title":"Purge Partition"},{"location":"docs/query-guide/#table","text":"A table is a stored version of an stream or a table of events. Its schema is defined via the table definition that is similar to a stream definition. These events are by default stored in-memory , but Siddhi also provides store extensions to work with data/events stored in various data stores through the table abstraction. Purpose Tables allow Siddhi to work with stored events. By defining a schema for tables Siddhi enables them to be processed by queries using their defined attributes with the streaming data. You can also interactively query the state of the stored events in the table. Syntax The syntax for a new table definition is as follows: define table table name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are configured in a table definition: Parameter Description table name The name of the table defined. ( PascalCase is used for table name as a convention.) attribute name The schema of the table is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . Example The following defines a table named RoomTypeTable with roomNo and type attributes of data types int and string respectively. define table RoomTypeTable ( roomNo int, type string ); Primary Keys Tables can be configured with primary keys to avoid the duplication of data. Primary keys are configured by including the @PrimaryKey( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have only one @PrimaryKey annotation. The number of attributes supported differ based on the table implementations. When more than one attribute is used for the primary key, the uniqueness of the events stored in the table is determined based on the combination of values for those attributes. Examples This query creates an event table with the symbol attribute as the primary key. Therefore each entry in this table must have a unique value for symbol attribute. @PrimaryKey('symbol') define table StockTable (symbol string, price float, volume long); Indexes Indexes allow tables to be searched/modified much faster. Indexes are configured by including the @Index( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have 0-1 @Index annotations. Support for the @Index annotation and the number of attributes supported differ based on the table implementations. When more then one attribute is used for index, each one of them is used to index the table for fast access of the data. Indexes can be configured together with primary keys. Examples This query creates an indexed event table named RoomTypeTable with the roomNo attribute as the index key. @Index('roomNo') define table RoomTypeTable (roomNo int, type string);","title":"Table"},{"location":"docs/query-guide/#store","text":"Store is a table that refers to data/events stored in data stores outside of Siddhi such as RDBMS, Cassandra, etc. Store is defined via the @store annotation, and the store schema is defined via a table definition associated with it. Purpose Store allows Siddhi to search, retrieve and manipulate data stored in external data stores through Siddhi queries. Syntax The syntax for a defining store and it's associated table definition is as follows: @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN') define table TableName (attribute1 Type1, attributeN TypeN); Example The following defines a RDBMS data store pointing to a MySQL database with name hotel hosted in loacalhost:3306 having a table RoomTypeTable with columns roomNo of INTEGER and type of VARCHAR(255) mapped to Siddhi data types int and string respectively. @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/hotel\", username=\"siddhi\", password=\"123\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table RoomTypeTable ( roomNo int, type string ); Supported Store Types The following is a list of currently supported store types: RDBMS (MySQL, Oracle, SQL Server, PostgreSQL, DB2, H2) MongoDB Operators on Table (and Store) The following operators can be performed on tables (and stores).","title":"Store"},{"location":"docs/query-guide/#insert","text":"This allows events to be inserted into tables. This is similar to inserting events into streams. Warning If the table is defined with primary keys, and if you insert duplicate data, primary key constrain violations can occur. In such cases use the update or insert into operation. Syntax from input stream select attribute name , attribute name , ... insert into table Similar to streams, you need to use the current events , expired events or the all events keyword between insert and into keywords in order to insert only the specific output event types. For more information, see output event type Example This query inserts all the events from the TempStream stream to the TempTable table. from TempStream select * insert into TempTable;","title":"Insert"},{"location":"docs/query-guide/#join-table","text":"This allows a stream to retrieve information from a table in a streaming manner. Note Joins can also be performed with two streams , aggregation or against externally named windows . Syntax from input stream join table on condition select ( input stream | table ). attribute name , ( input stream | table ). attribute name , ... insert into output stream Note A table can only be joint with a stream. Two tables cannot be joint because there must be at least one active entity to trigger the join operation. Example This Siddhi App performs a join to retrieve the room type from RoomTypeTable table based on the room number, so that it can filter the events related to server-room s. define table RoomTypeTable (roomNo int, type string); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream join RoomTypeTable on RoomTypeTable.roomNo == TempStream.roomNo select deviceID, RoomTypeTable.type as roomType, type, temp having roomType == 'server-room' insert into ServerRoomTempStream; Supported join types Table join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the table. The output is generated only if there is a matching event in both the stream and the table. Left outer join The left outer join operation allows you to join a stream on left side with a table on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right table by having null values for the attributes of the right table. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a table on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left table.","title":"Join (Table)"},{"location":"docs/query-guide/#delete","text":"To delete selected events that are stored in a table. Syntax from input stream select attribute name , attribute name , ... delete table (for output event type )? on condition The condition element specifies the basis on which events are selected to be deleted. When specifying the condition, table attributes should be referred to with the table name. To execute delete for specific output event types, use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see output event type Note Table attributes must be always referred to with the table name as follows: table name . attibute name Example In this example, the script deletes a record in the RoomTypeTable table if it has a value for the roomNo attribute that matches the value for the roomNumber attribute of an event in the DeleteStream stream. define table RoomTypeTable (roomNo int, type string); define stream DeleteStream (roomNumber int); from DeleteStream delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber;","title":"Delete"},{"location":"docs/query-guide/#update","text":"This operator updates selected event attributes stored in a table based on a condition. Syntax from input stream select attribute name , attribute name , ... update table (for output event type )? set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition The condition element specifies the basis on which events are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. To execute an update for specific output event types use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see output event type . Note Table attributes must be always referred to with the table name as shown below: table name . attibute name . Example This Siddhi application updates the room occupancy in the RoomOccupancyTable table for each room number based on new arrivals and exits from the UpdateStream stream. define table RoomOccupancyTable (roomNo int, people int); define stream UpdateStream (roomNumber int, arrival int, exit int); from UpdateStream select * update RoomOccupancyTable set RoomOccupancyTable.people = RoomOccupancyTable.people + arrival - exit on RoomOccupancyTable.roomNo == roomNumber;","title":"Update"},{"location":"docs/query-guide/#update-or-insert","text":"This allows you update if the event attributes already exist in the table based on a condition, or else insert the entry as a new attribute. Syntax from input stream select attribute name , attribute name , ... update or insert into table (for output event type )? set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which events are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note When the attribute to the right is a table attribute, the operations supported differ based on the database type. To execute update upon specific output event types use the current events , expired events or the all events keyword with for as shown in the syntax. To understand more see output event type . Note Table attributes should be always referred to with the table name as table name . attibute name . Example The following query update for events in the UpdateTable event table that have room numbers that match the same in the UpdateStream stream. When such events are found in the event table, they are updated. When a room number available in the stream is not found in the event table, it is inserted from the stream. define table RoomAssigneeTable (roomNo int, type string, assignee string); define stream RoomAssigneeStream (roomNumber int, type string, assignee string); from RoomAssigneeStream select roomNumber as roomNo, type, assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;","title":"Update or Insert"},{"location":"docs/query-guide/#in","text":"This allows the stream to check whether the expected value exists in the table as a part of a conditional operation. Syntax from input stream [ condition in table ] select attribute name , attribute name , ... insert into output stream The condition element specifies the basis on which events are selected to be compared. When constructing the condition , the table attribute must be always referred to with the table name as shown below: table . attibute name . Example This Siddhi application filters only room numbers that are listed in the ServerRoomTable table. define table ServerRoomTable (roomNo int); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream[ServerRoomTable.roomNo == roomNo in ServerRoomTable] insert into ServerRoomTempStream;","title":"In"},{"location":"docs/query-guide/#named-aggregation","text":"Named aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition . Purpose Named aggregation allows you to retrieve the aggregate values for different time durations. That is, it allows you to obtain aggregates such as sum , count , avg , min , max , count and distinctCount of stream attributes for durations such as sec , min , hour , etc. This is of considerable importance in many Analytics scenarios because aggregate values are often needed for several time periods. Furthermore, this ensures that the aggregations are not lost due to unexpected system failures because aggregates can be stored in different persistence stores . Syntax @store(type=\" store type \", ...) @purge(enable=\" true or false \",interval= purging interval ,@retentionPeriod( granularity = retention period , ...) ) define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; The above syntax includes the following: Item Description @store This annotation is used to refer to the data store where the calculated aggregate results are stored. This annotation is optional. When no annotation is provided, the data is stored in the in-memory store. @purge This annotation is used to configure purging in aggregation granularities. If this annotation is not provided, the default purging mentioned above is applied. If you want to disable automatic data purging, you can use this annotation as follows: '@purge(enable=false) /You should disable data purging if the aggregation query in included in the Siddhi application for read-only purposes. @retentionPeriod This annotation is used to specify the length of time the data needs to be retained when carrying out data purging. If this annotation is not provided, the default retention period is applied. aggregator name This specifies a unique name for the aggregation so that it can be referred when accessing aggregate results. input stream The stream that feeds the aggregation. Note! this stream should be already defined. group by attribute name The group by clause is optional. If it is included in a Siddhi application, aggregate values are calculated per each group by attribute. If it is not used, all the events are aggregated together. by timestamp attribute This clause is optional. This defines the attribute that should be used as the timestamp. If this clause is not used, the event time is used by default. The timestamp could be given as either a string or a long value. If it is a long value, the unix timestamp in milliseconds is expected (e.g. 1496289950000 ). If it is a string value, the supported formats are yyyy - MM - dd HH : mm : ss (if time is in GMT) and yyyy - MM - dd HH : mm : ss Z (if time is not in GMT), here the ISO 8601 UTC offset must be provided for Z . (e.g., +05:30 , -11:00 ). time periods Time periods can be specified as a range where the minimum and the maximum value are separated by three dots, or as comma-separated values. e.g., A range can be specified as sec...year where aggregation is done per second, minute, hour, day, month and year. Comma-separated values can be specified as min, hour. Skipping time durations (e.g., min, day where the hour duration is skipped) when specifying comma-separated values is supported only from v4.1.1 onwards Aggregation's granularity data holders are automatically purged every 15 minutes. When carrying out data purging, the retention period you have specified for each granularity in the named aggregation query is taken into account. The retention period defined for a granularity needs to be greater than or equal to its minimum retention period as specified in the table below. If no valid retention period is defined for a granularity, the default retention period (as specified in the table below) is applied. Granularity Default retention Minimum retention second 120 seconds 120 seconds minute 24 hours 120 minutes hour 30 days 25 hours day 1 year 32 days month All 13 month year All none Note Aggregation is carried out at calendar start times for each granularity with the GMT timezone Note The same aggregation can be defined in multiple Siddhi apps for joining, however, only one siddhi app should carry out the processing (i.e. the aggregation input stream should only feed events to one aggregation definition). Example This Siddhi Application defines an aggregation named TradeAggregation to calculate the average and sum for the price attribute of events arriving at the TradeStream stream. These aggregates are calculated per every time granularity in the second-year range. define stream TradeStream (symbol string, price double, volume long, timestamp long); @purge(enable='true', interval='10 sec',@retentionPeriod(sec='120 sec',min='24 hours',hours='30 days',days='1 year',months='all',years='all')) define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year;","title":"Named Aggregation"},{"location":"docs/query-guide/#distributed-aggregation","text":"Distributed Aggregation allows you to partially process aggregations in different shards. This allows Siddhi app in one shard to be responsible only for processing a part of the aggregation. However for this, all aggregations must be based on a common physical database(@store). Syntax @store(type=\" store type \", ...) @PartitionById define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; Following table includes the annotation to be used to enable distributed aggregation, Item Description @PartitionById If the annotation is given, then the distributed aggregation is enabled. Further this can be disabled by using enable element, @PartitionById(enable='false') . Further, following system properties are also available, System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yesio false Note ShardIds should not be changed after the first configuration in order to keep data consistency.","title":"Distributed Aggregation"},{"location":"docs/query-guide/#join-aggregation","text":"This allows a stream to retrieve calculated aggregate values from the aggregation. Note A join can also be performed with two streams , with a table and a stream, or with a stream against externally named windows . Syntax A join with aggregation is similer to the join with table , but with additional within and per clauses. from input stream join aggrigation on join condition within time range per time granularity select attribute name , attribute name , ... insert into output stream ; Apart from constructs of table join this includes the following. Please note that the 'on' condition is optional : Item Description within time range This allows you to specify the time interval for which the aggregate values need to be retrieved. This can be specified by providing the start and end time separated by a comma as string or long values, or by using the wildcard string specifying the data range. For details refer examples. per time granularity This specifies the time granularity by which the aggregate values must be grouped and returned. e.g., If you specify days , the retrieved aggregate values are grouped for each day within the selected time interval. within and per clauses also accept attribute values from the stream. The timestamp of the aggregations can be accessed through the AGG_TIMESTAMP attribute. Example Following aggregation definition will be used for the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select AGG_TIMESTAMP, symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) define stream StockStream (symbol string, value int); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; This query retrieves hourly aggregations within the day 2014-02-15 . define stream StockStream (symbol string, value int); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within \"2014-02-15 **:**:** +05:30\" per \"hours\" select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; This query retrieves all aggregations per perValue stream attribute within the time period between timestamps 1496200000000 and 1596434876000 . define stream StockStream (symbol string, value int, perValue string); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within 1496200000000L, 1596434876000L per S.perValue select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; Supported join types Aggregation join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the aggregation. The output is generated only if there is a matching event in the stream and the aggregation. Left outer join The left outer join operation allows you to join a stream on left side with a aggregation on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right aggregation by having null values for the attributes of the right aggregation. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a aggregation on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left aggregation.","title":"Join (Aggregation)"},{"location":"docs/query-guide/#named-window","text":"A named window is a window that can be shared across multiple queries. Events can be inserted to a named window from one or more queries and it can produce output events based on the named window type. Syntax The syntax for a named window is as follows: define window window name ( attribute name attribute type , attribute name attribute type , ... ) window type ( parameter , parameter , \u2026) output event type ; The following parameters are configured in a table definition: Parameter Description window name The name of the window defined. ( PascalCase is used for window names as a convention.) attribute name The schema of the window is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . window type ( parameter , ...) The window type associated with the window and its parameters. output output event type This is optional. Keywords such as current events , expired events and all events (the default) can be used to specify when the window output should be exposed. For more information, see output event type . Examples Returning all output when events arrive and when events expire from the window. In this query, the output event type is not specified. Therefore, it returns both current and expired events as the output. define window SensorWindow (name string, value float, roomNo int, deviceID string) timeBatch(1 second); + Returning an output only when events expire from the window. In this query, the output event type of the window is `expired events`. Therefore, it only returns the events that have expired from the window as the output. define window SensorWindow (name string, value float, roomNo int, deviceID string) timeBatch(1 second) output expired events; Operators on Named Windows The following operators can be performed on named windows.","title":"Named Window"},{"location":"docs/query-guide/#insert_1","text":"This allows events to be inserted into windows. This is similar to inserting events into streams. Syntax from input stream select attribute name , attribute name , ... insert into window To insert only events of a specific output event type, add the current events , expired events or the all events keyword between insert and into keywords (similar to how it is done for streams). For more information, see output event type . Example This query inserts all events from the TempStream stream to the OneMinTempWindow window. define stream TempStream(tempId string, temp double); define window OneMinTempWindow(tempId string, temp double) time(1 min); from TempStream select * insert into OneMinTempWindow;","title":"Insert"},{"location":"docs/query-guide/#join-window","text":"To allow a stream to retrieve information from a window based on a condition. Note A join can also be performed with two streams , aggregation or with tables tables . Syntax from input stream join window on condition select ( input stream | window ). attribute name , ( input stream | window ). attribute name , ... insert into output stream Example This Siddhi Application performs a join count the number of temperature events having more then 40 degrees within the last 2 minutes. define window TwoMinTempWindow (roomNo int, temp double) time(2 min); define stream CheckStream (requestId string); from CheckStream as C join TwoMinTempWindow as T on T.temp 40 select requestId, count(T.temp) as count insert into HighTempCountStream; Supported join types Window join supports following operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join two windows or a stream with a window. The output is generated only if there is a matching event in both stream/window. Left outer join The left outer join operation allows you to join two windows or a stream with a window to be merged based on a condition. Here, it returns all the events of left stream/window even if there are no matching events in the right stream/window by having null values for the attributes of the right stream/window. Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join two windows or a stream with a window. It returns all the events of the right stream/window even if there are no matching events in the left stream/window. Full outer join The full outer join combines the results of left outer join and right outer join . full outer join is used as the keyword to join two windows or a stream with a window. Here, output event are generated for each incoming event even if there are no matching events in the other stream/window.","title":"Join (Window)"},{"location":"docs/query-guide/#from","text":"A window can be an input to a query, similar to streams. Note !!! When window is used as an input to a query, another window cannot be applied on top of this. Syntax from window select attribute name , attribute name , ... insert into output stream Example This Siddhi Application calculates the maximum temperature within the last 5 minutes. define window FiveMinTempWindow (roomNo int, temp double) time(5 min); from FiveMinTempWindow select max(temp) as maxValue, roomNo insert into MaxSensorReadingStream;","title":"From"},{"location":"docs/query-guide/#trigger","text":"Triggers allow events to be periodically generated. Trigger definition can be used to define a trigger. A trigger also works like a stream with a predefined schema. Purpose For some use cases the system should be able to periodically generate events based on a specified time interval to perform some periodic executions. A trigger can be performed for a 'start' operation, for a given time interval , or for a given ' cron expression ' . Syntax The syntax for a trigger definition is as follows. define trigger trigger name at ('start'| every time interval | ' cron expression '); Similar to streams, triggers can be used as inputs. They adhere to the following stream definition and produce the triggered_time attribute of the long type. define stream trigger name (triggered_time long); The following types of triggeres are currently supported: Trigger type Description 'start' An event is triggered when Siddhi is started. every time interval An event is triggered periodically at the given time interval. ' cron expression ' An event is triggered periodically based on the given cron expression. For configuration details, see quartz-scheduler . Examples Triggering events regularly at specific time intervals The following query triggers events every 5 minutes. define trigger FiveMinTriggerStream at every 5 min; Triggering events at a specific time on specified days The following query triggers an event at 10.15 AM on every weekdays. define trigger FiveMinTriggerStream at '0 15 10 ? * MON-FRI';","title":"Trigger"},{"location":"docs/query-guide/#script","text":"Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Function definitions can be used to define these scripts. Function parameters are passed into the function logic as Object[] and with the name data . Purpose Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Syntax The syntax for a Script definition is as follows. define function function name [ language name ] return return type { operation of the function }; The following parameters are configured when defining a script. Parameter Description function name The name of the function ( camelCase is used for the function name) as a convention. language name The name of the programming language used to define the script, such as javascript , r and scala . return type The attribute type of the function\u2019s return. This can be int , long , float , double , string , bool or object . Here the function implementer should be responsible for returning the output attribute on the defined return type for proper functionality. operation of the function Here, the execution logic of the function is added. This logic should be written in the language specified under the language name , and it should return the output in the data type specified via the return type parameter. Examples This query performs concatenation using JavaScript, and returns the output as a string. define function concatFn[javascript] return string { var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var responce = str1 + str2 + str3; return responce; }; define stream TempStream(deviceID long, roomNo int, temp double); from TempStream select concatFn(roomNo,'-',deviceID) as id, temp insert into DeviceTempStream;","title":"Script"},{"location":"docs/query-guide/#store-query","text":"Siddhi store queries are a set of on-demand queries that can be used to perform operations on Siddhi tables, windows, and aggregators. Purpose Store queries allow you to execute the following operations on Siddhi tables, windows, and aggregators without the intervention of streams. Queries supported for tables: SELECT INSERT DELETE UPDATE UPDATE OR INSERT Queries supported for windows and aggregators: SELECT This is be done by submitting the store query to the Siddhi application runtime using its query() method. In order to execute store queries, the Siddhi application of the Siddhi application runtime you are using, should have a store defined, which contains the table that needs to be queried. Example If you need to query the table named RoomTypeTable the it should have been defined in the Siddhi application. In order to execute a store query on RoomTypeTable , you need to submit the store query using query() method of SiddhiAppRuntime instance as below. siddhiAppRuntime.query( store query );","title":"Store Query"},{"location":"docs/query-guide/#tablewindow-select","text":"The SELECT store query retrieves records from the specified table or window, based on the given condition. Syntax from table/window on condition ? select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example This query retrieves room numbers and types of the rooms starting from room no 10. from roomTypeTable on roomNo = 10; select roomNo, type","title":"(Table/Window) Select"},{"location":"docs/query-guide/#aggregation-select","text":"The SELECT store query retrieves records from the specified aggregation, based on the given condition, time range, and granularity. Syntax from aggregation on condition ? within time range per time granularity select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example Following aggregation definition will be used for the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) from TradeAggregation within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select symbol, total, avgPrice ; This query retrieves hourly aggregations of \"FB\" symbol within the day 2014-02-15 . from TradeAggregation on symbol == \"FB\" within \"2014-02-15 **:**:** +05:30\" per \"hours\" select symbol, total, avgPrice;","title":"(Aggregation) Select"},{"location":"docs/query-guide/#insert_2","text":"This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax select attribute name , attribute name , ... insert into table ; Example This store query inserts a new record to the table RoomOccupancyTable , with the specified attribute values. select 10 as roomNo, 2 as people insert into RoomOccupancyTable","title":"Insert"},{"location":"docs/query-guide/#delete_1","text":"The DELETE store query deletes selected records from a specified table. Syntax select ? delete table on conditional expresssion The condition element specifies the basis on which records are selected to be deleted. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example In this example, query deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection which has 10 as the actual value. select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; delete RoomTypeTable on RoomTypeTable.roomNo == 10;","title":"Delete"},{"location":"docs/query-guide/#update_1","text":"The UPDATE store query updates selected attributes stored in a specific table, based on a given condition. Syntax select attribute name , attribute name , ...? update table set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition The condition element specifies the basis on which records are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query updates the room occupancy by increasing the value of people by 1, in the RoomOccupancyTable table for each room number greater than 10. select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber; update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + 1 on RoomTypeTable.roomNo == 10;","title":"Update"},{"location":"docs/query-guide/#update-or-insert_1","text":"This allows you to update selected attributes if a record that meets the given conditions already exists in the specified table. If a matching record does not exist, the entry is inserted as a new record. Syntax select attribute name , attribute name , ... update or insert into table set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which records are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query tries to update the records in the RoomAssigneeTable table that have room numbers that match the same in the selection. If such records are not found, it inserts a new record based on the values provided in the selection. select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;","title":"Update or Insert"},{"location":"docs/query-guide/#extensions","text":"Siddhi supports an extension architecture to enhance its functionality by incorporating other libraries in a seamless manner. Purpose Extensions are supported because, Siddhi core cannot have all the functionality that's needed for all the use cases, mostly use cases require different type of functionality, and for some cases there can be gaps and you need to write the functionality by yourself. All extensions have a namespace. This is used to identify the relevant extensions together, and to let you specifically call the extension. Syntax Extensions follow the following syntax; namespace : function name ( parameter , parameter , ... ) The following parameters are configured when referring a script function. Parameter Description namespace Allows Siddhi to identify the extension without conflict function name The name of the function referred. parameter The function input parameter for function execution. Extension Types Siddhi supports following extension types: Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute. This can be used to manipulate existing event attributes to generate new attributes like any Function operation. This is implemented by extending io.siddhi.core.executor.function.FunctionExecutor . Example : math:sin(x) Here, the sin function of math extension returns the sin value for the x parameter. Aggregate Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute with aggregated results. This can be used in conjunction with a window in order to find the aggregated results based on the given window like any Aggregate Function operation. This is implemented by extending io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor . Example : custom:std(x) Here, the std aggregate function of custom extension returns the standard deviation of the x value based on its assigned window query. Window This allows events to be collected, generated, dropped and expired anytime without altering the event format based on the given input parameters, similar to any other Window operator. This is implemented by extending io.siddhi.core.query.processor.stream.window.WindowProcessor . Example : custom:unique(key) Here, the unique window of the custom extension retains one event for each unique key parameter. Stream Function This allows events to be generated or dropped only during event arrival and altered by adding one or more attributes to it. This is implemented by extending io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor . Example : custom:pol2cart(theta,rho) Here, the pol2cart function of the custom extension returns all the events by calculating the cartesian coordinates x y and adding them as new attributes to the events. Stream Processor This allows events to be collected, generated, dropped and expired anytime by altering the event format by adding one or more attributes to it based on the given input parameters. Implemented by extending io.siddhi.core.query.processor.stream.StreamProcessor . Example : custom:perMinResults( parameter , parameter , ...) Here, the perMinResults function of the custom extension returns all events by adding one or more attributes to the events based on the conversion logic. Altered events are output every minute regardless of event arrivals. Sink Sinks provide a way to publish Siddhi events to external systems in the preferred data format. Sinks publish events from the streams via multiple transports to external endpoints in various data formats. Implemented by extending io.siddhi.core.stream.output.sink.Sink . Example : @sink(type='sink_type', static_option_key1='static_option_value1') To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as above Source Source allows Siddhi to consume events from external systems , and map the events to adhere to the associated stream. Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. Implemented by extending io.siddhi.core.stream.input.source.Source . Example : @source(type='source_type', static.option.key1='static_option_value1') To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as above Store You can use Store extension type to work with data/events stored in various data stores through the table abstraction . You can find more information about these extension types under the heading 'Extension types' in this document. Implemented by extending io.siddhi.core.table.record.AbstractRecordTable . Script Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Implemented by extending io.siddhi.core.function.Script . Source Mapper Each @source configuration has a mapping denoted by the @map annotation that converts the incoming messages format to Siddhi events .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SourceMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Sink Mapper Each @sink configuration has a mapping denoted by the @map annotation that converts the outgoing Siddhi events to configured messages format .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SinkMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Example A window extension created with namespace foo and function name unique can be referred as follows: from StockExchangeStream[price = 20]#window.foo:unique(symbol) select symbol, price insert into StockQuote Available Extensions Siddhi currently has several pre written extensions that are available here We value your contribution on improving Siddhi and its extensions further.","title":"Extensions"},{"location":"docs/query-guide/#writing-custom-extensions","text":"Custom extensions can be written in order to cater use case specific logic that are not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension types and the related maven archetypes are given below. You can use these archetypes to generate Maven projects for each extension type. Follow the procedure for the required archetype, based on your project: Note When using the generated archetype please make sure you complete the @Extension annotation with proper values. This annotation will be used to identify and document the extension, hence your extension will not work without @Extension annotation. siddhi-execution Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Extension Types . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DgroupId=io.siddhi.extension.execution -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfFunction Name of the custom function to be created Y - _nameSpaceOfFunction Namespace of the function, used to grouped similar custom functions Y - groupIdPostfix Namespace of the function is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-execution- classNameOfAggregateFunction Class name of the Aggregate Function N $ classNameOfFunction Class name of the Function N $ classNameOfStreamFunction Class name of the Stream Function N $ classNameOfStreamProcessor Class name of the Stream Processor N $ classNameOfWindow Class name of the Window N $ To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-io Siddhi-io provides following extension types: Sink Source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: * The Source extension type gets inputs to your Siddhi application. * The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DgroupId=io.siddhi.extension.io -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _IOType Type of IO for which Siddhi-io extension is written Y - groupIdPostfix Type of the IO is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-io- classNameOfSink Class name of the Sink N classNameOfSource Class name of the Source N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-map Siddhi-map provides following extension types, Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints (such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DgroupId=io.siddhi.extension.map -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _mapType Type of Mapper for which Siddhi-map extension is written Y - groupIdPostfix Type of the Map is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-map- classNameOfSinkMapper Class name of the Sink Mapper N classNameOfSourceMapper Class name of the Source Mapper N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-script Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Extension Types . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DgroupId=io.siddhi.extension.script -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfScript Name of Custom Script for which Siddhi-script extension is written Y - groupIdPostfix Name of the Script is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-script- classNameOfScript Class name of the Script N Eval To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-store Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Extension Types . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DgroupId=io.siddhi.extension.store -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _storeType Type of Store for which Siddhi-store extension is written Y - groupIdPostfix Type of the Store is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-store- className Class name of the Store N To confirm that all property values are correct, type Y in the console. If not, press N .","title":"Writing Custom Extensions"},{"location":"docs/query-guide/#configuring-and-monitoring-siddhi-applications","text":"","title":"Configuring and Monitoring Siddhi Applications"},{"location":"docs/query-guide/#multi-threading-and-asynchronous-processing","text":"When @Async annotation is added to the Streams it enable the Streams to introduce asynchronous and multi-threading behaviour. @Async(buffer.size='256', workers='2', batch.size.max='5') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following elements are configured with this annotation. Annotation Description Default Value buffer.size The size of the event buffer that will be used to handover the execution to other threads. - workers Number of worker threads that will be be used to process the buffered events. 1 batch.size.max The maximum number of events that will be processed together by a worker thread at a given time. buffer.size","title":"Multi-threading and Asynchronous Processing"},{"location":"docs/query-guide/#statistics","text":"Use @app:statistics app level annotation to evaluate the performance of an application, you can enable the statistics of a Siddhi application to be published. This is done via the @app:statistics annotation that can be added to a Siddhi application as shown in the following example. @app:statistics(reporter = 'console') The following elements are configured with this annotation. Annotation Description Default Value reporter The interface in which statistics for the Siddhi application are published. Possible values are as follows: console jmx console interval The time interval (in seconds) at which the statistics for the Siddhi application are reported. 60 include If this parameter is added, only the types of metrics you specify are included in the reporting. The required metric types can be specified as a comma-separated list. It is also possible to use wild cards All ( . ) The metrics are reported in the following format. io.siddhi.SiddhiApps. SiddhiAppName .Siddhi. Component Type . Component Name . Metrics name The following table lists the types of metrics supported for different Siddhi application component types. Component Type Metrics Type Stream Throughput The size of the buffer if parallel processing is enabled via the @async annotation. Trigger Throughput (Trigger and Stream) Source Throughput Sink Throughput Mapper Latency Input/output throughput Table Memory Throughput (For all operations) Throughput (For all operations) Query Memory Latency Window Throughput (For all operations) Latency (For all operation) Partition Throughput (For all operations) Latency (For all operation) e.g., the following is a Siddhi application that includes the @app annotation to report performance statistics. @App:name('TestMetrics') @App:Statistics(reporter = 'console') define stream TestStream (message string); @info(name='logQuery') from TestSream#log(\"Message:\") insert into TempSream; Statistics are reported for this Siddhi application as shown in the extract below. Click to view the extract 11/26/17 8:01:20 PM ============================================================ -- Gauges ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.memory value = 5760 io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.size value = 0 -- Meters ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Sources.TestStream.http.throughput count = 0 mean rate = 0.00 events/second 1-minute rate = 0.00 events/second 5-minute rate = 0.00 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TempSream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second -- Timers ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.latency count = 2 mean rate = 0.11 calls/second 1-minute rate = 0.34 calls/second 5-minute rate = 0.39 calls/second 15-minute rate = 0.40 calls/second min = 0.61 milliseconds max = 1.08 milliseconds mean = 0.84 milliseconds stddev = 0.23 milliseconds median = 0.61 milliseconds 75% < = 1.08 milliseconds 95% < = 1.08 milliseconds 98% < = 1.08 milliseconds 99% < = 1.08 milliseconds 99.9% < = 1.08 milliseconds","title":"Statistics"},{"location":"docs/query-guide/#event-playback","text":"When @app:playback annotation is added to the app, the timestamp of the event (specified via an attribute) is treated as the current time. This results in events being processed faster. The following elements are configured with this annotation. Annotation Description idle.time If no events are received during a time interval specified (in milliseconds) via this element, the Siddhi system time is incremented by a number of seconds specified via the increment element. increment The number of seconds by which the Siddhi system time must be incremented if no events are received during the time interval specified via the idle.time element. e.g., In the following example, the Siddhi system time is incremented by two seconds if no events arrive for a time interval of 100 milliseconds. @app:playback(idle.time = '100 millisecond', increment = '2 sec')","title":"Event Playback"},{"location":"docs/siddhi-as-a-docker-microservice/","text":"Siddhi 5.0 as a Docker Microservice This section provides information on running Siddhi Apps on Docker. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Docker Microservice is as follows. Pull the the latest Siddhi Runner image from Siddhiio Docker Hub . docker pull siddhiio/siddhi-runner-alpine:latest Start SiddhiApps with the runner config by executing the following docker command. docker run -it -v local-siddhi-file-path : siddhi-file-mount-path -v local-conf-file-path : conf-file-mount-path siddhiio/siddhi-runner-alpine:latest -Dapps= siddhi-file-mount-path -Dconfig= conf-file-mount-path E.g., docker run -it -v /home/me/siddhi-apps:/apps -v /home/me/siddhi-configs:/configs siddhiio/siddhi-runner-alpine:latest -Dapps=/apps/Foo.siddhi -Dconfig=/configs/siddhi-config.yaml Running multiple SiddhiApps in one runner instance. To run multiple SiddhiApps in one runtime instance, have all SiddhiApps in a directory, mount the directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Docker Microservice refer Siddhi Config Guide . Samples Running Siddhi App Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /CountOverTime.siddhi:/apps/CountOverTime.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false} Running with runner config When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can be configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following command docker run -it -p 8006:8006 -p 9443:9443 -v local-absolute-siddhi-file-path /ConsumeAndStore.siddhi:/apps/ConsumeAndStore.siddhi -v local-absolute-config-yaml-path /TestDb.yaml:/conf/TestDb.yaml siddhiio/siddhi-runner-alpine -Dapps=/apps/ConsumeAndStore.siddhi -Dconfig=/conf/TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] } Running with environmental/system variables Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set the below environment variables by passing them during the docker run command: THRESHOLD=20 TO_EMAIL= to email address EMAIL_ADDRESS= gmail address EMAIL_USERNAME= gmail username EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding them to the end of the docker run command . -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password Run the SiddhiApp by executing following command. docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /TemplatedFilterAndEmail.siddhi:/apps/TemplatedFilterAndEmail.siddhi -v local-absolute-config-yaml-path /EmailConfig.yaml:/conf/EmailConfig.yaml -e THRESHOLD=20 -e TO_EMAIL= to email address -e EMAIL_ADDRESS= gmail address -e EMAIL_USERNAME= gmail username -e EMAIL_PASSWORD= gmail password siddhiio/siddhi-runner-alpine -Dapps=/apps/TemplatedFilterAndEmail.siddhi -Dconfig=/conf/EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Siddhi Docker Microservice"},{"location":"docs/siddhi-as-a-docker-microservice/#siddhi-50-as-a-docker-microservice","text":"This section provides information on running Siddhi Apps on Docker. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Docker Microservice is as follows. Pull the the latest Siddhi Runner image from Siddhiio Docker Hub . docker pull siddhiio/siddhi-runner-alpine:latest Start SiddhiApps with the runner config by executing the following docker command. docker run -it -v local-siddhi-file-path : siddhi-file-mount-path -v local-conf-file-path : conf-file-mount-path siddhiio/siddhi-runner-alpine:latest -Dapps= siddhi-file-mount-path -Dconfig= conf-file-mount-path E.g., docker run -it -v /home/me/siddhi-apps:/apps -v /home/me/siddhi-configs:/configs siddhiio/siddhi-runner-alpine:latest -Dapps=/apps/Foo.siddhi -Dconfig=/configs/siddhi-config.yaml Running multiple SiddhiApps in one runner instance. To run multiple SiddhiApps in one runtime instance, have all SiddhiApps in a directory, mount the directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Docker Microservice refer Siddhi Config Guide .","title":"Siddhi 5.0 as a Docker Microservice"},{"location":"docs/siddhi-as-a-docker-microservice/#samples","text":"","title":"Samples"},{"location":"docs/siddhi-as-a-docker-microservice/#running-siddhi-app","text":"Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /CountOverTime.siddhi:/apps/CountOverTime.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false}","title":"Running Siddhi App"},{"location":"docs/siddhi-as-a-docker-microservice/#running-with-runner-config","text":"When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can be configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following command docker run -it -p 8006:8006 -p 9443:9443 -v local-absolute-siddhi-file-path /ConsumeAndStore.siddhi:/apps/ConsumeAndStore.siddhi -v local-absolute-config-yaml-path /TestDb.yaml:/conf/TestDb.yaml siddhiio/siddhi-runner-alpine -Dapps=/apps/ConsumeAndStore.siddhi -Dconfig=/conf/TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] }","title":"Running with runner config"},{"location":"docs/siddhi-as-a-docker-microservice/#running-with-environmentalsystem-variables","text":"Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set the below environment variables by passing them during the docker run command: THRESHOLD=20 TO_EMAIL= to email address EMAIL_ADDRESS= gmail address EMAIL_USERNAME= gmail username EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding them to the end of the docker run command . -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password Run the SiddhiApp by executing following command. docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /TemplatedFilterAndEmail.siddhi:/apps/TemplatedFilterAndEmail.siddhi -v local-absolute-config-yaml-path /EmailConfig.yaml:/conf/EmailConfig.yaml -e THRESHOLD=20 -e TO_EMAIL= to email address -e EMAIL_ADDRESS= gmail address -e EMAIL_USERNAME= gmail username -e EMAIL_PASSWORD= gmail password siddhiio/siddhi-runner-alpine -Dapps=/apps/TemplatedFilterAndEmail.siddhi -Dconfig=/conf/EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Running with environmental/system variables"},{"location":"docs/siddhi-as-a-java-library/","text":"Siddhi 5.0 as a Java library Siddhi can be used as a library in any Java program (including in OSGi runtimes) just by adding Siddhi and its extension jars as dependencies. Find a sample Siddhi project that's implemented as a Java program using Maven here , this can be used as a reference for any based implementation. Following are the mandatory dependencies that need to be added to the Maven pom.xml file (or to the program classpath). dependency groupId io.siddhi /groupId artifactId siddhi-core /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-api /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-compiler /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-annotations /artifactId version 5.x.x /version /dependency Sample Sample Java class using Siddhi is as follows.","title":"Siddhi Java library"},{"location":"docs/siddhi-as-a-java-library/#siddhi-50-as-a-java-library","text":"Siddhi can be used as a library in any Java program (including in OSGi runtimes) just by adding Siddhi and its extension jars as dependencies. Find a sample Siddhi project that's implemented as a Java program using Maven here , this can be used as a reference for any based implementation. Following are the mandatory dependencies that need to be added to the Maven pom.xml file (or to the program classpath). dependency groupId io.siddhi /groupId artifactId siddhi-core /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-api /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-compiler /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-annotations /artifactId version 5.x.x /version /dependency","title":"Siddhi 5.0 as a Java library"},{"location":"docs/siddhi-as-a-java-library/#sample","text":"Sample Java class using Siddhi is as follows.","title":"Sample"},{"location":"docs/siddhi-as-a-kubernetes-microservice/","text":"Siddhi 5.0 as a Kubernetes Microservice This section provides information on running Siddhi Apps natively in Kubernetes via Siddhi Kubernetes Operator. Siddhi can be configured in SiddhiProcess kind and passed to the CRD for deployment. Here, the Siddhi applications containing stream processing logic can be written inline in SiddhiProcess yaml or passed as .siddhi files via configmaps. SiddhiProcess yaml can also be configured with the necessary system configurations. Prerequisites A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster Admin privileges to install Siddhi operator Minikube Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine (GKE) Cluster To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \"your-address@email.com\" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com Docker for Mac Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation . Install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.1.1/prerequisites.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.1.1/siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE siddhi-operator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m Deploy and run Siddhi App Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Here we will creating a very simple Siddhi stream processing application that consumes events via HTTP, filers the input events on the type 'monitored' and logs the output on the console. This can be created using a SiddhiProcess YAML file as given below. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Kubernetes Microservice refer Siddhi Config Guide . To deploy the above Siddhi app in your Kubernetes cluster, copy to a YAML file with name monitor-app.yaml and execute the following command. kubectl create -f absolute-yaml-file-path /monitor-app.yaml tls secret Within the SiddhiProcess, a tls secret named siddhi-tls is configured. If a Kubernetes secret with the same name does not exist in the Kubernetes cluster, the NGINX will ignore it and use a self-generated certificate. Configuring a secret will be necessary for calling HTTPS endpoints, refer deploy and run Siddhi apps with HTTPS section for more details. If the monitor-app is deployed successfully, the created SiddhiProcess, deployment, service, and ingress can be viewed as follows. $ kubectl get SiddhiProcesses NAME AGE monitor-app 2m $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE monitor-app 1 1 1 1 1m siddhi-operator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 10d monitor-app ClusterIP 10.101.242.132 none 8280/TCP 1m siddhi-operator ClusterIP 10.111.138.250 none 8383/TCP 1m siddhi-parser LoadBalancer 10.102.172.142 pending 9090:31830/TCP 1m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Invoke Siddhi Applications To invoke the Siddhi App, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Docker for Mac For Docker for Mac, you have to use 0.0.0.0 as the external IP. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ https://siddhi/monitor-app/8280/example \\ -H 'Content-Type: application/json' \\ -d '{ \"type\": \"monitored\", \"deviceID\": \"001\", \"power\": 341 }' -k Note: Here -k option is used to turn off curl's verification of the certificate. View Siddhi Process Logs Since the output of monitor-app is logged, you can see the output by monitoring the associated pod's logs. To find the monitor-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Here, the pod starting with the SiddhiProcess name (in this case monitor-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs monitor-app-7f8584875f-krz6t [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false} Get Siddhi process status List Siddhi processes List the Siddhi process using the kubectl get sps or kubectl get SiddhiProcesses commands as follows. $ kubectl get sps NAME AGE monitor-app 2m $ kubectl get SiddhiProcesses NAME AGE monitor-app 2m View Siddhi process configs Get the Siddhi process configuration details using kubectl describe sp command as follows. $ kubectl describe sp monitor-app Name: monitor-app Namespace: default Labels: none Annotations: none API Version: siddhi.io/v1alpha1 Kind: SiddhiProcess Metadata: Creation Timestamp: 2019-04-18T18:05:39Z Generation: 1 Resource Version: 497702 Self Link: /apis/siddhi.io/v1alpha1/namespaces/default/siddhiprocesses/monitor-app UID: 92b2293b-6204-11e9-996c-0800279e6dba Spec: Env: Name: RECEIVER_URL Value: http://0.0.0.0:8280/example Name: BASIC_AUTH_ENABLED Value: false Pod: Image: siddhiio/siddhi-runner-alpine Image Tag: 0.1.0 Query: @App:name(\"MonitorApp\") @App:description(\"Description of the plan\") @sink(type='log', prefix='LOGGER') @source(type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='${BASIC_AUTH_ENABLED}', @map(type='json')) define stream DevicePowerStream (type string, deviceID string, power int); define stream MonitorDevicesPowerStream(deviceID string, power int); @info(name='monitored-filter') from DevicePowerStream[type == 'monitored'] select deviceID, power insert into MonitorDevicesPowerStream; Siddhi . Runner . Configs: state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Status: Nodes: nil Status: Running Events: none Get the Siddhi process YAML using kubectl get sp command as follows. $ kubectl get sp monitor-app -o yaml apiVersion: siddhi.io/v1alpha1 kind: SiddhiProcess metadata: creationTimestamp: 2019-04-18T18:05:39Z generation: 1 name: monitor-app namespace: default resourceVersion: \"497702\" selfLink: /apis/siddhi.io/v1alpha1/namespaces/default/siddhiprocesses/monitor-app uid: 92b2293b-6204-11e9-996c-0800279e6dba spec: env: - name: RECEIVER_URL value: http://0.0.0.0:8280/example - name: BASIC_AUTH_ENABLED value: \"false\" pod: image: siddhiio/siddhi-runner-alpine imageTag: 0.1.0 query: \"@App:name(\\\"MonitorApp\\\")\\n@App:description(\\\"Description of the plan\\\") \\n\\n@sink(type='log', prefix='LOGGER')\\n@source(type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='${BASIC_AUTH_ENABLED}', @map(type='json'))\\ndefine stream DevicePowerStream (type string, deviceID string, power int);\\n\\n\\ndefine stream MonitorDevicesPowerStream(deviceID string, power int);\\n\\n@info(name='monitored-filter')\\nfrom DevicePowerStream[type == 'monitored']\\nselect deviceID, power\\ninsert into MonitorDevicesPowerStream;\\n\" siddhi.runner.configs: | state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence status: nodes: null status: Running View Siddhi process logs To view the Siddhi process logs, first get the Siddhi process pods using the kubectl get pods command as follows. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Then to retrieve the Siddhi process logs, run kubectl logs pod name command. Here pod name should be replaced with the name of the pod that starts with the relevant SiddhiProcess's name. A sample output logs is of this command is as follows. $ kubectl logs monitor-app-7f8584875f-krz6t JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0 RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-04-20 3:58:57,734] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner osgi [2019-04-20 03:59:00,208] INFO {org.wso2.carbon.config.reader.ConfigFileReader} - Default deployment configuration updated with provided custom configuration file monitor-app-deployment.yaml [2019-04-20 03:59:01,551] INFO {org.wso2.msf4j.internal.websocket.WebSocketServerSC} - All required capabilities are available of WebSocket service component is available. [2019-04-20 03:59:01,584] INFO {org.wso2.carbon.metrics.core.config.model.JmxReporterConfig} - Creating JMX reporter for Metrics with domain 'org.wso2.carbon.metrics' [2019-04-20 03:59:01,609] INFO {org.wso2.msf4j.analytics.metrics.MetricsComponent} - Metrics Component is activated [2019-04-20 03:59:01,614] INFO {org.wso2.carbon.databridge.agent.internal.DataAgentDS} - Successfully deployed Agent Server [2019-04-20 03:59:02,219] INFO {io.siddhi.distribution.core.internal.ServiceComponent} - Periodic state persistence started with an interval of 5 using io.siddhi.distribution.core.persistence.FileSystemPersistenceStore [2019-04-20 03:59:02,229] INFO {io.siddhi.distribution.event.simulator.core.service.CSVFileDeployer} - CSV file deployer initiated. [2019-04-20 03:59:02,233] INFO {io.siddhi.distribution.event.simulator.core.service.SimulationConfigDeployer} - Simulation config deployer initiated. [2019-04-20 03:59:02,279] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiverServiceComponent} - org.wso2.carbon.databridge.receiver.binary.internal.Service Component is activated [2019-04-20 03:59:02,312] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary SSL Transport on port : 9712 [2019-04-20 03:59:02,321] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary TCP Transport on port : 9612 [2019-04-20 03:59:02,322] INFO {org.wso2.carbon.databridge.receiver.thrift.internal.ThriftDataReceiverDS} - Service Component is activated [2019-04-20 03:59:02,344] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift Server started at 0.0.0.0 [2019-04-20 03:59:02,356] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift SSL port : 7711 [2019-04-20 03:59:02,363] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift port : 7611 [2019-04-20 03:59:02,449] INFO {org.wso2.msf4j.internal.MicroservicesServerSC} - All microservices are available [2019-04-20 03:59:02,516] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-04-20 03:59:02,520] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-04-20 03:59:03,068] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Periodic State persistence enabled. Restoring last persisted state of MonitorApp [2019-04-20 03:59:03,075] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 8280 [2019-04-20 03:59:03,077] INFO {org.wso2.extension.siddhi.io.http.source.HttpConnectorPortBindingListener} - HTTP source 0.0.0.0:8280 has been started [2019-04-20 03:59:03,084] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Siddhi App MonitorApp deployed successfully [2019-04-20 03:59:03,093] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 5.941 sec [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false} Deploy and run Siddhi App using configmaps Siddhi operator allows you to deploy Siddhi app configurations via configmaps instead of just adding them inline. Through this you can also run multiple Siddhi Apps in a single SiddhiProcess. This can be done by passing the configmaps containing Siddhi app files to the SiddhiProcess's apps configuration as follows. apps: - config-map-name1 - config-map-name2 Sample on deploying and running Siddhi Apps via configmaps Here we will creating a very simple Siddhi application as follows, that consumes events via HTTP, filers the input events on type 'monitored' and logs the output on the console. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Save the above Siddhi App file as MonitorApp.siddhi , and use this file to create a Kubernetes config map with the name monitor-app-cm . This can be achieved by running the following command. kubectl create configmap monitor-app-cm --from-file= absolute-file-path /MonitorApp.siddhi The created config map can be added to SiddhiProcess YAML under the apps entry as follows. Save the YAML file as monitor-app.yaml , and use the following command to deploy the SiddhiProcess. kubectl create -f absolute-yaml-file-path /monitor-app.yaml Using a config, created from a directory containing multiple Siddhi files SiddhiProcess's apps configuration also supports a config map that is created from a directory containing multiple Siddhi files. Use kubectl create configmap siddhi-apps --from-file= DIRECTORY_PATH command to create a config map from a directory. Invoke Siddhi Applications To invoke the Siddhi Apps, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ https://siddhi/monitor-app/8280/example \\ -H 'Content-Type: application/json' \\ -d '{ \"type\": \"monitored\", \"deviceID\": \"001\", \"power\": 341 }' -k Note: Here -k option is used to turn off curl's verification of the certificate. View Siddhi Process Logs Since the output of monitor-app is logged, you can see the output by monitoring the associated pod's logs. To find the monitor-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Here, the pod starting with the SiddhiProcess name (in this case monitor-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs monitor-app-7f8584875f-krz6t [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false} Deploy Siddhi Apps without Ingress creation By default, Siddhi operator creates an NGINX ingress and exposes your HTTP/HTTPS through that ingress. If you need to disable automatic ingress creation, you have to change the AUTO_INGRESS_CREATION value in the Siddhi operator.yaml file to false or null as below. Deploy and run Siddhi App with HTTPS Configuring tls will allow Siddhi ingress NGINX to expose HTTPS endpoints of your Siddhi Apps. To do so, created a Kubernetes secret and add that to the SiddhiProcess's tls configuration as following. tls: ingressSecret: siddhi-tls Sample on deploying and running Siddhi App with HTTPS First, you need to create a certificate using the following commands. For more details about the certificate creation refers this . openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout siddhi.key -out siddhi.crt -subj \"/CN=siddhi/O=siddhi\" After that, create a kubernetes secret called siddhi-tls , which we intended to add to the TLS configurations using the following command. kubectl create secret tls siddhi-tls --key siddhi.key --cert siddhi.crt The created secret then need to be added to the created SiddhiProcess's tls configuration as following. When this is done Siddhi operator will now enable TLS support via the NGINX ingress, and you will be able to access all the HTTPS endpoints. Invoke Siddhi Applications You can use now send the events to following HTTPS endpoint. https://siddhi/monitor-app/8280/example Further, you can use the following CURL command to send a request to the deployed siddhi applications via HTTPS. curl --cacert siddhi.crt -X POST \\ https://siddhi/monitor-app/8280/example \\ -H 'Content-Type: application/json' \\ -d '{ \"type\": \"monitored\", \"deviceID\": \"001\", \"power\": 341 }' View Siddhi Process Logs The output logs show the event that you sent using the previous CURL command. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-667c97c898-rrtfs 1/1 Running 0 2m siddhi-operator-79dcc45959-fkk4d 1/1 Running 0 3m siddhi-parser-64d4cd86ff-k8b87 1/1 Running 0 3m $ kubectl logs monitor-app-667c97c898-rrtfs JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0 RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-05-06 5:36:54,894] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner osgi [2019-05-06 05:36:57,692] INFO {org.wso2.msf4j.internal.websocket.WebSocketServerSC} - All required capabilities are available of WebSocket service component is available. [2019-05-06 05:36:57,749] INFO {org.wso2.carbon.metrics.core.config.model.JmxReporterConfig} - Creating JMX reporter for Metrics with domain 'org.wso2.carbon.metrics' [2019-05-06 05:36:57,779] INFO {org.wso2.msf4j.analytics.metrics.MetricsComponent} - Metrics Component is activated [2019-05-06 05:36:57,784] INFO {org.wso2.carbon.databridge.agent.internal.DataAgentDS} - Successfully deployed Agent Server [2019-05-06 05:36:58,292] INFO {io.siddhi.distribution.event.simulator.core.service.CSVFileDeployer} - CSV file deployer initiated. [2019-05-06 05:36:58,295] INFO {io.siddhi.distribution.event.simulator.core.service.SimulationConfigDeployer} - Simulation config deployer initiated. [2019-05-06 05:36:58,331] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiverServiceComponent} - org.wso2.carbon.databridge.receiver.binary.internal.Service Component is activated [2019-05-06 05:36:58,342] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary SSL Transport on port : 9712 [2019-05-06 05:36:58,343] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary TCP Transport on port : 9612 [2019-05-06 05:36:58,343] INFO {org.wso2.carbon.databridge.receiver.thrift.internal.ThriftDataReceiverDS} - Service Component is activated [2019-05-06 05:36:58,360] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift Server started at 0.0.0.0 [2019-05-06 05:36:58,369] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift SSL port : 7711 [2019-05-06 05:36:58,371] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift port : 7611 [2019-05-06 05:36:58,466] INFO {org.wso2.msf4j.internal.MicroservicesServerSC} - All microservices are available [2019-05-06 05:36:58,567] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-05-06 05:36:58,574] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-05-06 05:36:59,091] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 8280 [2019-05-06 05:36:59,092] INFO {org.wso2.extension.siddhi.io.http.source.HttpConnectorPortBindingListener} - HTTP source 0.0.0.0:8280 has been started [2019-05-06 05:36:59,093] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Siddhi App MonitorApp deployed successfully [2019-05-06 05:36:59,100] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 4.710 sec [2019-05-06 05:39:33,804] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1557121173802, data=[monitored, 001, 341], isExpired=false}","title":"Siddhi Kubernetes Microservice"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#siddhi-50-as-a-kubernetes-microservice","text":"This section provides information on running Siddhi Apps natively in Kubernetes via Siddhi Kubernetes Operator. Siddhi can be configured in SiddhiProcess kind and passed to the CRD for deployment. Here, the Siddhi applications containing stream processing logic can be written inline in SiddhiProcess yaml or passed as .siddhi files via configmaps. SiddhiProcess yaml can also be configured with the necessary system configurations.","title":"Siddhi 5.0 as a Kubernetes Microservice"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#prerequisites","text":"A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster Admin privileges to install Siddhi operator Minikube Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine (GKE) Cluster To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \"your-address@email.com\" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com Docker for Mac Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation .","title":"Prerequisites"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#install-siddhi-operator","text":"To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.1.1/prerequisites.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.1.1/siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE siddhi-operator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m","title":"Install Siddhi Operator"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app","text":"Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Here we will creating a very simple Siddhi stream processing application that consumes events via HTTP, filers the input events on the type 'monitored' and logs the output on the console. This can be created using a SiddhiProcess YAML file as given below. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Kubernetes Microservice refer Siddhi Config Guide . To deploy the above Siddhi app in your Kubernetes cluster, copy to a YAML file with name monitor-app.yaml and execute the following command. kubectl create -f absolute-yaml-file-path /monitor-app.yaml tls secret Within the SiddhiProcess, a tls secret named siddhi-tls is configured. If a Kubernetes secret with the same name does not exist in the Kubernetes cluster, the NGINX will ignore it and use a self-generated certificate. Configuring a secret will be necessary for calling HTTPS endpoints, refer deploy and run Siddhi apps with HTTPS section for more details. If the monitor-app is deployed successfully, the created SiddhiProcess, deployment, service, and ingress can be viewed as follows. $ kubectl get SiddhiProcesses NAME AGE monitor-app 2m $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE monitor-app 1 1 1 1 1m siddhi-operator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 10d monitor-app ClusterIP 10.101.242.132 none 8280/TCP 1m siddhi-operator ClusterIP 10.111.138.250 none 8383/TCP 1m siddhi-parser LoadBalancer 10.102.172.142 pending 9090:31830/TCP 1m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Invoke Siddhi Applications To invoke the Siddhi App, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Docker for Mac For Docker for Mac, you have to use 0.0.0.0 as the external IP. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ https://siddhi/monitor-app/8280/example \\ -H 'Content-Type: application/json' \\ -d '{ \"type\": \"monitored\", \"deviceID\": \"001\", \"power\": 341 }' -k Note: Here -k option is used to turn off curl's verification of the certificate. View Siddhi Process Logs Since the output of monitor-app is logged, you can see the output by monitoring the associated pod's logs. To find the monitor-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Here, the pod starting with the SiddhiProcess name (in this case monitor-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs monitor-app-7f8584875f-krz6t [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false}","title":"Deploy and run Siddhi App"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#get-siddhi-process-status","text":"","title":"Get Siddhi process status"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#list-siddhi-processes","text":"List the Siddhi process using the kubectl get sps or kubectl get SiddhiProcesses commands as follows. $ kubectl get sps NAME AGE monitor-app 2m $ kubectl get SiddhiProcesses NAME AGE monitor-app 2m","title":"List Siddhi processes"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#view-siddhi-process-configs","text":"Get the Siddhi process configuration details using kubectl describe sp command as follows. $ kubectl describe sp monitor-app Name: monitor-app Namespace: default Labels: none Annotations: none API Version: siddhi.io/v1alpha1 Kind: SiddhiProcess Metadata: Creation Timestamp: 2019-04-18T18:05:39Z Generation: 1 Resource Version: 497702 Self Link: /apis/siddhi.io/v1alpha1/namespaces/default/siddhiprocesses/monitor-app UID: 92b2293b-6204-11e9-996c-0800279e6dba Spec: Env: Name: RECEIVER_URL Value: http://0.0.0.0:8280/example Name: BASIC_AUTH_ENABLED Value: false Pod: Image: siddhiio/siddhi-runner-alpine Image Tag: 0.1.0 Query: @App:name(\"MonitorApp\") @App:description(\"Description of the plan\") @sink(type='log', prefix='LOGGER') @source(type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='${BASIC_AUTH_ENABLED}', @map(type='json')) define stream DevicePowerStream (type string, deviceID string, power int); define stream MonitorDevicesPowerStream(deviceID string, power int); @info(name='monitored-filter') from DevicePowerStream[type == 'monitored'] select deviceID, power insert into MonitorDevicesPowerStream; Siddhi . Runner . Configs: state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Status: Nodes: nil Status: Running Events: none Get the Siddhi process YAML using kubectl get sp command as follows. $ kubectl get sp monitor-app -o yaml apiVersion: siddhi.io/v1alpha1 kind: SiddhiProcess metadata: creationTimestamp: 2019-04-18T18:05:39Z generation: 1 name: monitor-app namespace: default resourceVersion: \"497702\" selfLink: /apis/siddhi.io/v1alpha1/namespaces/default/siddhiprocesses/monitor-app uid: 92b2293b-6204-11e9-996c-0800279e6dba spec: env: - name: RECEIVER_URL value: http://0.0.0.0:8280/example - name: BASIC_AUTH_ENABLED value: \"false\" pod: image: siddhiio/siddhi-runner-alpine imageTag: 0.1.0 query: \"@App:name(\\\"MonitorApp\\\")\\n@App:description(\\\"Description of the plan\\\") \\n\\n@sink(type='log', prefix='LOGGER')\\n@source(type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='${BASIC_AUTH_ENABLED}', @map(type='json'))\\ndefine stream DevicePowerStream (type string, deviceID string, power int);\\n\\n\\ndefine stream MonitorDevicesPowerStream(deviceID string, power int);\\n\\n@info(name='monitored-filter')\\nfrom DevicePowerStream[type == 'monitored']\\nselect deviceID, power\\ninsert into MonitorDevicesPowerStream;\\n\" siddhi.runner.configs: | state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence status: nodes: null status: Running","title":"View Siddhi process configs"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#view-siddhi-process-logs","text":"To view the Siddhi process logs, first get the Siddhi process pods using the kubectl get pods command as follows. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Then to retrieve the Siddhi process logs, run kubectl logs pod name command. Here pod name should be replaced with the name of the pod that starts with the relevant SiddhiProcess's name. A sample output logs is of this command is as follows. $ kubectl logs monitor-app-7f8584875f-krz6t JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0 RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-04-20 3:58:57,734] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner osgi [2019-04-20 03:59:00,208] INFO {org.wso2.carbon.config.reader.ConfigFileReader} - Default deployment configuration updated with provided custom configuration file monitor-app-deployment.yaml [2019-04-20 03:59:01,551] INFO {org.wso2.msf4j.internal.websocket.WebSocketServerSC} - All required capabilities are available of WebSocket service component is available. [2019-04-20 03:59:01,584] INFO {org.wso2.carbon.metrics.core.config.model.JmxReporterConfig} - Creating JMX reporter for Metrics with domain 'org.wso2.carbon.metrics' [2019-04-20 03:59:01,609] INFO {org.wso2.msf4j.analytics.metrics.MetricsComponent} - Metrics Component is activated [2019-04-20 03:59:01,614] INFO {org.wso2.carbon.databridge.agent.internal.DataAgentDS} - Successfully deployed Agent Server [2019-04-20 03:59:02,219] INFO {io.siddhi.distribution.core.internal.ServiceComponent} - Periodic state persistence started with an interval of 5 using io.siddhi.distribution.core.persistence.FileSystemPersistenceStore [2019-04-20 03:59:02,229] INFO {io.siddhi.distribution.event.simulator.core.service.CSVFileDeployer} - CSV file deployer initiated. [2019-04-20 03:59:02,233] INFO {io.siddhi.distribution.event.simulator.core.service.SimulationConfigDeployer} - Simulation config deployer initiated. [2019-04-20 03:59:02,279] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiverServiceComponent} - org.wso2.carbon.databridge.receiver.binary.internal.Service Component is activated [2019-04-20 03:59:02,312] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary SSL Transport on port : 9712 [2019-04-20 03:59:02,321] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary TCP Transport on port : 9612 [2019-04-20 03:59:02,322] INFO {org.wso2.carbon.databridge.receiver.thrift.internal.ThriftDataReceiverDS} - Service Component is activated [2019-04-20 03:59:02,344] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift Server started at 0.0.0.0 [2019-04-20 03:59:02,356] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift SSL port : 7711 [2019-04-20 03:59:02,363] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift port : 7611 [2019-04-20 03:59:02,449] INFO {org.wso2.msf4j.internal.MicroservicesServerSC} - All microservices are available [2019-04-20 03:59:02,516] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-04-20 03:59:02,520] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-04-20 03:59:03,068] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Periodic State persistence enabled. Restoring last persisted state of MonitorApp [2019-04-20 03:59:03,075] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 8280 [2019-04-20 03:59:03,077] INFO {org.wso2.extension.siddhi.io.http.source.HttpConnectorPortBindingListener} - HTTP source 0.0.0.0:8280 has been started [2019-04-20 03:59:03,084] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Siddhi App MonitorApp deployed successfully [2019-04-20 03:59:03,093] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 5.941 sec [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false}","title":"View Siddhi process logs"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-using-configmaps","text":"Siddhi operator allows you to deploy Siddhi app configurations via configmaps instead of just adding them inline. Through this you can also run multiple Siddhi Apps in a single SiddhiProcess. This can be done by passing the configmaps containing Siddhi app files to the SiddhiProcess's apps configuration as follows. apps: - config-map-name1 - config-map-name2 Sample on deploying and running Siddhi Apps via configmaps Here we will creating a very simple Siddhi application as follows, that consumes events via HTTP, filers the input events on type 'monitored' and logs the output on the console. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Save the above Siddhi App file as MonitorApp.siddhi , and use this file to create a Kubernetes config map with the name monitor-app-cm . This can be achieved by running the following command. kubectl create configmap monitor-app-cm --from-file= absolute-file-path /MonitorApp.siddhi The created config map can be added to SiddhiProcess YAML under the apps entry as follows. Save the YAML file as monitor-app.yaml , and use the following command to deploy the SiddhiProcess. kubectl create -f absolute-yaml-file-path /monitor-app.yaml Using a config, created from a directory containing multiple Siddhi files SiddhiProcess's apps configuration also supports a config map that is created from a directory containing multiple Siddhi files. Use kubectl create configmap siddhi-apps --from-file= DIRECTORY_PATH command to create a config map from a directory. Invoke Siddhi Applications To invoke the Siddhi Apps, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ https://siddhi/monitor-app/8280/example \\ -H 'Content-Type: application/json' \\ -d '{ \"type\": \"monitored\", \"deviceID\": \"001\", \"power\": 341 }' -k Note: Here -k option is used to turn off curl's verification of the certificate. View Siddhi Process Logs Since the output of monitor-app is logged, you can see the output by monitoring the associated pod's logs. To find the monitor-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Here, the pod starting with the SiddhiProcess name (in this case monitor-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs monitor-app-7f8584875f-krz6t [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false}","title":"Deploy and run Siddhi App using configmaps"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-siddhi-apps-without-ingress-creation","text":"By default, Siddhi operator creates an NGINX ingress and exposes your HTTP/HTTPS through that ingress. If you need to disable automatic ingress creation, you have to change the AUTO_INGRESS_CREATION value in the Siddhi operator.yaml file to false or null as below.","title":"Deploy Siddhi Apps without Ingress creation"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-with-https","text":"Configuring tls will allow Siddhi ingress NGINX to expose HTTPS endpoints of your Siddhi Apps. To do so, created a Kubernetes secret and add that to the SiddhiProcess's tls configuration as following. tls: ingressSecret: siddhi-tls Sample on deploying and running Siddhi App with HTTPS First, you need to create a certificate using the following commands. For more details about the certificate creation refers this . openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout siddhi.key -out siddhi.crt -subj \"/CN=siddhi/O=siddhi\" After that, create a kubernetes secret called siddhi-tls , which we intended to add to the TLS configurations using the following command. kubectl create secret tls siddhi-tls --key siddhi.key --cert siddhi.crt The created secret then need to be added to the created SiddhiProcess's tls configuration as following. When this is done Siddhi operator will now enable TLS support via the NGINX ingress, and you will be able to access all the HTTPS endpoints. Invoke Siddhi Applications You can use now send the events to following HTTPS endpoint. https://siddhi/monitor-app/8280/example Further, you can use the following CURL command to send a request to the deployed siddhi applications via HTTPS. curl --cacert siddhi.crt -X POST \\ https://siddhi/monitor-app/8280/example \\ -H 'Content-Type: application/json' \\ -d '{ \"type\": \"monitored\", \"deviceID\": \"001\", \"power\": 341 }' View Siddhi Process Logs The output logs show the event that you sent using the previous CURL command. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-667c97c898-rrtfs 1/1 Running 0 2m siddhi-operator-79dcc45959-fkk4d 1/1 Running 0 3m siddhi-parser-64d4cd86ff-k8b87 1/1 Running 0 3m $ kubectl logs monitor-app-667c97c898-rrtfs JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0 RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-05-06 5:36:54,894] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner osgi [2019-05-06 05:36:57,692] INFO {org.wso2.msf4j.internal.websocket.WebSocketServerSC} - All required capabilities are available of WebSocket service component is available. [2019-05-06 05:36:57,749] INFO {org.wso2.carbon.metrics.core.config.model.JmxReporterConfig} - Creating JMX reporter for Metrics with domain 'org.wso2.carbon.metrics' [2019-05-06 05:36:57,779] INFO {org.wso2.msf4j.analytics.metrics.MetricsComponent} - Metrics Component is activated [2019-05-06 05:36:57,784] INFO {org.wso2.carbon.databridge.agent.internal.DataAgentDS} - Successfully deployed Agent Server [2019-05-06 05:36:58,292] INFO {io.siddhi.distribution.event.simulator.core.service.CSVFileDeployer} - CSV file deployer initiated. [2019-05-06 05:36:58,295] INFO {io.siddhi.distribution.event.simulator.core.service.SimulationConfigDeployer} - Simulation config deployer initiated. [2019-05-06 05:36:58,331] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiverServiceComponent} - org.wso2.carbon.databridge.receiver.binary.internal.Service Component is activated [2019-05-06 05:36:58,342] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary SSL Transport on port : 9712 [2019-05-06 05:36:58,343] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary TCP Transport on port : 9612 [2019-05-06 05:36:58,343] INFO {org.wso2.carbon.databridge.receiver.thrift.internal.ThriftDataReceiverDS} - Service Component is activated [2019-05-06 05:36:58,360] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift Server started at 0.0.0.0 [2019-05-06 05:36:58,369] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift SSL port : 7711 [2019-05-06 05:36:58,371] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift port : 7611 [2019-05-06 05:36:58,466] INFO {org.wso2.msf4j.internal.MicroservicesServerSC} - All microservices are available [2019-05-06 05:36:58,567] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-05-06 05:36:58,574] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-05-06 05:36:59,091] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 8280 [2019-05-06 05:36:59,092] INFO {org.wso2.extension.siddhi.io.http.source.HttpConnectorPortBindingListener} - HTTP source 0.0.0.0:8280 has been started [2019-05-06 05:36:59,093] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Siddhi App MonitorApp deployed successfully [2019-05-06 05:36:59,100] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 4.710 sec [2019-05-06 05:39:33,804] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1557121173802, data=[monitored, 001, 341], isExpired=false}","title":"Deploy and run Siddhi App with HTTPS"},{"location":"docs/siddhi-as-a-local-microservice/","text":"Siddhi 5.0 as a Local Microservice This section provides information on running Siddhi Apps on Bare Metal or VM. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Local Microservice is as follows. Download the latest Siddhi Runner distribution Unzip the siddhi-runner-x.x.x.zip Start SiddhiApps with the runner config by executing the following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file -Dconfig= config-yaml-file Windows : bin\\runner.bat -Dapps= siddhi-file -Dconfig= config-yaml-file Running Multiple SiddhiApps in one runner. To run multiple SiddhiApps in one runtime, have all SiddhiApps in a directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Local Microservice refer Siddhi Config Guide . Samples Running Siddhi App Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /CountOverTime.siddhi Windows : bin\\runner.bat -Dapps= absolute-siddhi-file-path \\CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false} Running with runner config When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can by configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /ConsumeAndStore.siddhi \\ -Dconfig= absolute-config-yaml-path /TestDb.yaml Windows : bin\\runner.sh -Dapps= absolute-siddhi-file-path \\ConsumeAndStore.siddhi ^ -Dconfig= absolute-config-yaml-path \\TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] } Running with environmental/system variables Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set environment variables by running following in the termial Siddhi is about to run: export THRESHOLD=20 export TO_EMAIL= to email address export EMAIL_ADDRESS= gmail address export EMAIL_USERNAME= gmail username export EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password to the end of the runner startup script. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-file-path /TemplatedFilterAndEmail.siddhi \\ -Dconfig= absolute-config-yaml-path /EmailConfig.yaml Windows : bin\\runner.bat -Dapps= absolute-file-path \\TemplatedFilterAndEmail.siddhi ^ -Dconfig= absolute-config-yaml-path \\EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Siddhi Local Microservice"},{"location":"docs/siddhi-as-a-local-microservice/#siddhi-50-as-a-local-microservice","text":"This section provides information on running Siddhi Apps on Bare Metal or VM. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Local Microservice is as follows. Download the latest Siddhi Runner distribution Unzip the siddhi-runner-x.x.x.zip Start SiddhiApps with the runner config by executing the following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file -Dconfig= config-yaml-file Windows : bin\\runner.bat -Dapps= siddhi-file -Dconfig= config-yaml-file Running Multiple SiddhiApps in one runner. To run multiple SiddhiApps in one runtime, have all SiddhiApps in a directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Local Microservice refer Siddhi Config Guide .","title":"Siddhi 5.0 as a Local Microservice"},{"location":"docs/siddhi-as-a-local-microservice/#samples","text":"","title":"Samples"},{"location":"docs/siddhi-as-a-local-microservice/#running-siddhi-app","text":"Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /CountOverTime.siddhi Windows : bin\\runner.bat -Dapps= absolute-siddhi-file-path \\CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false}","title":"Running Siddhi App"},{"location":"docs/siddhi-as-a-local-microservice/#running-with-runner-config","text":"When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can by configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /ConsumeAndStore.siddhi \\ -Dconfig= absolute-config-yaml-path /TestDb.yaml Windows : bin\\runner.sh -Dapps= absolute-siddhi-file-path \\ConsumeAndStore.siddhi ^ -Dconfig= absolute-config-yaml-path \\TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] }","title":"Running with runner config"},{"location":"docs/siddhi-as-a-local-microservice/#running-with-environmentalsystem-variables","text":"Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set environment variables by running following in the termial Siddhi is about to run: export THRESHOLD=20 export TO_EMAIL= to email address export EMAIL_ADDRESS= gmail address export EMAIL_USERNAME= gmail username export EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password to the end of the runner startup script. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-file-path /TemplatedFilterAndEmail.siddhi \\ -Dconfig= absolute-config-yaml-path /EmailConfig.yaml Windows : bin\\runner.bat -Dapps= absolute-file-path \\TemplatedFilterAndEmail.siddhi ^ -Dconfig= absolute-config-yaml-path \\EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Running with environmental/system variables"},{"location":"docs/tooling/","text":"Siddhi 5.0 Tooling Siddhi Editor Siddhi provides tooling that supports following features to develop and test stream processing applications: Text Query Editor with syntax highlighting and advanced auto completion support. Event Simulator and Debugger to test Siddhi Applications. Graphical Query Editor with drag and drop query building support. Graphical Query Editor Text Query Editor","title":"Tooling"},{"location":"docs/tooling/#siddhi-50-tooling","text":"","title":"Siddhi 5.0 Tooling"},{"location":"docs/tooling/#siddhi-editor","text":"Siddhi provides tooling that supports following features to develop and test stream processing applications: Text Query Editor with syntax highlighting and advanced auto completion support. Event Simulator and Debugger to test Siddhi Applications. Graphical Query Editor with drag and drop query building support. Graphical Query Editor Text Query Editor","title":"Siddhi Editor"},{"location":"docs/api/5.0.0/","text":"API Docs - v5.0.0 Core and (Aggregate Function) Returns the results of AND operation for all the events. Origin: siddhi-core:5.0.0 Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) Calculates the average for all the events. Origin: siddhi-core:5.0.0 Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) Returns the count of all the events. Origin: siddhi-core:5.0.0 Syntax LONG count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) This returns the count of distinct occurrences for a given arg. Origin: siddhi-core:5.0.0 Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) Returns the maximum value for all the events. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) Returns the minimum value for all the events. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) Returns the results of OR operation for all the events. Origin: siddhi-core:5.0.0 Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) Returns the calculated standard deviation for all the events. Origin: siddhi-core:5.0.0 Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) Returns the sum for all the events. Origin: siddhi-core:5.0.0 Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Origin: siddhi-core:5.0.0 Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) Generates a UUID (Universally Unique Identifier). Origin: siddhi-core:5.0.0 Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) Converts the first input parameter according to the convertedTo parameter. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) Includes the given input parameter in a java.util.HashSet and returns the set. Origin: siddhi-core:5.0.0 Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) Returns the current timestamp of siddhi application in milliseconds. Origin: siddhi-core:5.0.0 Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) Returns the timestamp of the processed event. Origin: siddhi-core:5.0.0 Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) Checks whether the parameter is an instance of Boolean or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) Checks whether the parameter is an instance of Double or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) Checks whether the parameter is an instance of Float or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) Checks whether the parameter is an instance of Integer or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) Checks whether the parameter is an instance of Long or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) Checks whether the parameter is an instance of String or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) Returns the maximum value of the input parameters. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) Returns the minimum value of the input parameters. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) Returns the size of an object of type java.util.Set. Origin: siddhi-core:5.0.0 Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Origin: siddhi-core:5.0.0 Syntax pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) The logger logs the message on the given priority with or without processed event. Origin: siddhi-core:5.0.0 Syntax log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log(\"INFO\", \"Sample Event :\", true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log(\"Sample Event :\", true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log(\"Sample Event :\", fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log(\"Sample Event :\") select * insert into barStream; This will log message and fooStream:events. batch (Window) A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Origin: siddhi-core:5.0.0 Syntax batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Origin: siddhi-core:5.0.0 Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) A delay window holds events for a specific time period that is regarded as a delay period before processing them. Origin: siddhi-core:5.0.0 Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Origin: siddhi-core:5.0.0 Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Origin: siddhi-core:5.0.0 Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Origin: siddhi-core:5.0.0 Syntax frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Origin: siddhi-core:5.0.0 Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Origin: siddhi-core:5.0.0 Syntax lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Origin: siddhi-core:5.0.0 Syntax lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Origin: siddhi-core:5.0.0 Syntax session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Origin: siddhi-core:5.0.0 Syntax sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Origin: siddhi-core:5.0.0 Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Origin: siddhi-core:5.0.0 Syntax timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Origin: siddhi-core:5.0.0 Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Json getBool (Function) This method returns a 'boolean' value, either 'true' or 'false', based on the valuespecified against the JSON element present in the given path.In case there is no valid boolean value found in the given path, the method still returns 'false'. Origin: siddhi-execution-json:2.0.0 Syntax BOOL json:getBool( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the boolean value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getBool' function fetches theboolean value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getBool(json,\"$.name\") as name insert into OutputStream; This returns the boolean value of the JSON input in the given path. The results are directed to the 'OutputStream' stream. getDouble (Function) This method returns the double value of the JSON element present in the given path. If there is no valid double value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax DOUBLE json:getDouble( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getDouble' function fetches thedouble value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getDouble(json,\"$.name\") as name insert into OutputStream; This returns the double value of the given path. The results aredirected to the 'OutputStream' stream. getFloat (Function) This method returns the float value of the JSON element present in the given path.If there is no valid float value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax FLOAT json:getFloat( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getFloat' function fetches thevalue. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getFloat(json,\"$.name\") as name insert into OutputStream; This returns the float value of the JSON input in the given path. The results aredirected to the 'OutputStream' stream. getInt (Function) This method returns the integer value of the JSON element present in the given path. If there is no valid integer value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax INT json:getInt( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getInt' function fetches theinteger value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getInt(json,\"$.name\") as name insert into OutputStream; This returns the integer value of the JSON input in the given path. The resultsare directed to the 'OutputStream' stream. getLong (Function) This returns the long value of the JSON element present in the given path. Ifthere is no valid long value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax LONG json:getLong( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the JSON element from which the 'getLong' functionfetches the long value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getLong(json,\"$.name\") as name insert into OutputStream; This returns the long value of the JSON input in the given path. The results aredirected to 'OutputStream' stream. getObject (Function) This returns the object of the JSON element present in the given path. Origin: siddhi-execution-json:2.0.0 Syntax OBJECT json:getObject( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getObject' function fetches theobject. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getObject(json,\"$.name\") as name insert into OutputStream; This returns the object of the JSON input in the given path. The results are directed to the 'OutputStream' stream. getString (Function) This returns the string value of the JSON element present in the given path. Origin: siddhi-execution-json:2.0.0 Syntax STRING json:getString( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the JSON input from which the 'getString' function fetches the string value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getString(json,\"$.name\") as name insert into OutputStream; This returns the string value of the JSON input in the given path. The results are directed to the 'OutputStream' stream. isExists (Function) This method checks whether there is a JSON element present in the given path or not.If there is a valid JSON element in the given path, it returns 'true'. If there is no valid JSON element, it returns 'false' Origin: siddhi-execution-json:2.0.0 Syntax BOOL json:isExists( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input in a given path, on which the function performs the search forJSON elements. STRING OBJECT No No path The path that contains the input JSON on which the function performs the search. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:isExists(json,\"$.name\") as name insert into OutputStream; This returns either true or false based on the existence of a JSON element in a given path. The results are directed to the 'OutputStream' stream. setElement (Function) This method allows to insert elements into a given JSON present in a specific path. If there is no valid path given, it returns the original JSON. Otherwise, it returns the new JSON. Origin: siddhi-execution-json:2.0.0 Syntax OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT jsonelement, STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input into which is this function inserts the new value. STRING OBJECT No No path The path on the JSON input which is used to insert the given element. STRING No No jsonelement The JSON element which is inserted by the function into the input JSON. STRING BOOL DOUBLE FLOAT INT LONG OBJECT No No key The key which is used to insert the given element into the input JSON. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:setElement(json,\"$.name\") as name insert into OutputStream; This returns the JSON object present in the given path with the newly inserted JSONelement. The results are directed to the 'OutputStream' stream. toObject (Function) This method returns the JSON object related to a given JSON string. Origin: siddhi-execution-json:2.0.0 Syntax OBJECT json:toObject( STRING json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON string from which the function generates the JSON object. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:toJson(json) as jsonObject insert into OutputStream; This returns the JSON object corresponding to the given JSON string.The results aredirected to the 'OutputStream' stream. toString (Function) This method returns the JSON string corresponding to a given JSON object. Origin: siddhi-execution-json:2.0.0 Syntax STRING json:toString( OBJECT json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON object from which the function generates a JSON string. OBJECT No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:toString(json) as jsonString insert into OutputStream; This returns the JSON string corresponding to a given JSON object. The results are directed to the 'OutputStream' stream. tokenize (Stream Processor) This tokenizes the given json according the path provided Origin: siddhi-execution-json:2.0.0 Syntax json:tokenize( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input json that should be tokenized using the given path. STRING OBJECT No No path The path that is used to tokenize the given json STRING No No fail.on.missing.attribute If this parameter is set to 'true' and a json is not provided in the given path, the event is dropped. If the parameter is set to 'false', the unavailability of a json in the specified path results in the event being created with a 'null' value for the json element. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The json element retrieved based on the given path and the json. STRING Examples EXAMPLE 1 define stream InputStream (json string,path string); @info(name = 'query1') from InputStream#json:tokenize(json, path) select jsonElement insert into OutputStream; This query performs a tokenization for the given json using the path specified. If the specified path provides a json array, it generates events for each element in that array by adding an additional attributes as the 'jsonElement' to the stream. e.g., jsonInput - {name:\"John\",enrolledSubjects:[\"Mathematics\",\"Physics\"]}, path - \" .enrolledSubjects\" /code br If we use the configuration in this example, it generates two events with the attributes \"Mathematics\", \"Physics\". br If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream. br code e.g., jsonInput - {name:\"John\",age:25}, path - \" .enrolledSubjects\" </code><br>&nbsp;If we use the configuration in this example, it generates two events with the attributes \"Mathematics\", \"Physics\".<br>If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream. <br><code> e.g., jsonInput - {name:\"John\",age:25}, path - \" .age\" tokenizeAsObject (Stream Processor) This tokenizes the given JSON based on the path provided and returns the response as an object. Origin: siddhi-execution-json:2.0.0 Syntax json:tokenizeAsObject( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input json that is tokenized using the given path. STRING OBJECT No No path The path of the input JSON that the function tokenizes. STRING No No fail.on.missing.attribute If this parameter is set to 'true' and a JSON is not provided in the given path, the event is dropped. If the parameter is set to 'false', the unavailability of a JSON in the specified path results in the event being created with a 'null' value for the json element. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path and the JSON. OBJECT Examples EXAMPLE 1 define stream InputStream (json string,path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select jsonElement insert into OutputStream; This query performs a tokenization for the given JSON using the path specified. If the specified path provides a JSON array, it generates events for each element in the specified json array by adding an additional attribute as the 'jsonElement' into the stream. e.g., jsonInput - {name:\"John\",enrolledSubjects:[\"Mathematics\",\"Physics\"]}, path - \" .enrolledSubjects\" /code br If we use the configuration in the above example, it generates two events with the attributes \"Mathematics\" and \"Physics\". br If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream br code e.g., jsonInput - {name:\"John\",age:25}, path - \" .enrolledSubjects\" </code><br>If we use the configuration in the above example, it generates two events with the attributes \"Mathematics\" and \"Physics\".<br>If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream <br><code> e.g., jsonInput - {name:\"John\",age:25}, path - \" .age\" Math percentile (Aggregate Function) This functions returns the pth percentile value of a given argument. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:percentile( INT|LONG|FLOAT|DOUBLE arg, DOUBLE p) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value of the parameter whose percentile should be found. INT LONG FLOAT DOUBLE No No p Estimate of the percentile to be found (pth percentile) where p is any number greater than 0 or lesser than or equal to 100. DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (sensorId int, temperature double); from InValueStream select math:percentile(temperature, 97.0) as percentile insert into OutMediationStream; This function returns the percentile value based on the argument given. For example, math:percentile(temperature, 97.0) returns the 97 th percentile value of all the temperature events. abs (Function) This function returns the absolute value of the given parameter. It wraps the java.lang.Math.abs() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:abs( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The parameter whose absolute value is found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:abs(inValue) as absValue insert into OutMediationStream; Irrespective of whether the 'invalue' in the input stream holds a value of abs(3) or abs(-3),the function returns 3 since the absolute value of both 3 and -3 is 3. The result directed to OutMediationStream stream. acos (Function) If -1 = p1 = 1, this function returns the arc-cosine (inverse cosine) value of p1.If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.acos() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:acos( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-cosine (inverse cosine) value is found. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:acos(inValue) as acosValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-cosine value of it and returns the arc-cosine value to the output stream, OutMediationStream. For example, acos(0.5) returns 1.0471975511965979. asin (Function) If -1 = p1 = 1, this function returns the arc-sin (inverse sine) value of p1. If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.asin() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:asin( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-sin (inverse sine) value is found. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:asin(inValue) as asinValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-sin value of it and returns the arc-sin value to the output stream, OutMediationStream. For example, asin(0.5) returns 0.5235987755982989. atan (Function) 1. If a single p1 is received, this function returns the arc-tangent (inverse tangent) value of p1 . 2. If p1 is received along with an optional p1 , it considers them as x and y coordinates and returns the arc-tangent (inverse tangent) value. The returned value is in radian scale. This function wraps the java.lang.Math.atan() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-tangent (inverse tangent) is found. If the optional second parameter is given this represents the x coordinate of the (x,y) coordinate pair. INT LONG FLOAT DOUBLE No No p1 This optional parameter represents the y coordinate of the (x,y) coordinate pair. 0D INT LONG FLOAT DOUBLE Yes No Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:atan(inValue1, inValue2) as convertedValue insert into OutMediationStream; If the 'inValue1' in the input stream is given, the function calculates the arc-tangent value of it and returns the arc-tangent value to the output stream, OutMediationStream. If both the 'inValue1' and 'inValue2' are given, then the function considers them to be x and y coordinates respectively and returns the calculated arc-tangent value to the output stream, OutMediationStream. For example, atan(12d, 5d) returns 1.1760052070951352. bin (Function) This function returns a string representation of the p1 argument, that is of either 'integer' or 'long' data type, as an unsigned integer in base 2. It wraps the java.lang.Integer.toBinaryString and java.lang.Long.toBinaryString` methods. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:bin( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value in either 'integer' or 'long', that should be converted into an unsigned integer of base 2. INT LONG No No Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:bin(inValue) as binValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function converts it into an unsigned integer in base 2 and directs the output to the output stream, OutMediationStream. For example, bin(9) returns '1001'. cbrt (Function) This function returns the cube-root of 'p1' which is in radians. It wraps the java.lang.Math.cbrt() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:cbrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cube-root should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cbrt(inValue) as cbrtValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cube-root value for the same and directs the output to the output stream, OutMediationStream. For example, cbrt(17d) returns 2.5712815906582356. ceil (Function) This function returns the smallest double value, i.e., the closest to the negative infinity, that is greater than or equal to the p1 argument, and is equal to a mathematical integer. It wraps the java.lang.Math.ceil() method. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:ceil( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose ceiling value is found. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ceil(inValue) as ceilingValue insert into OutMediationStream; This function calculates the ceiling value of the given 'inValue' and directs the result to 'OutMediationStream' output stream. For example, ceil(423.187d) returns 424.0. conv (Function) This function converts a from the fromBase base to the toBase base. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:conv( STRING a, INT from.base, INT to.base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic a The value whose base should be changed. Input should be given as a 'String'. STRING No No from.base The source base of the input parameter 'a'. INT No No to.base The target base that the input parameter 'a' should be converted into. INT No No Examples EXAMPLE 1 define stream InValueStream (inValue string,fromBase int,toBase int); from InValueStream select math:conv(inValue,fromBase,toBase) as convertedValue insert into OutMediationStream; If the 'inValue' in the input stream is given, and the base in which it currently resides in and the base to which it should be converted to is specified, the function converts it into a string in the target base and directs it to the output stream, OutMediationStream. For example, conv(\"7f\", 16, 10) returns \"127\". copySign (Function) This function returns a value of an input with the received magnitude and sign of another input. It wraps the java.lang.Math.copySign() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:copySign( INT|LONG|FLOAT|DOUBLE magnitude, INT|LONG|FLOAT|DOUBLE sign) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic magnitude The magnitude of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No No sign The sign of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:copySign(inValue1,inValue2) as copysignValue insert into OutMediationStream; If two values are provided as 'inValue1' and 'inValue2', the function copies the magnitude and sign of the second argument into the first one and directs the result to the output stream, OutMediatonStream. For example, copySign(5.6d, -3.0d) returns -5.6. cos (Function) This function returns the cosine of p1 which is in radians. It wraps the java.lang.Math.cos() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:cos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cosine value should be found.The input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cos(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cos(6d) returns 0.9601702866503661. cosh (Function) This function returns the hyperbolic cosine of p1 which is in radians. It wraps the java.lang.Math.cosh() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:cosh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic cosine should be found. The input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cosh(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the hyperbolic cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cosh (6d) returns 201.7156361224559. e (Function) This function returns the java.lang.Math.E constant, which is the closest double value to e, where e is the base of the natural logarithms. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:e() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:e() as eValue insert into OutMediationStream; This function returns the constant, 2.7182818284590452354 which is the closest double value to e and directs the output to 'OutMediationStream' output stream. exp (Function) This function returns the Euler's number e raised to the power of p1 . It wraps the java.lang.Math.exp() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:exp( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The power that the Euler's number e is raised to. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:exp(inValue) as expValue insert into OutMediationStream; If the 'inValue' in the inputstream holds a value, this function calculates the corresponding Euler's number 'e' and directs it to the output stream, OutMediationStream. For example, exp(10.23) returns 27722.51006805505. floor (Function) This function wraps the java.lang.Math.floor() function and returns the largest value, i.e., closest to the positive infinity, that is less than or equal to p1 , and is equal to a mathematical integer. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:floor( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose floor value should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:floor(inValue) as floorValue insert into OutMediationStream; This function calculates the floor value of the given 'inValue' input and directs the output to the 'OutMediationStream' output stream. For example, (10.23) returns 10.0. getExponent (Function) This function returns the unbiased exponent that is used in the representation of p1 . This function wraps the java.lang.Math.getExponent() function. Origin: siddhi-execution-math:5.0.0 Syntax INT math:getExponent( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of whose unbiased exponent representation should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:getExponent(inValue) as expValue insert into OutMediationStream; This function calculates the unbiased exponent of a given input, 'inValue' and directs the result to the 'OutMediationStream' output stream. For example, getExponent(60984.1) returns 15. hex (Function) This function wraps the java.lang.Double.toHexString() function. It returns a hexadecimal string representation of the input, p1`. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:hex( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hexadecimal value should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue int); from InValueStream select math:hex(inValue) as hexString insert into OutMediationStream; If the 'inValue' in the input stream is provided, the function converts this into its corresponding hexadecimal format and directs the output to the output stream, OutMediationStream. For example, hex(200) returns \"c8\". isInfinite (Function) This function wraps the java.lang.Float.isInfinite() and java.lang.Double.isInfinite() and returns true if p1 is infinitely large in magnitude and false if otherwise. Origin: siddhi-execution-math:5.0.0 Syntax BOOL math:isInfinite( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 This is the value of the parameter that the function determines to be either infinite or finite. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isInfinite(inValue1) as isInfinite insert into OutMediationStream; If the value given in the 'inValue' in the input stream is of infinitely large magnitude, the function returns the value, 'true' and directs the result to the output stream, OutMediationStream'. For example, isInfinite(java.lang.Double.POSITIVE_INFINITY) returns true. isNan (Function) This function wraps the java.lang.Float.isNaN() and java.lang.Double.isNaN() functions and returns true if p1 is NaN (Not-a-Number), and returns false if otherwise. Origin: siddhi-execution-math:5.0.0 Syntax BOOL math:isNan( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter which the function determines to be either NaN or a number. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isNan(inValue1) as isNaN insert into OutMediationStream; If the 'inValue1' in the input stream has a value that is undefined, then the function considers it as an 'NaN' value and directs 'True' to the output stream, OutMediationStream. For example, isNan(java.lang.Math.log(-12d)) returns true. ln (Function) This function returns the natural logarithm (base e) of p1 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:ln( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose natural logarithm (base e) should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ln(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates its natural logarithm (base e) and directs the results to the output stream, 'OutMeditionStream'. For example, ln(11.453) returns 2.438251704415579. log (Function) This function returns the logarithm of the received number as per the given base . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:log( INT|LONG|FLOAT|DOUBLE number, INT|LONG|FLOAT|DOUBLE base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic number The value of the parameter whose base should be changed. INT LONG FLOAT DOUBLE No No base The base value of the ouput. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (number double, base double); from InValueStream select math:log(number, base) as logValue insert into OutMediationStream; If the number and the base to which it has to be converted into is given in the input stream, the function calculates the number to the base specified and directs the result to the output stream, OutMediationStream. For example, log(34, 2f) returns 5.08746284125034. log10 (Function) This function returns the base 10 logarithm of p1 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:log10( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 10 logarithm should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log10(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 10 logarithm of the same and directs the result to the output stream, OutMediatioStream. For example, log10(19.234) returns 1.2840696117100832. log2 (Function) This function returns the base 2 logarithm of p1 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:log2( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 2 logarithm should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log2(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 2 logarithm of the same and returns the value to the output stream, OutMediationStream. For example log2(91d) returns 6.507794640198696. max (Function) This function returns the greater value of p1 and p2 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:max( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values to be compared in order to find the larger value of the two INT LONG FLOAT DOUBLE No No p2 The input value to be compared with 'p1' in order to find the larger value of the two. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:max(inValue1,inValue2) as maxValue insert into OutMediationStream; If two input values 'inValue1, and 'inValue2' are given, the function compares them and directs the larger value to the output stream, OutMediationStream. For example, max(123.67d, 91) returns 123.67. min (Function) This function returns the smaller value of p1 and p2 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:min( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values that are to be compared in order to find the smaller value. INT LONG FLOAT DOUBLE No No p2 The input value that is to be compared with 'p1' in order to find the smaller value. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:min(inValue1,inValue2) as minValue insert into OutMediationStream; If two input values, 'inValue1' and 'inValue2' are given, the function compares them and directs the smaller value of the two to the output stream, OutMediationStream. For example, min(123.67d, 91) returns 91. oct (Function) This function converts the input parameter p1 to octal. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:oct( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose octal representation should be found. INT LONG No No Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:oct(inValue) as octValue insert into OutMediationStream; If the 'inValue' in the input stream is given, this function calculates the octal value corresponding to the same and directs it to the output stream, OutMediationStream. For example, oct(99l) returns \"143\". parseDouble (Function) This function returns the double value of the string received. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:parseDouble( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a double value. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseDouble(inValue) as output insert into OutMediationStream; If the 'inValue' in the input stream holds a value, this function converts it into the corresponding double value and directs it to the output stream, OutMediationStream. For example, parseDouble(\"123\") returns 123.0. parseFloat (Function) This function returns the float value of the received string. Origin: siddhi-execution-math:5.0.0 Syntax FLOAT math:parseFloat( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a float value. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseFloat(inValue) as output insert into OutMediationStream; The function converts the input value given in 'inValue',into its corresponding float value and directs the result into the output stream, OutMediationStream. For example, parseFloat(\"123\") returns 123.0. parseInt (Function) This function returns the integer value of the received string. Origin: siddhi-execution-math:5.0.0 Syntax INT math:parseInt( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to an integer. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseInt(inValue) as output insert into OutMediationStream; The function converts the 'inValue' into its corresponding integer value and directs the output to the output stream, OutMediationStream. For example, parseInt(\"123\") returns 123. parseLong (Function) This function returns the long value of the string received. Origin: siddhi-execution-math:5.0.0 Syntax LONG math:parseLong( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to a long value. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseLong(inValue) as output insert into OutMediationStream; The function converts the 'inValue' to its corresponding long value and directs the result to the output stream, OutMediationStream. For example, parseLong(\"123\") returns 123. pi (Function) This function returns the java.lang.Math.PI constant, which is the closest value to pi, i.e., the ratio of the circumference of a circle to its diameter. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:pi() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:pi() as piValue insert into OutMediationStream; pi() always returns 3.141592653589793. power (Function) This function raises the given value to a given power. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:power( INT|LONG|FLOAT|DOUBLE value, INT|LONG|FLOAT|DOUBLE to.power) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value The value that should be raised to the power of 'to.power' input parameter. INT LONG FLOAT DOUBLE No No to.power The power to which the 'value' input parameter should be raised. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:power(inValue1,inValue2) as powerValue insert into OutMediationStream; This function raises the 'inValue1' to the power of 'inValue2' and directs the output to the output stream, 'OutMediationStream. For example, (5.6d, 3.0d) returns 175.61599999999996. rand (Function) This returns a stream of pseudo-random numbers when a sequence of calls are sent to the rand() . Optionally, it is possible to define a seed, i.e., rand(seed) using which the pseudo-random numbers are generated. These functions internally use the java.util.Random class. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:rand( INT|LONG seed) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic seed An optional seed value that will be used to generate the random number sequence. defaultSeed INT LONG Yes No Examples EXAMPLE 1 define stream InValueStream (symbol string, price long, volume long); from InValueStream select symbol, math:rand() as randNumber select math:oct(inValue) as octValue insert into OutMediationStream; In the example given above, a random double value between 0 and 1 will be generated using math:rand(). round (Function) This function returns the value of the input argument rounded off to the closest integer/long value. Origin: siddhi-execution-math:5.0.0 Syntax INT|LONG math:round( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be rounded off to the closest integer/long value. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:round(inValue) as roundValue insert into OutMediationStream; The function rounds off 'inValue1' to the closest int/long value and directs the output to the output stream, 'OutMediationStream'. For example, round(3252.353) returns 3252. signum (Function) This returns +1, 0, or -1 for the given positive, zero and negative values respectively. This function wraps the java.lang.Math.signum() function. Origin: siddhi-execution-math:5.0.0 Syntax INT math:signum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be checked to be positive, negative or zero. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:signum(inValue) as sign insert into OutMediationStream; The function evaluates the 'inValue' given to be positive, negative or zero and directs the result to the output stream, 'OutMediationStream'. For example, signum(-6.32d) returns -1. sin (Function) This returns the sine of the value given in radians. This function wraps the java.lang.Math.sin() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:sin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sin(inValue) as sinValue insert into OutMediationStream; The function calculates the sine value of the given 'inValue' and directs the output to the output stream, 'OutMediationStream. For example, sin(6d) returns -0.27941549819892586. sinh (Function) This returns the hyperbolic sine of the value given in radians. This function wraps the java.lang.Math.sinh() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:sinh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sinh(inValue) as sinhValue insert into OutMediationStream; This function calculates the hyperbolic sine value of 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sinh(6d) returns 201.71315737027922. sqrt (Function) This function returns the square-root of the given value. It wraps the java.lang.Math.sqrt() s function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:sqrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose square-root value should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sqrt(inValue) as sqrtValue insert into OutMediationStream; The function calculates the square-root value of the 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sqrt(4d) returns 2. tan (Function) This function returns the tan of the given value in radians. It wraps the java.lang.Math.tan() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:tan( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose tan value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tan(inValue) as tanValue insert into OutMediationStream; This function calculates the tan value of the 'inValue' given and directs the output to the output stream, 'OutMediationStream'. For example, tan(6d) returns -0.29100619138474915. tanh (Function) This function returns the hyperbolic tangent of the value given in radians. It wraps the java.lang.Math.tanh() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:tanh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic tangent value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tanh(inValue) as tanhValue insert into OutMediationStream; If the 'inVaue' in the input stream is given, this function calculates the hyperbolic tangent value of the same and directs the output to 'OutMediationStream' stream. For example, tanh(6d) returns 0.9999877116507956. toDegrees (Function) This function converts the value given in radians to degrees. It wraps the java.lang.Math.toDegrees() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:toDegrees( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in radians that should be converted to degrees. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toDegrees(inValue) as degreesValue insert into OutMediationStream; The function converts the 'inValue' in the input stream from radians to degrees and directs the output to 'OutMediationStream' output stream. For example, toDegrees(6d) returns 343.77467707849394. toRadians (Function) This function converts the value given in degrees to radians. It wraps the java.lang.Math.toRadians() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:toRadians( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in degrees that should be converted to radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toRadians(inValue) as radiansValue insert into OutMediationStream; This function converts the input, from degrees to radians and directs the result to 'OutMediationStream' output stream. For example, toRadians(6d) returns 0.10471975511965977. Rdbms cud (Stream Processor) This function performs SQL CUD (INSERT, UPDATE, DELETE) queries on WSO2 datasources. Note: This function is only available when running Siddhi with WSO2 SP. Origin: siddhi-store-rdbms:6.0.0 Syntax rdbms:cud( STRING datasource.name, STRING query, STRING parameter.n) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the WSO2 datasource for which the query should be performed. STRING No No query The update, delete, or insert query(formatted according to the relevant database type) that needs to be performed. STRING No No parameter.n If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING No No System Parameters Name Description Default Value Possible Parameters perform.CUD.operations If this parameter is set to 'true', the RDBMS CUD function is enabled to perform CUD operations. false true false Extra Return Attributes Name Description Possible Types numRecords The number of records manipulated by the query. INT Examples EXAMPLE 1 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName='abc' where customerName='xyz'\") select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. EXAMPLE 2 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName=? where customerName=?\", changedName, previousName) select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. Here the values of attributes changedName and previousName in the event will be set to the query. query (Stream Processor) This function performs SQL retrieval queries on WSO2 datasources. Note: This function is only available when running Siddhi with WSO2 SP. Origin: siddhi-store-rdbms:6.0.0 Syntax rdbms:query( STRING datasource.name, STRING query, STRING parameter.n, STRING attribute.definition.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the WSO2 datasource for which the query should be performed. STRING No No query The select query(formatted according to the relevant database type) that needs to be performed STRING No No parameter.n If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING No No attribute.definition.list This is provided as a comma-separated list in the ' AttributeName AttributeType ' format. The SQL query is expected to return the attributes in the given order. e.g., If one attribute is defined here, the SQL query should return one column result set. If more than one column is returned, then the first column is processed. The Siddhi data types supported are 'STRING', 'INT', 'LONG', 'DOUBLE', 'FLOAT', and 'BOOL'. Mapping of the Siddhi data type to the database data type can be done as follows, Siddhi Datatype - Datasource Datatype STRING - CHAR , VARCHAR , LONGVARCHAR INT - INTEGER LONG - BIGINT DOUBLE - DOUBLE FLOAT - REAL BOOL - BIT STRING No No Extra Return Attributes Name Description Possible Types attributeName The return attributes will be the ones defined in the parameter attribute.definition.list . STRING INT LONG DOUBLE FLOAT BOOL Examples EXAMPLE 1 from TriggerStream#rdbms:query('SAMPLE_DB', 'select * from Transactions_Table', 'creditcardno string, country string, transaction string, amount int') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). EXAMPLE 2 from TriggerStream#rdbms:query('SAMPLE_DB', 'select * from where country=? ', countrySearchWord, 'creditcardno string, country string, transaction string, amount int') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). countrySearchWord value from the event will be set in the query when querying the datasource. Regex find (Function) These methods attempt to find the subsequence of the 'inputSequence' that matches the given 'regex' pattern. Origin: siddhi-execution-regex:5.0.0 Syntax BOOL regex:find( STRING regex, STRING input.sequence, INT starting.index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression that is matched to a sequence in order to find the subsequence of the same. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No starting.index The starting index of the input sequence from where the input sequence ismatched with the given regex pattern. eg: 1, 2. INT No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string); from InputStream select inputSequence , regex:find(\\d\\d(.*)WSO2, 21 products are produced by WSO2 currently) as aboutWSO2 insert into OutputStream; This method attempts to find the subsequence of the 'inputSequence' that matches the regex pattern, \\d\\d(.*)WSO2. It returns true as a subsequence exists. EXAMPLE 2 define stream InputStream (inputSequence string, price long, regex string); from InputStream select inputSequence , regex:find(\\d\\d(.*)WSO2, 21 products are produced currently) as aboutWSO2 insert into OutputStream; This method attempts to find the subsequence of the 'inputSequence' that matches the regex pattern, \\d\\d(.*)WSO2 . It returns 'false' as a subsequence does not exist. EXAMPLE 3 define stream InputStream (inputSequence string, price long, regex string); from InputStream select inputSequence , regex:find(\\d\\d(.*)WSO2, 21 products are produced within 10 years by WSO2 currently by WSO2 employees, 30) as aboutWSO2 insert into OutputStream; This method attempts to find the subsequence of the 'inputSequence' that matches the regex pattern, \\d\\d(.*)WSO2 starting from index 30. It returns 'true' since a subsequence exists. group (Function) This method returns the input subsequence captured by the given group during the previous match operation. Origin: siddhi-execution-regex:5.0.0 Syntax STRING regex:group( STRING regex, STRING input.sequence, INT group.id) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No group.id The given group id of the regex expression. For example, 0, 1, 2, etc. INT No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:group(\\d\\d(.*)(WSO2.*), 21 products are produced within 10 years by WSO2 currently by WSO2 employees, 3) insert into OutputStream; This function returns 'WSO2 employees', the input subsequence captured within the given groupID, 3 after grouping the 'inputSequence' according to the regex pattern, \\d\\d(. )(WSO2. ). lookingAt (Function) This method attempts to match the 'inputSequence', from the beginning, against the 'regex' pattern. Origin: siddhi-execution-regex:5.0.0 Syntax BOOL regex:lookingAt( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:lookingAt(\\d\\d(.*)(WSO2.*), 21 products are produced by WSO2 currently in Sri Lanka) This method attempts to match the 'inputSequence' against the regex pattern, \\d\\d(. )(WSO2. ) from the beginning. Since it matches, the function returns 'true'. EXAMPLE 2 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:lookingAt(WSO2(.*)middleware(.*), sample test string and WSO2 is situated in trace and it's a middleware company) This method attempts to match the 'inputSequence' against the regex pattern, WSO2(. )middleware(. ) from the beginning. Since it does not match, the function returns false. matches (Function) This method attempts to match the entire 'inputSequence' against the 'regex' pattern. Origin: siddhi-execution-regex:5.0.0 Syntax BOOL regex:matches( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:matches(WSO2(.*)middleware(.*), WSO2 is situated in trace and its a middleware company) This method attempts to match the entire 'inputSequence' against WSO2(. )middleware(. ) regex pattern. Since it matches, it returns 'true'. EXAMPLE 2 define stream inputStream (inputSequence string, price long, regex string, group int); from inputStream select inputSequence, regex:matches(WSO2(.*)middleware, WSO2 is situated in trace and its a middleware company) This method attempts to match the entire 'inputSequence' against WSO2(.*)middleware regex pattern. Since it does not match, it returns 'false'. Sink email (Sink) The email sink uses the 'smtp' server to publish events via emails. The events can be published in 'text', 'xml' or 'json' formats. The user can define email sink parameters in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or in the stream definition. The email sink first checks the stream definition for parameters, and if they are no configured there, it checks the 'deployment.yaml' file. If the parameters are not configured in either place, default values are considered for optional parameters. If you need to configure server system parameters that are not provided as options in the stream definition, then those parameters need to be defined them in the 'deployment.yaml' file under 'email sink properties'. For more information about the SMTP server parameters, see https://javaee.github.io/javamail/SMTP-Transport. Further, some email accounts are required to enable the 'access to less secure apps' option. For gmail accounts, you can enable this option via https://myaccount.google.com/lesssecureapps. Origin: siddhi-io-email:2.0.1 Syntax @sink(type=\"email\", username=\" STRING \", address=\" STRING \", password=\" STRING \", host=\" STRING \", port=\" INT \", ssl.enable=\" BOOL \", auth=\" BOOL \", content.type=\" STRING \", subject=\" STRING \", to=\" STRING \", cc=\" STRING \", bcc=\" STRING \", attachments=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The username of the email account that is used to send emails. e.g., 'abc' is the username of the 'abc@gmail.com' account. STRING No No address The address of the email account that is used to send emails. STRING No No password The password of the email account. STRING No No host The host name of the SMTP server. e.g., 'smtp.gmail.com' is a host name for a gmail account. The default value 'smtp.gmail.com' is only valid if the email account is a gmail account. smtp.gmail.com STRING Yes No port The port that is used to create the connection. '465' the default value is only valid is SSL is enabled. INT Yes No ssl.enable This parameter specifies whether the connection should be established via a secure connection or not. The value can be either 'true' or 'false'. If it is 'true', then the connection is establish via the 493 port which is a secure connection. true BOOL Yes No auth This parameter specifies whether to use the 'AUTH' command when authenticating or not. If the parameter is set to 'true', an attempt is made to authenticate the user using the 'AUTH' command. true BOOL Yes No content.type The content type can be either 'text/plain' or 'text/html'. text/plain STRING Yes No subject The subject of the mail to be send. STRING No Yes to The address of the 'to' recipient. If there are more than one 'to' recipients, then all the required addresses can be given as a comma-separated list. STRING No Yes cc The address of the 'cc' recipient. If there are more than one 'cc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No bcc The address of the 'bcc' recipient. If there are more than one 'bcc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No attachments File paths of the files that need to be attached to the email. These paths should be absolute paths. They can be either directories or files . If the path is to a directory, all the files located at the first level (i.e., not within another sub directory) are attached. None STRING Yes Yes System Parameters Name Description Default Value Possible Parameters mail.smtp.ssl.trust If this parameter is se, and a socket factory has not been specified, it enables the use of a MailSSLSocketFactory. If this parameter is set to \" \", all the hosts are trusted. If it is set to a whitespace-separated list of hosts, only those specified hosts are trusted. If not, the hosts trusted depends on the certificate presented by the server. String mail.smtp.connectiontimeout The socket connection timeout value in milliseconds. infinite timeout Any Integer mail.smtp.timeout The socket I/O timeout value in milliseconds. infinite timeout Any Integer mail.smtp.from The email address to use for the SMTP MAIL command. This sets the envelope return address. Defaults to msg.getFrom() or InternetAddress.getLocalAddress(). Any valid email address mail.smtp.localport The local port number to bind to when creating the SMTP socket. Defaults to the port number picked by the Socket class. Any Integer mail.smtp.ehlo If this parameter is set to 'false', you must not attempt to sign in with the EHLO command. true true or false mail.smtp.auth.login.disable If this is set to 'true', it is not allowed to use the 'AUTH LOGIN' command. false true or false mail.smtp.auth.plain.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH PLAIN' command. false true or false mail.smtp.auth.digest-md5.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH DIGEST-MD5' command. false true or false mail.smtp.auth.ntlm.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH NTLM' command false true or false mail.smtp.auth.ntlm.domain The NTLM authentication domain. None The valid NTLM authentication domain name. mail.smtp.auth.ntlm.flags NTLM protocol-specific flags. For more details, see http://curl.haxx.se/rfc/ntlm.html#theNtlmFlags. None Valid NTLM protocol-specific flags. mail.smtp.dsn.notify The NOTIFY option to the RCPT command. None Either 'NEVER', or a combination of 'SUCCESS', 'FAILURE', and 'DELAY' (separated by commas). mail.smtp.dsn.ret The 'RET' option to the 'MAIL' command. None Either 'FULL' or 'HDRS'. mail.smtp.sendpartial If this parameter is set to 'true' and a message is addressed to both valid and invalid addresses, the message is sent with a log that reports the partial failure with a 'SendFailedException' error. If this parameter is set to 'false' (which is default), the message is not sent to any of the recipients when the recipient lists contain one or more invalid addresses. false true or false mail.smtp.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.smtp.sasl.mechanisms Enter a space or a comma-separated list of SASL mechanism names that the system shouldt try to use. None mail.smtp.sasl.authorizationid The authorization ID to be used in the SASL authentication. If no value is specified, the authentication ID (i.e., username) is used. username Valid ID mail.smtp.sasl.realm The realm to be used with the 'DIGEST-MD5' authentication. None mail.smtp.quitwait If this parameter is set to 'false', the 'QUIT' command is issued and the connection is immediately closed. If this parameter is set to 'true' (which is default), the transport waits for the response to the QUIT command. false true or false mail.smtp.reportsuccess If this parameter is set to 'true', the transport to includes an 'SMTPAddressSucceededException' for each address to which the message is successfully delivered. false true or false mail.smtp.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create SMTP sockets. None Socket Factory mail.smtp.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory interface'. This class is used to create SMTP sockets. None mail.smtp.socketFactory.fallback If this parameter is set to 'true', the failure to create a socket using the specified socket factory class causes the socket to be created using the 'java.net.Socket' class. true true or false mail.smtp.socketFactory.port This specifies the port to connect to when using the specified socket factory. 25 Valid port number mail.smtp.ssl.protocols This specifies the SSL protocols that need to be enabled for the SSL connections. None This parameter specifies a whitespace separated list of tokens that are acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. mail.smtp.starttls.enable If this parameter is set to 'true', it is possible to issue the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.smtp.starttls.required If this parameter is set to 'true', it is required to use the 'STARTTLS' command. If the server does not support the 'STARTTLS' command, or if the command fails, the connection method will fail. false true or false mail.smtp.socks.host This specifies the host name of a SOCKS5 proxy server to be used for the connections to the mail server. None mail.smtp.socks.port This specifies the port number for the SOCKS5 proxy server. This needs to be used only if the proxy server is not using the standard port number 1080. 1080 valid port number mail.smtp.auth.ntlm.disable If this parameter is set to 'true', the AUTH NTLM command cannot be issued. false true or false mail.smtp.mailextension The extension string to be appended to the MAIL command. None mail.smtp.userset If this parameter is set to 'true', you should use the 'RSET' command instead of the 'NOOP' command in the 'isConnected' method. In some scenarios, 'sendmail' responds slowly after many 'NOOP' commands. This is avoided by using 'RSET' instead. false true or false Examples EXAMPLE 1 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to publish events via an email sink based on the values provided for the mandatory parameters. As shown in the example, it publishes events from the 'FooStream' in 'json' format as emails to the specified 'to' recipients via the email sink. The email is sent from the 'sender.account@gmail.com' email address via a secure connection. EXAMPLE 2 @sink(type='email', @map(type ='json'), subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to configure the query parameters and the system parameters in the 'deployment.yaml' file. Corresponding parameters need to be configured under 'email', and namespace:'sink' as follows: siddhi: extensions: - extension: name:'email' namespace:'sink' properties: username: sender's email username address: sender's email address password: sender's email password As shown in the example, events from the FooStream are published in 'json' format via the email sink as emails to the given 'to' recipients. The email is sent from the 'sender.account@gmail.com' address via a secure connection. EXAMPLE 3 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.com)define stream FooStream (name string, age int, country string); This example illustrates how to publish events via the email sink. Events from the 'FooStream' stream are published in 'xml' format via the email sink as a text/html message and sent to the specified 'to', 'cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value of the 'name' parameter in the corresponding output event. EXAMPLE 4 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.comattachments= '{{attachments}}')define stream FooStream (name string, age int, country string, attachments string); This example illustrates how to publish events via the email sink. Here, the email also contains attachments. Events from the FooStream are published in 'xml' format via the email sink as a 'text/html' message to the specified 'to','cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value for the 'name' parameter in the corresponding output event. The attachments included in the email message are the local files available in the path specified as the value for the 'attachments' attribute. http (Sink) This extension publish the HTTP events in any HTTP method POST, GET, PUT, DELETE via HTTP or https protocols. As the additional features this component can provide basic authentication as well as user can publish events using custom client truststore files when publishing events via https protocol. And also user can add any number of headers including HTTP_METHOD header for each event dynamically. Following content types will be set by default according to the type of sink mapper used. You can override them by setting the new content types in headers. - TEXT : text/plain - XML : application/xml - JSON : application/json - KEYVALUE : application/x-www-form-urlencoded Origin: siddhi-io-http:2.0.4 Syntax @sink(type=\"http\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", client.enable.session.creation=\" STRING \", follow.redirect=\" BOOL \", max.redirect.count=\" INT \", tls.store.type=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configuration=\" STRING \", client.bootstrap.nodelay=\" BOOL \", client.bootstrap.keepalive=\" BOOL \", client.bootstrap.sendbuffersize=\" INT \", client.bootstrap.recievebuffersize=\" INT \", client.bootstrap.connect.timeout=\" INT \", client.bootstrap.socket.reuse=\" BOOL \", client.bootstrap.socket.timeout=\" STRING \", client.threadpool.configurations=\" STRING \", client.connection.pool.count=\" INT \", client.max.active.connections.per.pool=\" INT \", client.min.idle.connections.per.pool=\" INT \", client.max.idle.connections.per.pool=\" INT \", client.min.eviction.idle.time=\" STRING \", sender.thread.count=\" STRING \", event.group.executor.thread.size=\" STRING \", max.wait.for.client.connection.pool=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", refresh.token=\" STRING \", token.url=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published via HTTP. This is a mandatory parameter and if this is not specified, an error is logged in the CLI. If user wants to enable SSL for the events, use https instead of http in the publisher.url.e.g., http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No basic.auth.username The username to be included in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No basic.auth.password The password to include in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No https.truststore.file The file path to the location of the truststore of the client that sends the HTTP events through 'https' protocol. A custom client-truststore can be specified if required. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified and the protocol of URL is 'https' then, the system uses default password. wso2carbon STRING Yes No headers The headers that should be included as HTTP request headers. There can be any number of headers concatenated in following format. \"'header1:value1','header2:value2'\". User can include Content-Type header if he needs to use a specific content-type for the payload. Or else, system decides the Content-Type by considering the type of sink mapper, in following way. - @map(xml):application/xml - @map(json):application/json - @map(text):plain/text ) - if user does not include any mapping type then the system gets 'plain/text' as default Content-Type header. Note that providing content-length as a header is not supported. The size of the payload will be automatically calculated and included in the content-length header. STRING Yes No method For HTTP events, HTTP_METHOD header should be included as a request header. If the parameter is null then system uses 'POST' as a default header. POST STRING Yes No socket.idle.timeout Socket timeout value in millisecond 6000 INT Yes No chunk.disabled This parameter is used to disable/enable chunked transfer encoding false BOOL Yes No ssl.protocol The SSL protocol version TLS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No client.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No follow.redirect Redirect related enabled. true BOOL Yes No max.redirect.count Maximum redirect count. 5 INT Yes No tls.store.type TLS store type to be used. JKS STRING Yes No proxy.host Proxy server host null STRING Yes No proxy.port Proxy server port null STRING Yes No proxy.username Proxy server username null STRING Yes No proxy.password Proxy server password null STRING Yes No client.bootstrap.configuration Client bootsrap configurations. Expected format of these parameters is as follows: \"'client.bootstrap.nodelay:xxx','client.bootstrap.keepalive:xxx'\" TODO STRING Yes No client.bootstrap.nodelay Http client no delay. true BOOL Yes No client.bootstrap.keepalive Http client keep alive. true BOOL Yes No client.bootstrap.sendbuffersize Http client send buffer size. 1048576 INT Yes No client.bootstrap.recievebuffersize Http client receive buffer size. 1048576 INT Yes No client.bootstrap.connect.timeout Http client connection timeout. 15000 INT Yes No client.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No client.bootstrap.socket.timeout Http client socket timeout. 15 STRING Yes No client.threadpool.configurations Thread pool configuration. Expected format of these parameters is as follows: \"'client.connection.pool.count:xxx','client.max.active.connections.per.pool:xxx'\" TODO STRING Yes No client.connection.pool.count Connection pool count. 0 INT Yes No client.max.active.connections.per.pool Active connections per pool. -1 INT Yes No client.min.idle.connections.per.pool Minimum ideal connection per pool. 0 INT Yes No client.max.idle.connections.per.pool Maximum ideal connection per pool. 100 INT Yes No client.min.eviction.idle.time Minimum eviction idle time. 5 * 60 * 1000 STRING Yes No sender.thread.count Http sender thread count. 20 STRING Yes No event.group.executor.thread.size Event group executor thread size. 15 STRING Yes No max.wait.for.client.connection.pool Maximum wait for client connection pool. 60000 STRING Yes No oauth.username The username to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No oauth.password The password to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No consumer.key consumer key for the Http request. It is only applicable for for Oauth requests STRING Yes No consumer.secret consumer secret for the Http request. It is only applicable for for Oauth requests STRING Yes No refresh.token refresh token for the Http request. It is only applicable for for Oauth requests STRING Yes No token.url token url for generate a new access token. It is only applicable for for Oauth requests STRING Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapBossGroupSize property to configure number of boss threads, which accepts incoming connections until the ports are unbound. Once connection accepts successfully, boss thread passes the accepted channel to one of the worker threads. Number of available processors Any integer clientBootstrapWorkerGroupSize property to configure number of worker threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer clientBootstrapClientGroupSize property to configure number of client threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client-truststore.jks trustStorePassword The default truststore password. wso2carbon Truststore password Examples EXAMPLE 1 @sink(type='http',publisher.url='http://localhost:8009/foo', method='{{method}}',headers=\"'content-type:xml','content-length:94'\", client.bootstrap.configuration=\"'client.bootstrap.socket.timeout:20', 'client.bootstrap.worker.group.size:10'\", client.pool.configuration=\"'client.connection.pool.count:10','client.max.active.connections.per.pool:1'\", @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody String, method string, headers string); If it is xml mapping expected input should be in following format for FooStream: { events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events , POST, Content-Length:24#Content-Location:USA#Retry-After:120 } Above event will generate output as below. ~Output http event payload events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events ~Output http event headers Content-Length:24, Content-Location:'USA', Retry-After:120, Content-Type:'application/xml', HTTP_METHOD:'POST', ~Output http event properties HTTP_METHOD:'POST', HOST:'localhost', PORT:8009, PROTOCOL:'http', TO:'/foo' http-request (Sink) This extension publish the HTTP events in any HTTP method POST, GET, PUT, DELETE via HTTP or https protocols. As the additional features this component can provide basic authentication as well as user can publish events using custom client truststore files when publishing events via https protocol. And also user can add any number of headers including HTTP_METHOD header for each event dynamically. Following content types will be set by default according to the type of sink mapper used. You can override them by setting the new content types in headers. - TEXT : text/plain - XML : application/xml - JSON : application/json - KEYVALUE : application/x-www-form-urlencoded HTTP request sink is correlated with the The HTTP reponse source, through a unique sink.id .It sends the request to the defined url and the response is received by the response source which has the same 'sink.id'. Origin: siddhi-io-http:2.0.4 Syntax @sink(type=\"http-request\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", client.enable.session.creation=\" STRING \", follow.redirect=\" BOOL \", max.redirect.count=\" INT \", tls.store.type=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configuration=\" STRING \", client.bootstrap.nodelay=\" BOOL \", client.bootstrap.keepalive=\" BOOL \", client.bootstrap.sendbuffersize=\" INT \", client.bootstrap.recievebuffersize=\" INT \", client.bootstrap.connect.timeout=\" INT \", client.bootstrap.socket.reuse=\" BOOL \", client.bootstrap.socket.timeout=\" STRING \", client.threadpool.configurations=\" STRING \", client.connection.pool.count=\" INT \", client.max.active.connections.per.pool=\" INT \", client.min.idle.connections.per.pool=\" INT \", client.max.idle.connections.per.pool=\" INT \", client.min.eviction.idle.time=\" STRING \", sender.thread.count=\" STRING \", event.group.executor.thread.size=\" STRING \", max.wait.for.client.connection.pool=\" STRING \", sink.id=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", refresh.token=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published via HTTP. This is a mandatory parameter and if this is not specified, an error is logged in the CLI. If user wants to enable SSL for the events, use https instead of http in the publisher.url. e.g., http://localhost:8080/endpoint , https://localhost:8080/endpoint This can be used as a dynamic parameter as well. STRING No Yes basic.auth.username The username to be included in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No basic.auth.password The password to include in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No https.truststore.file The file path to the location of the truststore of the client that sends the HTTP events through 'https' protocol. A custom client-truststore can be specified if required. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified and the protocol of URL is 'https' then, the system uses default password. wso2carbon STRING Yes No headers The headers that should be included as HTTP request headers. There can be any number of headers concatenated in following format. \"'header1:value1','header2:value2'\". User can include Content-Type header if he needs to use a specific content-type for the payload. Or else, system decides the Content-Type by considering the type of sink mapper, in following way. - @map(xml):application/xml - @map(json):application/json - @map(text):plain/text ) - if user does not include any mapping type then the system gets 'plain/text' as default Content-Type header. Note that providing content-length as a header is not supported. The size of the payload will be automatically calculated and included in the content-length header. STRING Yes No method For HTTP events, HTTP_METHOD header should be included as a request header. If the parameter is null then system uses 'POST' as a default header. POST STRING Yes No socket.idle.timeout Socket timeout value in millisecond 6000 INT Yes No chunk.disabled port: Port number of the remote service false BOOL Yes No ssl.protocol The SSL protocol version TLS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No client.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No follow.redirect Redirect related enabled. true BOOL Yes No max.redirect.count Maximum redirect count. 5 INT Yes No tls.store.type TLS store type to be used. JKS STRING Yes No proxy.host Proxy server host null STRING Yes No proxy.port Proxy server port null STRING Yes No proxy.username Proxy server username null STRING Yes No proxy.password Proxy server password null STRING Yes No client.bootstrap.configuration Client bootsrap configurations. Expected format of these parameters is as follows: \"'client.bootstrap.nodelay:xxx','client.bootstrap.keepalive:xxx'\" TODO STRING Yes No client.bootstrap.nodelay Http client no delay. true BOOL Yes No client.bootstrap.keepalive Http client keep alive. true BOOL Yes No client.bootstrap.sendbuffersize Http client send buffer size. 1048576 INT Yes No client.bootstrap.recievebuffersize Http client receive buffer size. 1048576 INT Yes No client.bootstrap.connect.timeout Http client connection timeout. 15000 INT Yes No client.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No client.bootstrap.socket.timeout Http client socket timeout. 15 STRING Yes No client.threadpool.configurations Thread pool configuration. Expected format of these parameters is as follows: \"'client.connection.pool.count:xxx','client.max.active.connections.per.pool:xxx'\" TODO STRING Yes No client.connection.pool.count Connection pool count. 0 INT Yes No client.max.active.connections.per.pool Active connections per pool. -1 INT Yes No client.min.idle.connections.per.pool Minimum ideal connection per pool. 0 INT Yes No client.max.idle.connections.per.pool Maximum ideal connection per pool. 100 INT Yes No client.min.eviction.idle.time Minimum eviction idle time. 5 * 60 * 1000 STRING Yes No sender.thread.count Http sender thread count. 20 STRING Yes No event.group.executor.thread.size Event group executor thread size. 15 STRING Yes No max.wait.for.client.connection.pool Maximum wait for client connection pool. 60000 STRING Yes No sink.id Identifier of the sink. This is used to co-relate with the corresponding http-response source which needs to process the repose for the request sent by this sink. STRING No No downloading.enabled If this is set to 'true' then the response received by the response source will be written to a file. If downloading is enabled, the download.path parameter is mandatory. false BOOL Yes No download.path If downloading is enabled, the path of the file which is going to be downloaded should be specified using 'download.path' parameter. This should be an absolute path including the file name. null STRING Yes Yes oauth.username The username to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No oauth.password The password to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No consumer.key consumer key for the Http request. It is only applicable for for Oauth requests STRING Yes No consumer.secret consumer secret for the Http request. It is only applicable for for Oauth requests STRING Yes No refresh.token refresh token for the Http request. It is only applicable for for Oauth requests STRING Yes No Examples EXAMPLE 1 @sink(type='http-request', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody String, method string, headers string); @source(type='http-response', sink.id='foo', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', fileName='A[1]'))) define stream responseStream2xx(fileName string, headers string); @source(type='http-response', sink.id='foo', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream responseStream4xx(errorMsg string); In above example, the payload body for 'FooStream' will be in following format. { events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events , This message will sent as the body of a POST request with the content-type 'application/xml' to the endpoint defined as the 'publisher.url' and in order to process the responses for these requests, there should be a source of type 'http-response' defined with the same sink id 'foo' in the siddhi app. The responses with 2xx status codes will be received by the http-response source which has the http.status.code defined by the regex '2\\d+'. If the response has a 4xx status code, it will be received by the http-response source which has the http.status.code defined by the regex '4\\d+'. EXAMPLE 2 define stream FooStream (name String, id int, headers String, downloadPath string); @sink(type='http-request', downloading.enabled='true', download.path='{{downloadPath}}',publisher.url='http://localhost:8005/files', method='GET', headers='{{headers}}',sink.id='download-sink', @map(type='json')) define stream BarStream (name String, id int, headers String, downloadPath string); @source(type='http-response', sink.id='download-sink', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', fileName='A[1]'))) define stream responseStream2xx(fileName string, headers string); @source(type='http-response', sink.id='download-sink', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream responseStream4xx(errorMsg string); In above example, http-request sink will send a GET request to the publisher url and the requested file will be received as the response by a corresponding http-response source. If the http status code of the response is a successful one (2xx), it will be received by the http-response source which has the http.status.code '2\\d+' and downloaded as a local file. Then the event received to the responseStream2xx will have the headers included in the request and the downloaded file name. If the http status code of the response is a 4xx code, it will be received by the http-response source which has the http.status.code '4\\d+'. Then the event received to the responseStream4xx will have the response message body in text format. http-response (Sink) HTTP response sink is correlated with the The HTTP request source, through a unique source.id , and it send a response to the HTTP request source having the same source.id . The response message can be formatted in text , XML or JSON and can be sent with appropriate headers. Origin: siddhi-io-http:2.0.4 Syntax @sink(type=\"http-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier of the source. STRING No No message.id Identifier of the message. STRING No Yes headers The headers that should be included as HTTP response headers. There can be any number of headers concatenated on following format. \"'header1:value1','header2:value2'\" User can include content-type header if he/she need to have any specific type for payload. If not system get the mapping type as the content-Type header (ie. @map(xml) : application/xml , @map(json) : application/json , @map(text) : plain/text ) and if user does not include any mapping type then system gets the plain/text as default Content-Type header. If user does not include Content-Length header then system calculate the bytes size of payload and include it as content-length header. STRING Yes No Examples EXAMPLE 1 @sink(type='http-response', source.id='sampleSourceId', message.id='{{messageId}}', headers=\"'content-type:json','content-length:94'\"@map(type='json', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody String, messageId string, headers string); If it is json mapping expected input should be in following format for FooStream: { {\"events\": {\"event\": \"symbol\":WSO2, \"price\":55.6, \"volume\":100, } }, 0cf708b1-7eae-440b-a93e-e72f801b486a, Content-Length:24#Content-Location:USA } Above event will generate response for the matching source message as below. ~Output http event payload {\"events\": {\"event\": \"symbol\":WSO2, \"price\":55.6, \"volume\":100, } } ~Output http event headers Content-Length:24, Content-Location:'USA', Content-Type:'application/json' inMemory (Sink) In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Origin: siddhi-core:5.0.0 Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation. kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Origin: siddhi-io-kafka:5.0.0 Syntax @sink(type=\"kafka\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", sequence.id=\" STRING \", key=\" STRING \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0 th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Origin: siddhi-io-kafka:5.0.0 Syntax @sink(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", sequence.id=\" STRING \", key=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0 th ) partition of the brokers in two data centers log (Sink) This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Origin: siddhi-core:5.0.0 Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. nats (Sink) NATS Sink allows users to subscribe to a NATS broker and publish messages. Origin: siddhi-io-nats:2.0.1 Syntax @sink(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS sink should publish to. STRING No Yes bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client publishing/connecting to the NATS broker. Should be unique for each client connecting to the server/cluster. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No Examples EXAMPLE 1 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with all supporting configurations. With the following configuration the sink identified as 'nats-client' will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. EXAMPLE 2 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with mandatory configurations. With the following configuration the sink identified with an auto generated client id will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. tcp (Sink) A Siddhi application can be configured to publish events via the TCP transport by adding the @Sink(type = 'tcp') annotation at the top of an event stream definition. Origin: siddhi-io-tcp:3.0.1 Syntax @sink(type=\"tcp\", url=\" STRING \", sync=\" STRING \", tcp.no.delay=\" BOOL \", keep.alive=\" BOOL \", worker.threads=\" INT|LONG \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The URL to which outgoing events should be published via TCP. The URL should adhere to tcp:// host : port / context format. STRING No No sync This parameter defines whether the events should be published in a synchronized manner or not. If sync = 'true', then the worker will wait for the ack after sending the message. Else it will not wait for an ack. false STRING Yes Yes tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true BOOL Yes No keep.alive This property defines whether the server should be kept alive when there are no connections available. true BOOL Yes No worker.threads Number of threads to publish events. 10 INT LONG Yes No Examples EXAMPLE 1 @Sink(type = 'tcp', url='tcp://localhost:8080/abc', sync='true' @map(type='binary')) define stream Foo (attribute1 string, attribute2 int); A sink of type 'tcp' has been defined. All events arriving at Foo stream via TCP transport will be sent to the url tcp://localhost:8080/abc in a synchronous manner. Sinkmapper binary (Sink Mapper) This section explains how to map events processed via Siddhi in order to publish them in the binary format. Origin: siddhi-map-binary:2.0.0 Syntax @sink(..., @map(type=\"binary\") Examples EXAMPLE 1 @sink(type='inMemory', topic='WSO2', @map(type='binary')) define stream FooStream (symbol string, price float, volume long); This will publish Siddhi event in binary format. csv (Sink Mapper) This output mapper extension allows you to convert Siddhi events processed by the WSO2 SP to CSV message before publishing them. You can either use custom placeholder to map a custom CSV message or use pre-defined CSV format where event conversion takes place without extra configurations. Origin: siddhi-map-csv:2.0.0 Syntax @sink(..., @map(type=\"csv\", delimiter=\" STRING \", header=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter This parameter used to separate the output CSV data, when converting a Siddhi event to CSV format, , STRING Yes No header This parameter specifies whether the CSV messages will be generated with header or not. If this parameter is set to true, message will be generated with header false BOOL Yes No event.grouping.enabled If this parameter is set to true , events are grouped via a line.separator when multiple events are received. It is required to specify a value for the System.lineSeparator() when the value for this parameter is true . false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv')) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a default CSV output mapping, which will generate output as follows: WSO2,55.6,100 OS supported line separator If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume OS supported line separator WSO2-55.6-100 OS supported line separator EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv',header='true',delimiter='-',@payload(symbol='0',price='2',volume='1')))define stream BarStream (symbol string, price float,volume long); Above configuration will perform a custom CSV mapping. Here, user can add custom place order in the @payload. The place order indicates that where the attribute name's value will be appear in the output message, The output will be produced output as follows: WSO2,100,55.6 If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume WSO2-55.6-100 OS supported line separator If event grouping is enabled, then the output is as follows: WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator json (Sink Mapper) This extension is an Event to JSON output mapper. Transports that publish messages can utilize this extension to convert Siddhi events to JSON messages. You can either send a pre-defined JSON format or a custom JSON message. Origin: siddhi-map-json:5.0.1 Syntax @sink(..., @map(type=\"json\", validate.json=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.json If this property is set to true , it enables JSON validation for the JSON messages generated. When validation is carried out, messages that do not adhere to proper JSON standards are dropped. This property is set to 'false' by default. false BOOL Yes No enclosing.element This specifies the enclosing element to be used if multiple events are sent in the same JSON message. Siddhi treats the child elements of the given enclosing element as events and executes JSON expressions on them. If an enclosing.element is not provided, the multiple event scenario is disregarded and JSON path is evaluated based on the root element. $ STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Above configuration does a default JSON input mapping that generates the output given below. { \"event\":{ \"symbol\":WSO2, \"price\":55.6, \"volume\":100 } } EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='json', enclosing.element='$.portfolio', validate.json='true', @payload( \"\"\"{\"StockData\":{\"Symbol\":\"{{symbol}}\",\"Price\":{{price}}}\"\"\"))) define stream BarStream (symbol string, price float, volume long); The above configuration performs a custom JSON mapping that generates the following JSON message as the output. {\"portfolio\":{ \"StockData\":{ \"Symbol\":WSO2, \"Price\":55.6 } } } keyvalue (Sink Mapper) The Event to Key-Value Map output mapper extension allows you to convert Siddhi events processed by WSO2 SP to key-value map events before publishing them. You can either use pre-defined keys where conversion takes place without extra configurations, or use custom keys with which the messages can be published. Origin: siddhi-map-keyvalue:2.0.0 Syntax @sink(..., @map(type=\"keyvalue\") Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default Key-Value output mapping. The expected output is something similar to the following:symbol:'WSO2' price : 55.6f volume: 100L EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='symbol',b='price',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where values are passed as objects. Values for symbol , price , and volume attributes are published with the keys a , b and c respectively. The expected output is a map similar to the following: a:'WSO2' b : 55.6f c: 100L EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='{{symbol}} is here',b='`price`',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where the values of the a and b attributes are strings and c is object. The expected output should be a Map similar to the following 'WSO2 is here' b : 'price' c: 100L passThrough (Sink Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.0.0 Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. text (Sink Mapper) This extension is a Event to Text output mapper. Transports that publish text messages can utilize this extension to convert the Siddhi events to text messages. Users can use a pre-defined text format where event conversion is carried out without any additional configurations, or use custom placeholder(using {{ and }} or {{{ and }}} ) to map custom text messages. All variables are HTML escaped by default. For example: & is replaced with amp; \" is replaced with quot; = is replaced with #61; If you want to return unescaped HTML, use the triple mustache {{{ instead of double {{ . Origin: siddhi-map-text:2.0.0 Syntax @sink(..., @map(type=\"text\", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.grouping.enabled If this parameter is set to true , events are grouped via a delimiter when multiple events are received. It is required to specify a value for the delimiter parameter when the value for this parameter is true . false BOOL Yes No delimiter This parameter specifies how events are separated when a grouped event is received. This must be a whole line and not a single character. ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n whereas Windows uses \\r\\n as the end of line character. \\n STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping with event grouping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{symbol}}/{{volume}}, SensorPrice : Rs{{price}}/=, Value : {{volume}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected output is as follows: SensorID : wso2/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {wso2,1000,100} EXAMPLE 4 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true', @payload(\"Stock price of {{symbol}} is {{price}}\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping with event grouping. The expected output is as follows: Stock price of WSO2 is 55.6 ~ Stock price of WSO2 is 55.6 ~ Stock price of WSO2 is 55.6 for the following siddhi event. {WSO2,55.6,10} EXAMPLE 5 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{{symbol}}}/{{{volume}}}, SensorPrice : Rs{{{price}}}/=, Value : {{{volume}}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping to return unescaped HTML. The expected output is as follows: SensorID : a b/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {a b,1000,100} xml (Sink Mapper) This mapper converts Siddhi output events to XML before they are published via transports that publish in XML format. Users can either send a pre-defined XML format or a custom XML message containing event data. Origin: siddhi-map-xml:5.0.0 Syntax @sink(..., @map(type=\"xml\", validate.xml=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.xml This parameter specifies whether the XML messages generated should be validated or not. If this parameter is set to true, messages that do not adhere to proper XML standards are dropped. false BOOL Yes No enclosing.element When an enclosing element is specified, the child elements (e.g., the immediate child elements) of that element are considered as events. This is useful when you need to send multiple events in a single XML message. When an enclosing element is not specified, one XML message per every event will be emitted without enclosing. None in custom mapping and events in default mapping STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping which will generate below output events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='xml', enclosing.element=' portfolio ', validate.xml='true', @payload( \" StockData Symbol {{symbol}} /Symbol Price {{price}} /Price /StockData \"))) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. Inside @payload you can specify the custom template that you want to send the messages out and addd placeholders to places where you need to add event attributes.Above config will produce below output XML message portfolio StockData Symbol WSO2 /Symbol Price 55.6 /Price /StockData /portfolio Source cdc (Source) The CDC source receives events when change events (i.e., INSERT, UPDATE, DELETE) are triggered for a database table. Events are received in the 'key-value' format. The key values of the map of a CDC change event are as follows. For insert: Keys are specified as columns of the table. For delete: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For update: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For 'polling' mode: Keys are specified as the coloumns of the table. See parameter: mode for supported databases and change events. Origin: siddhi-io-cdc:2.0.0 Syntax @source(type=\"cdc\", url=\" STRING \", mode=\" STRING \", jdbc.driver.name=\" STRING \", username=\" STRING \", password=\" STRING \", pool.properties=\" STRING \", datasource.name=\" STRING \", table.name=\" STRING \", polling.column=\" STRING \", polling.interval=\" INT \", operation=\" STRING \", connector.properties=\" STRING \", database.server.id=\" STRING \", database.server.name=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The connection URL to the database. F=The format used is: 'jdbc:mysql:// host : port / database_name ' STRING No No mode Mode to capture the change data. The type of events that can be received, and the required parameters differ based on the mode. The mode can be one of the following: 'polling': This mode uses a column named 'polling.column' to monitor the given table. It captures change events of the 'RDBMS', 'INSERT, and 'UPDATE' types. 'listening': This mode uses logs to monitor the given table. It currently supports change events only of the 'MySQL', 'INSERT', 'UPDATE', and 'DELETE' types. listening STRING Yes No jdbc.driver.name The driver class name for connecting the database. It is required to specify a value for this parameter when the mode is 'polling'. STRING Yes No username The username to be used for accessing the database. This user needs to have the 'SELECT', 'RELOAD', 'SHOW DATABASES', 'REPLICATION SLAVE', and 'REPLICATION CLIENT'privileges for the change data capturing table (specified via the 'table.name' parameter). To operate in the polling mode, the user needs 'SELECT' privileges. STRING No No password The password of the username you specified for accessing the database. STRING No No pool.properties The pool parameters for the database connection can be specified as key-value pairs. STRING Yes No datasource.name Name of the wso2 datasource to connect to the database. When datasource name is provided, the URL, username and password are not needed. A datasource based connection is given more priority over the URL based connection. This parameter is applicable only when the mode is set to 'polling', and it can be applied only when you use this extension with WSO2 Stream Processor. STRING Yes No table.name The name of the table that needs to be monitored for data changes. STRING No No polling.column The column name that is polled to capture the change data. It is recommended to have a TIMESTAMP field as the 'polling.column' in order to capture the inserts and updates. Numeric auto-incremental fields and char fields can also be used as 'polling.column'. However, note that fields of these types only support insert change capturing, and the possibility of using a char field also depends on how the data is input. It is required to enter a value for this parameter when the mode is 'polling'. STRING Yes No polling.interval The time interval (specified in seconds) to poll the given table for changes. This parameter is applicable only when the mode is set to 'polling'. 1 INT Yes No operation The change event operation you want to carry out. Possible values are 'insert', 'update' or 'delete'. It is required to specify a value when the mode is 'listening'. This parameter is not case sensitive. STRING No No connector.properties Here, you can specify Debezium connector properties as a comma-separated string. The properties specified here are given more priority over the parameters. This parameter is applicable only for the 'listening' mode. Empty_String STRING Yes No database.server.id An ID to be used when joining MySQL database cluster to read the bin log. This should be a unique integer between 1 to 2^32. This parameter is applicable only when the mode is 'listening'. Random integer between 5400 and 6400 STRING Yes No database.server.name A logical name that identifies and provides a namespace for the database server. This parameter is applicable only when the mode is 'listening'. {host}_{port} STRING Yes No Examples EXAMPLE 1 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'insert', @map(type='keyvalue', @attributes(id = 'id', name = 'name'))) define stream inputStream (id string, name string); In this example, the CDC source listens to the row insertions that are made in the 'students' table with the column name, and the ID. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 2 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'update', @map(type='keyvalue', @attributes(id = 'id', name = 'name', before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, id string, before_name string , name string); In this example, the CDC source listens to the row updates that are made in the 'students' table. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 3 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'delete', @map(type='keyvalue', @attributes(before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, before_name string); In this example, the CDC source listens to the row deletions made in the 'students' table. This table belongs to the 'SimpleDB' database that can be accessed via the given URL. EXAMPLE 4 @source(type = 'cdc', mode='polling', polling.column = 'id', jdbc.driver.name = 'com.mysql.jdbc.Driver', url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. 'id' that is specified as the polling colum' is an auto incremental field. The connection to the database is made via the URL, username, password, and the JDBC driver name. EXAMPLE 5 @source(type = 'cdc', mode='polling', polling.column = 'id', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The given polling column is a char column with the 'S001, S002, ... .' pattern. The connection to the database is made via a data source named 'SimpleDB'. Note that the 'datasource.name' parameter works only with the Stream Processor. EXAMPLE 6 @source(type = 'cdc', mode='polling', polling.column = 'last_updated', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue')) define stream inputStream (name string); In this example, the CDC source polls the 'students' table for inserts and updates. The polling column is a timestamp field. email (Source) The 'Email' source allows you to receive events via emails. An 'Email' source can be configured using the 'imap' or 'pop3' server to receive events. This allows you to filter the messages that satisfy the criteria specified under the 'search term' option. The email source parameters can be defined in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or the stream definition. If the parameter configurations are not available in either place, the default values are considered (i.e., if default values are available). If you need to configure server system parameters that are not provided as options in the stream definition, they need to be defined in the 'deployment yaml' file under 'email source properties'. For more information about 'imap' and 'pop3' server system parameters, see the following. [JavaMail Reference Implementation - IMAP Store](https://javaee.github.io/javamail/IMAP-Store) [JavaMail Reference Implementation - POP3 Store Store](https://javaee.github.io/javamail/POP3-Store) Origin: siddhi-io-email:2.0.1 Syntax @source(type=\"email\", username=\" STRING \", password=\" STRING \", store=\" STRING \", host=\" STRING \", port=\" INT \", folder=\" STRING \", search.term=\" STRING \", polling.interval=\" LONG \", action.after.processed=\" STRING \", folder.to.move=\" STRING \", content.type=\" STRING \", ssl.enable=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The user name of the email account. e.g., 'wso2mail' is the username of the 'wso2mail@gmail.com' mail account. STRING No No password The password of the email account STRING No No store The store type that used to receive emails. Possible values are 'imap' and 'pop3'. imap STRING Yes No host The host name of the server (e.g., 'imap.gmail.com' is the host name for a gmail account with an IMAP store.). The default value 'imap.gmail.com' is only valid if the email account is a gmail account with IMAP enabled. If store type is 'imap', then the default value is 'imap.gmail.com'. If the store type is 'pop3', then thedefault value is 'pop3.gmail.com'. STRING Yes No port The port that is used to create the connection. '993', the default value is valid only if the store is 'imap' and ssl-enabled. INT Yes No folder The name of the folder to which the emails should be fetched. INBOX STRING Yes No search.term The option that includes conditions such as key-value pairs to search for emails. In a string search term, the key and the value should be separated by a semicolon (';'). Each key-value pair must be within inverted commas (' '). The string search term can define multiple comma-separated key-value pairs. This string search term currently supports only the 'subject', 'from', 'to', 'bcc', and 'cc' keys. e.g., if you enter 'subject:DAS, from:carbon, bcc:wso2', the search term creates a search term instance that filters emails that contain 'DAS' in the subject, 'carbon' in the 'from' address, and 'wso2' in one of the 'bcc' addresses. The string search term carries out sub string matching that is case-sensitive. If '@' in included in the value for any key other than the 'subject' key, it checks for an address that is equal to the value given. e.g., If you search for 'abc@', the string search terms looks for an address that contains 'abc' before the '@' symbol. None STRING Yes No polling.interval This defines the time interval in seconds at which th email source should poll the account to check for new mail arrivals.in seconds. 600 LONG Yes No action.after.processed The action to be performed by the email source for the processed mail. Possible values are as follows: 'FLAGGED': Sets the flag as 'flagged'. 'SEEN': Sets the flag as 'read'. 'ANSWERED': Sets the flag as 'answered'. 'DELETE': Deletes tha mail after the polling cycle. 'MOVE': Moves the mail to the folder specified in the 'folder.to.move' parameter. If the folder specified is 'pop3', then the only option available is 'DELETE'. NONE STRING Yes No folder.to.move The name of the folder to which the mail must be moved once it is processed. If the action after processing is 'MOVE', it is required to specify a value for this parameter. STRING No No content.type The content type of the email. It can be either 'text/plain' or 'text/html.' text/plain STRING Yes No ssl.enable If this is set to 'true', a secure port is used to establish the connection. The possible values are 'true' and 'false'. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters mail.imap.partialfetch This determines whether the IMAP partial-fetch capability should be used. true true or false mail.imap.fetchsize The partial fetch size in bytes. 16K value in bytes mail.imap.peek If this is set to 'true', the IMAP PEEK option should be used when fetching body parts to avoid setting the 'SEEN' flag on messages. The default value is 'false'. This can be overridden on a per-message basis by the 'setPeek method' in 'IMAPMessage'. false true or false mail.imap.connectiontimeout The socket connection timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.timeout The socket read timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.writetimeout The socket write timeout value in milliseconds. This timeout is implemented by using a 'java.util.concurrent.ScheduledExecutorService' per connection that schedules a thread to close the socket if the timeout period elapses. Therefore, the overhead of using this timeout is one thread per connection. infinity timeout Any Integer value mail.imap.statuscachetimeout The timeout value in milliseconds for the cache of 'STATUS' command response. 1000ms Time out in miliseconds mail.imap.appendbuffersize The maximum size of a message to buffer in memory when appending to an IMAP folder. None Any Integer value mail.imap.connectionpoolsize The maximum number of available connections in the connection pool. 1 Any Integer value mail.imap.connectionpooltimeout The timeout value in milliseconds for connection pool connections. 45000ms Any Integer mail.imap.separatestoreconnection If this parameter is set to 'true', it indicates that a dedicated store connection needs to be used for store commands. true true or false mail.imap.auth.login.disable If this is set to 'true', it is not possible to use the non-standard 'AUTHENTICATE LOGIN' command instead of the plain 'LOGIN' command. false true or false mail.imap.auth.plain.disable If this is set to 'true', the 'AUTHENTICATE PLAIN' command cannot be used. false true or false mail.imap.auth.ntlm.disable If true, prevents use of the AUTHENTICATE NTLM command. false true or false mail.imap.proxyauth.user If the server supports the PROXYAUTH extension, this property specifies the name of the user to act as. Authentication to log in to the server is carried out using the administrator's credentials. After authentication, the IMAP provider issues the 'PROXYAUTH' command with the user name specified in this property. None Valid string value mail.imap.localaddress The local address (host name) to bind to when creating the IMAP socket. Defaults to the address picked by the Socket class. Valid string value mail.imap.localport The local port number to bind to when creating the IMAP socket. Defaults to the port number picked by the Socket class. Valid String value mail.imap.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.imap.sasl.mechanisms A list of SASL mechanism names that the system should to try to use. The names can be separated by spaces or commas. None Valid string value mail.imap.sasl.authorizationid The authorization ID to use in the SASL authentication. If this parameter is not set, the authentication ID (username) is used. Valid string value mail.imap.sasl.realm The realm to use with SASL authentication mechanisms that require a realm, such as 'DIGEST-MD5'. None Valid string value mail.imap.auth.ntlm.domain The NTLM authentication domain. None Valid string value The NTLM authentication domain. NTLM protocol-specific flags. None Valid integer value mail.imap.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create IMAP sockets. None Valid SocketFactory mail.imap.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create IMAP sockets. None Valid string mail.imap.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. true true or false mail.imap.socketFactory.port This specifies the port to connect to when using the specified socket factory. If this parameter is not set, the default port is used. 143 Valid Integer mail.imap.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by RFC 2595. false true or false mail.imap.ssl.trust If this parameter is set and a socket factory has not been specified, it enables the use of a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If this parameter specifies list of hosts separated by white spaces, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.imap.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class this class is used to create IMAP SSL sockets. None SSL Socket Factory mail.imap.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create IMAP SSL sockets. None Valid String mail.imap.ssl.socketFactory.port This specifies the port to connect to when using the specified socket factory. the default port 993 is used. valid port number mail.imap.ssl.protocols This specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid string mail.imap.starttls.enable If this parameter is set to 'true', it is possible to use the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.imap.socks.host This specifies the host name of a 'SOCKS5' proxy server that is used to connect to the mail server. None Valid String mail.imap.socks.port This specifies the port number for the 'SOCKS5' proxy server. This is needed if the proxy server is not using the standard port number 1080. 1080 Valid String mail.imap.minidletime This property sets the delay in milliseconds. 10 milliseconds time in seconds (Integer) mail.imap.enableimapevents If this property is set to 'true', it enables special IMAP-specific events to be delivered to the 'ConnectionListener' of the store. The unsolicited responses received during the idle method of the store are sent as connection events with 'IMAPStore.RESPONSE' as the type. The event's message is the raw IMAP response string. false true or false mail.imap.folder.class The class name of a subclass of 'com.sun.mail.imap.IMAPFolder'. The subclass can be used to provide support for additional IMAP commands. The subclass must have public constructors of the form 'public MyIMAPFolder'(String fullName, char separator, IMAPStore store, Boolean isNamespace) and public 'MyIMAPFolder'(ListInfo li, IMAPStore store) None Valid String mail.pop3.connectiontimeout The socket connection timeout value in milliseconds. Infinite timeout Integer value mail.pop3.timeout The socket I/O timeout value in milliseconds. Infinite timeout Integer value mail.pop3.message.class The class name of a subclass of 'com.sun.mail.pop3.POP3Message'. None Valid String mail.pop3.localaddress The local address (host name) to bind to when creating the POP3 socket. Defaults to the address picked by the Socket class. Valid String mail.pop3.localport The local port number to bind to when creating the POP3 socket. Defaults to the port number picked by the Socket class. Valid port number mail.pop3.apop.enable If this parameter is set to 'true', use 'APOP' instead of 'USER/PASS' to log in to the 'POP3' server (if the 'POP3' server supports 'APOP'). APOP sends a digest of the password instead of clearing the text password. false true or false mail.pop3.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create 'POP3' sockets. None Socket Factory mail.pop3.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create 'POP3' sockets. None Valid String mail.pop3.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. false true or false mail.pop3.socketFactory.port This specifies the port to connect to when using the specified socket factory. Default port Valid port number mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', check the server identity as specified by RFC 2595. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3' SSL sockets. None SSL Socket Factory mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by 'RFC 2595'. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to '*', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. Trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3 SSL' sockets. None SSL Socket Factory mail.pop3.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create 'POP3 SSL' sockets. None Valid String mail.pop3.ssl.socketFactory.p This parameter pecifies the port to connect to when using the specified socket factory. 995 Valid Integer mail.pop3.ssl.protocols This parameter specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid String mail.pop3.starttls.enable If this parameter is set to 'true', it is possible to use the 'STLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.pop3.starttls.required If this parameter is set to 'true', it is required to use the 'STLS' command. The connect method fails if the server does not support the 'STLS' command or if the command fails. false true or false mail.pop3.socks.host This parameter specifies the host name of a 'SOCKS5' proxy server that can be used to connect to the mail server. None Valid String mail.pop3.socks.port This parameter specifies the port number for the 'SOCKS5' proxy server. None Valid String mail.pop3.disabletop If this parameter is set to 'true', the 'POP3 TOP' command is not used to fetch message headers. false true or false mail.pop3.forgettopheaders If this parameter is set to 'true', the headers that might have been retrieved using the 'POP3 TOP' command is forgotten and replaced by the headers retrieved when the 'POP3 RETR' command is executed. false true or false mail.pop3.filecache.enable If this parameter is set to 'true', the 'POP3' provider caches message data in a temporary file instead of caching them in memory. Messages are only added to the cache when accessing the message content. Message headers are always cached in memory (on demand). The file cache is removed when the folder is closed or the JVM terminates. false true or false mail.pop3.filecache.dir If the file cache is enabled, this property is used to override the default directory used by the JDK for temporary files. None Valid String mail.pop3.cachewriteto This parameter controls the behavior of the 'writeTo' method on a 'POP3' message object. If the parameter is set to 'true', the message content has not been cached yet, and the 'ignoreList' is null, the message is cached before being written. If not, the message is streamed directly to the output stream without being cached. false true or false mail.pop3.keepmessagecontent If this property is set to 'true', a hard reference to the cached content is retained, preventing the memory from being reused until the folder is closed, or until the cached content is explicitly invalidated (using the 'invalidate' method). false true or false Examples EXAMPLE 1 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. In this example, only the required parameters are defined in the stream definition. The default values are taken for the other parameters. The search term is not defined, and therefore, all the new messages in the inbox folder are polled and taken. EXAMPLE 2 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',store = 'imap',host = 'imap.gmail.com',port = '993',searchTerm = 'subject:Stream Processor, from: from.account@ , cc: cc.account',polling.interval='500',action.after.processed='DELETE',content.type='text/html,)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. The email source polls the mail account every 500 seconds to check whether any new mails have arrived. It processes new mails only if they satisfy the conditions specified for the email search term (the value for 'from' of the email message should be 'from.account@. host name ', and the message should contain 'cc.account' in the cc receipient list and the word 'Stream Processor' in the mail subject). in this example, the action after processing is 'DELETE'. Therefore,after processing the event, corresponding mail is deleted from the mail folder. http (Source) The HTTP source receives POST requests via HTTP or HTTPS in format such as text , XML and JSON . In WSO2 SP, if required, you can enable basic authentication to ensure that events are received only from users who are authorized to access the service. Origin: siddhi-io-http:2.0.4 Syntax @source(type=\"http\", receiver.url=\" STRING \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", server.enable.session.creation=\" STRING \", server.supported.snimatchers=\" STRING \", server.suported.server.names=\" STRING \", request.size.validation.configuration=\" STRING \", request.size.validation=\" STRING \", request.size.validation.maximum.value=\" STRING \", request.size.validation.reject.status.code=\" STRING \", request.size.validation.reject.message=\" STRING \", request.size.validation.reject.message.content.type=\" STRING \", header.size.validation=\" STRING \", header.validation.maximum.request.line=\" STRING \", header.validation.maximum.size=\" STRING \", header.validation.maximum.chunk.size=\" STRING \", header.validation.reject.status.code=\" STRING \", header.validation.reject.message=\" STRING \", header.validation.reject.message.content.type=\" STRING \", server.bootstrap.configuration=\" OBJECT \", server.bootstrap.nodelay=\" BOOL \", server.bootstrap.keepalive=\" BOOL \", server.bootstrap.sendbuffersize=\" INT \", server.bootstrap.recievebuffersize=\" INT \", server.bootstrap.connect.timeout=\" INT \", server.bootstrap.socket.reuse=\" BOOL \", server.bootstrap.socket.timeout=\" BOOL \", server.bootstrap.socket.backlog=\" BOOL \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL to which the events should be received. User can provide any valid url and if the url is not provided the system will use the following format http://0.0.0.0:9763/ appNAme / streamName If the user want to use SSL the url should be given in following format https://localhost:8080/ streamName http://0.0.0.0:9763/ / STRING Yes No basic.auth.enabled This works only in WSO2 SP. If this is set to true , basic authentication is enabled for incoming events, and the credentials with which each event is sent are verified to ensure that the user is authorized to access the service. If basic authentication fails, the event is not authenticated and an authentication error is logged in the CLI. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. The value is 1 by default. This will ensure that the events are directed to the event stream in the same order in which they arrive. By increasing this value the performance might increase at the cost of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection. 120000 INT Yes No ssl.verify.client The type of client certificate verification. null STRING Yes No ssl.protocol ssl/tls related options TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No server.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No server.supported.snimatchers Http SNIMatcher to be added. This parameter should include under parameters Ex: 'server.supported.snimatchers:SNIMatcher' null STRING Yes No server.suported.server.names Http supported servers. This parameter should include under parameters Ex: 'server.suported.server.names:server' null STRING Yes No request.size.validation.configuration Parameters that responsible for validating the http request and request headers. Expected format of these parameters is as follows: \"'request.size.validation:xxx','request.size.validation.maximum.value:xxx'\" null STRING Yes No request.size.validation To enable the request size validation. false STRING Yes No request.size.validation.maximum.value If request size is validated then maximum size. Integer.MAX_VALUE STRING Yes No request.size.validation.reject.status.code If request is exceed maximum size and request.size.validation is enabled then status code to be send as response. 401 STRING Yes No request.size.validation.reject.message If request is exceed maximum size and request.size.validation is enabled then status message to be send as response. Message is bigger than the valid size STRING Yes No request.size.validation.reject.message.content.type If request is exceed maximum size and request.size.validation is enabled then content type to be send as response. plain/text STRING Yes No header.size.validation To enable the header size validation. false STRING Yes No header.validation.maximum.request.line If header header validation is enabled then the maximum request line. 4096 STRING Yes No header.validation.maximum.size If header header validation is enabled then the maximum expected header size. 8192 STRING Yes No header.validation.maximum.chunk.size If header header validation is enabled then the maximum expected chunk size. 8192 STRING Yes No header.validation.reject.status.code 401 If header is exceed maximum size and header.size.validation is enabled then status code to be send as response. STRING Yes No header.validation.reject.message If header is exceed maximum size and header.size.validation is enabled then message to be send as response. Message header is bigger than the valid size STRING Yes No header.validation.reject.message.content.type If header is exceed maximum size and header.size.validation is enabled then content type to be send as response. plain/text STRING Yes No server.bootstrap.configuration Parameters that for bootstrap configurations of the server. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null OBJECT Yes No server.bootstrap.nodelay Http server no delay. true BOOL Yes No server.bootstrap.keepalive Http server keep alive. true BOOL Yes No server.bootstrap.sendbuffersize Http server send buffer size. 1048576 INT Yes No server.bootstrap.recievebuffersize Http server receive buffer size. 1048576 INT Yes No server.bootstrap.connect.timeout Http server connection timeout. 15000 INT Yes No server.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No server.bootstrap.socket.timeout Http server socket timeout. 15 BOOL Yes No server.bootstrap.socket.backlog THttp server socket backlog. 100 BOOL Yes No trace.log.enabled Http traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize property to configure number of boss threads, which accepts incoming connections until the ports are unbound. Once connection accepts successfully, boss thread passes the accepted channel to one of the worker threads. Number of available processors Any integer serverBootstrapWorkerGroupSize property to configure number of worker threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer serverBootstrapClientGroupSize property to configure number of client threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultHttpPort The default port if the default scheme is 'http'. 8280 Any valid port defaultHttpsPort The default port if the default scheme is 'https'. 8243 Any valid port defaultScheme The default protocol. http http https keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to wso2carbon.jks file keyStorePassword The default keystore password. wso2carbon String of keystore password Examples EXAMPLE 1 @source(type='http', receiver.url='http://localhost:9055/endpoints/RecPro', socketIdleTimeout='150000', parameters=\"'ciphers : TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256', 'sslEnabledProtocols:TLSv1.1,TLSv1.2'\",request.size.validation.configuration=\"request.size.validation:true\",server.bootstrap.configuration=\"server.bootstrap.socket.timeout:25\" @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above source listenerConfiguration performs a default XML input mapping. The expected input is as follows: events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events If basic authentication is enabled via the basic.auth.enabled='true setting, each input event is also expected to contain the Authorization:'Basic encodeBase64(username:Password)' header. http-request (Source) The HTTP request is correlated with the HTTP response sink, through a unique source.id , and for each POST requests it receives via HTTP or HTTPS in format such as text , XML and JSON it sends the response via the HTTP response sink. The individual request and response messages are correlated at the sink using the message.id of the events. If required, you can enable basic authentication at the source to ensure that events are received only from users who are authorized to access the service. Origin: siddhi-io-http:2.0.4 Syntax @source(type=\"http-request\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", server.enable.session.creation=\" STRING \", server.supported.snimatchers=\" STRING \", server.suported.server.names=\" STRING \", request.size.validation.configuration=\" STRING \", request.size.validation=\" STRING \", request.size.validation.maximum.value=\" STRING \", request.size.validation.reject.status.code=\" STRING \", request.size.validation.reject.message=\" STRING \", request.size.validation.reject.message.content.type=\" STRING \", header.size.validation=\" STRING \", header.validation.maximum.request.line=\" STRING \", header.validation.maximum.size=\" STRING \", header.validation.maximum.chunk.size=\" STRING \", header.validation.reject.status.code=\" STRING \", header.validation.reject.message=\" STRING \", header.validation.reject.message.content.type=\" STRING \", server.bootstrap.configuration=\" OBJECT \", server.bootstrap.nodelay=\" BOOL \", server.bootstrap.keepalive=\" BOOL \", server.bootstrap.sendbuffersize=\" INT \", server.bootstrap.recievebuffersize=\" INT \", server.bootstrap.connect.timeout=\" INT \", server.bootstrap.socket.reuse=\" BOOL \", server.bootstrap.socket.timeout=\" BOOL \", server.bootstrap.socket.backlog=\" BOOL \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL to which the events should be received. User can provide any valid url and if the url is not provided the system will use the following format http://0.0.0.0:9763/ appNAme / streamName If the user want to use SSL the url should be given in following format https://localhost:8080/ streamName http://0.0.0.0:9763/ / STRING Yes No source.id Identifier need to map the source to sink. STRING No No connection.timeout Connection timeout in milliseconds. If the mapped http-response sink does not get a correlated message, after this timeout value, a timeout response is sent 120000 INT Yes No basic.auth.enabled If this is set to true , basic authentication is enabled for incoming events, and the credentials with which each event is sent are verified to ensure that the user is authorized to access the service. If basic authentication fails, the event is not authenticated and an authentication error is logged in the CLI. By default this values 'false' false STRING Yes No worker.count The number of active worker threads to serve the incoming events. The value is 1 by default. This will ensure that the events are directed to the event stream in the same order in which they arrive. By increasing this value the performance might increase at the cost of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection. 120000 INT Yes No ssl.verify.client The type of client certificate verification. null STRING Yes No ssl.protocol ssl/tls related options TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No server.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No server.supported.snimatchers Http SNIMatcher to be added. This parameter should include under parameters Ex: 'server.supported.snimatchers:SNIMatcher' null STRING Yes No server.suported.server.names Http supported servers. This parameter should include under parameters Ex: 'server.suported.server.names:server' null STRING Yes No request.size.validation.configuration Parameters that responsible for validating the http request and request headers. Expected format of these parameters is as follows: \"'request.size.validation:xxx','request.size.validation.maximum.value:xxx'\" null STRING Yes No request.size.validation To enable the request size validation. false STRING Yes No request.size.validation.maximum.value If request size is validated then maximum size. Integer.MAX_VALUE STRING Yes No request.size.validation.reject.status.code If request is exceed maximum size and request.size.validation is enabled then status code to be send as response. 401 STRING Yes No request.size.validation.reject.message If request is exceed maximum size and request.size.validation is enabled then status message to be send as response. Message is bigger than the valid size STRING Yes No request.size.validation.reject.message.content.type If request is exceed maximum size and request.size.validation is enabled then content type to be send as response. plain/text STRING Yes No header.size.validation To enable the header size validation. false STRING Yes No header.validation.maximum.request.line If header header validation is enabled then the maximum request line. 4096 STRING Yes No header.validation.maximum.size If header header validation is enabled then the maximum expected header size. 8192 STRING Yes No header.validation.maximum.chunk.size If header header validation is enabled then the maximum expected chunk size. 8192 STRING Yes No header.validation.reject.status.code 401 If header is exceed maximum size and header.size.validation is enabled then status code to be send as response. STRING Yes No header.validation.reject.message If header is exceed maximum size and header.size.validation is enabled then message to be send as response. Message header is bigger than the valid size STRING Yes No header.validation.reject.message.content.type If header is exceed maximum size and header.size.validation is enabled then content type to be send as response. plain/text STRING Yes No server.bootstrap.configuration Parameters that for bootstrap configurations of the server. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null OBJECT Yes No server.bootstrap.nodelay Http server no delay. true BOOL Yes No server.bootstrap.keepalive Http server keep alive. true BOOL Yes No server.bootstrap.sendbuffersize Http server send buffer size. 1048576 INT Yes No server.bootstrap.recievebuffersize Http server receive buffer size. 1048576 INT Yes No server.bootstrap.connect.timeout Http server connection timeout. 15000 INT Yes No server.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No server.bootstrap.socket.timeout Http server socket timeout. 15 BOOL Yes No server.bootstrap.socket.backlog THttp server socket backlog. 100 BOOL Yes No trace.log.enabled Http traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize property to configure number of boss threads, which accepts incoming connections until the ports are unbound. Once connection accepts successfully, boss thread passes the accepted channel to one of the worker threads. Number of available processors Any integer serverBootstrapWorkerGroupSize property to configure number of worker threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer serverBootstrapClientGroupSize property to configure number of client threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultHttpPort The default port if the default scheme is 'http'. 8280 Any valid port defaultHttpsPort The default port if the default scheme is 'https'. 8243 Any valid port defaultScheme The default protocol. http http https keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to wso2carbon.jks file keyStorePassword The default keystore password. wso2carbon String of keystore password certPassword The default cert password. wso2carbon String of cert password Examples EXAMPLE 1 @source(type='http-request', source.id='sampleSourceId, receiver.url='http://localhost:9055/endpoints/RecPro', connection.timeout='150000', parameters=\"'ciphers : TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256', 'sslEnabledProtocols:TLSv1.1,TLSv1.2'\", request.size.validation.configuration=\"request.size.validation:true\", server.bootstrap.configuration=\"server.bootstrap.socket.timeout:25\", @map(type='json, @attributes(messageId='trp:messageId', symbol='$.events.event.symbol', price='$.events.event.price', volume='$.events.event.volume'))) define stream FooStream (messageId string, symbol string, price float, volume long); The expected input is as follows: {\"events\": {\"event\": \"symbol\":WSO2, \"price\":55.6, \"volume\":100, } } If basic authentication is enabled via the basic.auth.enabled='true setting, each input event is also expected to contain the Authorization:'Basic encodeBase64(username:Password)' header. http-response (Source) The http-response source co-relates with http-request sink with the parameter 'sink.id'. This receives responses for the requests sent by the http-request sink which has the same sink id. Response messages can be in formats such as TEXT, JSON and XML. In order to handle the responses with different http status codes, user is allowed to defined the acceptable response source code using the parameter 'http.status.code' Origin: siddhi-io-http:2.0.4 Syntax @source(type=\"http-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id This parameter is used to map the http-response source to a http-request sink. Then this source will accepts the response messages for the requests sent by corresponding http-request sink. STRING No No http.status.code Acceptable http status code for the responses. This can be a complete string or a regex. Only the responses with matching status codes to the defined value, will be received by the http-response source. Eg: 'http.status.code = '200', http.status.code = '2\\d+'' 200 STRING Yes No allow.streaming.responses If responses can be received multiple times for a single request, this option should be enabled. If this is not enabled, for every request, response will be extracted only once. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-request', downloading.enabled='true', publisher.url='http://localhost:8005/registry/employee', method='POST', headers='{{headers}}',sink.id='employee-info', @map(type='json')) define stream BarStream (name String, id int, headers String, downloadPath string); @source(type='http-response' , sink.id='employee-info', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(message='A[1]'))) define stream responseStream2xx(message string);@source(type='http-response' , sink.id='employee-info', http.status.code='4\\\\d+' , @map(type='text', regex.A='((.|\\n)*)', @attributes(message='A[1]'))) define stream responseStream4xx(message string); In above example, the defined http-request sink will send a POST requests to the endpoint defined by 'publisher.url'. Then for those requests, the source with the response code '2\\d+' and sink.id 'employee-info' will receive the responses with 2xx status codes. The http-response source which has 'employee-info' as the 'sink.id' and '4\\d+' as the http.response.code will receive all the responses with 4xx status codes. . Then the body of the response message will be extracted using text mapper and converted into siddhi events. . inMemory (Source) In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Origin: siddhi-core:5.0.0 Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport. kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Origin: siddhi-io-kafka:5.0.0 Syntax @source(type=\"kafka\", bootstrap.servers=\" STRING \", topic.list=\" STRING \", group.id=\" STRING \", threading.option=\" STRING \", partition.no.list=\" STRING \", seq.enabled=\" BOOL \", is.binary.message=\" BOOL \", topic.offset.map=\" STRING \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51 st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Origin: siddhi-io-kafka:5.0.0 Syntax @source(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream. nats (Source) NATS Source allows users to subscribe to a NATS broker and receive messages. It has the ability to receive all the message types supported by NATS. Origin: siddhi-io-nats:2.0.1 Syntax @source(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", queue.group.name=\" STRING \", durable.name=\" STRING \", subscription.sequence=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS Source should subscribe to. STRING No No bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client subscribing/connecting to the NATS broker. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No queue.group.name This can be used when there is a requirement to share the load of a NATS subject. Clients belongs to the same queue group share the subscription load. None STRING Yes No durable.name This can be used to subscribe to a subject from the last acknowledged message when a client or connection failure happens. The client can be uniquely identified using the tuple (client.id, durable.name). None STRING Yes No subscription.sequence This can be used to subscribe to a subject from a given number of message sequence. All the messages from the given point of sequence number will be passed to the client. If not provided then the either the persisted value or 0 will be used. None STRING Yes No Examples EXAMPLE 1 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with all supporting configurations.With the following configuration the source identified as 'nats-client' will subscribes to a subject named as 'SP_NATS_INPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This subscription will receive all the messages from 100 th in the subject. EXAMPLE 2 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', ) define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with mandatory configurations.With the following configuration the source identified with an auto generated client id will subscribes to a subject named as 'SP_NATS_INTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This will receive all available messages in the subject tcp (Source) A Siddhi application can be configured to receive events via the TCP transport by adding the @Source(type = 'tcp') annotation at the top of an event stream definition. When this is defined the associated stream will receive events from the TCP transport on the host and port defined in the system. Origin: siddhi-io-tcp:3.0.1 Syntax @source(type=\"tcp\", context=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic context The URL 'context' that should be used to receive the events. / STRING Yes No System Parameters Name Description Default Value Possible Parameters host Tcp server host. 0.0.0.0 Any valid host or IP port Tcp server port. 9892 Any integer representing valid port receiver.threads Number of threads to receive connections. 10 Any positive integer worker.threads Number of threads to serve events. 10 Any positive integer tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true true false keep.alive This property defines whether the server should be kept alive when there are no connections available. true true false Examples EXAMPLE 1 @Source(type = 'tcp', context='abc', @map(type='binary')) define stream Foo (attribute1 string, attribute2 int ); Under this configuration, events are received via the TCP transport on default host,port, abc context, and they are passed to Foo stream for processing. Sourcemapper binary (Source Mapper) This extension is a binary input mapper that converts events received in binary format to Siddhi events before they are processed. Origin: siddhi-map-binary:2.0.0 Syntax @source(..., @map(type=\"binary\") Examples EXAMPLE 1 @source(type='inMemory', topic='WSO2', @map(type='binary'))define stream FooStream (symbol string, price float, volume long); This query performs a mapping to convert an event of the binary format to a Siddhi event. csv (Source Mapper) This extension is used to convert CSV message to Siddhi event input mapper. You can either receive pre-defined CSV message where event conversion takes place without extra configurations,or receive custom CSV message where a custom place order to map from custom CSV message. Origin: siddhi-map-csv:2.0.0 Syntax @source(..., @map(type=\"csv\", delimiter=\" STRING \", header.present=\" BOOL \", fail.on.unknown.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter When converting a CSV format message to Siddhi event, this parameter indicatesinput CSV message's data should be split by this parameter , STRING Yes No header.present When converting a CSV format message to Siddhi event, this parameter indicates whether CSV message has header or not. This can either have value true or false.If it's set to false then it indicates that CSV message has't header. false BOOL Yes No fail.on.unknown.attribute This parameter specifies how unknown attributes should be handled. If it's set to true and one or more attributes don't havevalues, then SP will drop that message. If this parameter is set to false , the Stream Processor adds the required attribute's values to such events with a null value and the event is converted to a Siddhi event. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='csv')) define stream FooStream (symbol string, price float, volume int); Above configuration will do a default CSV input mapping. Expected input will look like below: WSO2 ,55.6 , 100OR \"WSO2,No10,Palam Groove Rd,Col-03\" ,55.6 , 100If header.present is true and delimiter is \"-\", then the input is as follows: symbol-price-volumeWSO2-55.6-100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='csv',header='true', @attributes(symbol = \"2\", price = \"0\", volume = \"1\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom CSV mapping. Here, user can add place order of each attribute in the @attribute. The place order indicates where the attribute name's value has appeared in the input.Expected input will look like below: 55.6,100,WSO2 OR55.6,100,\"WSO2,No10,Palm Groove Rd,Col-03\" If header is true and delimiter is \"-\", then the output is as follows: price-volume-symbol 55.6-100-WSO2 If group events is enabled then input should be as follows: price-volume-symbol 55.6-100-WSO2System.lineSeparator() 55.6-100-IBMSystem.lineSeparator() 55.6-100-IFSSystem.lineSeparator() json (Source Mapper) This extension is a JSON-to-Event input mapper. Transports that accept JSON messages can utilize this extension to convert an incoming JSON message into a Siddhi event. Users can either send a pre-defined JSON format, where event conversion happens without any configurations, or use the JSON path to map from a custom JSON message. In default mapping, the JSON string of the event can be enclosed by the element \"event\", though optional. Origin: siddhi-map-json:5.0.1 Syntax @source(..., @map(type=\"json\", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic enclosing.element This is used to specify the enclosing element when sending multiple events in the same JSON message. Mapper treats the child elements of a given enclosing element as events and executes the JSON path expressions on these child elements. If the enclosing.element is not provided then the multiple-event scenario is disregarded and the JSON path is evaluated based on the root element. $ STRING Yes No fail.on.missing.attribute This parameter allows users to handle unknown attributes.The value of this can either be true or false. By default it is true. If a JSON execution fails or returns null, mapper drops that message. However, setting this property to false prompts mapper to send an event with a null value to Siddhi, where users can handle it as required, ie., assign a default value.) true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For a single event, the input is required to be in one of the following formats: { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } or { \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For multiple events, the input is required to be in one of the following formats: [ {\"event\":{\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80}} ] or [ {\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}, {\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}, {\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80} ] EXAMPLE 3 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"company.symbol\", price = \"price\", volume = \"volume\"))) This configuration performs a custom JSON mapping. For a single event, the expected input is similar to the one shown below: .{ \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"WSO2\" }, \"price\":55.6 } } EXAMPLE 4 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream FooStream (symbol string, price float, volume long); The configuration performs a custom JSON mapping. For multiple events, expected input looks as follows. .{\"portfolio\": [ {\"stock\":{\"volume\":100,\"company\":{\"symbol\":\"wso2\"},\"price\":56.6}}, {\"stock\":{\"volume\":200,\"company\":{\"symbol\":\"wso2\"},\"price\":57.6}} ] } keyvalue (Source Mapper) Key-Value Map to Event input mapper extension allows transports that accept events as key value maps to convert those events to Siddhi events. You can either receive pre-defined keys where conversion takes place without extra configurations, or use custom keys to map from the message. Origin: siddhi-map-keyvalue:2.0.0 Syntax @source(..., @map(type=\"keyvalue\", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic fail.on.missing.attribute If this parameter is set to true , if an event arrives without a matching key for a specific attribute in the connected stream, it is dropped and not processed by the Stream Processor. If this parameter is set to false the Stream Processor adds the required key to such events with a null value, and the event is converted to a Siddhi event so that you could handle them as required before they are further processed. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default key value input mapping. The expected input is a map similar to the following: symbol: 'WSO2' price: 55.6f volume: 100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='keyvalue', fail.on.missing.attribute='true', @attributes(symbol = 's', price = 'p', volume = 'v')))define stream FooStream (symbol string, price float, volume long); This query performs a custom key value input mapping. The matching keys for the symbol , price and volume attributes are be s , p, and v` respectively. The expected input is a map similar to the following: s: 'WSO2' p: 55.6 v: 100 passThrough (Source Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.0.0 Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source. text (Source Mapper) This extension is a text to Siddhi event input mapper. Transports that accept text messages can utilize this extension to convert the incoming text message to Siddhi event. Users can either use a pre-defined text format where event conversion happens without any additional configurations, or specify a regex to map a text message using custom configurations. Origin: siddhi-map-text:2.0.0 Syntax @source(..., @map(type=\"text\", regex.groupid=\" STRING \", fail.on.missing.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex.groupid This parameter specifies a regular expression group. The groupid can be any capital letter (e.g., regex.A,regex.B .. etc). You can specify any number of regular expression groups. In the attribute annotation, you need to map all attributes to the regular expression group with the matching group index. If you need to to enable custom mapping, it is required to specifythe matching group for each and every attribute. STRING No No fail.on.missing.attribute This parameter specifies how unknown attributes should be handled. If it is set to true a message is dropped if its execution fails, or if one or more attributes do not have values. If this parameter is set to false , null values are assigned to attributes with missing values, and messages with such attributes are not dropped. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No delimiter This parameter specifies how events must be separated when multiple events are received. This must be whole line and not a single character. ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n as the end of line character whereas windows uses \\r\\n . \\n STRING Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected input is as follows: symbol:\"WSO2\", price:55.6, volume:100 OR symbol:'WSO2', price:55.6, volume:100 If group events is enabled then input should be as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='text', fail.on.unknown.attribute = 'true', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected input is as follows: wos2 550 volume 100 If group events is enabled then input should be as follows: wos2 550 volume 100 ~ wos2 550 volume 100 ~ wos2 550 volume 100 xml (Source Mapper) This mapper converts XML input to Siddhi event. Transports which accepts XML messages can utilize this extension to convert the incoming XML message to Siddhi event. Users can either send a pre-defined XML format where event conversion will happen without any configs or can use xpath to map from a custom XML message. Origin: siddhi-map-xml:5.0.0 Syntax @source(..., @map(type=\"xml\", namespaces=\" STRING \", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic namespaces Used to provide namespaces used in the incoming XML message beforehand to configure xpath expressions. User can provide a comma separated list. If these are not provided xpath evaluations will fail None STRING Yes No enclosing.element Used to specify the enclosing element in case of sending multiple events in same XML message. WSO2 DAS will treat the child element of given enclosing element as events and execute xpath expressions on child elements. If enclosing.element is not provided multiple event scenario is disregarded and xpaths will be evaluated with respect to root element. Root element STRING Yes No fail.on.missing.attribute This can either have value true or false. By default it will be true. This attribute allows user to handle unknown attributes. By default if an xpath execution fails or returns null DAS will drop that message. However setting this property to false will prompt DAS to send and event with null value to Siddhi where user can handle it accordingly(ie. Assign a default value) True BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping. Expected input will look like below. events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='xml', namespaces = \"dt=urn:schemas-microsoft-com:datatypes\", enclosing.element=\"//portfolio\", @attributes(symbol = \"company/symbol\", price = \"price\", volume = \"volume\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. In the custom mapping user can add xpath expressions representing each event attribute using @attribute annotation. Expected input will look like below. portfolio xmlns:dt=\"urn:schemas-microsoft-com:datatypes\" stock exchange=\"nasdaq\" volume 100 /volume company symbol WSO2 /symbol /company price dt:type=\"number\" 55.6 /price /stock /portfolio Store rdbms (Store) This extension assigns data sources and connection instructions to event tables. It also implements read-write operations on connected datasources. Origin: siddhi-store-rdbms:6.0.0 Syntax @Store(type=\"rdbms\", jdbc.url=\" STRING \", username=\" STRING \", password=\" STRING \", jdbc.driver.name=\" STRING \", pool.properties=\" STRING \", jndi.resource=\" STRING \", datasource=\" STRING \", table.name=\" STRING \", field.length=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic jdbc.url The JDBC URL via which the RDBMS data store is accessed. STRING No No username The username to be used to access the RDBMS data store. STRING No No password The password to be used to access the RDBMS data store. STRING No No jdbc.driver.name The driver class name for connecting the RDBMS data store. STRING No No pool.properties Any pool parameters for the database connection must be specified as key-value pairs. null STRING Yes No jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account and the connection is attempted via JNDI lookup instead. null STRING Yes No datasource The name of the Carbon datasource that should be used for creating the connection with the database. If this is found, neither the pool properties nor the JNDI resource name described above are taken into account and the connection is attempted via Carbon datasources instead. null STRING Yes No table.name The name with which the event table should be persisted in the store. If no name is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The table name defined in the Siddhi App query. STRING Yes No field.length The number of characters that the values for fields of the 'STRING' type in the table definition must contain. Each required field must be provided as a comma-separated list of key-value pairs in the ' field.name : length ' format. If this is not specified, the default number of characters specific to the database type is considered. null STRING Yes No System Parameters Name Description Default Value Possible Parameters {{RDBMS-Name}}.maxVersion The latest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.minVersion The earliest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.tableCheckQuery The template query for the 'check table' operation in {{RDBMS-Name}}. H2 : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) MySQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Oracle : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Microsoft SQL Server : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) PostgreSQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) DB2. : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) N/A {{RDBMS-Name}}.tableCreateQuery The template query for the 'create table' operation in {{RDBMS-Name}}. H2 : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 MySQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 Oracle : SELECT 1 FROM {{TABLE_NAME}} WHERE rownum=1 Microsoft SQL Server : SELECT TOP 1 1 from {{TABLE_NAME}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.indexCreateQuery The template query for the 'create index' operation in {{RDBMS-Name}}. H2 : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) MySQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Oracle : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Microsoft SQL Server : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) {{TABLE_NAME}} ({{INDEX_COLUMNS}}) PostgreSQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) DB2. : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) N/A {{RDBMS-Name}}.recordInsertQuery The template query for the 'insert record' operation in {{RDBMS-Name}}. H2 : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) MySQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Oracle : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Microsoft SQL Server : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) PostgreSQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) DB2. : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) N/A {{RDBMS-Name}}.recordUpdateQuery The template query for the 'update record' operation in {{RDBMS-Name}}. H2 : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} MySQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Oracle : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Microsoft SQL Server : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} PostgreSQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} DB2. : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} N/A {{RDBMS-Name}}.recordSelectQuery The template query for the 'select record' operation in {{RDBMS-Name}}. H2 : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} DB2. : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.recordExistsQuery The template query for the 'check record existence' operation in {{RDBMS-Name}}. H2 : SELECT TOP 1 1 FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT COUNT(1) INTO existence FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT TOP 1 FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.recordDeleteQuery The query for the 'delete record' operation in {{RDBMS-Name}}. H2 : DELETE FROM {{TABLE_NAME}} {{CONDITION}} MySQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Oracle : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : DELETE FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} DB2. : DELETE FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.stringSize This defines the length for the string fields in {{RDBMS-Name}}. H2 : 254 MySQL : 254 Oracle : 254 Microsoft SQL Server : 254 PostgreSQL : 254 DB2. : 254 N/A {{RDBMS-Name}}.fieldSizeLimit This defines the field size limit for select/switch to big string type from the default string type if the 'bigStringType' is available in field type list. H2 : N/A MySQL : N/A Oracle : 2000 Microsoft SQL Server : N/A PostgreSQL : N/A DB2. : N/A 0 = n = INT_MAX {{RDBMS-Name}}.batchSize This defines the batch size when operations are performed for batches of events. H2 : 1000 MySQL : 1000 Oracle : 1000 Microsoft SQL Server : 1000 PostgreSQL : 1000 DB2. : 1000 N/A {{RDBMS-Name}}.batchEnable This specifies whether 'Update' and 'Insert' operations can be performed for batches of events or not. H2 : true MySQL : true Oracle (versions 12.0 and less) : false Oracle (versions 12.1 and above) : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.transactionSupported This is used to specify whether the JDBC connection that is used supports JDBC transactions or not. H2 : true MySQL : true Oracle : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.typeMapping.binaryType This is used to specify the binary data type. An attribute defines as 'object' type in Siddhi stream will be stored into RDBMS with this type. H2 : BLOB MySQL : BLOB Oracle : BLOB Microsoft SQL Server : VARBINARY(max) PostgreSQL : BYTEA DB2. : BLOB(64000) N/A {{RDBMS-Name}}.typeMapping.booleanType This is used to specify the boolean data type. An attribute defines as 'bool' type in Siddhi stream will be stored into RDBMS with this type. H2 : TINYINT(1) MySQL : TINYINT(1) Oracle : NUMBER(1) Microsoft SQL Server : BIT PostgreSQL : BOOLEAN DB2. : SMALLINT N/A {{RDBMS-Name}}.typeMapping.doubleType This is used to specify the double data type. An attribute defines as 'double' type in Siddhi stream will be stored into RDBMS with this type. H2 : DOUBLE MySQL : DOUBLE Oracle : NUMBER(19,4) Microsoft SQL Server : FLOAT(32) PostgreSQL : DOUBLE PRECISION DB2. : DOUBLE N/A {{RDBMS-Name}}.typeMapping.floatType This is used to specify the float data type. An attribute defines as 'float' type in Siddhi stream will be stored into RDBMS with this type. H2 : FLOAT MySQL : FLOAT Oracle : NUMBER(19,4) Microsoft SQL Server : REAL PostgreSQL : REAL DB2. : REAL N/A {{RDBMS-Name}}.typeMapping.integerType This is used to specify the integer data type. An attribute defines as 'int' type in Siddhi stream will be stored into RDBMS with this type. H2 : INTEGER MySQL : INTEGER Oracle : NUMBER(10) Microsoft SQL Server : INTEGER PostgreSQL : INTEGER DB2. : INTEGER N/A {{RDBMS-Name}}.typeMapping.longType This is used to specify the long data type. An attribute defines as 'long' type in Siddhi stream will be stored into RDBMS with this type. H2 : BIGINT MySQL : BIGINT Oracle : NUMBER(19) Microsoft SQL Server : BIGINT PostgreSQL : BIGINT DB2. : BIGINT N/A {{RDBMS-Name}}.typeMapping.stringType This is used to specify the string data type. An attribute defines as 'string' type in Siddhi stream will be stored into RDBMS with this type. H2 : VARCHAR(stringSize) MySQL : VARCHAR(stringSize) Oracle : VARCHAR(stringSize) Microsoft SQL Server : VARCHAR(stringSize) PostgreSQL : VARCHAR(stringSize) DB2. : VARCHAR(stringSize) N/A {{RDBMS-Name}}.typeMapping.bigStringType This is used to specify the big string data type. An attribute defines as 'string' type in Siddhi stream and field.length define in the annotation is greater than the fieldSizeLimit, will be stored into RDBMS with this type. H2 : N/A MySQL : N/A Oracle : CLOB Microsoft SQL Server : N/A PostgreSQL : N/A DB2.* : N/A N/A Examples EXAMPLE 1 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/stocks\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"volume\") define table StockTable (symbol string, price float, volume long); The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float', and 'long' respectively). The connection is made as specified by the parameters configured for the '@Store' annotation. The 'symbol' attribute is considered a unique field, and a DB index is created for it. EXAMPLE 2 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)] Str groupConcat (Aggregate Function) This function aggregates the received events by concatenating the keys in those events using a separator, e.g.,a comma (,) or a hyphen (-), and returns the concatenated key string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:groupConcat( STRING key, STRING separator, STRING distinct, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key The string that needs to be aggregated. STRING No No separator The separator that separates each string key after concatenating the keys. , STRING Yes No distinct This is used to only have distinct values in the concatenated string that is returned. false STRING Yes No order This parameter accepts 'ASC' or 'DESC' strings to sort the string keys in either ascending or descending order respectively. No order STRING Yes No Examples EXAMPLE 1 from InputStream#window.time(5 min) select str:groupConcat(\"key\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , it returns \"A,B,S,C,A\" to the 'OutputStream'. EXAMPLE 2 from InputStream#window.time(5 min) select groupConcat(\"key\",\"-\",true,\"ASC\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , specify the seperator as hyphen and choose the order to be ascending, the function returns \"A-B-C-S\" to the 'OutputStream'. charAt (Function) This function returns the 'char' value that is present at the given index position. of the input string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:charAt( STRING input.value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.value The input string of which the char value at the given position needs to be returned. STRING No No index The variable that specifies the index of the char value that needs to be returned. INT No No Examples EXAMPLE 1 charAt(\"WSO2\", 1) In this case, the functiion returns the character that exists at index 1. Hence, it returns 'S'. coalesce (Function) This returns the first input parameter value of the given argument, that is not null. Origin: siddhi-execution-string:5.0.1 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT str:coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT argn) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic argn It can have one or more input parameters in any data type. However, all the specified parameters are required to be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 coalesce(null, \"BBB\", \"CCC\") This returns the first input parameter that is not null. In this example, it returns \"BBB\". concat (Function) This function returns a string value that is obtained as a result of concatenating two or more input string values. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:concat( STRING argn) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic argn This can have two or more string type input parameters. STRING No No Examples EXAMPLE 1 concat(\"D533\", \"8JU^\", \"XYZ\") This returns a string value by concatenating two or more given arguments. In the example shown above, it returns \"D5338JU^XYZ\". contains (Function) This function returns true if the input.string contains the specified sequence of char values in the search.string . Origin: siddhi-execution-string:5.0.1 Syntax BOOL str:contains( STRING input.string, STRING search.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string Input string value. STRING No No search.string The string value to be searched for in the input.string . STRING No No Examples EXAMPLE 1 contains(\"21 products are produced by WSO2 currently\", \"WSO2\") This returns a boolean value as the output. In this case, it returns true . equalsIgnoreCase (Function) This returns a boolean value by comparing two strings lexicographically without considering the letter case. Origin: siddhi-execution-string:5.0.1 Syntax BOOL str:equalsIgnoreCase( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No No arg2 The second input string argument. This is compared with the first argument. STRING No No Examples EXAMPLE 1 equalsIgnoreCase(\"WSO2\", \"wso2\") This returns a boolean value as the output. In this scenario, it returns \"true\". fillTemplate (Function) This extension replaces the templated positions that are marked with an index value in a specified template with the strings provided. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:fillTemplate( STRING template, STRING|INT|LONG|DOUBLE|FLOAT|BOOL replacement.strings) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic template The string with templated fields that needs to be filled with the given strings. The format of the templated fields should be as follows: {{INDEX}} where 'INDEX' is an integer. This index is used to map the strings that are used to replace the templated fields. STRING No No replacement.strings The strings with which the templated positions in the template need to be replaced. The minimum of two arguments need to be included in the execution string. There is no upper limit on the number of arguments allowed to be included. STRING INT LONG DOUBLE FLOAT BOOL No No Examples EXAMPLE 1 str:fillTemplate(\"This is {{1}} for the {{2}} function\", 'an example', 'fillTemplate') In this example, the template is 'This is {{1}} for the {{2}} function'.Here, the templated string {{1}} is replaced with the 1 st string value provided, which is 'an example'. {{2}} is replaced with the 2 nd string provided, which is 'fillTemplate' The complete return string is 'This is an example for the fillTemplate function'. hex (Function) This function returns a hexadecimal string by converting each byte of each character in the input string to two hexadecimal digits. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:hex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the hexadecimal value. STRING No No Examples EXAMPLE 1 hex(\"MySQL\") This returns the hexadecimal value of the input.string. In this scenario, the output is \"4d7953514c\". length (Function) Returns the length of the input string. Origin: siddhi-execution-string:5.0.1 Syntax INT str:length( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the length. STRING No No Examples EXAMPLE 1 length(\"Hello World\") This outputs the length of the provided string. In this scenario, the, output is 11 . lower (Function) Converts the capital letters in the input string to the equivalent simple letters. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:lower( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to convert to the lower case (i.e., equivalent simple letters). STRING No No Examples EXAMPLE 1 lower(\"WSO2 cep \") This converts the capital letters in the input.string to the equivalent simple letters. In this scenario, the output is \"wso2 cep \". regexp (Function) Returns a boolean value based on the matchability of the input string and the given regular expression. Origin: siddhi-execution-string:5.0.1 Syntax BOOL str:regexp( STRING input.string, STRING regex) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to match with the given regular expression. STRING No No regex The regular expression to be matched with the input string. STRING No No Examples EXAMPLE 1 regexp(\"WSO2 abcdh\", \"WSO(.*h)\") This returns a boolean value after matching regular expression with the given string. In this scenario, it returns \"true\" as the output. repeat (Function) Repeats the input string for a specified number of times. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:repeat( STRING input.string, INT times) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that is repeated the number of times as defined by the user. STRING No No times The number of times the input.string needs to be repeated . INT No No Examples EXAMPLE 1 repeat(\"StRing 1\", 3) This returns a string value by repeating the string for a specified number of times. In this scenario, the output is \"StRing 1StRing 1StRing 1\". replaceAll (Function) Finds all the substrings of the input string that matches with the given expression, and replaces them with the given replacement string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:replaceAll( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No No regex The regular expression to be matched with the input string. STRING No No replacement.string The string with which each substring that matches the given expression should be replaced. STRING No No Examples EXAMPLE 1 replaceAll(\"hello hi hello\", 'hello', 'test') This returns a string after replacing the substrings of the input string with the replacement string. In this scenario, the output is \"test hi test\" . replaceFirst (Function) Finds the first substring of the input string that matches with the given regular expression, and replaces itwith the given replacement string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:replaceFirst( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be replaced. STRING No No regex The regular expression with which the input string should be matched. STRING No No replacement.string The string with which the first substring of input string that matches the regular expression should be replaced. STRING No No Examples EXAMPLE 1 replaceFirst(\"hello WSO2 A hello\", 'WSO2(.*)A', 'XXXX') This returns a string after replacing the first substring with the given replacement string. In this scenario, the output is \"hello XXXX hello\". reverse (Function) Returns the input string in the reverse order character-wise and string-wise. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:reverse( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be reversed. STRING No No Examples EXAMPLE 1 reverse(\"Hello World\") This outputs a string value by reversing the incoming input.string . In this scenario, the output is \"dlroW olleH\". split (Function) Splits the input.string into substrings using the value parsed in the split.string and returns the substring at the position specified in the group.number . Origin: siddhi-execution-string:5.0.1 Syntax STRING str:split( STRING input.string, STRING split.string, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No No split.string The string value to be used to split the input.string . STRING No No group.number The index of the split group INT No No Examples EXAMPLE 1 split(\"WSO2,ABM,NSFT\", \",\", 0) This splits the given input.string by given split.string and returns the string in the index given by group.number. In this scenario, the output will is \"WSO2\". strcmp (Function) Compares two strings lexicographically and returns an integer value. If both strings are equal, 0 is returned. If the first string is lexicographically greater than the second string, a positive value is returned. If the first string is lexicographically greater than the second string, a negative value is returned. Origin: siddhi-execution-string:5.0.1 Syntax INT str:strcmp( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No No arg2 The second input string argument that should be compared with the first argument lexicographically. STRING No No Examples EXAMPLE 1 strcmp(\"AbCDefghiJ KLMN\", 'Hello') This compares two strings lexicographically and outputs an integer value. substr (Function) Returns a substring of the input string by considering a subset or all of the following factors: starting index, length, regular expression, and regex group number. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:substr( STRING input.string, INT begin.index, INT length, STRING regex, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be processed. STRING No No begin.index Starting index to consider for the substring. INT No No length The length of the substring. INT No No regex The regular expression that should be matched with the input string. STRING No No group.number The regex group number INT No No Examples EXAMPLE 1 substr(\"AbCDefghiJ KLMN\", 4) This outputs the substring based on the given begin.index . In this scenario, the output is \"efghiJ KLMN\". EXAMPLE 2 substr(\"AbCDefghiJ KLMN\", 2, 4) This outputs the substring based on the given begin.index and length. In this scenario, the output is \"CDef\". EXAMPLE 3 substr(\"WSO2D efghiJ KLMN\", '^WSO2(.*)') This outputs the substring by applying the regex. In this scenario, the output is \"WSO2D efghiJ KLMN\". EXAMPLE 4 substr(\"WSO2 cep WSO2 XX E hi hA WSO2 heAllo\", 'WSO2(.*)A(.*)', 2) This outputs the substring by applying the regex and considering the group.number . In this scenario, the output is \" ello\". trim (Function) Returns a copy of the input string without the leading and trailing whitespace (if any). Origin: siddhi-execution-string:5.0.1 Syntax STRING str:trim( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that needs to be trimmed. STRING No No Examples EXAMPLE 1 trim(\" AbCDefghiJ KLMN \") This returns a copy of the input.string with the leading and/or trailing white-spaces omitted. In this scenario, the output is \"AbCDefghiJ KLMN\". unhex (Function) Returns a string by converting the hexadecimal characters in the input string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:unhex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The hexadecimal input string that needs to be converted to string. STRING No No Examples EXAMPLE 1 unhex(\"4d7953514c\") This converts the hexadecimal value to string. upper (Function) Converts the simple letters in the input string to the equivalent capital/block letters. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:upper( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be converted to the upper case (equivalent capital/block letters). STRING No No Examples EXAMPLE 1 upper(\"Hello World\") This converts the simple letters in the input.string to theequivalent capital letters. In this scenario, the output is \"HELLO WORLD\". tokenize (Stream Processor) This function splits the input string into tokens using a given regular expression and returns the split tokens. Origin: siddhi-execution-string:5.0.1 Syntax str:tokenize( STRING input.string, STRING regex, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string which needs to be split. STRING No No regex The string value which is used to tokenize the 'input.string'. STRING No No distinct This flag is used to return only distinct values. false BOOL Yes No Extra Return Attributes Name Description Possible Types token The attribute which contains a single token. STRING Examples EXAMPLE 1 define stream inputStream (str string); @info(name = 'query1') from inputStream#str:tokenize(str , ',') select text insert into outputStream; This query performs tokenization on the given string. If the str is \"Android,Windows8,iOS\", then the string is split into 3 events containing the token attribute values, i.e., Android , Windows8 and iOS .","title":"5.0.0"},{"location":"docs/api/5.0.0/#api-docs-v500","text":"","title":"API Docs - v5.0.0"},{"location":"docs/api/5.0.0/#core","text":"","title":"Core"},{"location":"docs/api/5.0.0/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Origin: siddhi-core:5.0.0 Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"docs/api/5.0.0/#avg-aggregate-function","text":"Calculates the average for all the events. Origin: siddhi-core:5.0.0 Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"docs/api/5.0.0/#count-aggregate-function","text":"Returns the count of all the events. Origin: siddhi-core:5.0.0 Syntax LONG count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"docs/api/5.0.0/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Origin: siddhi-core:5.0.0 Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"docs/api/5.0.0/#max-aggregate-function","text":"Returns the maximum value for all the events. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"docs/api/5.0.0/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"docs/api/5.0.0/#min-aggregate-function","text":"Returns the minimum value for all the events. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"docs/api/5.0.0/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"docs/api/5.0.0/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Origin: siddhi-core:5.0.0 Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"docs/api/5.0.0/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Origin: siddhi-core:5.0.0 Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"docs/api/5.0.0/#sum-aggregate-function","text":"Returns the sum for all the events. Origin: siddhi-core:5.0.0 Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"docs/api/5.0.0/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Origin: siddhi-core:5.0.0 Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"docs/api/5.0.0/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Origin: siddhi-core:5.0.0 Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"docs/api/5.0.0/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"docs/api/5.0.0/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"docs/api/5.0.0/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"docs/api/5.0.0/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Origin: siddhi-core:5.0.0 Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"docs/api/5.0.0/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Origin: siddhi-core:5.0.0 Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"docs/api/5.0.0/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"docs/api/5.0.0/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Origin: siddhi-core:5.0.0 Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"docs/api/5.0.0/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"docs/api/5.0.0/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"docs/api/5.0.0/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"docs/api/5.0.0/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"docs/api/5.0.0/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"docs/api/5.0.0/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"docs/api/5.0.0/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"docs/api/5.0.0/#maximum-function","text":"Returns the maximum value of the input parameters. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"docs/api/5.0.0/#minimum-function","text":"Returns the minimum value of the input parameters. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"docs/api/5.0.0/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Origin: siddhi-core:5.0.0 Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"docs/api/5.0.0/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Origin: siddhi-core:5.0.0 Syntax pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"docs/api/5.0.0/#log-stream-processor","text":"The logger logs the message on the given priority with or without processed event. Origin: siddhi-core:5.0.0 Syntax log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log(\"INFO\", \"Sample Event :\", true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log(\"Sample Event :\", true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log(\"Sample Event :\", fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log(\"Sample Event :\") select * insert into barStream; This will log message and fooStream:events.","title":"log (Stream Processor)"},{"location":"docs/api/5.0.0/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Origin: siddhi-core:5.0.0 Syntax batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"docs/api/5.0.0/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Origin: siddhi-core:5.0.0 Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"docs/api/5.0.0/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Origin: siddhi-core:5.0.0 Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"docs/api/5.0.0/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Origin: siddhi-core:5.0.0 Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"docs/api/5.0.0/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Origin: siddhi-core:5.0.0 Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/5.0.0/#frequent-window","text":"This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Origin: siddhi-core:5.0.0 Syntax frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"docs/api/5.0.0/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Origin: siddhi-core:5.0.0 Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"docs/api/5.0.0/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Origin: siddhi-core:5.0.0 Syntax lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"docs/api/5.0.0/#lossyfrequent-window","text":"This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Origin: siddhi-core:5.0.0 Syntax lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"docs/api/5.0.0/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Origin: siddhi-core:5.0.0 Syntax session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"docs/api/5.0.0/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Origin: siddhi-core:5.0.0 Syntax sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"docs/api/5.0.0/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Origin: siddhi-core:5.0.0 Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"docs/api/5.0.0/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Origin: siddhi-core:5.0.0 Syntax timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"docs/api/5.0.0/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Origin: siddhi-core:5.0.0 Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"docs/api/5.0.0/#json","text":"","title":"Json"},{"location":"docs/api/5.0.0/#getbool-function","text":"This method returns a 'boolean' value, either 'true' or 'false', based on the valuespecified against the JSON element present in the given path.In case there is no valid boolean value found in the given path, the method still returns 'false'. Origin: siddhi-execution-json:2.0.0 Syntax BOOL json:getBool( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the boolean value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getBool' function fetches theboolean value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getBool(json,\"$.name\") as name insert into OutputStream; This returns the boolean value of the JSON input in the given path. The results are directed to the 'OutputStream' stream.","title":"getBool (Function)"},{"location":"docs/api/5.0.0/#getdouble-function","text":"This method returns the double value of the JSON element present in the given path. If there is no valid double value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax DOUBLE json:getDouble( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getDouble' function fetches thedouble value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getDouble(json,\"$.name\") as name insert into OutputStream; This returns the double value of the given path. The results aredirected to the 'OutputStream' stream.","title":"getDouble (Function)"},{"location":"docs/api/5.0.0/#getfloat-function","text":"This method returns the float value of the JSON element present in the given path.If there is no valid float value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax FLOAT json:getFloat( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getFloat' function fetches thevalue. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getFloat(json,\"$.name\") as name insert into OutputStream; This returns the float value of the JSON input in the given path. The results aredirected to the 'OutputStream' stream.","title":"getFloat (Function)"},{"location":"docs/api/5.0.0/#getint-function","text":"This method returns the integer value of the JSON element present in the given path. If there is no valid integer value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax INT json:getInt( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getInt' function fetches theinteger value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getInt(json,\"$.name\") as name insert into OutputStream; This returns the integer value of the JSON input in the given path. The resultsare directed to the 'OutputStream' stream.","title":"getInt (Function)"},{"location":"docs/api/5.0.0/#getlong-function","text":"This returns the long value of the JSON element present in the given path. Ifthere is no valid long value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax LONG json:getLong( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the JSON element from which the 'getLong' functionfetches the long value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getLong(json,\"$.name\") as name insert into OutputStream; This returns the long value of the JSON input in the given path. The results aredirected to 'OutputStream' stream.","title":"getLong (Function)"},{"location":"docs/api/5.0.0/#getobject-function","text":"This returns the object of the JSON element present in the given path. Origin: siddhi-execution-json:2.0.0 Syntax OBJECT json:getObject( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getObject' function fetches theobject. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getObject(json,\"$.name\") as name insert into OutputStream; This returns the object of the JSON input in the given path. The results are directed to the 'OutputStream' stream.","title":"getObject (Function)"},{"location":"docs/api/5.0.0/#getstring-function","text":"This returns the string value of the JSON element present in the given path. Origin: siddhi-execution-json:2.0.0 Syntax STRING json:getString( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the JSON input from which the 'getString' function fetches the string value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getString(json,\"$.name\") as name insert into OutputStream; This returns the string value of the JSON input in the given path. The results are directed to the 'OutputStream' stream.","title":"getString (Function)"},{"location":"docs/api/5.0.0/#isexists-function","text":"This method checks whether there is a JSON element present in the given path or not.If there is a valid JSON element in the given path, it returns 'true'. If there is no valid JSON element, it returns 'false' Origin: siddhi-execution-json:2.0.0 Syntax BOOL json:isExists( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input in a given path, on which the function performs the search forJSON elements. STRING OBJECT No No path The path that contains the input JSON on which the function performs the search. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:isExists(json,\"$.name\") as name insert into OutputStream; This returns either true or false based on the existence of a JSON element in a given path. The results are directed to the 'OutputStream' stream.","title":"isExists (Function)"},{"location":"docs/api/5.0.0/#setelement-function","text":"This method allows to insert elements into a given JSON present in a specific path. If there is no valid path given, it returns the original JSON. Otherwise, it returns the new JSON. Origin: siddhi-execution-json:2.0.0 Syntax OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT jsonelement, STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input into which is this function inserts the new value. STRING OBJECT No No path The path on the JSON input which is used to insert the given element. STRING No No jsonelement The JSON element which is inserted by the function into the input JSON. STRING BOOL DOUBLE FLOAT INT LONG OBJECT No No key The key which is used to insert the given element into the input JSON. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:setElement(json,\"$.name\") as name insert into OutputStream; This returns the JSON object present in the given path with the newly inserted JSONelement. The results are directed to the 'OutputStream' stream.","title":"setElement (Function)"},{"location":"docs/api/5.0.0/#toobject-function","text":"This method returns the JSON object related to a given JSON string. Origin: siddhi-execution-json:2.0.0 Syntax OBJECT json:toObject( STRING json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON string from which the function generates the JSON object. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:toJson(json) as jsonObject insert into OutputStream; This returns the JSON object corresponding to the given JSON string.The results aredirected to the 'OutputStream' stream.","title":"toObject (Function)"},{"location":"docs/api/5.0.0/#tostring-function","text":"This method returns the JSON string corresponding to a given JSON object. Origin: siddhi-execution-json:2.0.0 Syntax STRING json:toString( OBJECT json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON object from which the function generates a JSON string. OBJECT No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:toString(json) as jsonString insert into OutputStream; This returns the JSON string corresponding to a given JSON object. The results are directed to the 'OutputStream' stream.","title":"toString (Function)"},{"location":"docs/api/5.0.0/#tokenize-stream-processor","text":"This tokenizes the given json according the path provided Origin: siddhi-execution-json:2.0.0 Syntax json:tokenize( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input json that should be tokenized using the given path. STRING OBJECT No No path The path that is used to tokenize the given json STRING No No fail.on.missing.attribute If this parameter is set to 'true' and a json is not provided in the given path, the event is dropped. If the parameter is set to 'false', the unavailability of a json in the specified path results in the event being created with a 'null' value for the json element. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The json element retrieved based on the given path and the json. STRING Examples EXAMPLE 1 define stream InputStream (json string,path string); @info(name = 'query1') from InputStream#json:tokenize(json, path) select jsonElement insert into OutputStream; This query performs a tokenization for the given json using the path specified. If the specified path provides a json array, it generates events for each element in that array by adding an additional attributes as the 'jsonElement' to the stream. e.g., jsonInput - {name:\"John\",enrolledSubjects:[\"Mathematics\",\"Physics\"]}, path - \" .enrolledSubjects\" /code br If we use the configuration in this example, it generates two events with the attributes \"Mathematics\", \"Physics\". br If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream. br code e.g., jsonInput - {name:\"John\",age:25}, path - \" .enrolledSubjects\" </code><br>&nbsp;If we use the configuration in this example, it generates two events with the attributes \"Mathematics\", \"Physics\".<br>If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream. <br><code> e.g., jsonInput - {name:\"John\",age:25}, path - \" .age\"","title":"tokenize (Stream Processor)"},{"location":"docs/api/5.0.0/#tokenizeasobject-stream-processor","text":"This tokenizes the given JSON based on the path provided and returns the response as an object. Origin: siddhi-execution-json:2.0.0 Syntax json:tokenizeAsObject( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input json that is tokenized using the given path. STRING OBJECT No No path The path of the input JSON that the function tokenizes. STRING No No fail.on.missing.attribute If this parameter is set to 'true' and a JSON is not provided in the given path, the event is dropped. If the parameter is set to 'false', the unavailability of a JSON in the specified path results in the event being created with a 'null' value for the json element. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path and the JSON. OBJECT Examples EXAMPLE 1 define stream InputStream (json string,path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select jsonElement insert into OutputStream; This query performs a tokenization for the given JSON using the path specified. If the specified path provides a JSON array, it generates events for each element in the specified json array by adding an additional attribute as the 'jsonElement' into the stream. e.g., jsonInput - {name:\"John\",enrolledSubjects:[\"Mathematics\",\"Physics\"]}, path - \" .enrolledSubjects\" /code br If we use the configuration in the above example, it generates two events with the attributes \"Mathematics\" and \"Physics\". br If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream br code e.g., jsonInput - {name:\"John\",age:25}, path - \" .enrolledSubjects\" </code><br>If we use the configuration in the above example, it generates two events with the attributes \"Mathematics\" and \"Physics\".<br>If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream <br><code> e.g., jsonInput - {name:\"John\",age:25}, path - \" .age\"","title":"tokenizeAsObject (Stream Processor)"},{"location":"docs/api/5.0.0/#math","text":"","title":"Math"},{"location":"docs/api/5.0.0/#percentile-aggregate-function","text":"This functions returns the pth percentile value of a given argument. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:percentile( INT|LONG|FLOAT|DOUBLE arg, DOUBLE p) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value of the parameter whose percentile should be found. INT LONG FLOAT DOUBLE No No p Estimate of the percentile to be found (pth percentile) where p is any number greater than 0 or lesser than or equal to 100. DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (sensorId int, temperature double); from InValueStream select math:percentile(temperature, 97.0) as percentile insert into OutMediationStream; This function returns the percentile value based on the argument given. For example, math:percentile(temperature, 97.0) returns the 97 th percentile value of all the temperature events.","title":"percentile (Aggregate Function)"},{"location":"docs/api/5.0.0/#abs-function","text":"This function returns the absolute value of the given parameter. It wraps the java.lang.Math.abs() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:abs( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The parameter whose absolute value is found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:abs(inValue) as absValue insert into OutMediationStream; Irrespective of whether the 'invalue' in the input stream holds a value of abs(3) or abs(-3),the function returns 3 since the absolute value of both 3 and -3 is 3. The result directed to OutMediationStream stream.","title":"abs (Function)"},{"location":"docs/api/5.0.0/#acos-function","text":"If -1 = p1 = 1, this function returns the arc-cosine (inverse cosine) value of p1.If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.acos() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:acos( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-cosine (inverse cosine) value is found. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:acos(inValue) as acosValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-cosine value of it and returns the arc-cosine value to the output stream, OutMediationStream. For example, acos(0.5) returns 1.0471975511965979.","title":"acos (Function)"},{"location":"docs/api/5.0.0/#asin-function","text":"If -1 = p1 = 1, this function returns the arc-sin (inverse sine) value of p1. If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.asin() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:asin( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-sin (inverse sine) value is found. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:asin(inValue) as asinValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-sin value of it and returns the arc-sin value to the output stream, OutMediationStream. For example, asin(0.5) returns 0.5235987755982989.","title":"asin (Function)"},{"location":"docs/api/5.0.0/#atan-function","text":"1. If a single p1 is received, this function returns the arc-tangent (inverse tangent) value of p1 . 2. If p1 is received along with an optional p1 , it considers them as x and y coordinates and returns the arc-tangent (inverse tangent) value. The returned value is in radian scale. This function wraps the java.lang.Math.atan() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-tangent (inverse tangent) is found. If the optional second parameter is given this represents the x coordinate of the (x,y) coordinate pair. INT LONG FLOAT DOUBLE No No p1 This optional parameter represents the y coordinate of the (x,y) coordinate pair. 0D INT LONG FLOAT DOUBLE Yes No Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:atan(inValue1, inValue2) as convertedValue insert into OutMediationStream; If the 'inValue1' in the input stream is given, the function calculates the arc-tangent value of it and returns the arc-tangent value to the output stream, OutMediationStream. If both the 'inValue1' and 'inValue2' are given, then the function considers them to be x and y coordinates respectively and returns the calculated arc-tangent value to the output stream, OutMediationStream. For example, atan(12d, 5d) returns 1.1760052070951352.","title":"atan (Function)"},{"location":"docs/api/5.0.0/#bin-function","text":"This function returns a string representation of the p1 argument, that is of either 'integer' or 'long' data type, as an unsigned integer in base 2. It wraps the java.lang.Integer.toBinaryString and java.lang.Long.toBinaryString` methods. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:bin( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value in either 'integer' or 'long', that should be converted into an unsigned integer of base 2. INT LONG No No Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:bin(inValue) as binValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function converts it into an unsigned integer in base 2 and directs the output to the output stream, OutMediationStream. For example, bin(9) returns '1001'.","title":"bin (Function)"},{"location":"docs/api/5.0.0/#cbrt-function","text":"This function returns the cube-root of 'p1' which is in radians. It wraps the java.lang.Math.cbrt() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:cbrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cube-root should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cbrt(inValue) as cbrtValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cube-root value for the same and directs the output to the output stream, OutMediationStream. For example, cbrt(17d) returns 2.5712815906582356.","title":"cbrt (Function)"},{"location":"docs/api/5.0.0/#ceil-function","text":"This function returns the smallest double value, i.e., the closest to the negative infinity, that is greater than or equal to the p1 argument, and is equal to a mathematical integer. It wraps the java.lang.Math.ceil() method. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:ceil( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose ceiling value is found. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ceil(inValue) as ceilingValue insert into OutMediationStream; This function calculates the ceiling value of the given 'inValue' and directs the result to 'OutMediationStream' output stream. For example, ceil(423.187d) returns 424.0.","title":"ceil (Function)"},{"location":"docs/api/5.0.0/#conv-function","text":"This function converts a from the fromBase base to the toBase base. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:conv( STRING a, INT from.base, INT to.base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic a The value whose base should be changed. Input should be given as a 'String'. STRING No No from.base The source base of the input parameter 'a'. INT No No to.base The target base that the input parameter 'a' should be converted into. INT No No Examples EXAMPLE 1 define stream InValueStream (inValue string,fromBase int,toBase int); from InValueStream select math:conv(inValue,fromBase,toBase) as convertedValue insert into OutMediationStream; If the 'inValue' in the input stream is given, and the base in which it currently resides in and the base to which it should be converted to is specified, the function converts it into a string in the target base and directs it to the output stream, OutMediationStream. For example, conv(\"7f\", 16, 10) returns \"127\".","title":"conv (Function)"},{"location":"docs/api/5.0.0/#copysign-function","text":"This function returns a value of an input with the received magnitude and sign of another input. It wraps the java.lang.Math.copySign() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:copySign( INT|LONG|FLOAT|DOUBLE magnitude, INT|LONG|FLOAT|DOUBLE sign) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic magnitude The magnitude of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No No sign The sign of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:copySign(inValue1,inValue2) as copysignValue insert into OutMediationStream; If two values are provided as 'inValue1' and 'inValue2', the function copies the magnitude and sign of the second argument into the first one and directs the result to the output stream, OutMediatonStream. For example, copySign(5.6d, -3.0d) returns -5.6.","title":"copySign (Function)"},{"location":"docs/api/5.0.0/#cos-function","text":"This function returns the cosine of p1 which is in radians. It wraps the java.lang.Math.cos() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:cos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cosine value should be found.The input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cos(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cos(6d) returns 0.9601702866503661.","title":"cos (Function)"},{"location":"docs/api/5.0.0/#cosh-function","text":"This function returns the hyperbolic cosine of p1 which is in radians. It wraps the java.lang.Math.cosh() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:cosh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic cosine should be found. The input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cosh(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the hyperbolic cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cosh (6d) returns 201.7156361224559.","title":"cosh (Function)"},{"location":"docs/api/5.0.0/#e-function","text":"This function returns the java.lang.Math.E constant, which is the closest double value to e, where e is the base of the natural logarithms. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:e() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:e() as eValue insert into OutMediationStream; This function returns the constant, 2.7182818284590452354 which is the closest double value to e and directs the output to 'OutMediationStream' output stream.","title":"e (Function)"},{"location":"docs/api/5.0.0/#exp-function","text":"This function returns the Euler's number e raised to the power of p1 . It wraps the java.lang.Math.exp() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:exp( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The power that the Euler's number e is raised to. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:exp(inValue) as expValue insert into OutMediationStream; If the 'inValue' in the inputstream holds a value, this function calculates the corresponding Euler's number 'e' and directs it to the output stream, OutMediationStream. For example, exp(10.23) returns 27722.51006805505.","title":"exp (Function)"},{"location":"docs/api/5.0.0/#floor-function","text":"This function wraps the java.lang.Math.floor() function and returns the largest value, i.e., closest to the positive infinity, that is less than or equal to p1 , and is equal to a mathematical integer. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:floor( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose floor value should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:floor(inValue) as floorValue insert into OutMediationStream; This function calculates the floor value of the given 'inValue' input and directs the output to the 'OutMediationStream' output stream. For example, (10.23) returns 10.0.","title":"floor (Function)"},{"location":"docs/api/5.0.0/#getexponent-function","text":"This function returns the unbiased exponent that is used in the representation of p1 . This function wraps the java.lang.Math.getExponent() function. Origin: siddhi-execution-math:5.0.0 Syntax INT math:getExponent( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of whose unbiased exponent representation should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:getExponent(inValue) as expValue insert into OutMediationStream; This function calculates the unbiased exponent of a given input, 'inValue' and directs the result to the 'OutMediationStream' output stream. For example, getExponent(60984.1) returns 15.","title":"getExponent (Function)"},{"location":"docs/api/5.0.0/#hex-function","text":"This function wraps the java.lang.Double.toHexString() function. It returns a hexadecimal string representation of the input, p1`. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:hex( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hexadecimal value should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue int); from InValueStream select math:hex(inValue) as hexString insert into OutMediationStream; If the 'inValue' in the input stream is provided, the function converts this into its corresponding hexadecimal format and directs the output to the output stream, OutMediationStream. For example, hex(200) returns \"c8\".","title":"hex (Function)"},{"location":"docs/api/5.0.0/#isinfinite-function","text":"This function wraps the java.lang.Float.isInfinite() and java.lang.Double.isInfinite() and returns true if p1 is infinitely large in magnitude and false if otherwise. Origin: siddhi-execution-math:5.0.0 Syntax BOOL math:isInfinite( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 This is the value of the parameter that the function determines to be either infinite or finite. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isInfinite(inValue1) as isInfinite insert into OutMediationStream; If the value given in the 'inValue' in the input stream is of infinitely large magnitude, the function returns the value, 'true' and directs the result to the output stream, OutMediationStream'. For example, isInfinite(java.lang.Double.POSITIVE_INFINITY) returns true.","title":"isInfinite (Function)"},{"location":"docs/api/5.0.0/#isnan-function","text":"This function wraps the java.lang.Float.isNaN() and java.lang.Double.isNaN() functions and returns true if p1 is NaN (Not-a-Number), and returns false if otherwise. Origin: siddhi-execution-math:5.0.0 Syntax BOOL math:isNan( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter which the function determines to be either NaN or a number. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isNan(inValue1) as isNaN insert into OutMediationStream; If the 'inValue1' in the input stream has a value that is undefined, then the function considers it as an 'NaN' value and directs 'True' to the output stream, OutMediationStream. For example, isNan(java.lang.Math.log(-12d)) returns true.","title":"isNan (Function)"},{"location":"docs/api/5.0.0/#ln-function","text":"This function returns the natural logarithm (base e) of p1 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:ln( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose natural logarithm (base e) should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ln(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates its natural logarithm (base e) and directs the results to the output stream, 'OutMeditionStream'. For example, ln(11.453) returns 2.438251704415579.","title":"ln (Function)"},{"location":"docs/api/5.0.0/#log-function","text":"This function returns the logarithm of the received number as per the given base . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:log( INT|LONG|FLOAT|DOUBLE number, INT|LONG|FLOAT|DOUBLE base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic number The value of the parameter whose base should be changed. INT LONG FLOAT DOUBLE No No base The base value of the ouput. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (number double, base double); from InValueStream select math:log(number, base) as logValue insert into OutMediationStream; If the number and the base to which it has to be converted into is given in the input stream, the function calculates the number to the base specified and directs the result to the output stream, OutMediationStream. For example, log(34, 2f) returns 5.08746284125034.","title":"log (Function)"},{"location":"docs/api/5.0.0/#log10-function","text":"This function returns the base 10 logarithm of p1 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:log10( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 10 logarithm should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log10(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 10 logarithm of the same and directs the result to the output stream, OutMediatioStream. For example, log10(19.234) returns 1.2840696117100832.","title":"log10 (Function)"},{"location":"docs/api/5.0.0/#log2-function","text":"This function returns the base 2 logarithm of p1 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:log2( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 2 logarithm should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log2(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 2 logarithm of the same and returns the value to the output stream, OutMediationStream. For example log2(91d) returns 6.507794640198696.","title":"log2 (Function)"},{"location":"docs/api/5.0.0/#max-function","text":"This function returns the greater value of p1 and p2 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:max( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values to be compared in order to find the larger value of the two INT LONG FLOAT DOUBLE No No p2 The input value to be compared with 'p1' in order to find the larger value of the two. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:max(inValue1,inValue2) as maxValue insert into OutMediationStream; If two input values 'inValue1, and 'inValue2' are given, the function compares them and directs the larger value to the output stream, OutMediationStream. For example, max(123.67d, 91) returns 123.67.","title":"max (Function)"},{"location":"docs/api/5.0.0/#min-function","text":"This function returns the smaller value of p1 and p2 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:min( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values that are to be compared in order to find the smaller value. INT LONG FLOAT DOUBLE No No p2 The input value that is to be compared with 'p1' in order to find the smaller value. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:min(inValue1,inValue2) as minValue insert into OutMediationStream; If two input values, 'inValue1' and 'inValue2' are given, the function compares them and directs the smaller value of the two to the output stream, OutMediationStream. For example, min(123.67d, 91) returns 91.","title":"min (Function)"},{"location":"docs/api/5.0.0/#oct-function","text":"This function converts the input parameter p1 to octal. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:oct( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose octal representation should be found. INT LONG No No Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:oct(inValue) as octValue insert into OutMediationStream; If the 'inValue' in the input stream is given, this function calculates the octal value corresponding to the same and directs it to the output stream, OutMediationStream. For example, oct(99l) returns \"143\".","title":"oct (Function)"},{"location":"docs/api/5.0.0/#parsedouble-function","text":"This function returns the double value of the string received. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:parseDouble( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a double value. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseDouble(inValue) as output insert into OutMediationStream; If the 'inValue' in the input stream holds a value, this function converts it into the corresponding double value and directs it to the output stream, OutMediationStream. For example, parseDouble(\"123\") returns 123.0.","title":"parseDouble (Function)"},{"location":"docs/api/5.0.0/#parsefloat-function","text":"This function returns the float value of the received string. Origin: siddhi-execution-math:5.0.0 Syntax FLOAT math:parseFloat( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a float value. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseFloat(inValue) as output insert into OutMediationStream; The function converts the input value given in 'inValue',into its corresponding float value and directs the result into the output stream, OutMediationStream. For example, parseFloat(\"123\") returns 123.0.","title":"parseFloat (Function)"},{"location":"docs/api/5.0.0/#parseint-function","text":"This function returns the integer value of the received string. Origin: siddhi-execution-math:5.0.0 Syntax INT math:parseInt( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to an integer. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseInt(inValue) as output insert into OutMediationStream; The function converts the 'inValue' into its corresponding integer value and directs the output to the output stream, OutMediationStream. For example, parseInt(\"123\") returns 123.","title":"parseInt (Function)"},{"location":"docs/api/5.0.0/#parselong-function","text":"This function returns the long value of the string received. Origin: siddhi-execution-math:5.0.0 Syntax LONG math:parseLong( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to a long value. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseLong(inValue) as output insert into OutMediationStream; The function converts the 'inValue' to its corresponding long value and directs the result to the output stream, OutMediationStream. For example, parseLong(\"123\") returns 123.","title":"parseLong (Function)"},{"location":"docs/api/5.0.0/#pi-function","text":"This function returns the java.lang.Math.PI constant, which is the closest value to pi, i.e., the ratio of the circumference of a circle to its diameter. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:pi() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:pi() as piValue insert into OutMediationStream; pi() always returns 3.141592653589793.","title":"pi (Function)"},{"location":"docs/api/5.0.0/#power-function","text":"This function raises the given value to a given power. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:power( INT|LONG|FLOAT|DOUBLE value, INT|LONG|FLOAT|DOUBLE to.power) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value The value that should be raised to the power of 'to.power' input parameter. INT LONG FLOAT DOUBLE No No to.power The power to which the 'value' input parameter should be raised. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:power(inValue1,inValue2) as powerValue insert into OutMediationStream; This function raises the 'inValue1' to the power of 'inValue2' and directs the output to the output stream, 'OutMediationStream. For example, (5.6d, 3.0d) returns 175.61599999999996.","title":"power (Function)"},{"location":"docs/api/5.0.0/#rand-function","text":"This returns a stream of pseudo-random numbers when a sequence of calls are sent to the rand() . Optionally, it is possible to define a seed, i.e., rand(seed) using which the pseudo-random numbers are generated. These functions internally use the java.util.Random class. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:rand( INT|LONG seed) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic seed An optional seed value that will be used to generate the random number sequence. defaultSeed INT LONG Yes No Examples EXAMPLE 1 define stream InValueStream (symbol string, price long, volume long); from InValueStream select symbol, math:rand() as randNumber select math:oct(inValue) as octValue insert into OutMediationStream; In the example given above, a random double value between 0 and 1 will be generated using math:rand().","title":"rand (Function)"},{"location":"docs/api/5.0.0/#round-function","text":"This function returns the value of the input argument rounded off to the closest integer/long value. Origin: siddhi-execution-math:5.0.0 Syntax INT|LONG math:round( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be rounded off to the closest integer/long value. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:round(inValue) as roundValue insert into OutMediationStream; The function rounds off 'inValue1' to the closest int/long value and directs the output to the output stream, 'OutMediationStream'. For example, round(3252.353) returns 3252.","title":"round (Function)"},{"location":"docs/api/5.0.0/#signum-function","text":"This returns +1, 0, or -1 for the given positive, zero and negative values respectively. This function wraps the java.lang.Math.signum() function. Origin: siddhi-execution-math:5.0.0 Syntax INT math:signum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be checked to be positive, negative or zero. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:signum(inValue) as sign insert into OutMediationStream; The function evaluates the 'inValue' given to be positive, negative or zero and directs the result to the output stream, 'OutMediationStream'. For example, signum(-6.32d) returns -1.","title":"signum (Function)"},{"location":"docs/api/5.0.0/#sin-function","text":"This returns the sine of the value given in radians. This function wraps the java.lang.Math.sin() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:sin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sin(inValue) as sinValue insert into OutMediationStream; The function calculates the sine value of the given 'inValue' and directs the output to the output stream, 'OutMediationStream. For example, sin(6d) returns -0.27941549819892586.","title":"sin (Function)"},{"location":"docs/api/5.0.0/#sinh-function","text":"This returns the hyperbolic sine of the value given in radians. This function wraps the java.lang.Math.sinh() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:sinh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sinh(inValue) as sinhValue insert into OutMediationStream; This function calculates the hyperbolic sine value of 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sinh(6d) returns 201.71315737027922.","title":"sinh (Function)"},{"location":"docs/api/5.0.0/#sqrt-function","text":"This function returns the square-root of the given value. It wraps the java.lang.Math.sqrt() s function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:sqrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose square-root value should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sqrt(inValue) as sqrtValue insert into OutMediationStream; The function calculates the square-root value of the 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sqrt(4d) returns 2.","title":"sqrt (Function)"},{"location":"docs/api/5.0.0/#tan-function","text":"This function returns the tan of the given value in radians. It wraps the java.lang.Math.tan() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:tan( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose tan value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tan(inValue) as tanValue insert into OutMediationStream; This function calculates the tan value of the 'inValue' given and directs the output to the output stream, 'OutMediationStream'. For example, tan(6d) returns -0.29100619138474915.","title":"tan (Function)"},{"location":"docs/api/5.0.0/#tanh-function","text":"This function returns the hyperbolic tangent of the value given in radians. It wraps the java.lang.Math.tanh() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:tanh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic tangent value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tanh(inValue) as tanhValue insert into OutMediationStream; If the 'inVaue' in the input stream is given, this function calculates the hyperbolic tangent value of the same and directs the output to 'OutMediationStream' stream. For example, tanh(6d) returns 0.9999877116507956.","title":"tanh (Function)"},{"location":"docs/api/5.0.0/#todegrees-function","text":"This function converts the value given in radians to degrees. It wraps the java.lang.Math.toDegrees() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:toDegrees( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in radians that should be converted to degrees. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toDegrees(inValue) as degreesValue insert into OutMediationStream; The function converts the 'inValue' in the input stream from radians to degrees and directs the output to 'OutMediationStream' output stream. For example, toDegrees(6d) returns 343.77467707849394.","title":"toDegrees (Function)"},{"location":"docs/api/5.0.0/#toradians-function","text":"This function converts the value given in degrees to radians. It wraps the java.lang.Math.toRadians() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:toRadians( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in degrees that should be converted to radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toRadians(inValue) as radiansValue insert into OutMediationStream; This function converts the input, from degrees to radians and directs the result to 'OutMediationStream' output stream. For example, toRadians(6d) returns 0.10471975511965977.","title":"toRadians (Function)"},{"location":"docs/api/5.0.0/#rdbms","text":"","title":"Rdbms"},{"location":"docs/api/5.0.0/#cud-stream-processor","text":"This function performs SQL CUD (INSERT, UPDATE, DELETE) queries on WSO2 datasources. Note: This function is only available when running Siddhi with WSO2 SP. Origin: siddhi-store-rdbms:6.0.0 Syntax rdbms:cud( STRING datasource.name, STRING query, STRING parameter.n) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the WSO2 datasource for which the query should be performed. STRING No No query The update, delete, or insert query(formatted according to the relevant database type) that needs to be performed. STRING No No parameter.n If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING No No System Parameters Name Description Default Value Possible Parameters perform.CUD.operations If this parameter is set to 'true', the RDBMS CUD function is enabled to perform CUD operations. false true false Extra Return Attributes Name Description Possible Types numRecords The number of records manipulated by the query. INT Examples EXAMPLE 1 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName='abc' where customerName='xyz'\") select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. EXAMPLE 2 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName=? where customerName=?\", changedName, previousName) select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. Here the values of attributes changedName and previousName in the event will be set to the query.","title":"cud (Stream Processor)"},{"location":"docs/api/5.0.0/#query-stream-processor","text":"This function performs SQL retrieval queries on WSO2 datasources. Note: This function is only available when running Siddhi with WSO2 SP. Origin: siddhi-store-rdbms:6.0.0 Syntax rdbms:query( STRING datasource.name, STRING query, STRING parameter.n, STRING attribute.definition.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the WSO2 datasource for which the query should be performed. STRING No No query The select query(formatted according to the relevant database type) that needs to be performed STRING No No parameter.n If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING No No attribute.definition.list This is provided as a comma-separated list in the ' AttributeName AttributeType ' format. The SQL query is expected to return the attributes in the given order. e.g., If one attribute is defined here, the SQL query should return one column result set. If more than one column is returned, then the first column is processed. The Siddhi data types supported are 'STRING', 'INT', 'LONG', 'DOUBLE', 'FLOAT', and 'BOOL'. Mapping of the Siddhi data type to the database data type can be done as follows, Siddhi Datatype - Datasource Datatype STRING - CHAR , VARCHAR , LONGVARCHAR INT - INTEGER LONG - BIGINT DOUBLE - DOUBLE FLOAT - REAL BOOL - BIT STRING No No Extra Return Attributes Name Description Possible Types attributeName The return attributes will be the ones defined in the parameter attribute.definition.list . STRING INT LONG DOUBLE FLOAT BOOL Examples EXAMPLE 1 from TriggerStream#rdbms:query('SAMPLE_DB', 'select * from Transactions_Table', 'creditcardno string, country string, transaction string, amount int') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). EXAMPLE 2 from TriggerStream#rdbms:query('SAMPLE_DB', 'select * from where country=? ', countrySearchWord, 'creditcardno string, country string, transaction string, amount int') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). countrySearchWord value from the event will be set in the query when querying the datasource.","title":"query (Stream Processor)"},{"location":"docs/api/5.0.0/#regex","text":"","title":"Regex"},{"location":"docs/api/5.0.0/#find-function","text":"These methods attempt to find the subsequence of the 'inputSequence' that matches the given 'regex' pattern. Origin: siddhi-execution-regex:5.0.0 Syntax BOOL regex:find( STRING regex, STRING input.sequence, INT starting.index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression that is matched to a sequence in order to find the subsequence of the same. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No starting.index The starting index of the input sequence from where the input sequence ismatched with the given regex pattern. eg: 1, 2. INT No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string); from InputStream select inputSequence , regex:find(\\d\\d(.*)WSO2, 21 products are produced by WSO2 currently) as aboutWSO2 insert into OutputStream; This method attempts to find the subsequence of the 'inputSequence' that matches the regex pattern, \\d\\d(.*)WSO2. It returns true as a subsequence exists. EXAMPLE 2 define stream InputStream (inputSequence string, price long, regex string); from InputStream select inputSequence , regex:find(\\d\\d(.*)WSO2, 21 products are produced currently) as aboutWSO2 insert into OutputStream; This method attempts to find the subsequence of the 'inputSequence' that matches the regex pattern, \\d\\d(.*)WSO2 . It returns 'false' as a subsequence does not exist. EXAMPLE 3 define stream InputStream (inputSequence string, price long, regex string); from InputStream select inputSequence , regex:find(\\d\\d(.*)WSO2, 21 products are produced within 10 years by WSO2 currently by WSO2 employees, 30) as aboutWSO2 insert into OutputStream; This method attempts to find the subsequence of the 'inputSequence' that matches the regex pattern, \\d\\d(.*)WSO2 starting from index 30. It returns 'true' since a subsequence exists.","title":"find (Function)"},{"location":"docs/api/5.0.0/#group-function","text":"This method returns the input subsequence captured by the given group during the previous match operation. Origin: siddhi-execution-regex:5.0.0 Syntax STRING regex:group( STRING regex, STRING input.sequence, INT group.id) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No group.id The given group id of the regex expression. For example, 0, 1, 2, etc. INT No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:group(\\d\\d(.*)(WSO2.*), 21 products are produced within 10 years by WSO2 currently by WSO2 employees, 3) insert into OutputStream; This function returns 'WSO2 employees', the input subsequence captured within the given groupID, 3 after grouping the 'inputSequence' according to the regex pattern, \\d\\d(. )(WSO2. ).","title":"group (Function)"},{"location":"docs/api/5.0.0/#lookingat-function","text":"This method attempts to match the 'inputSequence', from the beginning, against the 'regex' pattern. Origin: siddhi-execution-regex:5.0.0 Syntax BOOL regex:lookingAt( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:lookingAt(\\d\\d(.*)(WSO2.*), 21 products are produced by WSO2 currently in Sri Lanka) This method attempts to match the 'inputSequence' against the regex pattern, \\d\\d(. )(WSO2. ) from the beginning. Since it matches, the function returns 'true'. EXAMPLE 2 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:lookingAt(WSO2(.*)middleware(.*), sample test string and WSO2 is situated in trace and it's a middleware company) This method attempts to match the 'inputSequence' against the regex pattern, WSO2(. )middleware(. ) from the beginning. Since it does not match, the function returns false.","title":"lookingAt (Function)"},{"location":"docs/api/5.0.0/#matches-function","text":"This method attempts to match the entire 'inputSequence' against the 'regex' pattern. Origin: siddhi-execution-regex:5.0.0 Syntax BOOL regex:matches( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:matches(WSO2(.*)middleware(.*), WSO2 is situated in trace and its a middleware company) This method attempts to match the entire 'inputSequence' against WSO2(. )middleware(. ) regex pattern. Since it matches, it returns 'true'. EXAMPLE 2 define stream inputStream (inputSequence string, price long, regex string, group int); from inputStream select inputSequence, regex:matches(WSO2(.*)middleware, WSO2 is situated in trace and its a middleware company) This method attempts to match the entire 'inputSequence' against WSO2(.*)middleware regex pattern. Since it does not match, it returns 'false'.","title":"matches (Function)"},{"location":"docs/api/5.0.0/#sink","text":"","title":"Sink"},{"location":"docs/api/5.0.0/#email-sink","text":"The email sink uses the 'smtp' server to publish events via emails. The events can be published in 'text', 'xml' or 'json' formats. The user can define email sink parameters in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or in the stream definition. The email sink first checks the stream definition for parameters, and if they are no configured there, it checks the 'deployment.yaml' file. If the parameters are not configured in either place, default values are considered for optional parameters. If you need to configure server system parameters that are not provided as options in the stream definition, then those parameters need to be defined them in the 'deployment.yaml' file under 'email sink properties'. For more information about the SMTP server parameters, see https://javaee.github.io/javamail/SMTP-Transport. Further, some email accounts are required to enable the 'access to less secure apps' option. For gmail accounts, you can enable this option via https://myaccount.google.com/lesssecureapps. Origin: siddhi-io-email:2.0.1 Syntax @sink(type=\"email\", username=\" STRING \", address=\" STRING \", password=\" STRING \", host=\" STRING \", port=\" INT \", ssl.enable=\" BOOL \", auth=\" BOOL \", content.type=\" STRING \", subject=\" STRING \", to=\" STRING \", cc=\" STRING \", bcc=\" STRING \", attachments=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The username of the email account that is used to send emails. e.g., 'abc' is the username of the 'abc@gmail.com' account. STRING No No address The address of the email account that is used to send emails. STRING No No password The password of the email account. STRING No No host The host name of the SMTP server. e.g., 'smtp.gmail.com' is a host name for a gmail account. The default value 'smtp.gmail.com' is only valid if the email account is a gmail account. smtp.gmail.com STRING Yes No port The port that is used to create the connection. '465' the default value is only valid is SSL is enabled. INT Yes No ssl.enable This parameter specifies whether the connection should be established via a secure connection or not. The value can be either 'true' or 'false'. If it is 'true', then the connection is establish via the 493 port which is a secure connection. true BOOL Yes No auth This parameter specifies whether to use the 'AUTH' command when authenticating or not. If the parameter is set to 'true', an attempt is made to authenticate the user using the 'AUTH' command. true BOOL Yes No content.type The content type can be either 'text/plain' or 'text/html'. text/plain STRING Yes No subject The subject of the mail to be send. STRING No Yes to The address of the 'to' recipient. If there are more than one 'to' recipients, then all the required addresses can be given as a comma-separated list. STRING No Yes cc The address of the 'cc' recipient. If there are more than one 'cc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No bcc The address of the 'bcc' recipient. If there are more than one 'bcc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No attachments File paths of the files that need to be attached to the email. These paths should be absolute paths. They can be either directories or files . If the path is to a directory, all the files located at the first level (i.e., not within another sub directory) are attached. None STRING Yes Yes System Parameters Name Description Default Value Possible Parameters mail.smtp.ssl.trust If this parameter is se, and a socket factory has not been specified, it enables the use of a MailSSLSocketFactory. If this parameter is set to \" \", all the hosts are trusted. If it is set to a whitespace-separated list of hosts, only those specified hosts are trusted. If not, the hosts trusted depends on the certificate presented by the server. String mail.smtp.connectiontimeout The socket connection timeout value in milliseconds. infinite timeout Any Integer mail.smtp.timeout The socket I/O timeout value in milliseconds. infinite timeout Any Integer mail.smtp.from The email address to use for the SMTP MAIL command. This sets the envelope return address. Defaults to msg.getFrom() or InternetAddress.getLocalAddress(). Any valid email address mail.smtp.localport The local port number to bind to when creating the SMTP socket. Defaults to the port number picked by the Socket class. Any Integer mail.smtp.ehlo If this parameter is set to 'false', you must not attempt to sign in with the EHLO command. true true or false mail.smtp.auth.login.disable If this is set to 'true', it is not allowed to use the 'AUTH LOGIN' command. false true or false mail.smtp.auth.plain.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH PLAIN' command. false true or false mail.smtp.auth.digest-md5.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH DIGEST-MD5' command. false true or false mail.smtp.auth.ntlm.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH NTLM' command false true or false mail.smtp.auth.ntlm.domain The NTLM authentication domain. None The valid NTLM authentication domain name. mail.smtp.auth.ntlm.flags NTLM protocol-specific flags. For more details, see http://curl.haxx.se/rfc/ntlm.html#theNtlmFlags. None Valid NTLM protocol-specific flags. mail.smtp.dsn.notify The NOTIFY option to the RCPT command. None Either 'NEVER', or a combination of 'SUCCESS', 'FAILURE', and 'DELAY' (separated by commas). mail.smtp.dsn.ret The 'RET' option to the 'MAIL' command. None Either 'FULL' or 'HDRS'. mail.smtp.sendpartial If this parameter is set to 'true' and a message is addressed to both valid and invalid addresses, the message is sent with a log that reports the partial failure with a 'SendFailedException' error. If this parameter is set to 'false' (which is default), the message is not sent to any of the recipients when the recipient lists contain one or more invalid addresses. false true or false mail.smtp.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.smtp.sasl.mechanisms Enter a space or a comma-separated list of SASL mechanism names that the system shouldt try to use. None mail.smtp.sasl.authorizationid The authorization ID to be used in the SASL authentication. If no value is specified, the authentication ID (i.e., username) is used. username Valid ID mail.smtp.sasl.realm The realm to be used with the 'DIGEST-MD5' authentication. None mail.smtp.quitwait If this parameter is set to 'false', the 'QUIT' command is issued and the connection is immediately closed. If this parameter is set to 'true' (which is default), the transport waits for the response to the QUIT command. false true or false mail.smtp.reportsuccess If this parameter is set to 'true', the transport to includes an 'SMTPAddressSucceededException' for each address to which the message is successfully delivered. false true or false mail.smtp.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create SMTP sockets. None Socket Factory mail.smtp.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory interface'. This class is used to create SMTP sockets. None mail.smtp.socketFactory.fallback If this parameter is set to 'true', the failure to create a socket using the specified socket factory class causes the socket to be created using the 'java.net.Socket' class. true true or false mail.smtp.socketFactory.port This specifies the port to connect to when using the specified socket factory. 25 Valid port number mail.smtp.ssl.protocols This specifies the SSL protocols that need to be enabled for the SSL connections. None This parameter specifies a whitespace separated list of tokens that are acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. mail.smtp.starttls.enable If this parameter is set to 'true', it is possible to issue the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.smtp.starttls.required If this parameter is set to 'true', it is required to use the 'STARTTLS' command. If the server does not support the 'STARTTLS' command, or if the command fails, the connection method will fail. false true or false mail.smtp.socks.host This specifies the host name of a SOCKS5 proxy server to be used for the connections to the mail server. None mail.smtp.socks.port This specifies the port number for the SOCKS5 proxy server. This needs to be used only if the proxy server is not using the standard port number 1080. 1080 valid port number mail.smtp.auth.ntlm.disable If this parameter is set to 'true', the AUTH NTLM command cannot be issued. false true or false mail.smtp.mailextension The extension string to be appended to the MAIL command. None mail.smtp.userset If this parameter is set to 'true', you should use the 'RSET' command instead of the 'NOOP' command in the 'isConnected' method. In some scenarios, 'sendmail' responds slowly after many 'NOOP' commands. This is avoided by using 'RSET' instead. false true or false Examples EXAMPLE 1 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to publish events via an email sink based on the values provided for the mandatory parameters. As shown in the example, it publishes events from the 'FooStream' in 'json' format as emails to the specified 'to' recipients via the email sink. The email is sent from the 'sender.account@gmail.com' email address via a secure connection. EXAMPLE 2 @sink(type='email', @map(type ='json'), subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to configure the query parameters and the system parameters in the 'deployment.yaml' file. Corresponding parameters need to be configured under 'email', and namespace:'sink' as follows: siddhi: extensions: - extension: name:'email' namespace:'sink' properties: username: sender's email username address: sender's email address password: sender's email password As shown in the example, events from the FooStream are published in 'json' format via the email sink as emails to the given 'to' recipients. The email is sent from the 'sender.account@gmail.com' address via a secure connection. EXAMPLE 3 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.com)define stream FooStream (name string, age int, country string); This example illustrates how to publish events via the email sink. Events from the 'FooStream' stream are published in 'xml' format via the email sink as a text/html message and sent to the specified 'to', 'cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value of the 'name' parameter in the corresponding output event. EXAMPLE 4 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.comattachments= '{{attachments}}')define stream FooStream (name string, age int, country string, attachments string); This example illustrates how to publish events via the email sink. Here, the email also contains attachments. Events from the FooStream are published in 'xml' format via the email sink as a 'text/html' message to the specified 'to','cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value for the 'name' parameter in the corresponding output event. The attachments included in the email message are the local files available in the path specified as the value for the 'attachments' attribute.","title":"email (Sink)"},{"location":"docs/api/5.0.0/#http-sink","text":"This extension publish the HTTP events in any HTTP method POST, GET, PUT, DELETE via HTTP or https protocols. As the additional features this component can provide basic authentication as well as user can publish events using custom client truststore files when publishing events via https protocol. And also user can add any number of headers including HTTP_METHOD header for each event dynamically. Following content types will be set by default according to the type of sink mapper used. You can override them by setting the new content types in headers. - TEXT : text/plain - XML : application/xml - JSON : application/json - KEYVALUE : application/x-www-form-urlencoded Origin: siddhi-io-http:2.0.4 Syntax @sink(type=\"http\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", client.enable.session.creation=\" STRING \", follow.redirect=\" BOOL \", max.redirect.count=\" INT \", tls.store.type=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configuration=\" STRING \", client.bootstrap.nodelay=\" BOOL \", client.bootstrap.keepalive=\" BOOL \", client.bootstrap.sendbuffersize=\" INT \", client.bootstrap.recievebuffersize=\" INT \", client.bootstrap.connect.timeout=\" INT \", client.bootstrap.socket.reuse=\" BOOL \", client.bootstrap.socket.timeout=\" STRING \", client.threadpool.configurations=\" STRING \", client.connection.pool.count=\" INT \", client.max.active.connections.per.pool=\" INT \", client.min.idle.connections.per.pool=\" INT \", client.max.idle.connections.per.pool=\" INT \", client.min.eviction.idle.time=\" STRING \", sender.thread.count=\" STRING \", event.group.executor.thread.size=\" STRING \", max.wait.for.client.connection.pool=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", refresh.token=\" STRING \", token.url=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published via HTTP. This is a mandatory parameter and if this is not specified, an error is logged in the CLI. If user wants to enable SSL for the events, use https instead of http in the publisher.url.e.g., http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No basic.auth.username The username to be included in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No basic.auth.password The password to include in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No https.truststore.file The file path to the location of the truststore of the client that sends the HTTP events through 'https' protocol. A custom client-truststore can be specified if required. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified and the protocol of URL is 'https' then, the system uses default password. wso2carbon STRING Yes No headers The headers that should be included as HTTP request headers. There can be any number of headers concatenated in following format. \"'header1:value1','header2:value2'\". User can include Content-Type header if he needs to use a specific content-type for the payload. Or else, system decides the Content-Type by considering the type of sink mapper, in following way. - @map(xml):application/xml - @map(json):application/json - @map(text):plain/text ) - if user does not include any mapping type then the system gets 'plain/text' as default Content-Type header. Note that providing content-length as a header is not supported. The size of the payload will be automatically calculated and included in the content-length header. STRING Yes No method For HTTP events, HTTP_METHOD header should be included as a request header. If the parameter is null then system uses 'POST' as a default header. POST STRING Yes No socket.idle.timeout Socket timeout value in millisecond 6000 INT Yes No chunk.disabled This parameter is used to disable/enable chunked transfer encoding false BOOL Yes No ssl.protocol The SSL protocol version TLS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No client.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No follow.redirect Redirect related enabled. true BOOL Yes No max.redirect.count Maximum redirect count. 5 INT Yes No tls.store.type TLS store type to be used. JKS STRING Yes No proxy.host Proxy server host null STRING Yes No proxy.port Proxy server port null STRING Yes No proxy.username Proxy server username null STRING Yes No proxy.password Proxy server password null STRING Yes No client.bootstrap.configuration Client bootsrap configurations. Expected format of these parameters is as follows: \"'client.bootstrap.nodelay:xxx','client.bootstrap.keepalive:xxx'\" TODO STRING Yes No client.bootstrap.nodelay Http client no delay. true BOOL Yes No client.bootstrap.keepalive Http client keep alive. true BOOL Yes No client.bootstrap.sendbuffersize Http client send buffer size. 1048576 INT Yes No client.bootstrap.recievebuffersize Http client receive buffer size. 1048576 INT Yes No client.bootstrap.connect.timeout Http client connection timeout. 15000 INT Yes No client.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No client.bootstrap.socket.timeout Http client socket timeout. 15 STRING Yes No client.threadpool.configurations Thread pool configuration. Expected format of these parameters is as follows: \"'client.connection.pool.count:xxx','client.max.active.connections.per.pool:xxx'\" TODO STRING Yes No client.connection.pool.count Connection pool count. 0 INT Yes No client.max.active.connections.per.pool Active connections per pool. -1 INT Yes No client.min.idle.connections.per.pool Minimum ideal connection per pool. 0 INT Yes No client.max.idle.connections.per.pool Maximum ideal connection per pool. 100 INT Yes No client.min.eviction.idle.time Minimum eviction idle time. 5 * 60 * 1000 STRING Yes No sender.thread.count Http sender thread count. 20 STRING Yes No event.group.executor.thread.size Event group executor thread size. 15 STRING Yes No max.wait.for.client.connection.pool Maximum wait for client connection pool. 60000 STRING Yes No oauth.username The username to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No oauth.password The password to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No consumer.key consumer key for the Http request. It is only applicable for for Oauth requests STRING Yes No consumer.secret consumer secret for the Http request. It is only applicable for for Oauth requests STRING Yes No refresh.token refresh token for the Http request. It is only applicable for for Oauth requests STRING Yes No token.url token url for generate a new access token. It is only applicable for for Oauth requests STRING Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapBossGroupSize property to configure number of boss threads, which accepts incoming connections until the ports are unbound. Once connection accepts successfully, boss thread passes the accepted channel to one of the worker threads. Number of available processors Any integer clientBootstrapWorkerGroupSize property to configure number of worker threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer clientBootstrapClientGroupSize property to configure number of client threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client-truststore.jks trustStorePassword The default truststore password. wso2carbon Truststore password Examples EXAMPLE 1 @sink(type='http',publisher.url='http://localhost:8009/foo', method='{{method}}',headers=\"'content-type:xml','content-length:94'\", client.bootstrap.configuration=\"'client.bootstrap.socket.timeout:20', 'client.bootstrap.worker.group.size:10'\", client.pool.configuration=\"'client.connection.pool.count:10','client.max.active.connections.per.pool:1'\", @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody String, method string, headers string); If it is xml mapping expected input should be in following format for FooStream: { events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events , POST, Content-Length:24#Content-Location:USA#Retry-After:120 } Above event will generate output as below. ~Output http event payload events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events ~Output http event headers Content-Length:24, Content-Location:'USA', Retry-After:120, Content-Type:'application/xml', HTTP_METHOD:'POST', ~Output http event properties HTTP_METHOD:'POST', HOST:'localhost', PORT:8009, PROTOCOL:'http', TO:'/foo'","title":"http (Sink)"},{"location":"docs/api/5.0.0/#http-request-sink","text":"This extension publish the HTTP events in any HTTP method POST, GET, PUT, DELETE via HTTP or https protocols. As the additional features this component can provide basic authentication as well as user can publish events using custom client truststore files when publishing events via https protocol. And also user can add any number of headers including HTTP_METHOD header for each event dynamically. Following content types will be set by default according to the type of sink mapper used. You can override them by setting the new content types in headers. - TEXT : text/plain - XML : application/xml - JSON : application/json - KEYVALUE : application/x-www-form-urlencoded HTTP request sink is correlated with the The HTTP reponse source, through a unique sink.id .It sends the request to the defined url and the response is received by the response source which has the same 'sink.id'. Origin: siddhi-io-http:2.0.4 Syntax @sink(type=\"http-request\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", client.enable.session.creation=\" STRING \", follow.redirect=\" BOOL \", max.redirect.count=\" INT \", tls.store.type=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configuration=\" STRING \", client.bootstrap.nodelay=\" BOOL \", client.bootstrap.keepalive=\" BOOL \", client.bootstrap.sendbuffersize=\" INT \", client.bootstrap.recievebuffersize=\" INT \", client.bootstrap.connect.timeout=\" INT \", client.bootstrap.socket.reuse=\" BOOL \", client.bootstrap.socket.timeout=\" STRING \", client.threadpool.configurations=\" STRING \", client.connection.pool.count=\" INT \", client.max.active.connections.per.pool=\" INT \", client.min.idle.connections.per.pool=\" INT \", client.max.idle.connections.per.pool=\" INT \", client.min.eviction.idle.time=\" STRING \", sender.thread.count=\" STRING \", event.group.executor.thread.size=\" STRING \", max.wait.for.client.connection.pool=\" STRING \", sink.id=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", refresh.token=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published via HTTP. This is a mandatory parameter and if this is not specified, an error is logged in the CLI. If user wants to enable SSL for the events, use https instead of http in the publisher.url. e.g., http://localhost:8080/endpoint , https://localhost:8080/endpoint This can be used as a dynamic parameter as well. STRING No Yes basic.auth.username The username to be included in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No basic.auth.password The password to include in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No https.truststore.file The file path to the location of the truststore of the client that sends the HTTP events through 'https' protocol. A custom client-truststore can be specified if required. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified and the protocol of URL is 'https' then, the system uses default password. wso2carbon STRING Yes No headers The headers that should be included as HTTP request headers. There can be any number of headers concatenated in following format. \"'header1:value1','header2:value2'\". User can include Content-Type header if he needs to use a specific content-type for the payload. Or else, system decides the Content-Type by considering the type of sink mapper, in following way. - @map(xml):application/xml - @map(json):application/json - @map(text):plain/text ) - if user does not include any mapping type then the system gets 'plain/text' as default Content-Type header. Note that providing content-length as a header is not supported. The size of the payload will be automatically calculated and included in the content-length header. STRING Yes No method For HTTP events, HTTP_METHOD header should be included as a request header. If the parameter is null then system uses 'POST' as a default header. POST STRING Yes No socket.idle.timeout Socket timeout value in millisecond 6000 INT Yes No chunk.disabled port: Port number of the remote service false BOOL Yes No ssl.protocol The SSL protocol version TLS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No client.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No follow.redirect Redirect related enabled. true BOOL Yes No max.redirect.count Maximum redirect count. 5 INT Yes No tls.store.type TLS store type to be used. JKS STRING Yes No proxy.host Proxy server host null STRING Yes No proxy.port Proxy server port null STRING Yes No proxy.username Proxy server username null STRING Yes No proxy.password Proxy server password null STRING Yes No client.bootstrap.configuration Client bootsrap configurations. Expected format of these parameters is as follows: \"'client.bootstrap.nodelay:xxx','client.bootstrap.keepalive:xxx'\" TODO STRING Yes No client.bootstrap.nodelay Http client no delay. true BOOL Yes No client.bootstrap.keepalive Http client keep alive. true BOOL Yes No client.bootstrap.sendbuffersize Http client send buffer size. 1048576 INT Yes No client.bootstrap.recievebuffersize Http client receive buffer size. 1048576 INT Yes No client.bootstrap.connect.timeout Http client connection timeout. 15000 INT Yes No client.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No client.bootstrap.socket.timeout Http client socket timeout. 15 STRING Yes No client.threadpool.configurations Thread pool configuration. Expected format of these parameters is as follows: \"'client.connection.pool.count:xxx','client.max.active.connections.per.pool:xxx'\" TODO STRING Yes No client.connection.pool.count Connection pool count. 0 INT Yes No client.max.active.connections.per.pool Active connections per pool. -1 INT Yes No client.min.idle.connections.per.pool Minimum ideal connection per pool. 0 INT Yes No client.max.idle.connections.per.pool Maximum ideal connection per pool. 100 INT Yes No client.min.eviction.idle.time Minimum eviction idle time. 5 * 60 * 1000 STRING Yes No sender.thread.count Http sender thread count. 20 STRING Yes No event.group.executor.thread.size Event group executor thread size. 15 STRING Yes No max.wait.for.client.connection.pool Maximum wait for client connection pool. 60000 STRING Yes No sink.id Identifier of the sink. This is used to co-relate with the corresponding http-response source which needs to process the repose for the request sent by this sink. STRING No No downloading.enabled If this is set to 'true' then the response received by the response source will be written to a file. If downloading is enabled, the download.path parameter is mandatory. false BOOL Yes No download.path If downloading is enabled, the path of the file which is going to be downloaded should be specified using 'download.path' parameter. This should be an absolute path including the file name. null STRING Yes Yes oauth.username The username to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No oauth.password The password to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No consumer.key consumer key for the Http request. It is only applicable for for Oauth requests STRING Yes No consumer.secret consumer secret for the Http request. It is only applicable for for Oauth requests STRING Yes No refresh.token refresh token for the Http request. It is only applicable for for Oauth requests STRING Yes No Examples EXAMPLE 1 @sink(type='http-request', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody String, method string, headers string); @source(type='http-response', sink.id='foo', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', fileName='A[1]'))) define stream responseStream2xx(fileName string, headers string); @source(type='http-response', sink.id='foo', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream responseStream4xx(errorMsg string); In above example, the payload body for 'FooStream' will be in following format. { events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events , This message will sent as the body of a POST request with the content-type 'application/xml' to the endpoint defined as the 'publisher.url' and in order to process the responses for these requests, there should be a source of type 'http-response' defined with the same sink id 'foo' in the siddhi app. The responses with 2xx status codes will be received by the http-response source which has the http.status.code defined by the regex '2\\d+'. If the response has a 4xx status code, it will be received by the http-response source which has the http.status.code defined by the regex '4\\d+'. EXAMPLE 2 define stream FooStream (name String, id int, headers String, downloadPath string); @sink(type='http-request', downloading.enabled='true', download.path='{{downloadPath}}',publisher.url='http://localhost:8005/files', method='GET', headers='{{headers}}',sink.id='download-sink', @map(type='json')) define stream BarStream (name String, id int, headers String, downloadPath string); @source(type='http-response', sink.id='download-sink', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', fileName='A[1]'))) define stream responseStream2xx(fileName string, headers string); @source(type='http-response', sink.id='download-sink', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream responseStream4xx(errorMsg string); In above example, http-request sink will send a GET request to the publisher url and the requested file will be received as the response by a corresponding http-response source. If the http status code of the response is a successful one (2xx), it will be received by the http-response source which has the http.status.code '2\\d+' and downloaded as a local file. Then the event received to the responseStream2xx will have the headers included in the request and the downloaded file name. If the http status code of the response is a 4xx code, it will be received by the http-response source which has the http.status.code '4\\d+'. Then the event received to the responseStream4xx will have the response message body in text format.","title":"http-request (Sink)"},{"location":"docs/api/5.0.0/#http-response-sink","text":"HTTP response sink is correlated with the The HTTP request source, through a unique source.id , and it send a response to the HTTP request source having the same source.id . The response message can be formatted in text , XML or JSON and can be sent with appropriate headers. Origin: siddhi-io-http:2.0.4 Syntax @sink(type=\"http-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier of the source. STRING No No message.id Identifier of the message. STRING No Yes headers The headers that should be included as HTTP response headers. There can be any number of headers concatenated on following format. \"'header1:value1','header2:value2'\" User can include content-type header if he/she need to have any specific type for payload. If not system get the mapping type as the content-Type header (ie. @map(xml) : application/xml , @map(json) : application/json , @map(text) : plain/text ) and if user does not include any mapping type then system gets the plain/text as default Content-Type header. If user does not include Content-Length header then system calculate the bytes size of payload and include it as content-length header. STRING Yes No Examples EXAMPLE 1 @sink(type='http-response', source.id='sampleSourceId', message.id='{{messageId}}', headers=\"'content-type:json','content-length:94'\"@map(type='json', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody String, messageId string, headers string); If it is json mapping expected input should be in following format for FooStream: { {\"events\": {\"event\": \"symbol\":WSO2, \"price\":55.6, \"volume\":100, } }, 0cf708b1-7eae-440b-a93e-e72f801b486a, Content-Length:24#Content-Location:USA } Above event will generate response for the matching source message as below. ~Output http event payload {\"events\": {\"event\": \"symbol\":WSO2, \"price\":55.6, \"volume\":100, } } ~Output http event headers Content-Length:24, Content-Location:'USA', Content-Type:'application/json'","title":"http-response (Sink)"},{"location":"docs/api/5.0.0/#inmemory-sink","text":"In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Origin: siddhi-core:5.0.0 Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation.","title":"inMemory (Sink)"},{"location":"docs/api/5.0.0/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Origin: siddhi-io-kafka:5.0.0 Syntax @sink(type=\"kafka\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", sequence.id=\" STRING \", key=\" STRING \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0 th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"docs/api/5.0.0/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Origin: siddhi-io-kafka:5.0.0 Syntax @sink(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", sequence.id=\" STRING \", key=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0 th ) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"docs/api/5.0.0/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Origin: siddhi-core:5.0.0 Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"docs/api/5.0.0/#nats-sink","text":"NATS Sink allows users to subscribe to a NATS broker and publish messages. Origin: siddhi-io-nats:2.0.1 Syntax @sink(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS sink should publish to. STRING No Yes bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client publishing/connecting to the NATS broker. Should be unique for each client connecting to the server/cluster. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No Examples EXAMPLE 1 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with all supporting configurations. With the following configuration the sink identified as 'nats-client' will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. EXAMPLE 2 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with mandatory configurations. With the following configuration the sink identified with an auto generated client id will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection.","title":"nats (Sink)"},{"location":"docs/api/5.0.0/#tcp-sink","text":"A Siddhi application can be configured to publish events via the TCP transport by adding the @Sink(type = 'tcp') annotation at the top of an event stream definition. Origin: siddhi-io-tcp:3.0.1 Syntax @sink(type=\"tcp\", url=\" STRING \", sync=\" STRING \", tcp.no.delay=\" BOOL \", keep.alive=\" BOOL \", worker.threads=\" INT|LONG \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The URL to which outgoing events should be published via TCP. The URL should adhere to tcp:// host : port / context format. STRING No No sync This parameter defines whether the events should be published in a synchronized manner or not. If sync = 'true', then the worker will wait for the ack after sending the message. Else it will not wait for an ack. false STRING Yes Yes tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true BOOL Yes No keep.alive This property defines whether the server should be kept alive when there are no connections available. true BOOL Yes No worker.threads Number of threads to publish events. 10 INT LONG Yes No Examples EXAMPLE 1 @Sink(type = 'tcp', url='tcp://localhost:8080/abc', sync='true' @map(type='binary')) define stream Foo (attribute1 string, attribute2 int); A sink of type 'tcp' has been defined. All events arriving at Foo stream via TCP transport will be sent to the url tcp://localhost:8080/abc in a synchronous manner.","title":"tcp (Sink)"},{"location":"docs/api/5.0.0/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"docs/api/5.0.0/#binary-sink-mapper","text":"This section explains how to map events processed via Siddhi in order to publish them in the binary format. Origin: siddhi-map-binary:2.0.0 Syntax @sink(..., @map(type=\"binary\") Examples EXAMPLE 1 @sink(type='inMemory', topic='WSO2', @map(type='binary')) define stream FooStream (symbol string, price float, volume long); This will publish Siddhi event in binary format.","title":"binary (Sink Mapper)"},{"location":"docs/api/5.0.0/#csv-sink-mapper","text":"This output mapper extension allows you to convert Siddhi events processed by the WSO2 SP to CSV message before publishing them. You can either use custom placeholder to map a custom CSV message or use pre-defined CSV format where event conversion takes place without extra configurations. Origin: siddhi-map-csv:2.0.0 Syntax @sink(..., @map(type=\"csv\", delimiter=\" STRING \", header=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter This parameter used to separate the output CSV data, when converting a Siddhi event to CSV format, , STRING Yes No header This parameter specifies whether the CSV messages will be generated with header or not. If this parameter is set to true, message will be generated with header false BOOL Yes No event.grouping.enabled If this parameter is set to true , events are grouped via a line.separator when multiple events are received. It is required to specify a value for the System.lineSeparator() when the value for this parameter is true . false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv')) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a default CSV output mapping, which will generate output as follows: WSO2,55.6,100 OS supported line separator If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume OS supported line separator WSO2-55.6-100 OS supported line separator EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv',header='true',delimiter='-',@payload(symbol='0',price='2',volume='1')))define stream BarStream (symbol string, price float,volume long); Above configuration will perform a custom CSV mapping. Here, user can add custom place order in the @payload. The place order indicates that where the attribute name's value will be appear in the output message, The output will be produced output as follows: WSO2,100,55.6 If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume WSO2-55.6-100 OS supported line separator If event grouping is enabled, then the output is as follows: WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator","title":"csv (Sink Mapper)"},{"location":"docs/api/5.0.0/#json-sink-mapper","text":"This extension is an Event to JSON output mapper. Transports that publish messages can utilize this extension to convert Siddhi events to JSON messages. You can either send a pre-defined JSON format or a custom JSON message. Origin: siddhi-map-json:5.0.1 Syntax @sink(..., @map(type=\"json\", validate.json=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.json If this property is set to true , it enables JSON validation for the JSON messages generated. When validation is carried out, messages that do not adhere to proper JSON standards are dropped. This property is set to 'false' by default. false BOOL Yes No enclosing.element This specifies the enclosing element to be used if multiple events are sent in the same JSON message. Siddhi treats the child elements of the given enclosing element as events and executes JSON expressions on them. If an enclosing.element is not provided, the multiple event scenario is disregarded and JSON path is evaluated based on the root element. $ STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Above configuration does a default JSON input mapping that generates the output given below. { \"event\":{ \"symbol\":WSO2, \"price\":55.6, \"volume\":100 } } EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='json', enclosing.element='$.portfolio', validate.json='true', @payload( \"\"\"{\"StockData\":{\"Symbol\":\"{{symbol}}\",\"Price\":{{price}}}\"\"\"))) define stream BarStream (symbol string, price float, volume long); The above configuration performs a custom JSON mapping that generates the following JSON message as the output. {\"portfolio\":{ \"StockData\":{ \"Symbol\":WSO2, \"Price\":55.6 } } }","title":"json (Sink Mapper)"},{"location":"docs/api/5.0.0/#keyvalue-sink-mapper","text":"The Event to Key-Value Map output mapper extension allows you to convert Siddhi events processed by WSO2 SP to key-value map events before publishing them. You can either use pre-defined keys where conversion takes place without extra configurations, or use custom keys with which the messages can be published. Origin: siddhi-map-keyvalue:2.0.0 Syntax @sink(..., @map(type=\"keyvalue\") Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default Key-Value output mapping. The expected output is something similar to the following:symbol:'WSO2' price : 55.6f volume: 100L EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='symbol',b='price',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where values are passed as objects. Values for symbol , price , and volume attributes are published with the keys a , b and c respectively. The expected output is a map similar to the following: a:'WSO2' b : 55.6f c: 100L EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='{{symbol}} is here',b='`price`',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where the values of the a and b attributes are strings and c is object. The expected output should be a Map similar to the following 'WSO2 is here' b : 'price' c: 100L","title":"keyvalue (Sink Mapper)"},{"location":"docs/api/5.0.0/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.0.0 Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"docs/api/5.0.0/#text-sink-mapper","text":"This extension is a Event to Text output mapper. Transports that publish text messages can utilize this extension to convert the Siddhi events to text messages. Users can use a pre-defined text format where event conversion is carried out without any additional configurations, or use custom placeholder(using {{ and }} or {{{ and }}} ) to map custom text messages. All variables are HTML escaped by default. For example: & is replaced with amp; \" is replaced with quot; = is replaced with #61; If you want to return unescaped HTML, use the triple mustache {{{ instead of double {{ . Origin: siddhi-map-text:2.0.0 Syntax @sink(..., @map(type=\"text\", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.grouping.enabled If this parameter is set to true , events are grouped via a delimiter when multiple events are received. It is required to specify a value for the delimiter parameter when the value for this parameter is true . false BOOL Yes No delimiter This parameter specifies how events are separated when a grouped event is received. This must be a whole line and not a single character. ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n whereas Windows uses \\r\\n as the end of line character. \\n STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping with event grouping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{symbol}}/{{volume}}, SensorPrice : Rs{{price}}/=, Value : {{volume}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected output is as follows: SensorID : wso2/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {wso2,1000,100} EXAMPLE 4 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true', @payload(\"Stock price of {{symbol}} is {{price}}\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping with event grouping. The expected output is as follows: Stock price of WSO2 is 55.6 ~ Stock price of WSO2 is 55.6 ~ Stock price of WSO2 is 55.6 for the following siddhi event. {WSO2,55.6,10} EXAMPLE 5 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{{symbol}}}/{{{volume}}}, SensorPrice : Rs{{{price}}}/=, Value : {{{volume}}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping to return unescaped HTML. The expected output is as follows: SensorID : a b/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {a b,1000,100}","title":"text (Sink Mapper)"},{"location":"docs/api/5.0.0/#xml-sink-mapper","text":"This mapper converts Siddhi output events to XML before they are published via transports that publish in XML format. Users can either send a pre-defined XML format or a custom XML message containing event data. Origin: siddhi-map-xml:5.0.0 Syntax @sink(..., @map(type=\"xml\", validate.xml=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.xml This parameter specifies whether the XML messages generated should be validated or not. If this parameter is set to true, messages that do not adhere to proper XML standards are dropped. false BOOL Yes No enclosing.element When an enclosing element is specified, the child elements (e.g., the immediate child elements) of that element are considered as events. This is useful when you need to send multiple events in a single XML message. When an enclosing element is not specified, one XML message per every event will be emitted without enclosing. None in custom mapping and events in default mapping STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping which will generate below output events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='xml', enclosing.element=' portfolio ', validate.xml='true', @payload( \" StockData Symbol {{symbol}} /Symbol Price {{price}} /Price /StockData \"))) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. Inside @payload you can specify the custom template that you want to send the messages out and addd placeholders to places where you need to add event attributes.Above config will produce below output XML message portfolio StockData Symbol WSO2 /Symbol Price 55.6 /Price /StockData /portfolio","title":"xml (Sink Mapper)"},{"location":"docs/api/5.0.0/#source","text":"","title":"Source"},{"location":"docs/api/5.0.0/#cdc-source","text":"The CDC source receives events when change events (i.e., INSERT, UPDATE, DELETE) are triggered for a database table. Events are received in the 'key-value' format. The key values of the map of a CDC change event are as follows. For insert: Keys are specified as columns of the table. For delete: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For update: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For 'polling' mode: Keys are specified as the coloumns of the table. See parameter: mode for supported databases and change events. Origin: siddhi-io-cdc:2.0.0 Syntax @source(type=\"cdc\", url=\" STRING \", mode=\" STRING \", jdbc.driver.name=\" STRING \", username=\" STRING \", password=\" STRING \", pool.properties=\" STRING \", datasource.name=\" STRING \", table.name=\" STRING \", polling.column=\" STRING \", polling.interval=\" INT \", operation=\" STRING \", connector.properties=\" STRING \", database.server.id=\" STRING \", database.server.name=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The connection URL to the database. F=The format used is: 'jdbc:mysql:// host : port / database_name ' STRING No No mode Mode to capture the change data. The type of events that can be received, and the required parameters differ based on the mode. The mode can be one of the following: 'polling': This mode uses a column named 'polling.column' to monitor the given table. It captures change events of the 'RDBMS', 'INSERT, and 'UPDATE' types. 'listening': This mode uses logs to monitor the given table. It currently supports change events only of the 'MySQL', 'INSERT', 'UPDATE', and 'DELETE' types. listening STRING Yes No jdbc.driver.name The driver class name for connecting the database. It is required to specify a value for this parameter when the mode is 'polling'. STRING Yes No username The username to be used for accessing the database. This user needs to have the 'SELECT', 'RELOAD', 'SHOW DATABASES', 'REPLICATION SLAVE', and 'REPLICATION CLIENT'privileges for the change data capturing table (specified via the 'table.name' parameter). To operate in the polling mode, the user needs 'SELECT' privileges. STRING No No password The password of the username you specified for accessing the database. STRING No No pool.properties The pool parameters for the database connection can be specified as key-value pairs. STRING Yes No datasource.name Name of the wso2 datasource to connect to the database. When datasource name is provided, the URL, username and password are not needed. A datasource based connection is given more priority over the URL based connection. This parameter is applicable only when the mode is set to 'polling', and it can be applied only when you use this extension with WSO2 Stream Processor. STRING Yes No table.name The name of the table that needs to be monitored for data changes. STRING No No polling.column The column name that is polled to capture the change data. It is recommended to have a TIMESTAMP field as the 'polling.column' in order to capture the inserts and updates. Numeric auto-incremental fields and char fields can also be used as 'polling.column'. However, note that fields of these types only support insert change capturing, and the possibility of using a char field also depends on how the data is input. It is required to enter a value for this parameter when the mode is 'polling'. STRING Yes No polling.interval The time interval (specified in seconds) to poll the given table for changes. This parameter is applicable only when the mode is set to 'polling'. 1 INT Yes No operation The change event operation you want to carry out. Possible values are 'insert', 'update' or 'delete'. It is required to specify a value when the mode is 'listening'. This parameter is not case sensitive. STRING No No connector.properties Here, you can specify Debezium connector properties as a comma-separated string. The properties specified here are given more priority over the parameters. This parameter is applicable only for the 'listening' mode. Empty_String STRING Yes No database.server.id An ID to be used when joining MySQL database cluster to read the bin log. This should be a unique integer between 1 to 2^32. This parameter is applicable only when the mode is 'listening'. Random integer between 5400 and 6400 STRING Yes No database.server.name A logical name that identifies and provides a namespace for the database server. This parameter is applicable only when the mode is 'listening'. {host}_{port} STRING Yes No Examples EXAMPLE 1 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'insert', @map(type='keyvalue', @attributes(id = 'id', name = 'name'))) define stream inputStream (id string, name string); In this example, the CDC source listens to the row insertions that are made in the 'students' table with the column name, and the ID. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 2 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'update', @map(type='keyvalue', @attributes(id = 'id', name = 'name', before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, id string, before_name string , name string); In this example, the CDC source listens to the row updates that are made in the 'students' table. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 3 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'delete', @map(type='keyvalue', @attributes(before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, before_name string); In this example, the CDC source listens to the row deletions made in the 'students' table. This table belongs to the 'SimpleDB' database that can be accessed via the given URL. EXAMPLE 4 @source(type = 'cdc', mode='polling', polling.column = 'id', jdbc.driver.name = 'com.mysql.jdbc.Driver', url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. 'id' that is specified as the polling colum' is an auto incremental field. The connection to the database is made via the URL, username, password, and the JDBC driver name. EXAMPLE 5 @source(type = 'cdc', mode='polling', polling.column = 'id', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The given polling column is a char column with the 'S001, S002, ... .' pattern. The connection to the database is made via a data source named 'SimpleDB'. Note that the 'datasource.name' parameter works only with the Stream Processor. EXAMPLE 6 @source(type = 'cdc', mode='polling', polling.column = 'last_updated', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue')) define stream inputStream (name string); In this example, the CDC source polls the 'students' table for inserts and updates. The polling column is a timestamp field.","title":"cdc (Source)"},{"location":"docs/api/5.0.0/#email-source","text":"The 'Email' source allows you to receive events via emails. An 'Email' source can be configured using the 'imap' or 'pop3' server to receive events. This allows you to filter the messages that satisfy the criteria specified under the 'search term' option. The email source parameters can be defined in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or the stream definition. If the parameter configurations are not available in either place, the default values are considered (i.e., if default values are available). If you need to configure server system parameters that are not provided as options in the stream definition, they need to be defined in the 'deployment yaml' file under 'email source properties'. For more information about 'imap' and 'pop3' server system parameters, see the following. [JavaMail Reference Implementation - IMAP Store](https://javaee.github.io/javamail/IMAP-Store) [JavaMail Reference Implementation - POP3 Store Store](https://javaee.github.io/javamail/POP3-Store) Origin: siddhi-io-email:2.0.1 Syntax @source(type=\"email\", username=\" STRING \", password=\" STRING \", store=\" STRING \", host=\" STRING \", port=\" INT \", folder=\" STRING \", search.term=\" STRING \", polling.interval=\" LONG \", action.after.processed=\" STRING \", folder.to.move=\" STRING \", content.type=\" STRING \", ssl.enable=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The user name of the email account. e.g., 'wso2mail' is the username of the 'wso2mail@gmail.com' mail account. STRING No No password The password of the email account STRING No No store The store type that used to receive emails. Possible values are 'imap' and 'pop3'. imap STRING Yes No host The host name of the server (e.g., 'imap.gmail.com' is the host name for a gmail account with an IMAP store.). The default value 'imap.gmail.com' is only valid if the email account is a gmail account with IMAP enabled. If store type is 'imap', then the default value is 'imap.gmail.com'. If the store type is 'pop3', then thedefault value is 'pop3.gmail.com'. STRING Yes No port The port that is used to create the connection. '993', the default value is valid only if the store is 'imap' and ssl-enabled. INT Yes No folder The name of the folder to which the emails should be fetched. INBOX STRING Yes No search.term The option that includes conditions such as key-value pairs to search for emails. In a string search term, the key and the value should be separated by a semicolon (';'). Each key-value pair must be within inverted commas (' '). The string search term can define multiple comma-separated key-value pairs. This string search term currently supports only the 'subject', 'from', 'to', 'bcc', and 'cc' keys. e.g., if you enter 'subject:DAS, from:carbon, bcc:wso2', the search term creates a search term instance that filters emails that contain 'DAS' in the subject, 'carbon' in the 'from' address, and 'wso2' in one of the 'bcc' addresses. The string search term carries out sub string matching that is case-sensitive. If '@' in included in the value for any key other than the 'subject' key, it checks for an address that is equal to the value given. e.g., If you search for 'abc@', the string search terms looks for an address that contains 'abc' before the '@' symbol. None STRING Yes No polling.interval This defines the time interval in seconds at which th email source should poll the account to check for new mail arrivals.in seconds. 600 LONG Yes No action.after.processed The action to be performed by the email source for the processed mail. Possible values are as follows: 'FLAGGED': Sets the flag as 'flagged'. 'SEEN': Sets the flag as 'read'. 'ANSWERED': Sets the flag as 'answered'. 'DELETE': Deletes tha mail after the polling cycle. 'MOVE': Moves the mail to the folder specified in the 'folder.to.move' parameter. If the folder specified is 'pop3', then the only option available is 'DELETE'. NONE STRING Yes No folder.to.move The name of the folder to which the mail must be moved once it is processed. If the action after processing is 'MOVE', it is required to specify a value for this parameter. STRING No No content.type The content type of the email. It can be either 'text/plain' or 'text/html.' text/plain STRING Yes No ssl.enable If this is set to 'true', a secure port is used to establish the connection. The possible values are 'true' and 'false'. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters mail.imap.partialfetch This determines whether the IMAP partial-fetch capability should be used. true true or false mail.imap.fetchsize The partial fetch size in bytes. 16K value in bytes mail.imap.peek If this is set to 'true', the IMAP PEEK option should be used when fetching body parts to avoid setting the 'SEEN' flag on messages. The default value is 'false'. This can be overridden on a per-message basis by the 'setPeek method' in 'IMAPMessage'. false true or false mail.imap.connectiontimeout The socket connection timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.timeout The socket read timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.writetimeout The socket write timeout value in milliseconds. This timeout is implemented by using a 'java.util.concurrent.ScheduledExecutorService' per connection that schedules a thread to close the socket if the timeout period elapses. Therefore, the overhead of using this timeout is one thread per connection. infinity timeout Any Integer value mail.imap.statuscachetimeout The timeout value in milliseconds for the cache of 'STATUS' command response. 1000ms Time out in miliseconds mail.imap.appendbuffersize The maximum size of a message to buffer in memory when appending to an IMAP folder. None Any Integer value mail.imap.connectionpoolsize The maximum number of available connections in the connection pool. 1 Any Integer value mail.imap.connectionpooltimeout The timeout value in milliseconds for connection pool connections. 45000ms Any Integer mail.imap.separatestoreconnection If this parameter is set to 'true', it indicates that a dedicated store connection needs to be used for store commands. true true or false mail.imap.auth.login.disable If this is set to 'true', it is not possible to use the non-standard 'AUTHENTICATE LOGIN' command instead of the plain 'LOGIN' command. false true or false mail.imap.auth.plain.disable If this is set to 'true', the 'AUTHENTICATE PLAIN' command cannot be used. false true or false mail.imap.auth.ntlm.disable If true, prevents use of the AUTHENTICATE NTLM command. false true or false mail.imap.proxyauth.user If the server supports the PROXYAUTH extension, this property specifies the name of the user to act as. Authentication to log in to the server is carried out using the administrator's credentials. After authentication, the IMAP provider issues the 'PROXYAUTH' command with the user name specified in this property. None Valid string value mail.imap.localaddress The local address (host name) to bind to when creating the IMAP socket. Defaults to the address picked by the Socket class. Valid string value mail.imap.localport The local port number to bind to when creating the IMAP socket. Defaults to the port number picked by the Socket class. Valid String value mail.imap.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.imap.sasl.mechanisms A list of SASL mechanism names that the system should to try to use. The names can be separated by spaces or commas. None Valid string value mail.imap.sasl.authorizationid The authorization ID to use in the SASL authentication. If this parameter is not set, the authentication ID (username) is used. Valid string value mail.imap.sasl.realm The realm to use with SASL authentication mechanisms that require a realm, such as 'DIGEST-MD5'. None Valid string value mail.imap.auth.ntlm.domain The NTLM authentication domain. None Valid string value The NTLM authentication domain. NTLM protocol-specific flags. None Valid integer value mail.imap.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create IMAP sockets. None Valid SocketFactory mail.imap.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create IMAP sockets. None Valid string mail.imap.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. true true or false mail.imap.socketFactory.port This specifies the port to connect to when using the specified socket factory. If this parameter is not set, the default port is used. 143 Valid Integer mail.imap.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by RFC 2595. false true or false mail.imap.ssl.trust If this parameter is set and a socket factory has not been specified, it enables the use of a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If this parameter specifies list of hosts separated by white spaces, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.imap.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class this class is used to create IMAP SSL sockets. None SSL Socket Factory mail.imap.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create IMAP SSL sockets. None Valid String mail.imap.ssl.socketFactory.port This specifies the port to connect to when using the specified socket factory. the default port 993 is used. valid port number mail.imap.ssl.protocols This specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid string mail.imap.starttls.enable If this parameter is set to 'true', it is possible to use the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.imap.socks.host This specifies the host name of a 'SOCKS5' proxy server that is used to connect to the mail server. None Valid String mail.imap.socks.port This specifies the port number for the 'SOCKS5' proxy server. This is needed if the proxy server is not using the standard port number 1080. 1080 Valid String mail.imap.minidletime This property sets the delay in milliseconds. 10 milliseconds time in seconds (Integer) mail.imap.enableimapevents If this property is set to 'true', it enables special IMAP-specific events to be delivered to the 'ConnectionListener' of the store. The unsolicited responses received during the idle method of the store are sent as connection events with 'IMAPStore.RESPONSE' as the type. The event's message is the raw IMAP response string. false true or false mail.imap.folder.class The class name of a subclass of 'com.sun.mail.imap.IMAPFolder'. The subclass can be used to provide support for additional IMAP commands. The subclass must have public constructors of the form 'public MyIMAPFolder'(String fullName, char separator, IMAPStore store, Boolean isNamespace) and public 'MyIMAPFolder'(ListInfo li, IMAPStore store) None Valid String mail.pop3.connectiontimeout The socket connection timeout value in milliseconds. Infinite timeout Integer value mail.pop3.timeout The socket I/O timeout value in milliseconds. Infinite timeout Integer value mail.pop3.message.class The class name of a subclass of 'com.sun.mail.pop3.POP3Message'. None Valid String mail.pop3.localaddress The local address (host name) to bind to when creating the POP3 socket. Defaults to the address picked by the Socket class. Valid String mail.pop3.localport The local port number to bind to when creating the POP3 socket. Defaults to the port number picked by the Socket class. Valid port number mail.pop3.apop.enable If this parameter is set to 'true', use 'APOP' instead of 'USER/PASS' to log in to the 'POP3' server (if the 'POP3' server supports 'APOP'). APOP sends a digest of the password instead of clearing the text password. false true or false mail.pop3.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create 'POP3' sockets. None Socket Factory mail.pop3.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create 'POP3' sockets. None Valid String mail.pop3.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. false true or false mail.pop3.socketFactory.port This specifies the port to connect to when using the specified socket factory. Default port Valid port number mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', check the server identity as specified by RFC 2595. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3' SSL sockets. None SSL Socket Factory mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by 'RFC 2595'. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to '*', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. Trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3 SSL' sockets. None SSL Socket Factory mail.pop3.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create 'POP3 SSL' sockets. None Valid String mail.pop3.ssl.socketFactory.p This parameter pecifies the port to connect to when using the specified socket factory. 995 Valid Integer mail.pop3.ssl.protocols This parameter specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid String mail.pop3.starttls.enable If this parameter is set to 'true', it is possible to use the 'STLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.pop3.starttls.required If this parameter is set to 'true', it is required to use the 'STLS' command. The connect method fails if the server does not support the 'STLS' command or if the command fails. false true or false mail.pop3.socks.host This parameter specifies the host name of a 'SOCKS5' proxy server that can be used to connect to the mail server. None Valid String mail.pop3.socks.port This parameter specifies the port number for the 'SOCKS5' proxy server. None Valid String mail.pop3.disabletop If this parameter is set to 'true', the 'POP3 TOP' command is not used to fetch message headers. false true or false mail.pop3.forgettopheaders If this parameter is set to 'true', the headers that might have been retrieved using the 'POP3 TOP' command is forgotten and replaced by the headers retrieved when the 'POP3 RETR' command is executed. false true or false mail.pop3.filecache.enable If this parameter is set to 'true', the 'POP3' provider caches message data in a temporary file instead of caching them in memory. Messages are only added to the cache when accessing the message content. Message headers are always cached in memory (on demand). The file cache is removed when the folder is closed or the JVM terminates. false true or false mail.pop3.filecache.dir If the file cache is enabled, this property is used to override the default directory used by the JDK for temporary files. None Valid String mail.pop3.cachewriteto This parameter controls the behavior of the 'writeTo' method on a 'POP3' message object. If the parameter is set to 'true', the message content has not been cached yet, and the 'ignoreList' is null, the message is cached before being written. If not, the message is streamed directly to the output stream without being cached. false true or false mail.pop3.keepmessagecontent If this property is set to 'true', a hard reference to the cached content is retained, preventing the memory from being reused until the folder is closed, or until the cached content is explicitly invalidated (using the 'invalidate' method). false true or false Examples EXAMPLE 1 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. In this example, only the required parameters are defined in the stream definition. The default values are taken for the other parameters. The search term is not defined, and therefore, all the new messages in the inbox folder are polled and taken. EXAMPLE 2 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',store = 'imap',host = 'imap.gmail.com',port = '993',searchTerm = 'subject:Stream Processor, from: from.account@ , cc: cc.account',polling.interval='500',action.after.processed='DELETE',content.type='text/html,)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. The email source polls the mail account every 500 seconds to check whether any new mails have arrived. It processes new mails only if they satisfy the conditions specified for the email search term (the value for 'from' of the email message should be 'from.account@. host name ', and the message should contain 'cc.account' in the cc receipient list and the word 'Stream Processor' in the mail subject). in this example, the action after processing is 'DELETE'. Therefore,after processing the event, corresponding mail is deleted from the mail folder.","title":"email (Source)"},{"location":"docs/api/5.0.0/#http-source","text":"The HTTP source receives POST requests via HTTP or HTTPS in format such as text , XML and JSON . In WSO2 SP, if required, you can enable basic authentication to ensure that events are received only from users who are authorized to access the service. Origin: siddhi-io-http:2.0.4 Syntax @source(type=\"http\", receiver.url=\" STRING \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", server.enable.session.creation=\" STRING \", server.supported.snimatchers=\" STRING \", server.suported.server.names=\" STRING \", request.size.validation.configuration=\" STRING \", request.size.validation=\" STRING \", request.size.validation.maximum.value=\" STRING \", request.size.validation.reject.status.code=\" STRING \", request.size.validation.reject.message=\" STRING \", request.size.validation.reject.message.content.type=\" STRING \", header.size.validation=\" STRING \", header.validation.maximum.request.line=\" STRING \", header.validation.maximum.size=\" STRING \", header.validation.maximum.chunk.size=\" STRING \", header.validation.reject.status.code=\" STRING \", header.validation.reject.message=\" STRING \", header.validation.reject.message.content.type=\" STRING \", server.bootstrap.configuration=\" OBJECT \", server.bootstrap.nodelay=\" BOOL \", server.bootstrap.keepalive=\" BOOL \", server.bootstrap.sendbuffersize=\" INT \", server.bootstrap.recievebuffersize=\" INT \", server.bootstrap.connect.timeout=\" INT \", server.bootstrap.socket.reuse=\" BOOL \", server.bootstrap.socket.timeout=\" BOOL \", server.bootstrap.socket.backlog=\" BOOL \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL to which the events should be received. User can provide any valid url and if the url is not provided the system will use the following format http://0.0.0.0:9763/ appNAme / streamName If the user want to use SSL the url should be given in following format https://localhost:8080/ streamName http://0.0.0.0:9763/ / STRING Yes No basic.auth.enabled This works only in WSO2 SP. If this is set to true , basic authentication is enabled for incoming events, and the credentials with which each event is sent are verified to ensure that the user is authorized to access the service. If basic authentication fails, the event is not authenticated and an authentication error is logged in the CLI. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. The value is 1 by default. This will ensure that the events are directed to the event stream in the same order in which they arrive. By increasing this value the performance might increase at the cost of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection. 120000 INT Yes No ssl.verify.client The type of client certificate verification. null STRING Yes No ssl.protocol ssl/tls related options TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No server.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No server.supported.snimatchers Http SNIMatcher to be added. This parameter should include under parameters Ex: 'server.supported.snimatchers:SNIMatcher' null STRING Yes No server.suported.server.names Http supported servers. This parameter should include under parameters Ex: 'server.suported.server.names:server' null STRING Yes No request.size.validation.configuration Parameters that responsible for validating the http request and request headers. Expected format of these parameters is as follows: \"'request.size.validation:xxx','request.size.validation.maximum.value:xxx'\" null STRING Yes No request.size.validation To enable the request size validation. false STRING Yes No request.size.validation.maximum.value If request size is validated then maximum size. Integer.MAX_VALUE STRING Yes No request.size.validation.reject.status.code If request is exceed maximum size and request.size.validation is enabled then status code to be send as response. 401 STRING Yes No request.size.validation.reject.message If request is exceed maximum size and request.size.validation is enabled then status message to be send as response. Message is bigger than the valid size STRING Yes No request.size.validation.reject.message.content.type If request is exceed maximum size and request.size.validation is enabled then content type to be send as response. plain/text STRING Yes No header.size.validation To enable the header size validation. false STRING Yes No header.validation.maximum.request.line If header header validation is enabled then the maximum request line. 4096 STRING Yes No header.validation.maximum.size If header header validation is enabled then the maximum expected header size. 8192 STRING Yes No header.validation.maximum.chunk.size If header header validation is enabled then the maximum expected chunk size. 8192 STRING Yes No header.validation.reject.status.code 401 If header is exceed maximum size and header.size.validation is enabled then status code to be send as response. STRING Yes No header.validation.reject.message If header is exceed maximum size and header.size.validation is enabled then message to be send as response. Message header is bigger than the valid size STRING Yes No header.validation.reject.message.content.type If header is exceed maximum size and header.size.validation is enabled then content type to be send as response. plain/text STRING Yes No server.bootstrap.configuration Parameters that for bootstrap configurations of the server. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null OBJECT Yes No server.bootstrap.nodelay Http server no delay. true BOOL Yes No server.bootstrap.keepalive Http server keep alive. true BOOL Yes No server.bootstrap.sendbuffersize Http server send buffer size. 1048576 INT Yes No server.bootstrap.recievebuffersize Http server receive buffer size. 1048576 INT Yes No server.bootstrap.connect.timeout Http server connection timeout. 15000 INT Yes No server.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No server.bootstrap.socket.timeout Http server socket timeout. 15 BOOL Yes No server.bootstrap.socket.backlog THttp server socket backlog. 100 BOOL Yes No trace.log.enabled Http traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize property to configure number of boss threads, which accepts incoming connections until the ports are unbound. Once connection accepts successfully, boss thread passes the accepted channel to one of the worker threads. Number of available processors Any integer serverBootstrapWorkerGroupSize property to configure number of worker threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer serverBootstrapClientGroupSize property to configure number of client threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultHttpPort The default port if the default scheme is 'http'. 8280 Any valid port defaultHttpsPort The default port if the default scheme is 'https'. 8243 Any valid port defaultScheme The default protocol. http http https keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to wso2carbon.jks file keyStorePassword The default keystore password. wso2carbon String of keystore password Examples EXAMPLE 1 @source(type='http', receiver.url='http://localhost:9055/endpoints/RecPro', socketIdleTimeout='150000', parameters=\"'ciphers : TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256', 'sslEnabledProtocols:TLSv1.1,TLSv1.2'\",request.size.validation.configuration=\"request.size.validation:true\",server.bootstrap.configuration=\"server.bootstrap.socket.timeout:25\" @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above source listenerConfiguration performs a default XML input mapping. The expected input is as follows: events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events If basic authentication is enabled via the basic.auth.enabled='true setting, each input event is also expected to contain the Authorization:'Basic encodeBase64(username:Password)' header.","title":"http (Source)"},{"location":"docs/api/5.0.0/#http-request-source","text":"The HTTP request is correlated with the HTTP response sink, through a unique source.id , and for each POST requests it receives via HTTP or HTTPS in format such as text , XML and JSON it sends the response via the HTTP response sink. The individual request and response messages are correlated at the sink using the message.id of the events. If required, you can enable basic authentication at the source to ensure that events are received only from users who are authorized to access the service. Origin: siddhi-io-http:2.0.4 Syntax @source(type=\"http-request\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", server.enable.session.creation=\" STRING \", server.supported.snimatchers=\" STRING \", server.suported.server.names=\" STRING \", request.size.validation.configuration=\" STRING \", request.size.validation=\" STRING \", request.size.validation.maximum.value=\" STRING \", request.size.validation.reject.status.code=\" STRING \", request.size.validation.reject.message=\" STRING \", request.size.validation.reject.message.content.type=\" STRING \", header.size.validation=\" STRING \", header.validation.maximum.request.line=\" STRING \", header.validation.maximum.size=\" STRING \", header.validation.maximum.chunk.size=\" STRING \", header.validation.reject.status.code=\" STRING \", header.validation.reject.message=\" STRING \", header.validation.reject.message.content.type=\" STRING \", server.bootstrap.configuration=\" OBJECT \", server.bootstrap.nodelay=\" BOOL \", server.bootstrap.keepalive=\" BOOL \", server.bootstrap.sendbuffersize=\" INT \", server.bootstrap.recievebuffersize=\" INT \", server.bootstrap.connect.timeout=\" INT \", server.bootstrap.socket.reuse=\" BOOL \", server.bootstrap.socket.timeout=\" BOOL \", server.bootstrap.socket.backlog=\" BOOL \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL to which the events should be received. User can provide any valid url and if the url is not provided the system will use the following format http://0.0.0.0:9763/ appNAme / streamName If the user want to use SSL the url should be given in following format https://localhost:8080/ streamName http://0.0.0.0:9763/ / STRING Yes No source.id Identifier need to map the source to sink. STRING No No connection.timeout Connection timeout in milliseconds. If the mapped http-response sink does not get a correlated message, after this timeout value, a timeout response is sent 120000 INT Yes No basic.auth.enabled If this is set to true , basic authentication is enabled for incoming events, and the credentials with which each event is sent are verified to ensure that the user is authorized to access the service. If basic authentication fails, the event is not authenticated and an authentication error is logged in the CLI. By default this values 'false' false STRING Yes No worker.count The number of active worker threads to serve the incoming events. The value is 1 by default. This will ensure that the events are directed to the event stream in the same order in which they arrive. By increasing this value the performance might increase at the cost of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection. 120000 INT Yes No ssl.verify.client The type of client certificate verification. null STRING Yes No ssl.protocol ssl/tls related options TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No server.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No server.supported.snimatchers Http SNIMatcher to be added. This parameter should include under parameters Ex: 'server.supported.snimatchers:SNIMatcher' null STRING Yes No server.suported.server.names Http supported servers. This parameter should include under parameters Ex: 'server.suported.server.names:server' null STRING Yes No request.size.validation.configuration Parameters that responsible for validating the http request and request headers. Expected format of these parameters is as follows: \"'request.size.validation:xxx','request.size.validation.maximum.value:xxx'\" null STRING Yes No request.size.validation To enable the request size validation. false STRING Yes No request.size.validation.maximum.value If request size is validated then maximum size. Integer.MAX_VALUE STRING Yes No request.size.validation.reject.status.code If request is exceed maximum size and request.size.validation is enabled then status code to be send as response. 401 STRING Yes No request.size.validation.reject.message If request is exceed maximum size and request.size.validation is enabled then status message to be send as response. Message is bigger than the valid size STRING Yes No request.size.validation.reject.message.content.type If request is exceed maximum size and request.size.validation is enabled then content type to be send as response. plain/text STRING Yes No header.size.validation To enable the header size validation. false STRING Yes No header.validation.maximum.request.line If header header validation is enabled then the maximum request line. 4096 STRING Yes No header.validation.maximum.size If header header validation is enabled then the maximum expected header size. 8192 STRING Yes No header.validation.maximum.chunk.size If header header validation is enabled then the maximum expected chunk size. 8192 STRING Yes No header.validation.reject.status.code 401 If header is exceed maximum size and header.size.validation is enabled then status code to be send as response. STRING Yes No header.validation.reject.message If header is exceed maximum size and header.size.validation is enabled then message to be send as response. Message header is bigger than the valid size STRING Yes No header.validation.reject.message.content.type If header is exceed maximum size and header.size.validation is enabled then content type to be send as response. plain/text STRING Yes No server.bootstrap.configuration Parameters that for bootstrap configurations of the server. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null OBJECT Yes No server.bootstrap.nodelay Http server no delay. true BOOL Yes No server.bootstrap.keepalive Http server keep alive. true BOOL Yes No server.bootstrap.sendbuffersize Http server send buffer size. 1048576 INT Yes No server.bootstrap.recievebuffersize Http server receive buffer size. 1048576 INT Yes No server.bootstrap.connect.timeout Http server connection timeout. 15000 INT Yes No server.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No server.bootstrap.socket.timeout Http server socket timeout. 15 BOOL Yes No server.bootstrap.socket.backlog THttp server socket backlog. 100 BOOL Yes No trace.log.enabled Http traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize property to configure number of boss threads, which accepts incoming connections until the ports are unbound. Once connection accepts successfully, boss thread passes the accepted channel to one of the worker threads. Number of available processors Any integer serverBootstrapWorkerGroupSize property to configure number of worker threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer serverBootstrapClientGroupSize property to configure number of client threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultHttpPort The default port if the default scheme is 'http'. 8280 Any valid port defaultHttpsPort The default port if the default scheme is 'https'. 8243 Any valid port defaultScheme The default protocol. http http https keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to wso2carbon.jks file keyStorePassword The default keystore password. wso2carbon String of keystore password certPassword The default cert password. wso2carbon String of cert password Examples EXAMPLE 1 @source(type='http-request', source.id='sampleSourceId, receiver.url='http://localhost:9055/endpoints/RecPro', connection.timeout='150000', parameters=\"'ciphers : TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256', 'sslEnabledProtocols:TLSv1.1,TLSv1.2'\", request.size.validation.configuration=\"request.size.validation:true\", server.bootstrap.configuration=\"server.bootstrap.socket.timeout:25\", @map(type='json, @attributes(messageId='trp:messageId', symbol='$.events.event.symbol', price='$.events.event.price', volume='$.events.event.volume'))) define stream FooStream (messageId string, symbol string, price float, volume long); The expected input is as follows: {\"events\": {\"event\": \"symbol\":WSO2, \"price\":55.6, \"volume\":100, } } If basic authentication is enabled via the basic.auth.enabled='true setting, each input event is also expected to contain the Authorization:'Basic encodeBase64(username:Password)' header.","title":"http-request (Source)"},{"location":"docs/api/5.0.0/#http-response-source","text":"The http-response source co-relates with http-request sink with the parameter 'sink.id'. This receives responses for the requests sent by the http-request sink which has the same sink id. Response messages can be in formats such as TEXT, JSON and XML. In order to handle the responses with different http status codes, user is allowed to defined the acceptable response source code using the parameter 'http.status.code' Origin: siddhi-io-http:2.0.4 Syntax @source(type=\"http-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id This parameter is used to map the http-response source to a http-request sink. Then this source will accepts the response messages for the requests sent by corresponding http-request sink. STRING No No http.status.code Acceptable http status code for the responses. This can be a complete string or a regex. Only the responses with matching status codes to the defined value, will be received by the http-response source. Eg: 'http.status.code = '200', http.status.code = '2\\d+'' 200 STRING Yes No allow.streaming.responses If responses can be received multiple times for a single request, this option should be enabled. If this is not enabled, for every request, response will be extracted only once. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-request', downloading.enabled='true', publisher.url='http://localhost:8005/registry/employee', method='POST', headers='{{headers}}',sink.id='employee-info', @map(type='json')) define stream BarStream (name String, id int, headers String, downloadPath string); @source(type='http-response' , sink.id='employee-info', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(message='A[1]'))) define stream responseStream2xx(message string);@source(type='http-response' , sink.id='employee-info', http.status.code='4\\\\d+' , @map(type='text', regex.A='((.|\\n)*)', @attributes(message='A[1]'))) define stream responseStream4xx(message string); In above example, the defined http-request sink will send a POST requests to the endpoint defined by 'publisher.url'. Then for those requests, the source with the response code '2\\d+' and sink.id 'employee-info' will receive the responses with 2xx status codes. The http-response source which has 'employee-info' as the 'sink.id' and '4\\d+' as the http.response.code will receive all the responses with 4xx status codes. . Then the body of the response message will be extracted using text mapper and converted into siddhi events. .","title":"http-response (Source)"},{"location":"docs/api/5.0.0/#inmemory-source","text":"In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Origin: siddhi-core:5.0.0 Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport.","title":"inMemory (Source)"},{"location":"docs/api/5.0.0/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Origin: siddhi-io-kafka:5.0.0 Syntax @source(type=\"kafka\", bootstrap.servers=\" STRING \", topic.list=\" STRING \", group.id=\" STRING \", threading.option=\" STRING \", partition.no.list=\" STRING \", seq.enabled=\" BOOL \", is.binary.message=\" BOOL \", topic.offset.map=\" STRING \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51 st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"docs/api/5.0.0/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Origin: siddhi-io-kafka:5.0.0 Syntax @source(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"docs/api/5.0.0/#nats-source","text":"NATS Source allows users to subscribe to a NATS broker and receive messages. It has the ability to receive all the message types supported by NATS. Origin: siddhi-io-nats:2.0.1 Syntax @source(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", queue.group.name=\" STRING \", durable.name=\" STRING \", subscription.sequence=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS Source should subscribe to. STRING No No bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client subscribing/connecting to the NATS broker. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No queue.group.name This can be used when there is a requirement to share the load of a NATS subject. Clients belongs to the same queue group share the subscription load. None STRING Yes No durable.name This can be used to subscribe to a subject from the last acknowledged message when a client or connection failure happens. The client can be uniquely identified using the tuple (client.id, durable.name). None STRING Yes No subscription.sequence This can be used to subscribe to a subject from a given number of message sequence. All the messages from the given point of sequence number will be passed to the client. If not provided then the either the persisted value or 0 will be used. None STRING Yes No Examples EXAMPLE 1 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with all supporting configurations.With the following configuration the source identified as 'nats-client' will subscribes to a subject named as 'SP_NATS_INPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This subscription will receive all the messages from 100 th in the subject. EXAMPLE 2 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', ) define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with mandatory configurations.With the following configuration the source identified with an auto generated client id will subscribes to a subject named as 'SP_NATS_INTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This will receive all available messages in the subject","title":"nats (Source)"},{"location":"docs/api/5.0.0/#tcp-source","text":"A Siddhi application can be configured to receive events via the TCP transport by adding the @Source(type = 'tcp') annotation at the top of an event stream definition. When this is defined the associated stream will receive events from the TCP transport on the host and port defined in the system. Origin: siddhi-io-tcp:3.0.1 Syntax @source(type=\"tcp\", context=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic context The URL 'context' that should be used to receive the events. / STRING Yes No System Parameters Name Description Default Value Possible Parameters host Tcp server host. 0.0.0.0 Any valid host or IP port Tcp server port. 9892 Any integer representing valid port receiver.threads Number of threads to receive connections. 10 Any positive integer worker.threads Number of threads to serve events. 10 Any positive integer tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true true false keep.alive This property defines whether the server should be kept alive when there are no connections available. true true false Examples EXAMPLE 1 @Source(type = 'tcp', context='abc', @map(type='binary')) define stream Foo (attribute1 string, attribute2 int ); Under this configuration, events are received via the TCP transport on default host,port, abc context, and they are passed to Foo stream for processing.","title":"tcp (Source)"},{"location":"docs/api/5.0.0/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"docs/api/5.0.0/#binary-source-mapper","text":"This extension is a binary input mapper that converts events received in binary format to Siddhi events before they are processed. Origin: siddhi-map-binary:2.0.0 Syntax @source(..., @map(type=\"binary\") Examples EXAMPLE 1 @source(type='inMemory', topic='WSO2', @map(type='binary'))define stream FooStream (symbol string, price float, volume long); This query performs a mapping to convert an event of the binary format to a Siddhi event.","title":"binary (Source Mapper)"},{"location":"docs/api/5.0.0/#csv-source-mapper","text":"This extension is used to convert CSV message to Siddhi event input mapper. You can either receive pre-defined CSV message where event conversion takes place without extra configurations,or receive custom CSV message where a custom place order to map from custom CSV message. Origin: siddhi-map-csv:2.0.0 Syntax @source(..., @map(type=\"csv\", delimiter=\" STRING \", header.present=\" BOOL \", fail.on.unknown.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter When converting a CSV format message to Siddhi event, this parameter indicatesinput CSV message's data should be split by this parameter , STRING Yes No header.present When converting a CSV format message to Siddhi event, this parameter indicates whether CSV message has header or not. This can either have value true or false.If it's set to false then it indicates that CSV message has't header. false BOOL Yes No fail.on.unknown.attribute This parameter specifies how unknown attributes should be handled. If it's set to true and one or more attributes don't havevalues, then SP will drop that message. If this parameter is set to false , the Stream Processor adds the required attribute's values to such events with a null value and the event is converted to a Siddhi event. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='csv')) define stream FooStream (symbol string, price float, volume int); Above configuration will do a default CSV input mapping. Expected input will look like below: WSO2 ,55.6 , 100OR \"WSO2,No10,Palam Groove Rd,Col-03\" ,55.6 , 100If header.present is true and delimiter is \"-\", then the input is as follows: symbol-price-volumeWSO2-55.6-100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='csv',header='true', @attributes(symbol = \"2\", price = \"0\", volume = \"1\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom CSV mapping. Here, user can add place order of each attribute in the @attribute. The place order indicates where the attribute name's value has appeared in the input.Expected input will look like below: 55.6,100,WSO2 OR55.6,100,\"WSO2,No10,Palm Groove Rd,Col-03\" If header is true and delimiter is \"-\", then the output is as follows: price-volume-symbol 55.6-100-WSO2 If group events is enabled then input should be as follows: price-volume-symbol 55.6-100-WSO2System.lineSeparator() 55.6-100-IBMSystem.lineSeparator() 55.6-100-IFSSystem.lineSeparator()","title":"csv (Source Mapper)"},{"location":"docs/api/5.0.0/#json-source-mapper","text":"This extension is a JSON-to-Event input mapper. Transports that accept JSON messages can utilize this extension to convert an incoming JSON message into a Siddhi event. Users can either send a pre-defined JSON format, where event conversion happens without any configurations, or use the JSON path to map from a custom JSON message. In default mapping, the JSON string of the event can be enclosed by the element \"event\", though optional. Origin: siddhi-map-json:5.0.1 Syntax @source(..., @map(type=\"json\", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic enclosing.element This is used to specify the enclosing element when sending multiple events in the same JSON message. Mapper treats the child elements of a given enclosing element as events and executes the JSON path expressions on these child elements. If the enclosing.element is not provided then the multiple-event scenario is disregarded and the JSON path is evaluated based on the root element. $ STRING Yes No fail.on.missing.attribute This parameter allows users to handle unknown attributes.The value of this can either be true or false. By default it is true. If a JSON execution fails or returns null, mapper drops that message. However, setting this property to false prompts mapper to send an event with a null value to Siddhi, where users can handle it as required, ie., assign a default value.) true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For a single event, the input is required to be in one of the following formats: { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } or { \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For multiple events, the input is required to be in one of the following formats: [ {\"event\":{\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80}} ] or [ {\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}, {\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}, {\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80} ] EXAMPLE 3 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"company.symbol\", price = \"price\", volume = \"volume\"))) This configuration performs a custom JSON mapping. For a single event, the expected input is similar to the one shown below: .{ \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"WSO2\" }, \"price\":55.6 } } EXAMPLE 4 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream FooStream (symbol string, price float, volume long); The configuration performs a custom JSON mapping. For multiple events, expected input looks as follows. .{\"portfolio\": [ {\"stock\":{\"volume\":100,\"company\":{\"symbol\":\"wso2\"},\"price\":56.6}}, {\"stock\":{\"volume\":200,\"company\":{\"symbol\":\"wso2\"},\"price\":57.6}} ] }","title":"json (Source Mapper)"},{"location":"docs/api/5.0.0/#keyvalue-source-mapper","text":"Key-Value Map to Event input mapper extension allows transports that accept events as key value maps to convert those events to Siddhi events. You can either receive pre-defined keys where conversion takes place without extra configurations, or use custom keys to map from the message. Origin: siddhi-map-keyvalue:2.0.0 Syntax @source(..., @map(type=\"keyvalue\", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic fail.on.missing.attribute If this parameter is set to true , if an event arrives without a matching key for a specific attribute in the connected stream, it is dropped and not processed by the Stream Processor. If this parameter is set to false the Stream Processor adds the required key to such events with a null value, and the event is converted to a Siddhi event so that you could handle them as required before they are further processed. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default key value input mapping. The expected input is a map similar to the following: symbol: 'WSO2' price: 55.6f volume: 100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='keyvalue', fail.on.missing.attribute='true', @attributes(symbol = 's', price = 'p', volume = 'v')))define stream FooStream (symbol string, price float, volume long); This query performs a custom key value input mapping. The matching keys for the symbol , price and volume attributes are be s , p, and v` respectively. The expected input is a map similar to the following: s: 'WSO2' p: 55.6 v: 100","title":"keyvalue (Source Mapper)"},{"location":"docs/api/5.0.0/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.0.0 Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"docs/api/5.0.0/#text-source-mapper","text":"This extension is a text to Siddhi event input mapper. Transports that accept text messages can utilize this extension to convert the incoming text message to Siddhi event. Users can either use a pre-defined text format where event conversion happens without any additional configurations, or specify a regex to map a text message using custom configurations. Origin: siddhi-map-text:2.0.0 Syntax @source(..., @map(type=\"text\", regex.groupid=\" STRING \", fail.on.missing.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex.groupid This parameter specifies a regular expression group. The groupid can be any capital letter (e.g., regex.A,regex.B .. etc). You can specify any number of regular expression groups. In the attribute annotation, you need to map all attributes to the regular expression group with the matching group index. If you need to to enable custom mapping, it is required to specifythe matching group for each and every attribute. STRING No No fail.on.missing.attribute This parameter specifies how unknown attributes should be handled. If it is set to true a message is dropped if its execution fails, or if one or more attributes do not have values. If this parameter is set to false , null values are assigned to attributes with missing values, and messages with such attributes are not dropped. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No delimiter This parameter specifies how events must be separated when multiple events are received. This must be whole line and not a single character. ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n as the end of line character whereas windows uses \\r\\n . \\n STRING Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected input is as follows: symbol:\"WSO2\", price:55.6, volume:100 OR symbol:'WSO2', price:55.6, volume:100 If group events is enabled then input should be as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='text', fail.on.unknown.attribute = 'true', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected input is as follows: wos2 550 volume 100 If group events is enabled then input should be as follows: wos2 550 volume 100 ~ wos2 550 volume 100 ~ wos2 550 volume 100","title":"text (Source Mapper)"},{"location":"docs/api/5.0.0/#xml-source-mapper","text":"This mapper converts XML input to Siddhi event. Transports which accepts XML messages can utilize this extension to convert the incoming XML message to Siddhi event. Users can either send a pre-defined XML format where event conversion will happen without any configs or can use xpath to map from a custom XML message. Origin: siddhi-map-xml:5.0.0 Syntax @source(..., @map(type=\"xml\", namespaces=\" STRING \", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic namespaces Used to provide namespaces used in the incoming XML message beforehand to configure xpath expressions. User can provide a comma separated list. If these are not provided xpath evaluations will fail None STRING Yes No enclosing.element Used to specify the enclosing element in case of sending multiple events in same XML message. WSO2 DAS will treat the child element of given enclosing element as events and execute xpath expressions on child elements. If enclosing.element is not provided multiple event scenario is disregarded and xpaths will be evaluated with respect to root element. Root element STRING Yes No fail.on.missing.attribute This can either have value true or false. By default it will be true. This attribute allows user to handle unknown attributes. By default if an xpath execution fails or returns null DAS will drop that message. However setting this property to false will prompt DAS to send and event with null value to Siddhi where user can handle it accordingly(ie. Assign a default value) True BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping. Expected input will look like below. events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='xml', namespaces = \"dt=urn:schemas-microsoft-com:datatypes\", enclosing.element=\"//portfolio\", @attributes(symbol = \"company/symbol\", price = \"price\", volume = \"volume\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. In the custom mapping user can add xpath expressions representing each event attribute using @attribute annotation. Expected input will look like below. portfolio xmlns:dt=\"urn:schemas-microsoft-com:datatypes\" stock exchange=\"nasdaq\" volume 100 /volume company symbol WSO2 /symbol /company price dt:type=\"number\" 55.6 /price /stock /portfolio","title":"xml (Source Mapper)"},{"location":"docs/api/5.0.0/#store","text":"","title":"Store"},{"location":"docs/api/5.0.0/#rdbms-store","text":"This extension assigns data sources and connection instructions to event tables. It also implements read-write operations on connected datasources. Origin: siddhi-store-rdbms:6.0.0 Syntax @Store(type=\"rdbms\", jdbc.url=\" STRING \", username=\" STRING \", password=\" STRING \", jdbc.driver.name=\" STRING \", pool.properties=\" STRING \", jndi.resource=\" STRING \", datasource=\" STRING \", table.name=\" STRING \", field.length=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic jdbc.url The JDBC URL via which the RDBMS data store is accessed. STRING No No username The username to be used to access the RDBMS data store. STRING No No password The password to be used to access the RDBMS data store. STRING No No jdbc.driver.name The driver class name for connecting the RDBMS data store. STRING No No pool.properties Any pool parameters for the database connection must be specified as key-value pairs. null STRING Yes No jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account and the connection is attempted via JNDI lookup instead. null STRING Yes No datasource The name of the Carbon datasource that should be used for creating the connection with the database. If this is found, neither the pool properties nor the JNDI resource name described above are taken into account and the connection is attempted via Carbon datasources instead. null STRING Yes No table.name The name with which the event table should be persisted in the store. If no name is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The table name defined in the Siddhi App query. STRING Yes No field.length The number of characters that the values for fields of the 'STRING' type in the table definition must contain. Each required field must be provided as a comma-separated list of key-value pairs in the ' field.name : length ' format. If this is not specified, the default number of characters specific to the database type is considered. null STRING Yes No System Parameters Name Description Default Value Possible Parameters {{RDBMS-Name}}.maxVersion The latest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.minVersion The earliest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.tableCheckQuery The template query for the 'check table' operation in {{RDBMS-Name}}. H2 : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) MySQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Oracle : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Microsoft SQL Server : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) PostgreSQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) DB2. : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) N/A {{RDBMS-Name}}.tableCreateQuery The template query for the 'create table' operation in {{RDBMS-Name}}. H2 : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 MySQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 Oracle : SELECT 1 FROM {{TABLE_NAME}} WHERE rownum=1 Microsoft SQL Server : SELECT TOP 1 1 from {{TABLE_NAME}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.indexCreateQuery The template query for the 'create index' operation in {{RDBMS-Name}}. H2 : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) MySQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Oracle : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Microsoft SQL Server : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) {{TABLE_NAME}} ({{INDEX_COLUMNS}}) PostgreSQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) DB2. : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) N/A {{RDBMS-Name}}.recordInsertQuery The template query for the 'insert record' operation in {{RDBMS-Name}}. H2 : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) MySQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Oracle : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Microsoft SQL Server : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) PostgreSQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) DB2. : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) N/A {{RDBMS-Name}}.recordUpdateQuery The template query for the 'update record' operation in {{RDBMS-Name}}. H2 : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} MySQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Oracle : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Microsoft SQL Server : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} PostgreSQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} DB2. : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} N/A {{RDBMS-Name}}.recordSelectQuery The template query for the 'select record' operation in {{RDBMS-Name}}. H2 : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} DB2. : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.recordExistsQuery The template query for the 'check record existence' operation in {{RDBMS-Name}}. H2 : SELECT TOP 1 1 FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT COUNT(1) INTO existence FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT TOP 1 FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.recordDeleteQuery The query for the 'delete record' operation in {{RDBMS-Name}}. H2 : DELETE FROM {{TABLE_NAME}} {{CONDITION}} MySQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Oracle : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : DELETE FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} DB2. : DELETE FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.stringSize This defines the length for the string fields in {{RDBMS-Name}}. H2 : 254 MySQL : 254 Oracle : 254 Microsoft SQL Server : 254 PostgreSQL : 254 DB2. : 254 N/A {{RDBMS-Name}}.fieldSizeLimit This defines the field size limit for select/switch to big string type from the default string type if the 'bigStringType' is available in field type list. H2 : N/A MySQL : N/A Oracle : 2000 Microsoft SQL Server : N/A PostgreSQL : N/A DB2. : N/A 0 = n = INT_MAX {{RDBMS-Name}}.batchSize This defines the batch size when operations are performed for batches of events. H2 : 1000 MySQL : 1000 Oracle : 1000 Microsoft SQL Server : 1000 PostgreSQL : 1000 DB2. : 1000 N/A {{RDBMS-Name}}.batchEnable This specifies whether 'Update' and 'Insert' operations can be performed for batches of events or not. H2 : true MySQL : true Oracle (versions 12.0 and less) : false Oracle (versions 12.1 and above) : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.transactionSupported This is used to specify whether the JDBC connection that is used supports JDBC transactions or not. H2 : true MySQL : true Oracle : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.typeMapping.binaryType This is used to specify the binary data type. An attribute defines as 'object' type in Siddhi stream will be stored into RDBMS with this type. H2 : BLOB MySQL : BLOB Oracle : BLOB Microsoft SQL Server : VARBINARY(max) PostgreSQL : BYTEA DB2. : BLOB(64000) N/A {{RDBMS-Name}}.typeMapping.booleanType This is used to specify the boolean data type. An attribute defines as 'bool' type in Siddhi stream will be stored into RDBMS with this type. H2 : TINYINT(1) MySQL : TINYINT(1) Oracle : NUMBER(1) Microsoft SQL Server : BIT PostgreSQL : BOOLEAN DB2. : SMALLINT N/A {{RDBMS-Name}}.typeMapping.doubleType This is used to specify the double data type. An attribute defines as 'double' type in Siddhi stream will be stored into RDBMS with this type. H2 : DOUBLE MySQL : DOUBLE Oracle : NUMBER(19,4) Microsoft SQL Server : FLOAT(32) PostgreSQL : DOUBLE PRECISION DB2. : DOUBLE N/A {{RDBMS-Name}}.typeMapping.floatType This is used to specify the float data type. An attribute defines as 'float' type in Siddhi stream will be stored into RDBMS with this type. H2 : FLOAT MySQL : FLOAT Oracle : NUMBER(19,4) Microsoft SQL Server : REAL PostgreSQL : REAL DB2. : REAL N/A {{RDBMS-Name}}.typeMapping.integerType This is used to specify the integer data type. An attribute defines as 'int' type in Siddhi stream will be stored into RDBMS with this type. H2 : INTEGER MySQL : INTEGER Oracle : NUMBER(10) Microsoft SQL Server : INTEGER PostgreSQL : INTEGER DB2. : INTEGER N/A {{RDBMS-Name}}.typeMapping.longType This is used to specify the long data type. An attribute defines as 'long' type in Siddhi stream will be stored into RDBMS with this type. H2 : BIGINT MySQL : BIGINT Oracle : NUMBER(19) Microsoft SQL Server : BIGINT PostgreSQL : BIGINT DB2. : BIGINT N/A {{RDBMS-Name}}.typeMapping.stringType This is used to specify the string data type. An attribute defines as 'string' type in Siddhi stream will be stored into RDBMS with this type. H2 : VARCHAR(stringSize) MySQL : VARCHAR(stringSize) Oracle : VARCHAR(stringSize) Microsoft SQL Server : VARCHAR(stringSize) PostgreSQL : VARCHAR(stringSize) DB2. : VARCHAR(stringSize) N/A {{RDBMS-Name}}.typeMapping.bigStringType This is used to specify the big string data type. An attribute defines as 'string' type in Siddhi stream and field.length define in the annotation is greater than the fieldSizeLimit, will be stored into RDBMS with this type. H2 : N/A MySQL : N/A Oracle : CLOB Microsoft SQL Server : N/A PostgreSQL : N/A DB2.* : N/A N/A Examples EXAMPLE 1 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/stocks\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"volume\") define table StockTable (symbol string, price float, volume long); The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float', and 'long' respectively). The connection is made as specified by the parameters configured for the '@Store' annotation. The 'symbol' attribute is considered a unique field, and a DB index is created for it. EXAMPLE 2 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)]","title":"rdbms (Store)"},{"location":"docs/api/5.0.0/#str","text":"","title":"Str"},{"location":"docs/api/5.0.0/#groupconcat-aggregate-function","text":"This function aggregates the received events by concatenating the keys in those events using a separator, e.g.,a comma (,) or a hyphen (-), and returns the concatenated key string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:groupConcat( STRING key, STRING separator, STRING distinct, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key The string that needs to be aggregated. STRING No No separator The separator that separates each string key after concatenating the keys. , STRING Yes No distinct This is used to only have distinct values in the concatenated string that is returned. false STRING Yes No order This parameter accepts 'ASC' or 'DESC' strings to sort the string keys in either ascending or descending order respectively. No order STRING Yes No Examples EXAMPLE 1 from InputStream#window.time(5 min) select str:groupConcat(\"key\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , it returns \"A,B,S,C,A\" to the 'OutputStream'. EXAMPLE 2 from InputStream#window.time(5 min) select groupConcat(\"key\",\"-\",true,\"ASC\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , specify the seperator as hyphen and choose the order to be ascending, the function returns \"A-B-C-S\" to the 'OutputStream'.","title":"groupConcat (Aggregate Function)"},{"location":"docs/api/5.0.0/#charat-function","text":"This function returns the 'char' value that is present at the given index position. of the input string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:charAt( STRING input.value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.value The input string of which the char value at the given position needs to be returned. STRING No No index The variable that specifies the index of the char value that needs to be returned. INT No No Examples EXAMPLE 1 charAt(\"WSO2\", 1) In this case, the functiion returns the character that exists at index 1. Hence, it returns 'S'.","title":"charAt (Function)"},{"location":"docs/api/5.0.0/#coalesce-function_1","text":"This returns the first input parameter value of the given argument, that is not null. Origin: siddhi-execution-string:5.0.1 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT str:coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT argn) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic argn It can have one or more input parameters in any data type. However, all the specified parameters are required to be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 coalesce(null, \"BBB\", \"CCC\") This returns the first input parameter that is not null. In this example, it returns \"BBB\".","title":"coalesce (Function)"},{"location":"docs/api/5.0.0/#concat-function","text":"This function returns a string value that is obtained as a result of concatenating two or more input string values. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:concat( STRING argn) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic argn This can have two or more string type input parameters. STRING No No Examples EXAMPLE 1 concat(\"D533\", \"8JU^\", \"XYZ\") This returns a string value by concatenating two or more given arguments. In the example shown above, it returns \"D5338JU^XYZ\".","title":"concat (Function)"},{"location":"docs/api/5.0.0/#contains-function","text":"This function returns true if the input.string contains the specified sequence of char values in the search.string . Origin: siddhi-execution-string:5.0.1 Syntax BOOL str:contains( STRING input.string, STRING search.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string Input string value. STRING No No search.string The string value to be searched for in the input.string . STRING No No Examples EXAMPLE 1 contains(\"21 products are produced by WSO2 currently\", \"WSO2\") This returns a boolean value as the output. In this case, it returns true .","title":"contains (Function)"},{"location":"docs/api/5.0.0/#equalsignorecase-function","text":"This returns a boolean value by comparing two strings lexicographically without considering the letter case. Origin: siddhi-execution-string:5.0.1 Syntax BOOL str:equalsIgnoreCase( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No No arg2 The second input string argument. This is compared with the first argument. STRING No No Examples EXAMPLE 1 equalsIgnoreCase(\"WSO2\", \"wso2\") This returns a boolean value as the output. In this scenario, it returns \"true\".","title":"equalsIgnoreCase (Function)"},{"location":"docs/api/5.0.0/#filltemplate-function","text":"This extension replaces the templated positions that are marked with an index value in a specified template with the strings provided. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:fillTemplate( STRING template, STRING|INT|LONG|DOUBLE|FLOAT|BOOL replacement.strings) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic template The string with templated fields that needs to be filled with the given strings. The format of the templated fields should be as follows: {{INDEX}} where 'INDEX' is an integer. This index is used to map the strings that are used to replace the templated fields. STRING No No replacement.strings The strings with which the templated positions in the template need to be replaced. The minimum of two arguments need to be included in the execution string. There is no upper limit on the number of arguments allowed to be included. STRING INT LONG DOUBLE FLOAT BOOL No No Examples EXAMPLE 1 str:fillTemplate(\"This is {{1}} for the {{2}} function\", 'an example', 'fillTemplate') In this example, the template is 'This is {{1}} for the {{2}} function'.Here, the templated string {{1}} is replaced with the 1 st string value provided, which is 'an example'. {{2}} is replaced with the 2 nd string provided, which is 'fillTemplate' The complete return string is 'This is an example for the fillTemplate function'.","title":"fillTemplate (Function)"},{"location":"docs/api/5.0.0/#hex-function_1","text":"This function returns a hexadecimal string by converting each byte of each character in the input string to two hexadecimal digits. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:hex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the hexadecimal value. STRING No No Examples EXAMPLE 1 hex(\"MySQL\") This returns the hexadecimal value of the input.string. In this scenario, the output is \"4d7953514c\".","title":"hex (Function)"},{"location":"docs/api/5.0.0/#length-function","text":"Returns the length of the input string. Origin: siddhi-execution-string:5.0.1 Syntax INT str:length( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the length. STRING No No Examples EXAMPLE 1 length(\"Hello World\") This outputs the length of the provided string. In this scenario, the, output is 11 .","title":"length (Function)"},{"location":"docs/api/5.0.0/#lower-function","text":"Converts the capital letters in the input string to the equivalent simple letters. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:lower( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to convert to the lower case (i.e., equivalent simple letters). STRING No No Examples EXAMPLE 1 lower(\"WSO2 cep \") This converts the capital letters in the input.string to the equivalent simple letters. In this scenario, the output is \"wso2 cep \".","title":"lower (Function)"},{"location":"docs/api/5.0.0/#regexp-function","text":"Returns a boolean value based on the matchability of the input string and the given regular expression. Origin: siddhi-execution-string:5.0.1 Syntax BOOL str:regexp( STRING input.string, STRING regex) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to match with the given regular expression. STRING No No regex The regular expression to be matched with the input string. STRING No No Examples EXAMPLE 1 regexp(\"WSO2 abcdh\", \"WSO(.*h)\") This returns a boolean value after matching regular expression with the given string. In this scenario, it returns \"true\" as the output.","title":"regexp (Function)"},{"location":"docs/api/5.0.0/#repeat-function","text":"Repeats the input string for a specified number of times. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:repeat( STRING input.string, INT times) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that is repeated the number of times as defined by the user. STRING No No times The number of times the input.string needs to be repeated . INT No No Examples EXAMPLE 1 repeat(\"StRing 1\", 3) This returns a string value by repeating the string for a specified number of times. In this scenario, the output is \"StRing 1StRing 1StRing 1\".","title":"repeat (Function)"},{"location":"docs/api/5.0.0/#replaceall-function","text":"Finds all the substrings of the input string that matches with the given expression, and replaces them with the given replacement string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:replaceAll( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No No regex The regular expression to be matched with the input string. STRING No No replacement.string The string with which each substring that matches the given expression should be replaced. STRING No No Examples EXAMPLE 1 replaceAll(\"hello hi hello\", 'hello', 'test') This returns a string after replacing the substrings of the input string with the replacement string. In this scenario, the output is \"test hi test\" .","title":"replaceAll (Function)"},{"location":"docs/api/5.0.0/#replacefirst-function","text":"Finds the first substring of the input string that matches with the given regular expression, and replaces itwith the given replacement string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:replaceFirst( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be replaced. STRING No No regex The regular expression with which the input string should be matched. STRING No No replacement.string The string with which the first substring of input string that matches the regular expression should be replaced. STRING No No Examples EXAMPLE 1 replaceFirst(\"hello WSO2 A hello\", 'WSO2(.*)A', 'XXXX') This returns a string after replacing the first substring with the given replacement string. In this scenario, the output is \"hello XXXX hello\".","title":"replaceFirst (Function)"},{"location":"docs/api/5.0.0/#reverse-function","text":"Returns the input string in the reverse order character-wise and string-wise. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:reverse( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be reversed. STRING No No Examples EXAMPLE 1 reverse(\"Hello World\") This outputs a string value by reversing the incoming input.string . In this scenario, the output is \"dlroW olleH\".","title":"reverse (Function)"},{"location":"docs/api/5.0.0/#split-function","text":"Splits the input.string into substrings using the value parsed in the split.string and returns the substring at the position specified in the group.number . Origin: siddhi-execution-string:5.0.1 Syntax STRING str:split( STRING input.string, STRING split.string, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No No split.string The string value to be used to split the input.string . STRING No No group.number The index of the split group INT No No Examples EXAMPLE 1 split(\"WSO2,ABM,NSFT\", \",\", 0) This splits the given input.string by given split.string and returns the string in the index given by group.number. In this scenario, the output will is \"WSO2\".","title":"split (Function)"},{"location":"docs/api/5.0.0/#strcmp-function","text":"Compares two strings lexicographically and returns an integer value. If both strings are equal, 0 is returned. If the first string is lexicographically greater than the second string, a positive value is returned. If the first string is lexicographically greater than the second string, a negative value is returned. Origin: siddhi-execution-string:5.0.1 Syntax INT str:strcmp( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No No arg2 The second input string argument that should be compared with the first argument lexicographically. STRING No No Examples EXAMPLE 1 strcmp(\"AbCDefghiJ KLMN\", 'Hello') This compares two strings lexicographically and outputs an integer value.","title":"strcmp (Function)"},{"location":"docs/api/5.0.0/#substr-function","text":"Returns a substring of the input string by considering a subset or all of the following factors: starting index, length, regular expression, and regex group number. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:substr( STRING input.string, INT begin.index, INT length, STRING regex, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be processed. STRING No No begin.index Starting index to consider for the substring. INT No No length The length of the substring. INT No No regex The regular expression that should be matched with the input string. STRING No No group.number The regex group number INT No No Examples EXAMPLE 1 substr(\"AbCDefghiJ KLMN\", 4) This outputs the substring based on the given begin.index . In this scenario, the output is \"efghiJ KLMN\". EXAMPLE 2 substr(\"AbCDefghiJ KLMN\", 2, 4) This outputs the substring based on the given begin.index and length. In this scenario, the output is \"CDef\". EXAMPLE 3 substr(\"WSO2D efghiJ KLMN\", '^WSO2(.*)') This outputs the substring by applying the regex. In this scenario, the output is \"WSO2D efghiJ KLMN\". EXAMPLE 4 substr(\"WSO2 cep WSO2 XX E hi hA WSO2 heAllo\", 'WSO2(.*)A(.*)', 2) This outputs the substring by applying the regex and considering the group.number . In this scenario, the output is \" ello\".","title":"substr (Function)"},{"location":"docs/api/5.0.0/#trim-function","text":"Returns a copy of the input string without the leading and trailing whitespace (if any). Origin: siddhi-execution-string:5.0.1 Syntax STRING str:trim( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that needs to be trimmed. STRING No No Examples EXAMPLE 1 trim(\" AbCDefghiJ KLMN \") This returns a copy of the input.string with the leading and/or trailing white-spaces omitted. In this scenario, the output is \"AbCDefghiJ KLMN\".","title":"trim (Function)"},{"location":"docs/api/5.0.0/#unhex-function","text":"Returns a string by converting the hexadecimal characters in the input string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:unhex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The hexadecimal input string that needs to be converted to string. STRING No No Examples EXAMPLE 1 unhex(\"4d7953514c\") This converts the hexadecimal value to string.","title":"unhex (Function)"},{"location":"docs/api/5.0.0/#upper-function","text":"Converts the simple letters in the input string to the equivalent capital/block letters. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:upper( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be converted to the upper case (equivalent capital/block letters). STRING No No Examples EXAMPLE 1 upper(\"Hello World\") This converts the simple letters in the input.string to theequivalent capital letters. In this scenario, the output is \"HELLO WORLD\".","title":"upper (Function)"},{"location":"docs/api/5.0.0/#tokenize-stream-processor_1","text":"This function splits the input string into tokens using a given regular expression and returns the split tokens. Origin: siddhi-execution-string:5.0.1 Syntax str:tokenize( STRING input.string, STRING regex, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string which needs to be split. STRING No No regex The string value which is used to tokenize the 'input.string'. STRING No No distinct This flag is used to return only distinct values. false BOOL Yes No Extra Return Attributes Name Description Possible Types token The attribute which contains a single token. STRING Examples EXAMPLE 1 define stream inputStream (str string); @info(name = 'query1') from inputStream#str:tokenize(str , ',') select text insert into outputStream; This query performs tokenization on the given string. If the str is \"Android,Windows8,iOS\", then the string is split into 3 events containing the token attribute values, i.e., Android , Windows8 and iOS .","title":"tokenize (Stream Processor)"},{"location":"docs/api/latest/","text":"API Docs - v5.0.0 Core and (Aggregate Function) Returns the results of AND operation for all the events. Origin: siddhi-core:5.0.0 Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) Calculates the average for all the events. Origin: siddhi-core:5.0.0 Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) Returns the count of all the events. Origin: siddhi-core:5.0.0 Syntax LONG count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) This returns the count of distinct occurrences for a given arg. Origin: siddhi-core:5.0.0 Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) Returns the maximum value for all the events. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) Returns the minimum value for all the events. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) Returns the results of OR operation for all the events. Origin: siddhi-core:5.0.0 Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) Returns the calculated standard deviation for all the events. Origin: siddhi-core:5.0.0 Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) Returns the sum for all the events. Origin: siddhi-core:5.0.0 Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Origin: siddhi-core:5.0.0 Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) Generates a UUID (Universally Unique Identifier). Origin: siddhi-core:5.0.0 Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) Converts the first input parameter according to the convertedTo parameter. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) Includes the given input parameter in a java.util.HashSet and returns the set. Origin: siddhi-core:5.0.0 Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) Returns the current timestamp of siddhi application in milliseconds. Origin: siddhi-core:5.0.0 Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) Returns the timestamp of the processed event. Origin: siddhi-core:5.0.0 Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) Checks whether the parameter is an instance of Boolean or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) Checks whether the parameter is an instance of Double or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) Checks whether the parameter is an instance of Float or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) Checks whether the parameter is an instance of Integer or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) Checks whether the parameter is an instance of Long or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) Checks whether the parameter is an instance of String or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) Returns the maximum value of the input parameters. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) Returns the minimum value of the input parameters. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) Returns the size of an object of type java.util.Set. Origin: siddhi-core:5.0.0 Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Origin: siddhi-core:5.0.0 Syntax pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) The logger logs the message on the given priority with or without processed event. Origin: siddhi-core:5.0.0 Syntax log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log(\"INFO\", \"Sample Event :\", true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log(\"Sample Event :\", true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log(\"Sample Event :\", fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log(\"Sample Event :\") select * insert into barStream; This will log message and fooStream:events. batch (Window) A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Origin: siddhi-core:5.0.0 Syntax batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Origin: siddhi-core:5.0.0 Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) A delay window holds events for a specific time period that is regarded as a delay period before processing them. Origin: siddhi-core:5.0.0 Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Origin: siddhi-core:5.0.0 Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Origin: siddhi-core:5.0.0 Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Origin: siddhi-core:5.0.0 Syntax frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Origin: siddhi-core:5.0.0 Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Origin: siddhi-core:5.0.0 Syntax lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Origin: siddhi-core:5.0.0 Syntax lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Origin: siddhi-core:5.0.0 Syntax session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Origin: siddhi-core:5.0.0 Syntax sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Origin: siddhi-core:5.0.0 Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Origin: siddhi-core:5.0.0 Syntax timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Origin: siddhi-core:5.0.0 Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Json getBool (Function) This method returns a 'boolean' value, either 'true' or 'false', based on the valuespecified against the JSON element present in the given path.In case there is no valid boolean value found in the given path, the method still returns 'false'. Origin: siddhi-execution-json:2.0.0 Syntax BOOL json:getBool( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the boolean value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getBool' function fetches theboolean value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getBool(json,\"$.name\") as name insert into OutputStream; This returns the boolean value of the JSON input in the given path. The results are directed to the 'OutputStream' stream. getDouble (Function) This method returns the double value of the JSON element present in the given path. If there is no valid double value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax DOUBLE json:getDouble( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getDouble' function fetches thedouble value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getDouble(json,\"$.name\") as name insert into OutputStream; This returns the double value of the given path. The results aredirected to the 'OutputStream' stream. getFloat (Function) This method returns the float value of the JSON element present in the given path.If there is no valid float value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax FLOAT json:getFloat( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getFloat' function fetches thevalue. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getFloat(json,\"$.name\") as name insert into OutputStream; This returns the float value of the JSON input in the given path. The results aredirected to the 'OutputStream' stream. getInt (Function) This method returns the integer value of the JSON element present in the given path. If there is no valid integer value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax INT json:getInt( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getInt' function fetches theinteger value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getInt(json,\"$.name\") as name insert into OutputStream; This returns the integer value of the JSON input in the given path. The resultsare directed to the 'OutputStream' stream. getLong (Function) This returns the long value of the JSON element present in the given path. Ifthere is no valid long value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax LONG json:getLong( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the JSON element from which the 'getLong' functionfetches the long value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getLong(json,\"$.name\") as name insert into OutputStream; This returns the long value of the JSON input in the given path. The results aredirected to 'OutputStream' stream. getObject (Function) This returns the object of the JSON element present in the given path. Origin: siddhi-execution-json:2.0.0 Syntax OBJECT json:getObject( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getObject' function fetches theobject. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getObject(json,\"$.name\") as name insert into OutputStream; This returns the object of the JSON input in the given path. The results are directed to the 'OutputStream' stream. getString (Function) This returns the string value of the JSON element present in the given path. Origin: siddhi-execution-json:2.0.0 Syntax STRING json:getString( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the JSON input from which the 'getString' function fetches the string value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getString(json,\"$.name\") as name insert into OutputStream; This returns the string value of the JSON input in the given path. The results are directed to the 'OutputStream' stream. isExists (Function) This method checks whether there is a JSON element present in the given path or not.If there is a valid JSON element in the given path, it returns 'true'. If there is no valid JSON element, it returns 'false' Origin: siddhi-execution-json:2.0.0 Syntax BOOL json:isExists( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input in a given path, on which the function performs the search forJSON elements. STRING OBJECT No No path The path that contains the input JSON on which the function performs the search. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:isExists(json,\"$.name\") as name insert into OutputStream; This returns either true or false based on the existence of a JSON element in a given path. The results are directed to the 'OutputStream' stream. setElement (Function) This method allows to insert elements into a given JSON present in a specific path. If there is no valid path given, it returns the original JSON. Otherwise, it returns the new JSON. Origin: siddhi-execution-json:2.0.0 Syntax OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT jsonelement, STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input into which is this function inserts the new value. STRING OBJECT No No path The path on the JSON input which is used to insert the given element. STRING No No jsonelement The JSON element which is inserted by the function into the input JSON. STRING BOOL DOUBLE FLOAT INT LONG OBJECT No No key The key which is used to insert the given element into the input JSON. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:setElement(json,\"$.name\") as name insert into OutputStream; This returns the JSON object present in the given path with the newly inserted JSONelement. The results are directed to the 'OutputStream' stream. toObject (Function) This method returns the JSON object related to a given JSON string. Origin: siddhi-execution-json:2.0.0 Syntax OBJECT json:toObject( STRING json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON string from which the function generates the JSON object. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:toJson(json) as jsonObject insert into OutputStream; This returns the JSON object corresponding to the given JSON string.The results aredirected to the 'OutputStream' stream. toString (Function) This method returns the JSON string corresponding to a given JSON object. Origin: siddhi-execution-json:2.0.0 Syntax STRING json:toString( OBJECT json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON object from which the function generates a JSON string. OBJECT No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:toString(json) as jsonString insert into OutputStream; This returns the JSON string corresponding to a given JSON object. The results are directed to the 'OutputStream' stream. tokenize (Stream Processor) This tokenizes the given json according the path provided Origin: siddhi-execution-json:2.0.0 Syntax json:tokenize( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input json that should be tokenized using the given path. STRING OBJECT No No path The path that is used to tokenize the given json STRING No No fail.on.missing.attribute If this parameter is set to 'true' and a json is not provided in the given path, the event is dropped. If the parameter is set to 'false', the unavailability of a json in the specified path results in the event being created with a 'null' value for the json element. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The json element retrieved based on the given path and the json. STRING Examples EXAMPLE 1 define stream InputStream (json string,path string); @info(name = 'query1') from InputStream#json:tokenize(json, path) select jsonElement insert into OutputStream; This query performs a tokenization for the given json using the path specified. If the specified path provides a json array, it generates events for each element in that array by adding an additional attributes as the 'jsonElement' to the stream. e.g., jsonInput - {name:\"John\",enrolledSubjects:[\"Mathematics\",\"Physics\"]}, path - \" .enrolledSubjects\" /code br If we use the configuration in this example, it generates two events with the attributes \"Mathematics\", \"Physics\". br If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream. br code e.g., jsonInput - {name:\"John\",age:25}, path - \" .enrolledSubjects\" </code><br>&nbsp;If we use the configuration in this example, it generates two events with the attributes \"Mathematics\", \"Physics\".<br>If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream. <br><code> e.g., jsonInput - {name:\"John\",age:25}, path - \" .age\" tokenizeAsObject (Stream Processor) This tokenizes the given JSON based on the path provided and returns the response as an object. Origin: siddhi-execution-json:2.0.0 Syntax json:tokenizeAsObject( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input json that is tokenized using the given path. STRING OBJECT No No path The path of the input JSON that the function tokenizes. STRING No No fail.on.missing.attribute If this parameter is set to 'true' and a JSON is not provided in the given path, the event is dropped. If the parameter is set to 'false', the unavailability of a JSON in the specified path results in the event being created with a 'null' value for the json element. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path and the JSON. OBJECT Examples EXAMPLE 1 define stream InputStream (json string,path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select jsonElement insert into OutputStream; This query performs a tokenization for the given JSON using the path specified. If the specified path provides a JSON array, it generates events for each element in the specified json array by adding an additional attribute as the 'jsonElement' into the stream. e.g., jsonInput - {name:\"John\",enrolledSubjects:[\"Mathematics\",\"Physics\"]}, path - \" .enrolledSubjects\" /code br If we use the configuration in the above example, it generates two events with the attributes \"Mathematics\" and \"Physics\". br If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream br code e.g., jsonInput - {name:\"John\",age:25}, path - \" .enrolledSubjects\" </code><br>If we use the configuration in the above example, it generates two events with the attributes \"Mathematics\" and \"Physics\".<br>If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream <br><code> e.g., jsonInput - {name:\"John\",age:25}, path - \" .age\" Math percentile (Aggregate Function) This functions returns the pth percentile value of a given argument. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:percentile( INT|LONG|FLOAT|DOUBLE arg, DOUBLE p) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value of the parameter whose percentile should be found. INT LONG FLOAT DOUBLE No No p Estimate of the percentile to be found (pth percentile) where p is any number greater than 0 or lesser than or equal to 100. DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (sensorId int, temperature double); from InValueStream select math:percentile(temperature, 97.0) as percentile insert into OutMediationStream; This function returns the percentile value based on the argument given. For example, math:percentile(temperature, 97.0) returns the 97 th percentile value of all the temperature events. abs (Function) This function returns the absolute value of the given parameter. It wraps the java.lang.Math.abs() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:abs( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The parameter whose absolute value is found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:abs(inValue) as absValue insert into OutMediationStream; Irrespective of whether the 'invalue' in the input stream holds a value of abs(3) or abs(-3),the function returns 3 since the absolute value of both 3 and -3 is 3. The result directed to OutMediationStream stream. acos (Function) If -1 = p1 = 1, this function returns the arc-cosine (inverse cosine) value of p1.If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.acos() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:acos( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-cosine (inverse cosine) value is found. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:acos(inValue) as acosValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-cosine value of it and returns the arc-cosine value to the output stream, OutMediationStream. For example, acos(0.5) returns 1.0471975511965979. asin (Function) If -1 = p1 = 1, this function returns the arc-sin (inverse sine) value of p1. If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.asin() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:asin( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-sin (inverse sine) value is found. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:asin(inValue) as asinValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-sin value of it and returns the arc-sin value to the output stream, OutMediationStream. For example, asin(0.5) returns 0.5235987755982989. atan (Function) 1. If a single p1 is received, this function returns the arc-tangent (inverse tangent) value of p1 . 2. If p1 is received along with an optional p1 , it considers them as x and y coordinates and returns the arc-tangent (inverse tangent) value. The returned value is in radian scale. This function wraps the java.lang.Math.atan() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-tangent (inverse tangent) is found. If the optional second parameter is given this represents the x coordinate of the (x,y) coordinate pair. INT LONG FLOAT DOUBLE No No p1 This optional parameter represents the y coordinate of the (x,y) coordinate pair. 0D INT LONG FLOAT DOUBLE Yes No Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:atan(inValue1, inValue2) as convertedValue insert into OutMediationStream; If the 'inValue1' in the input stream is given, the function calculates the arc-tangent value of it and returns the arc-tangent value to the output stream, OutMediationStream. If both the 'inValue1' and 'inValue2' are given, then the function considers them to be x and y coordinates respectively and returns the calculated arc-tangent value to the output stream, OutMediationStream. For example, atan(12d, 5d) returns 1.1760052070951352. bin (Function) This function returns a string representation of the p1 argument, that is of either 'integer' or 'long' data type, as an unsigned integer in base 2. It wraps the java.lang.Integer.toBinaryString and java.lang.Long.toBinaryString` methods. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:bin( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value in either 'integer' or 'long', that should be converted into an unsigned integer of base 2. INT LONG No No Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:bin(inValue) as binValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function converts it into an unsigned integer in base 2 and directs the output to the output stream, OutMediationStream. For example, bin(9) returns '1001'. cbrt (Function) This function returns the cube-root of 'p1' which is in radians. It wraps the java.lang.Math.cbrt() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:cbrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cube-root should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cbrt(inValue) as cbrtValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cube-root value for the same and directs the output to the output stream, OutMediationStream. For example, cbrt(17d) returns 2.5712815906582356. ceil (Function) This function returns the smallest double value, i.e., the closest to the negative infinity, that is greater than or equal to the p1 argument, and is equal to a mathematical integer. It wraps the java.lang.Math.ceil() method. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:ceil( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose ceiling value is found. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ceil(inValue) as ceilingValue insert into OutMediationStream; This function calculates the ceiling value of the given 'inValue' and directs the result to 'OutMediationStream' output stream. For example, ceil(423.187d) returns 424.0. conv (Function) This function converts a from the fromBase base to the toBase base. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:conv( STRING a, INT from.base, INT to.base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic a The value whose base should be changed. Input should be given as a 'String'. STRING No No from.base The source base of the input parameter 'a'. INT No No to.base The target base that the input parameter 'a' should be converted into. INT No No Examples EXAMPLE 1 define stream InValueStream (inValue string,fromBase int,toBase int); from InValueStream select math:conv(inValue,fromBase,toBase) as convertedValue insert into OutMediationStream; If the 'inValue' in the input stream is given, and the base in which it currently resides in and the base to which it should be converted to is specified, the function converts it into a string in the target base and directs it to the output stream, OutMediationStream. For example, conv(\"7f\", 16, 10) returns \"127\". copySign (Function) This function returns a value of an input with the received magnitude and sign of another input. It wraps the java.lang.Math.copySign() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:copySign( INT|LONG|FLOAT|DOUBLE magnitude, INT|LONG|FLOAT|DOUBLE sign) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic magnitude The magnitude of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No No sign The sign of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:copySign(inValue1,inValue2) as copysignValue insert into OutMediationStream; If two values are provided as 'inValue1' and 'inValue2', the function copies the magnitude and sign of the second argument into the first one and directs the result to the output stream, OutMediatonStream. For example, copySign(5.6d, -3.0d) returns -5.6. cos (Function) This function returns the cosine of p1 which is in radians. It wraps the java.lang.Math.cos() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:cos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cosine value should be found.The input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cos(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cos(6d) returns 0.9601702866503661. cosh (Function) This function returns the hyperbolic cosine of p1 which is in radians. It wraps the java.lang.Math.cosh() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:cosh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic cosine should be found. The input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cosh(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the hyperbolic cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cosh (6d) returns 201.7156361224559. e (Function) This function returns the java.lang.Math.E constant, which is the closest double value to e, where e is the base of the natural logarithms. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:e() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:e() as eValue insert into OutMediationStream; This function returns the constant, 2.7182818284590452354 which is the closest double value to e and directs the output to 'OutMediationStream' output stream. exp (Function) This function returns the Euler's number e raised to the power of p1 . It wraps the java.lang.Math.exp() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:exp( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The power that the Euler's number e is raised to. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:exp(inValue) as expValue insert into OutMediationStream; If the 'inValue' in the inputstream holds a value, this function calculates the corresponding Euler's number 'e' and directs it to the output stream, OutMediationStream. For example, exp(10.23) returns 27722.51006805505. floor (Function) This function wraps the java.lang.Math.floor() function and returns the largest value, i.e., closest to the positive infinity, that is less than or equal to p1 , and is equal to a mathematical integer. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:floor( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose floor value should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:floor(inValue) as floorValue insert into OutMediationStream; This function calculates the floor value of the given 'inValue' input and directs the output to the 'OutMediationStream' output stream. For example, (10.23) returns 10.0. getExponent (Function) This function returns the unbiased exponent that is used in the representation of p1 . This function wraps the java.lang.Math.getExponent() function. Origin: siddhi-execution-math:5.0.0 Syntax INT math:getExponent( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of whose unbiased exponent representation should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:getExponent(inValue) as expValue insert into OutMediationStream; This function calculates the unbiased exponent of a given input, 'inValue' and directs the result to the 'OutMediationStream' output stream. For example, getExponent(60984.1) returns 15. hex (Function) This function wraps the java.lang.Double.toHexString() function. It returns a hexadecimal string representation of the input, p1`. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:hex( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hexadecimal value should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue int); from InValueStream select math:hex(inValue) as hexString insert into OutMediationStream; If the 'inValue' in the input stream is provided, the function converts this into its corresponding hexadecimal format and directs the output to the output stream, OutMediationStream. For example, hex(200) returns \"c8\". isInfinite (Function) This function wraps the java.lang.Float.isInfinite() and java.lang.Double.isInfinite() and returns true if p1 is infinitely large in magnitude and false if otherwise. Origin: siddhi-execution-math:5.0.0 Syntax BOOL math:isInfinite( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 This is the value of the parameter that the function determines to be either infinite or finite. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isInfinite(inValue1) as isInfinite insert into OutMediationStream; If the value given in the 'inValue' in the input stream is of infinitely large magnitude, the function returns the value, 'true' and directs the result to the output stream, OutMediationStream'. For example, isInfinite(java.lang.Double.POSITIVE_INFINITY) returns true. isNan (Function) This function wraps the java.lang.Float.isNaN() and java.lang.Double.isNaN() functions and returns true if p1 is NaN (Not-a-Number), and returns false if otherwise. Origin: siddhi-execution-math:5.0.0 Syntax BOOL math:isNan( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter which the function determines to be either NaN or a number. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isNan(inValue1) as isNaN insert into OutMediationStream; If the 'inValue1' in the input stream has a value that is undefined, then the function considers it as an 'NaN' value and directs 'True' to the output stream, OutMediationStream. For example, isNan(java.lang.Math.log(-12d)) returns true. ln (Function) This function returns the natural logarithm (base e) of p1 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:ln( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose natural logarithm (base e) should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ln(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates its natural logarithm (base e) and directs the results to the output stream, 'OutMeditionStream'. For example, ln(11.453) returns 2.438251704415579. log (Function) This function returns the logarithm of the received number as per the given base . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:log( INT|LONG|FLOAT|DOUBLE number, INT|LONG|FLOAT|DOUBLE base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic number The value of the parameter whose base should be changed. INT LONG FLOAT DOUBLE No No base The base value of the ouput. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (number double, base double); from InValueStream select math:log(number, base) as logValue insert into OutMediationStream; If the number and the base to which it has to be converted into is given in the input stream, the function calculates the number to the base specified and directs the result to the output stream, OutMediationStream. For example, log(34, 2f) returns 5.08746284125034. log10 (Function) This function returns the base 10 logarithm of p1 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:log10( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 10 logarithm should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log10(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 10 logarithm of the same and directs the result to the output stream, OutMediatioStream. For example, log10(19.234) returns 1.2840696117100832. log2 (Function) This function returns the base 2 logarithm of p1 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:log2( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 2 logarithm should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log2(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 2 logarithm of the same and returns the value to the output stream, OutMediationStream. For example log2(91d) returns 6.507794640198696. max (Function) This function returns the greater value of p1 and p2 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:max( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values to be compared in order to find the larger value of the two INT LONG FLOAT DOUBLE No No p2 The input value to be compared with 'p1' in order to find the larger value of the two. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:max(inValue1,inValue2) as maxValue insert into OutMediationStream; If two input values 'inValue1, and 'inValue2' are given, the function compares them and directs the larger value to the output stream, OutMediationStream. For example, max(123.67d, 91) returns 123.67. min (Function) This function returns the smaller value of p1 and p2 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:min( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values that are to be compared in order to find the smaller value. INT LONG FLOAT DOUBLE No No p2 The input value that is to be compared with 'p1' in order to find the smaller value. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:min(inValue1,inValue2) as minValue insert into OutMediationStream; If two input values, 'inValue1' and 'inValue2' are given, the function compares them and directs the smaller value of the two to the output stream, OutMediationStream. For example, min(123.67d, 91) returns 91. oct (Function) This function converts the input parameter p1 to octal. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:oct( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose octal representation should be found. INT LONG No No Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:oct(inValue) as octValue insert into OutMediationStream; If the 'inValue' in the input stream is given, this function calculates the octal value corresponding to the same and directs it to the output stream, OutMediationStream. For example, oct(99l) returns \"143\". parseDouble (Function) This function returns the double value of the string received. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:parseDouble( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a double value. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseDouble(inValue) as output insert into OutMediationStream; If the 'inValue' in the input stream holds a value, this function converts it into the corresponding double value and directs it to the output stream, OutMediationStream. For example, parseDouble(\"123\") returns 123.0. parseFloat (Function) This function returns the float value of the received string. Origin: siddhi-execution-math:5.0.0 Syntax FLOAT math:parseFloat( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a float value. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseFloat(inValue) as output insert into OutMediationStream; The function converts the input value given in 'inValue',into its corresponding float value and directs the result into the output stream, OutMediationStream. For example, parseFloat(\"123\") returns 123.0. parseInt (Function) This function returns the integer value of the received string. Origin: siddhi-execution-math:5.0.0 Syntax INT math:parseInt( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to an integer. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseInt(inValue) as output insert into OutMediationStream; The function converts the 'inValue' into its corresponding integer value and directs the output to the output stream, OutMediationStream. For example, parseInt(\"123\") returns 123. parseLong (Function) This function returns the long value of the string received. Origin: siddhi-execution-math:5.0.0 Syntax LONG math:parseLong( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to a long value. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseLong(inValue) as output insert into OutMediationStream; The function converts the 'inValue' to its corresponding long value and directs the result to the output stream, OutMediationStream. For example, parseLong(\"123\") returns 123. pi (Function) This function returns the java.lang.Math.PI constant, which is the closest value to pi, i.e., the ratio of the circumference of a circle to its diameter. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:pi() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:pi() as piValue insert into OutMediationStream; pi() always returns 3.141592653589793. power (Function) This function raises the given value to a given power. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:power( INT|LONG|FLOAT|DOUBLE value, INT|LONG|FLOAT|DOUBLE to.power) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value The value that should be raised to the power of 'to.power' input parameter. INT LONG FLOAT DOUBLE No No to.power The power to which the 'value' input parameter should be raised. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:power(inValue1,inValue2) as powerValue insert into OutMediationStream; This function raises the 'inValue1' to the power of 'inValue2' and directs the output to the output stream, 'OutMediationStream. For example, (5.6d, 3.0d) returns 175.61599999999996. rand (Function) This returns a stream of pseudo-random numbers when a sequence of calls are sent to the rand() . Optionally, it is possible to define a seed, i.e., rand(seed) using which the pseudo-random numbers are generated. These functions internally use the java.util.Random class. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:rand( INT|LONG seed) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic seed An optional seed value that will be used to generate the random number sequence. defaultSeed INT LONG Yes No Examples EXAMPLE 1 define stream InValueStream (symbol string, price long, volume long); from InValueStream select symbol, math:rand() as randNumber select math:oct(inValue) as octValue insert into OutMediationStream; In the example given above, a random double value between 0 and 1 will be generated using math:rand(). round (Function) This function returns the value of the input argument rounded off to the closest integer/long value. Origin: siddhi-execution-math:5.0.0 Syntax INT|LONG math:round( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be rounded off to the closest integer/long value. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:round(inValue) as roundValue insert into OutMediationStream; The function rounds off 'inValue1' to the closest int/long value and directs the output to the output stream, 'OutMediationStream'. For example, round(3252.353) returns 3252. signum (Function) This returns +1, 0, or -1 for the given positive, zero and negative values respectively. This function wraps the java.lang.Math.signum() function. Origin: siddhi-execution-math:5.0.0 Syntax INT math:signum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be checked to be positive, negative or zero. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:signum(inValue) as sign insert into OutMediationStream; The function evaluates the 'inValue' given to be positive, negative or zero and directs the result to the output stream, 'OutMediationStream'. For example, signum(-6.32d) returns -1. sin (Function) This returns the sine of the value given in radians. This function wraps the java.lang.Math.sin() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:sin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sin(inValue) as sinValue insert into OutMediationStream; The function calculates the sine value of the given 'inValue' and directs the output to the output stream, 'OutMediationStream. For example, sin(6d) returns -0.27941549819892586. sinh (Function) This returns the hyperbolic sine of the value given in radians. This function wraps the java.lang.Math.sinh() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:sinh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sinh(inValue) as sinhValue insert into OutMediationStream; This function calculates the hyperbolic sine value of 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sinh(6d) returns 201.71315737027922. sqrt (Function) This function returns the square-root of the given value. It wraps the java.lang.Math.sqrt() s function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:sqrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose square-root value should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sqrt(inValue) as sqrtValue insert into OutMediationStream; The function calculates the square-root value of the 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sqrt(4d) returns 2. tan (Function) This function returns the tan of the given value in radians. It wraps the java.lang.Math.tan() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:tan( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose tan value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tan(inValue) as tanValue insert into OutMediationStream; This function calculates the tan value of the 'inValue' given and directs the output to the output stream, 'OutMediationStream'. For example, tan(6d) returns -0.29100619138474915. tanh (Function) This function returns the hyperbolic tangent of the value given in radians. It wraps the java.lang.Math.tanh() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:tanh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic tangent value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tanh(inValue) as tanhValue insert into OutMediationStream; If the 'inVaue' in the input stream is given, this function calculates the hyperbolic tangent value of the same and directs the output to 'OutMediationStream' stream. For example, tanh(6d) returns 0.9999877116507956. toDegrees (Function) This function converts the value given in radians to degrees. It wraps the java.lang.Math.toDegrees() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:toDegrees( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in radians that should be converted to degrees. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toDegrees(inValue) as degreesValue insert into OutMediationStream; The function converts the 'inValue' in the input stream from radians to degrees and directs the output to 'OutMediationStream' output stream. For example, toDegrees(6d) returns 343.77467707849394. toRadians (Function) This function converts the value given in degrees to radians. It wraps the java.lang.Math.toRadians() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:toRadians( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in degrees that should be converted to radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toRadians(inValue) as radiansValue insert into OutMediationStream; This function converts the input, from degrees to radians and directs the result to 'OutMediationStream' output stream. For example, toRadians(6d) returns 0.10471975511965977. Rdbms cud (Stream Processor) This function performs SQL CUD (INSERT, UPDATE, DELETE) queries on WSO2 datasources. Note: This function is only available when running Siddhi with WSO2 SP. Origin: siddhi-store-rdbms:6.0.0 Syntax rdbms:cud( STRING datasource.name, STRING query, STRING parameter.n) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the WSO2 datasource for which the query should be performed. STRING No No query The update, delete, or insert query(formatted according to the relevant database type) that needs to be performed. STRING No No parameter.n If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING No No System Parameters Name Description Default Value Possible Parameters perform.CUD.operations If this parameter is set to 'true', the RDBMS CUD function is enabled to perform CUD operations. false true false Extra Return Attributes Name Description Possible Types numRecords The number of records manipulated by the query. INT Examples EXAMPLE 1 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName='abc' where customerName='xyz'\") select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. EXAMPLE 2 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName=? where customerName=?\", changedName, previousName) select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. Here the values of attributes changedName and previousName in the event will be set to the query. query (Stream Processor) This function performs SQL retrieval queries on WSO2 datasources. Note: This function is only available when running Siddhi with WSO2 SP. Origin: siddhi-store-rdbms:6.0.0 Syntax rdbms:query( STRING datasource.name, STRING query, STRING parameter.n, STRING attribute.definition.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the WSO2 datasource for which the query should be performed. STRING No No query The select query(formatted according to the relevant database type) that needs to be performed STRING No No parameter.n If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING No No attribute.definition.list This is provided as a comma-separated list in the ' AttributeName AttributeType ' format. The SQL query is expected to return the attributes in the given order. e.g., If one attribute is defined here, the SQL query should return one column result set. If more than one column is returned, then the first column is processed. The Siddhi data types supported are 'STRING', 'INT', 'LONG', 'DOUBLE', 'FLOAT', and 'BOOL'. Mapping of the Siddhi data type to the database data type can be done as follows, Siddhi Datatype - Datasource Datatype STRING - CHAR , VARCHAR , LONGVARCHAR INT - INTEGER LONG - BIGINT DOUBLE - DOUBLE FLOAT - REAL BOOL - BIT STRING No No Extra Return Attributes Name Description Possible Types attributeName The return attributes will be the ones defined in the parameter attribute.definition.list . STRING INT LONG DOUBLE FLOAT BOOL Examples EXAMPLE 1 from TriggerStream#rdbms:query('SAMPLE_DB', 'select * from Transactions_Table', 'creditcardno string, country string, transaction string, amount int') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). EXAMPLE 2 from TriggerStream#rdbms:query('SAMPLE_DB', 'select * from where country=? ', countrySearchWord, 'creditcardno string, country string, transaction string, amount int') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). countrySearchWord value from the event will be set in the query when querying the datasource. Regex find (Function) These methods attempt to find the subsequence of the 'inputSequence' that matches the given 'regex' pattern. Origin: siddhi-execution-regex:5.0.0 Syntax BOOL regex:find( STRING regex, STRING input.sequence, INT starting.index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression that is matched to a sequence in order to find the subsequence of the same. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No starting.index The starting index of the input sequence from where the input sequence ismatched with the given regex pattern. eg: 1, 2. INT No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string); from InputStream select inputSequence , regex:find(\\d\\d(.*)WSO2, 21 products are produced by WSO2 currently) as aboutWSO2 insert into OutputStream; This method attempts to find the subsequence of the 'inputSequence' that matches the regex pattern, \\d\\d(.*)WSO2. It returns true as a subsequence exists. EXAMPLE 2 define stream InputStream (inputSequence string, price long, regex string); from InputStream select inputSequence , regex:find(\\d\\d(.*)WSO2, 21 products are produced currently) as aboutWSO2 insert into OutputStream; This method attempts to find the subsequence of the 'inputSequence' that matches the regex pattern, \\d\\d(.*)WSO2 . It returns 'false' as a subsequence does not exist. EXAMPLE 3 define stream InputStream (inputSequence string, price long, regex string); from InputStream select inputSequence , regex:find(\\d\\d(.*)WSO2, 21 products are produced within 10 years by WSO2 currently by WSO2 employees, 30) as aboutWSO2 insert into OutputStream; This method attempts to find the subsequence of the 'inputSequence' that matches the regex pattern, \\d\\d(.*)WSO2 starting from index 30. It returns 'true' since a subsequence exists. group (Function) This method returns the input subsequence captured by the given group during the previous match operation. Origin: siddhi-execution-regex:5.0.0 Syntax STRING regex:group( STRING regex, STRING input.sequence, INT group.id) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No group.id The given group id of the regex expression. For example, 0, 1, 2, etc. INT No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:group(\\d\\d(.*)(WSO2.*), 21 products are produced within 10 years by WSO2 currently by WSO2 employees, 3) insert into OutputStream; This function returns 'WSO2 employees', the input subsequence captured within the given groupID, 3 after grouping the 'inputSequence' according to the regex pattern, \\d\\d(. )(WSO2. ). lookingAt (Function) This method attempts to match the 'inputSequence', from the beginning, against the 'regex' pattern. Origin: siddhi-execution-regex:5.0.0 Syntax BOOL regex:lookingAt( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:lookingAt(\\d\\d(.*)(WSO2.*), 21 products are produced by WSO2 currently in Sri Lanka) This method attempts to match the 'inputSequence' against the regex pattern, \\d\\d(. )(WSO2. ) from the beginning. Since it matches, the function returns 'true'. EXAMPLE 2 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:lookingAt(WSO2(.*)middleware(.*), sample test string and WSO2 is situated in trace and it's a middleware company) This method attempts to match the 'inputSequence' against the regex pattern, WSO2(. )middleware(. ) from the beginning. Since it does not match, the function returns false. matches (Function) This method attempts to match the entire 'inputSequence' against the 'regex' pattern. Origin: siddhi-execution-regex:5.0.0 Syntax BOOL regex:matches( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:matches(WSO2(.*)middleware(.*), WSO2 is situated in trace and its a middleware company) This method attempts to match the entire 'inputSequence' against WSO2(. )middleware(. ) regex pattern. Since it matches, it returns 'true'. EXAMPLE 2 define stream inputStream (inputSequence string, price long, regex string, group int); from inputStream select inputSequence, regex:matches(WSO2(.*)middleware, WSO2 is situated in trace and its a middleware company) This method attempts to match the entire 'inputSequence' against WSO2(.*)middleware regex pattern. Since it does not match, it returns 'false'. Sink email (Sink) The email sink uses the 'smtp' server to publish events via emails. The events can be published in 'text', 'xml' or 'json' formats. The user can define email sink parameters in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or in the stream definition. The email sink first checks the stream definition for parameters, and if they are no configured there, it checks the 'deployment.yaml' file. If the parameters are not configured in either place, default values are considered for optional parameters. If you need to configure server system parameters that are not provided as options in the stream definition, then those parameters need to be defined them in the 'deployment.yaml' file under 'email sink properties'. For more information about the SMTP server parameters, see https://javaee.github.io/javamail/SMTP-Transport. Further, some email accounts are required to enable the 'access to less secure apps' option. For gmail accounts, you can enable this option via https://myaccount.google.com/lesssecureapps. Origin: siddhi-io-email:2.0.1 Syntax @sink(type=\"email\", username=\" STRING \", address=\" STRING \", password=\" STRING \", host=\" STRING \", port=\" INT \", ssl.enable=\" BOOL \", auth=\" BOOL \", content.type=\" STRING \", subject=\" STRING \", to=\" STRING \", cc=\" STRING \", bcc=\" STRING \", attachments=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The username of the email account that is used to send emails. e.g., 'abc' is the username of the 'abc@gmail.com' account. STRING No No address The address of the email account that is used to send emails. STRING No No password The password of the email account. STRING No No host The host name of the SMTP server. e.g., 'smtp.gmail.com' is a host name for a gmail account. The default value 'smtp.gmail.com' is only valid if the email account is a gmail account. smtp.gmail.com STRING Yes No port The port that is used to create the connection. '465' the default value is only valid is SSL is enabled. INT Yes No ssl.enable This parameter specifies whether the connection should be established via a secure connection or not. The value can be either 'true' or 'false'. If it is 'true', then the connection is establish via the 493 port which is a secure connection. true BOOL Yes No auth This parameter specifies whether to use the 'AUTH' command when authenticating or not. If the parameter is set to 'true', an attempt is made to authenticate the user using the 'AUTH' command. true BOOL Yes No content.type The content type can be either 'text/plain' or 'text/html'. text/plain STRING Yes No subject The subject of the mail to be send. STRING No Yes to The address of the 'to' recipient. If there are more than one 'to' recipients, then all the required addresses can be given as a comma-separated list. STRING No Yes cc The address of the 'cc' recipient. If there are more than one 'cc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No bcc The address of the 'bcc' recipient. If there are more than one 'bcc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No attachments File paths of the files that need to be attached to the email. These paths should be absolute paths. They can be either directories or files . If the path is to a directory, all the files located at the first level (i.e., not within another sub directory) are attached. None STRING Yes Yes System Parameters Name Description Default Value Possible Parameters mail.smtp.ssl.trust If this parameter is se, and a socket factory has not been specified, it enables the use of a MailSSLSocketFactory. If this parameter is set to \" \", all the hosts are trusted. If it is set to a whitespace-separated list of hosts, only those specified hosts are trusted. If not, the hosts trusted depends on the certificate presented by the server. String mail.smtp.connectiontimeout The socket connection timeout value in milliseconds. infinite timeout Any Integer mail.smtp.timeout The socket I/O timeout value in milliseconds. infinite timeout Any Integer mail.smtp.from The email address to use for the SMTP MAIL command. This sets the envelope return address. Defaults to msg.getFrom() or InternetAddress.getLocalAddress(). Any valid email address mail.smtp.localport The local port number to bind to when creating the SMTP socket. Defaults to the port number picked by the Socket class. Any Integer mail.smtp.ehlo If this parameter is set to 'false', you must not attempt to sign in with the EHLO command. true true or false mail.smtp.auth.login.disable If this is set to 'true', it is not allowed to use the 'AUTH LOGIN' command. false true or false mail.smtp.auth.plain.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH PLAIN' command. false true or false mail.smtp.auth.digest-md5.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH DIGEST-MD5' command. false true or false mail.smtp.auth.ntlm.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH NTLM' command false true or false mail.smtp.auth.ntlm.domain The NTLM authentication domain. None The valid NTLM authentication domain name. mail.smtp.auth.ntlm.flags NTLM protocol-specific flags. For more details, see http://curl.haxx.se/rfc/ntlm.html#theNtlmFlags. None Valid NTLM protocol-specific flags. mail.smtp.dsn.notify The NOTIFY option to the RCPT command. None Either 'NEVER', or a combination of 'SUCCESS', 'FAILURE', and 'DELAY' (separated by commas). mail.smtp.dsn.ret The 'RET' option to the 'MAIL' command. None Either 'FULL' or 'HDRS'. mail.smtp.sendpartial If this parameter is set to 'true' and a message is addressed to both valid and invalid addresses, the message is sent with a log that reports the partial failure with a 'SendFailedException' error. If this parameter is set to 'false' (which is default), the message is not sent to any of the recipients when the recipient lists contain one or more invalid addresses. false true or false mail.smtp.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.smtp.sasl.mechanisms Enter a space or a comma-separated list of SASL mechanism names that the system shouldt try to use. None mail.smtp.sasl.authorizationid The authorization ID to be used in the SASL authentication. If no value is specified, the authentication ID (i.e., username) is used. username Valid ID mail.smtp.sasl.realm The realm to be used with the 'DIGEST-MD5' authentication. None mail.smtp.quitwait If this parameter is set to 'false', the 'QUIT' command is issued and the connection is immediately closed. If this parameter is set to 'true' (which is default), the transport waits for the response to the QUIT command. false true or false mail.smtp.reportsuccess If this parameter is set to 'true', the transport to includes an 'SMTPAddressSucceededException' for each address to which the message is successfully delivered. false true or false mail.smtp.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create SMTP sockets. None Socket Factory mail.smtp.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory interface'. This class is used to create SMTP sockets. None mail.smtp.socketFactory.fallback If this parameter is set to 'true', the failure to create a socket using the specified socket factory class causes the socket to be created using the 'java.net.Socket' class. true true or false mail.smtp.socketFactory.port This specifies the port to connect to when using the specified socket factory. 25 Valid port number mail.smtp.ssl.protocols This specifies the SSL protocols that need to be enabled for the SSL connections. None This parameter specifies a whitespace separated list of tokens that are acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. mail.smtp.starttls.enable If this parameter is set to 'true', it is possible to issue the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.smtp.starttls.required If this parameter is set to 'true', it is required to use the 'STARTTLS' command. If the server does not support the 'STARTTLS' command, or if the command fails, the connection method will fail. false true or false mail.smtp.socks.host This specifies the host name of a SOCKS5 proxy server to be used for the connections to the mail server. None mail.smtp.socks.port This specifies the port number for the SOCKS5 proxy server. This needs to be used only if the proxy server is not using the standard port number 1080. 1080 valid port number mail.smtp.auth.ntlm.disable If this parameter is set to 'true', the AUTH NTLM command cannot be issued. false true or false mail.smtp.mailextension The extension string to be appended to the MAIL command. None mail.smtp.userset If this parameter is set to 'true', you should use the 'RSET' command instead of the 'NOOP' command in the 'isConnected' method. In some scenarios, 'sendmail' responds slowly after many 'NOOP' commands. This is avoided by using 'RSET' instead. false true or false Examples EXAMPLE 1 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to publish events via an email sink based on the values provided for the mandatory parameters. As shown in the example, it publishes events from the 'FooStream' in 'json' format as emails to the specified 'to' recipients via the email sink. The email is sent from the 'sender.account@gmail.com' email address via a secure connection. EXAMPLE 2 @sink(type='email', @map(type ='json'), subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to configure the query parameters and the system parameters in the 'deployment.yaml' file. Corresponding parameters need to be configured under 'email', and namespace:'sink' as follows: siddhi: extensions: - extension: name:'email' namespace:'sink' properties: username: sender's email username address: sender's email address password: sender's email password As shown in the example, events from the FooStream are published in 'json' format via the email sink as emails to the given 'to' recipients. The email is sent from the 'sender.account@gmail.com' address via a secure connection. EXAMPLE 3 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.com)define stream FooStream (name string, age int, country string); This example illustrates how to publish events via the email sink. Events from the 'FooStream' stream are published in 'xml' format via the email sink as a text/html message and sent to the specified 'to', 'cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value of the 'name' parameter in the corresponding output event. EXAMPLE 4 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.comattachments= '{{attachments}}')define stream FooStream (name string, age int, country string, attachments string); This example illustrates how to publish events via the email sink. Here, the email also contains attachments. Events from the FooStream are published in 'xml' format via the email sink as a 'text/html' message to the specified 'to','cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value for the 'name' parameter in the corresponding output event. The attachments included in the email message are the local files available in the path specified as the value for the 'attachments' attribute. http (Sink) This extension publish the HTTP events in any HTTP method POST, GET, PUT, DELETE via HTTP or https protocols. As the additional features this component can provide basic authentication as well as user can publish events using custom client truststore files when publishing events via https protocol. And also user can add any number of headers including HTTP_METHOD header for each event dynamically. Following content types will be set by default according to the type of sink mapper used. You can override them by setting the new content types in headers. - TEXT : text/plain - XML : application/xml - JSON : application/json - KEYVALUE : application/x-www-form-urlencoded Origin: siddhi-io-http:2.0.4 Syntax @sink(type=\"http\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", client.enable.session.creation=\" STRING \", follow.redirect=\" BOOL \", max.redirect.count=\" INT \", tls.store.type=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configuration=\" STRING \", client.bootstrap.nodelay=\" BOOL \", client.bootstrap.keepalive=\" BOOL \", client.bootstrap.sendbuffersize=\" INT \", client.bootstrap.recievebuffersize=\" INT \", client.bootstrap.connect.timeout=\" INT \", client.bootstrap.socket.reuse=\" BOOL \", client.bootstrap.socket.timeout=\" STRING \", client.threadpool.configurations=\" STRING \", client.connection.pool.count=\" INT \", client.max.active.connections.per.pool=\" INT \", client.min.idle.connections.per.pool=\" INT \", client.max.idle.connections.per.pool=\" INT \", client.min.eviction.idle.time=\" STRING \", sender.thread.count=\" STRING \", event.group.executor.thread.size=\" STRING \", max.wait.for.client.connection.pool=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", refresh.token=\" STRING \", token.url=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published via HTTP. This is a mandatory parameter and if this is not specified, an error is logged in the CLI. If user wants to enable SSL for the events, use https instead of http in the publisher.url.e.g., http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No basic.auth.username The username to be included in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No basic.auth.password The password to include in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No https.truststore.file The file path to the location of the truststore of the client that sends the HTTP events through 'https' protocol. A custom client-truststore can be specified if required. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified and the protocol of URL is 'https' then, the system uses default password. wso2carbon STRING Yes No headers The headers that should be included as HTTP request headers. There can be any number of headers concatenated in following format. \"'header1:value1','header2:value2'\". User can include Content-Type header if he needs to use a specific content-type for the payload. Or else, system decides the Content-Type by considering the type of sink mapper, in following way. - @map(xml):application/xml - @map(json):application/json - @map(text):plain/text ) - if user does not include any mapping type then the system gets 'plain/text' as default Content-Type header. Note that providing content-length as a header is not supported. The size of the payload will be automatically calculated and included in the content-length header. STRING Yes No method For HTTP events, HTTP_METHOD header should be included as a request header. If the parameter is null then system uses 'POST' as a default header. POST STRING Yes No socket.idle.timeout Socket timeout value in millisecond 6000 INT Yes No chunk.disabled This parameter is used to disable/enable chunked transfer encoding false BOOL Yes No ssl.protocol The SSL protocol version TLS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No client.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No follow.redirect Redirect related enabled. true BOOL Yes No max.redirect.count Maximum redirect count. 5 INT Yes No tls.store.type TLS store type to be used. JKS STRING Yes No proxy.host Proxy server host null STRING Yes No proxy.port Proxy server port null STRING Yes No proxy.username Proxy server username null STRING Yes No proxy.password Proxy server password null STRING Yes No client.bootstrap.configuration Client bootsrap configurations. Expected format of these parameters is as follows: \"'client.bootstrap.nodelay:xxx','client.bootstrap.keepalive:xxx'\" TODO STRING Yes No client.bootstrap.nodelay Http client no delay. true BOOL Yes No client.bootstrap.keepalive Http client keep alive. true BOOL Yes No client.bootstrap.sendbuffersize Http client send buffer size. 1048576 INT Yes No client.bootstrap.recievebuffersize Http client receive buffer size. 1048576 INT Yes No client.bootstrap.connect.timeout Http client connection timeout. 15000 INT Yes No client.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No client.bootstrap.socket.timeout Http client socket timeout. 15 STRING Yes No client.threadpool.configurations Thread pool configuration. Expected format of these parameters is as follows: \"'client.connection.pool.count:xxx','client.max.active.connections.per.pool:xxx'\" TODO STRING Yes No client.connection.pool.count Connection pool count. 0 INT Yes No client.max.active.connections.per.pool Active connections per pool. -1 INT Yes No client.min.idle.connections.per.pool Minimum ideal connection per pool. 0 INT Yes No client.max.idle.connections.per.pool Maximum ideal connection per pool. 100 INT Yes No client.min.eviction.idle.time Minimum eviction idle time. 5 * 60 * 1000 STRING Yes No sender.thread.count Http sender thread count. 20 STRING Yes No event.group.executor.thread.size Event group executor thread size. 15 STRING Yes No max.wait.for.client.connection.pool Maximum wait for client connection pool. 60000 STRING Yes No oauth.username The username to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No oauth.password The password to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No consumer.key consumer key for the Http request. It is only applicable for for Oauth requests STRING Yes No consumer.secret consumer secret for the Http request. It is only applicable for for Oauth requests STRING Yes No refresh.token refresh token for the Http request. It is only applicable for for Oauth requests STRING Yes No token.url token url for generate a new access token. It is only applicable for for Oauth requests STRING Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapBossGroupSize property to configure number of boss threads, which accepts incoming connections until the ports are unbound. Once connection accepts successfully, boss thread passes the accepted channel to one of the worker threads. Number of available processors Any integer clientBootstrapWorkerGroupSize property to configure number of worker threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer clientBootstrapClientGroupSize property to configure number of client threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client-truststore.jks trustStorePassword The default truststore password. wso2carbon Truststore password Examples EXAMPLE 1 @sink(type='http',publisher.url='http://localhost:8009/foo', method='{{method}}',headers=\"'content-type:xml','content-length:94'\", client.bootstrap.configuration=\"'client.bootstrap.socket.timeout:20', 'client.bootstrap.worker.group.size:10'\", client.pool.configuration=\"'client.connection.pool.count:10','client.max.active.connections.per.pool:1'\", @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody String, method string, headers string); If it is xml mapping expected input should be in following format for FooStream: { events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events , POST, Content-Length:24#Content-Location:USA#Retry-After:120 } Above event will generate output as below. ~Output http event payload events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events ~Output http event headers Content-Length:24, Content-Location:'USA', Retry-After:120, Content-Type:'application/xml', HTTP_METHOD:'POST', ~Output http event properties HTTP_METHOD:'POST', HOST:'localhost', PORT:8009, PROTOCOL:'http', TO:'/foo' http-request (Sink) This extension publish the HTTP events in any HTTP method POST, GET, PUT, DELETE via HTTP or https protocols. As the additional features this component can provide basic authentication as well as user can publish events using custom client truststore files when publishing events via https protocol. And also user can add any number of headers including HTTP_METHOD header for each event dynamically. Following content types will be set by default according to the type of sink mapper used. You can override them by setting the new content types in headers. - TEXT : text/plain - XML : application/xml - JSON : application/json - KEYVALUE : application/x-www-form-urlencoded HTTP request sink is correlated with the The HTTP reponse source, through a unique sink.id .It sends the request to the defined url and the response is received by the response source which has the same 'sink.id'. Origin: siddhi-io-http:2.0.4 Syntax @sink(type=\"http-request\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", client.enable.session.creation=\" STRING \", follow.redirect=\" BOOL \", max.redirect.count=\" INT \", tls.store.type=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configuration=\" STRING \", client.bootstrap.nodelay=\" BOOL \", client.bootstrap.keepalive=\" BOOL \", client.bootstrap.sendbuffersize=\" INT \", client.bootstrap.recievebuffersize=\" INT \", client.bootstrap.connect.timeout=\" INT \", client.bootstrap.socket.reuse=\" BOOL \", client.bootstrap.socket.timeout=\" STRING \", client.threadpool.configurations=\" STRING \", client.connection.pool.count=\" INT \", client.max.active.connections.per.pool=\" INT \", client.min.idle.connections.per.pool=\" INT \", client.max.idle.connections.per.pool=\" INT \", client.min.eviction.idle.time=\" STRING \", sender.thread.count=\" STRING \", event.group.executor.thread.size=\" STRING \", max.wait.for.client.connection.pool=\" STRING \", sink.id=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", refresh.token=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published via HTTP. This is a mandatory parameter and if this is not specified, an error is logged in the CLI. If user wants to enable SSL for the events, use https instead of http in the publisher.url. e.g., http://localhost:8080/endpoint , https://localhost:8080/endpoint This can be used as a dynamic parameter as well. STRING No Yes basic.auth.username The username to be included in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No basic.auth.password The password to include in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No https.truststore.file The file path to the location of the truststore of the client that sends the HTTP events through 'https' protocol. A custom client-truststore can be specified if required. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified and the protocol of URL is 'https' then, the system uses default password. wso2carbon STRING Yes No headers The headers that should be included as HTTP request headers. There can be any number of headers concatenated in following format. \"'header1:value1','header2:value2'\". User can include Content-Type header if he needs to use a specific content-type for the payload. Or else, system decides the Content-Type by considering the type of sink mapper, in following way. - @map(xml):application/xml - @map(json):application/json - @map(text):plain/text ) - if user does not include any mapping type then the system gets 'plain/text' as default Content-Type header. Note that providing content-length as a header is not supported. The size of the payload will be automatically calculated and included in the content-length header. STRING Yes No method For HTTP events, HTTP_METHOD header should be included as a request header. If the parameter is null then system uses 'POST' as a default header. POST STRING Yes No socket.idle.timeout Socket timeout value in millisecond 6000 INT Yes No chunk.disabled port: Port number of the remote service false BOOL Yes No ssl.protocol The SSL protocol version TLS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No client.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No follow.redirect Redirect related enabled. true BOOL Yes No max.redirect.count Maximum redirect count. 5 INT Yes No tls.store.type TLS store type to be used. JKS STRING Yes No proxy.host Proxy server host null STRING Yes No proxy.port Proxy server port null STRING Yes No proxy.username Proxy server username null STRING Yes No proxy.password Proxy server password null STRING Yes No client.bootstrap.configuration Client bootsrap configurations. Expected format of these parameters is as follows: \"'client.bootstrap.nodelay:xxx','client.bootstrap.keepalive:xxx'\" TODO STRING Yes No client.bootstrap.nodelay Http client no delay. true BOOL Yes No client.bootstrap.keepalive Http client keep alive. true BOOL Yes No client.bootstrap.sendbuffersize Http client send buffer size. 1048576 INT Yes No client.bootstrap.recievebuffersize Http client receive buffer size. 1048576 INT Yes No client.bootstrap.connect.timeout Http client connection timeout. 15000 INT Yes No client.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No client.bootstrap.socket.timeout Http client socket timeout. 15 STRING Yes No client.threadpool.configurations Thread pool configuration. Expected format of these parameters is as follows: \"'client.connection.pool.count:xxx','client.max.active.connections.per.pool:xxx'\" TODO STRING Yes No client.connection.pool.count Connection pool count. 0 INT Yes No client.max.active.connections.per.pool Active connections per pool. -1 INT Yes No client.min.idle.connections.per.pool Minimum ideal connection per pool. 0 INT Yes No client.max.idle.connections.per.pool Maximum ideal connection per pool. 100 INT Yes No client.min.eviction.idle.time Minimum eviction idle time. 5 * 60 * 1000 STRING Yes No sender.thread.count Http sender thread count. 20 STRING Yes No event.group.executor.thread.size Event group executor thread size. 15 STRING Yes No max.wait.for.client.connection.pool Maximum wait for client connection pool. 60000 STRING Yes No sink.id Identifier of the sink. This is used to co-relate with the corresponding http-response source which needs to process the repose for the request sent by this sink. STRING No No downloading.enabled If this is set to 'true' then the response received by the response source will be written to a file. If downloading is enabled, the download.path parameter is mandatory. false BOOL Yes No download.path If downloading is enabled, the path of the file which is going to be downloaded should be specified using 'download.path' parameter. This should be an absolute path including the file name. null STRING Yes Yes oauth.username The username to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No oauth.password The password to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No consumer.key consumer key for the Http request. It is only applicable for for Oauth requests STRING Yes No consumer.secret consumer secret for the Http request. It is only applicable for for Oauth requests STRING Yes No refresh.token refresh token for the Http request. It is only applicable for for Oauth requests STRING Yes No Examples EXAMPLE 1 @sink(type='http-request', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody String, method string, headers string); @source(type='http-response', sink.id='foo', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', fileName='A[1]'))) define stream responseStream2xx(fileName string, headers string); @source(type='http-response', sink.id='foo', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream responseStream4xx(errorMsg string); In above example, the payload body for 'FooStream' will be in following format. { events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events , This message will sent as the body of a POST request with the content-type 'application/xml' to the endpoint defined as the 'publisher.url' and in order to process the responses for these requests, there should be a source of type 'http-response' defined with the same sink id 'foo' in the siddhi app. The responses with 2xx status codes will be received by the http-response source which has the http.status.code defined by the regex '2\\d+'. If the response has a 4xx status code, it will be received by the http-response source which has the http.status.code defined by the regex '4\\d+'. EXAMPLE 2 define stream FooStream (name String, id int, headers String, downloadPath string); @sink(type='http-request', downloading.enabled='true', download.path='{{downloadPath}}',publisher.url='http://localhost:8005/files', method='GET', headers='{{headers}}',sink.id='download-sink', @map(type='json')) define stream BarStream (name String, id int, headers String, downloadPath string); @source(type='http-response', sink.id='download-sink', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', fileName='A[1]'))) define stream responseStream2xx(fileName string, headers string); @source(type='http-response', sink.id='download-sink', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream responseStream4xx(errorMsg string); In above example, http-request sink will send a GET request to the publisher url and the requested file will be received as the response by a corresponding http-response source. If the http status code of the response is a successful one (2xx), it will be received by the http-response source which has the http.status.code '2\\d+' and downloaded as a local file. Then the event received to the responseStream2xx will have the headers included in the request and the downloaded file name. If the http status code of the response is a 4xx code, it will be received by the http-response source which has the http.status.code '4\\d+'. Then the event received to the responseStream4xx will have the response message body in text format. http-response (Sink) HTTP response sink is correlated with the The HTTP request source, through a unique source.id , and it send a response to the HTTP request source having the same source.id . The response message can be formatted in text , XML or JSON and can be sent with appropriate headers. Origin: siddhi-io-http:2.0.4 Syntax @sink(type=\"http-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier of the source. STRING No No message.id Identifier of the message. STRING No Yes headers The headers that should be included as HTTP response headers. There can be any number of headers concatenated on following format. \"'header1:value1','header2:value2'\" User can include content-type header if he/she need to have any specific type for payload. If not system get the mapping type as the content-Type header (ie. @map(xml) : application/xml , @map(json) : application/json , @map(text) : plain/text ) and if user does not include any mapping type then system gets the plain/text as default Content-Type header. If user does not include Content-Length header then system calculate the bytes size of payload and include it as content-length header. STRING Yes No Examples EXAMPLE 1 @sink(type='http-response', source.id='sampleSourceId', message.id='{{messageId}}', headers=\"'content-type:json','content-length:94'\"@map(type='json', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody String, messageId string, headers string); If it is json mapping expected input should be in following format for FooStream: { {\"events\": {\"event\": \"symbol\":WSO2, \"price\":55.6, \"volume\":100, } }, 0cf708b1-7eae-440b-a93e-e72f801b486a, Content-Length:24#Content-Location:USA } Above event will generate response for the matching source message as below. ~Output http event payload {\"events\": {\"event\": \"symbol\":WSO2, \"price\":55.6, \"volume\":100, } } ~Output http event headers Content-Length:24, Content-Location:'USA', Content-Type:'application/json' inMemory (Sink) In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Origin: siddhi-core:5.0.0 Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation. kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Origin: siddhi-io-kafka:5.0.0 Syntax @sink(type=\"kafka\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", sequence.id=\" STRING \", key=\" STRING \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0 th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Origin: siddhi-io-kafka:5.0.0 Syntax @sink(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", sequence.id=\" STRING \", key=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0 th ) partition of the brokers in two data centers log (Sink) This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Origin: siddhi-core:5.0.0 Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. nats (Sink) NATS Sink allows users to subscribe to a NATS broker and publish messages. Origin: siddhi-io-nats:2.0.1 Syntax @sink(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS sink should publish to. STRING No Yes bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client publishing/connecting to the NATS broker. Should be unique for each client connecting to the server/cluster. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No Examples EXAMPLE 1 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with all supporting configurations. With the following configuration the sink identified as 'nats-client' will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. EXAMPLE 2 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with mandatory configurations. With the following configuration the sink identified with an auto generated client id will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. tcp (Sink) A Siddhi application can be configured to publish events via the TCP transport by adding the @Sink(type = 'tcp') annotation at the top of an event stream definition. Origin: siddhi-io-tcp:3.0.1 Syntax @sink(type=\"tcp\", url=\" STRING \", sync=\" STRING \", tcp.no.delay=\" BOOL \", keep.alive=\" BOOL \", worker.threads=\" INT|LONG \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The URL to which outgoing events should be published via TCP. The URL should adhere to tcp:// host : port / context format. STRING No No sync This parameter defines whether the events should be published in a synchronized manner or not. If sync = 'true', then the worker will wait for the ack after sending the message. Else it will not wait for an ack. false STRING Yes Yes tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true BOOL Yes No keep.alive This property defines whether the server should be kept alive when there are no connections available. true BOOL Yes No worker.threads Number of threads to publish events. 10 INT LONG Yes No Examples EXAMPLE 1 @Sink(type = 'tcp', url='tcp://localhost:8080/abc', sync='true' @map(type='binary')) define stream Foo (attribute1 string, attribute2 int); A sink of type 'tcp' has been defined. All events arriving at Foo stream via TCP transport will be sent to the url tcp://localhost:8080/abc in a synchronous manner. Sinkmapper binary (Sink Mapper) This section explains how to map events processed via Siddhi in order to publish them in the binary format. Origin: siddhi-map-binary:2.0.0 Syntax @sink(..., @map(type=\"binary\") Examples EXAMPLE 1 @sink(type='inMemory', topic='WSO2', @map(type='binary')) define stream FooStream (symbol string, price float, volume long); This will publish Siddhi event in binary format. csv (Sink Mapper) This output mapper extension allows you to convert Siddhi events processed by the WSO2 SP to CSV message before publishing them. You can either use custom placeholder to map a custom CSV message or use pre-defined CSV format where event conversion takes place without extra configurations. Origin: siddhi-map-csv:2.0.0 Syntax @sink(..., @map(type=\"csv\", delimiter=\" STRING \", header=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter This parameter used to separate the output CSV data, when converting a Siddhi event to CSV format, , STRING Yes No header This parameter specifies whether the CSV messages will be generated with header or not. If this parameter is set to true, message will be generated with header false BOOL Yes No event.grouping.enabled If this parameter is set to true , events are grouped via a line.separator when multiple events are received. It is required to specify a value for the System.lineSeparator() when the value for this parameter is true . false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv')) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a default CSV output mapping, which will generate output as follows: WSO2,55.6,100 OS supported line separator If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume OS supported line separator WSO2-55.6-100 OS supported line separator EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv',header='true',delimiter='-',@payload(symbol='0',price='2',volume='1')))define stream BarStream (symbol string, price float,volume long); Above configuration will perform a custom CSV mapping. Here, user can add custom place order in the @payload. The place order indicates that where the attribute name's value will be appear in the output message, The output will be produced output as follows: WSO2,100,55.6 If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume WSO2-55.6-100 OS supported line separator If event grouping is enabled, then the output is as follows: WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator json (Sink Mapper) This extension is an Event to JSON output mapper. Transports that publish messages can utilize this extension to convert Siddhi events to JSON messages. You can either send a pre-defined JSON format or a custom JSON message. Origin: siddhi-map-json:5.0.1 Syntax @sink(..., @map(type=\"json\", validate.json=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.json If this property is set to true , it enables JSON validation for the JSON messages generated. When validation is carried out, messages that do not adhere to proper JSON standards are dropped. This property is set to 'false' by default. false BOOL Yes No enclosing.element This specifies the enclosing element to be used if multiple events are sent in the same JSON message. Siddhi treats the child elements of the given enclosing element as events and executes JSON expressions on them. If an enclosing.element is not provided, the multiple event scenario is disregarded and JSON path is evaluated based on the root element. $ STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Above configuration does a default JSON input mapping that generates the output given below. { \"event\":{ \"symbol\":WSO2, \"price\":55.6, \"volume\":100 } } EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='json', enclosing.element='$.portfolio', validate.json='true', @payload( \"\"\"{\"StockData\":{\"Symbol\":\"{{symbol}}\",\"Price\":{{price}}}\"\"\"))) define stream BarStream (symbol string, price float, volume long); The above configuration performs a custom JSON mapping that generates the following JSON message as the output. {\"portfolio\":{ \"StockData\":{ \"Symbol\":WSO2, \"Price\":55.6 } } } keyvalue (Sink Mapper) The Event to Key-Value Map output mapper extension allows you to convert Siddhi events processed by WSO2 SP to key-value map events before publishing them. You can either use pre-defined keys where conversion takes place without extra configurations, or use custom keys with which the messages can be published. Origin: siddhi-map-keyvalue:2.0.0 Syntax @sink(..., @map(type=\"keyvalue\") Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default Key-Value output mapping. The expected output is something similar to the following:symbol:'WSO2' price : 55.6f volume: 100L EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='symbol',b='price',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where values are passed as objects. Values for symbol , price , and volume attributes are published with the keys a , b and c respectively. The expected output is a map similar to the following: a:'WSO2' b : 55.6f c: 100L EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='{{symbol}} is here',b='`price`',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where the values of the a and b attributes are strings and c is object. The expected output should be a Map similar to the following 'WSO2 is here' b : 'price' c: 100L passThrough (Sink Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.0.0 Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. text (Sink Mapper) This extension is a Event to Text output mapper. Transports that publish text messages can utilize this extension to convert the Siddhi events to text messages. Users can use a pre-defined text format where event conversion is carried out without any additional configurations, or use custom placeholder(using {{ and }} or {{{ and }}} ) to map custom text messages. All variables are HTML escaped by default. For example: & is replaced with amp; \" is replaced with quot; = is replaced with #61; If you want to return unescaped HTML, use the triple mustache {{{ instead of double {{ . Origin: siddhi-map-text:2.0.0 Syntax @sink(..., @map(type=\"text\", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.grouping.enabled If this parameter is set to true , events are grouped via a delimiter when multiple events are received. It is required to specify a value for the delimiter parameter when the value for this parameter is true . false BOOL Yes No delimiter This parameter specifies how events are separated when a grouped event is received. This must be a whole line and not a single character. ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n whereas Windows uses \\r\\n as the end of line character. \\n STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping with event grouping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{symbol}}/{{volume}}, SensorPrice : Rs{{price}}/=, Value : {{volume}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected output is as follows: SensorID : wso2/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {wso2,1000,100} EXAMPLE 4 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true', @payload(\"Stock price of {{symbol}} is {{price}}\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping with event grouping. The expected output is as follows: Stock price of WSO2 is 55.6 ~ Stock price of WSO2 is 55.6 ~ Stock price of WSO2 is 55.6 for the following siddhi event. {WSO2,55.6,10} EXAMPLE 5 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{{symbol}}}/{{{volume}}}, SensorPrice : Rs{{{price}}}/=, Value : {{{volume}}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping to return unescaped HTML. The expected output is as follows: SensorID : a b/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {a b,1000,100} xml (Sink Mapper) This mapper converts Siddhi output events to XML before they are published via transports that publish in XML format. Users can either send a pre-defined XML format or a custom XML message containing event data. Origin: siddhi-map-xml:5.0.0 Syntax @sink(..., @map(type=\"xml\", validate.xml=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.xml This parameter specifies whether the XML messages generated should be validated or not. If this parameter is set to true, messages that do not adhere to proper XML standards are dropped. false BOOL Yes No enclosing.element When an enclosing element is specified, the child elements (e.g., the immediate child elements) of that element are considered as events. This is useful when you need to send multiple events in a single XML message. When an enclosing element is not specified, one XML message per every event will be emitted without enclosing. None in custom mapping and events in default mapping STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping which will generate below output events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='xml', enclosing.element=' portfolio ', validate.xml='true', @payload( \" StockData Symbol {{symbol}} /Symbol Price {{price}} /Price /StockData \"))) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. Inside @payload you can specify the custom template that you want to send the messages out and addd placeholders to places where you need to add event attributes.Above config will produce below output XML message portfolio StockData Symbol WSO2 /Symbol Price 55.6 /Price /StockData /portfolio Source cdc (Source) The CDC source receives events when change events (i.e., INSERT, UPDATE, DELETE) are triggered for a database table. Events are received in the 'key-value' format. The key values of the map of a CDC change event are as follows. For insert: Keys are specified as columns of the table. For delete: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For update: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For 'polling' mode: Keys are specified as the coloumns of the table. See parameter: mode for supported databases and change events. Origin: siddhi-io-cdc:2.0.0 Syntax @source(type=\"cdc\", url=\" STRING \", mode=\" STRING \", jdbc.driver.name=\" STRING \", username=\" STRING \", password=\" STRING \", pool.properties=\" STRING \", datasource.name=\" STRING \", table.name=\" STRING \", polling.column=\" STRING \", polling.interval=\" INT \", operation=\" STRING \", connector.properties=\" STRING \", database.server.id=\" STRING \", database.server.name=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The connection URL to the database. F=The format used is: 'jdbc:mysql:// host : port / database_name ' STRING No No mode Mode to capture the change data. The type of events that can be received, and the required parameters differ based on the mode. The mode can be one of the following: 'polling': This mode uses a column named 'polling.column' to monitor the given table. It captures change events of the 'RDBMS', 'INSERT, and 'UPDATE' types. 'listening': This mode uses logs to monitor the given table. It currently supports change events only of the 'MySQL', 'INSERT', 'UPDATE', and 'DELETE' types. listening STRING Yes No jdbc.driver.name The driver class name for connecting the database. It is required to specify a value for this parameter when the mode is 'polling'. STRING Yes No username The username to be used for accessing the database. This user needs to have the 'SELECT', 'RELOAD', 'SHOW DATABASES', 'REPLICATION SLAVE', and 'REPLICATION CLIENT'privileges for the change data capturing table (specified via the 'table.name' parameter). To operate in the polling mode, the user needs 'SELECT' privileges. STRING No No password The password of the username you specified for accessing the database. STRING No No pool.properties The pool parameters for the database connection can be specified as key-value pairs. STRING Yes No datasource.name Name of the wso2 datasource to connect to the database. When datasource name is provided, the URL, username and password are not needed. A datasource based connection is given more priority over the URL based connection. This parameter is applicable only when the mode is set to 'polling', and it can be applied only when you use this extension with WSO2 Stream Processor. STRING Yes No table.name The name of the table that needs to be monitored for data changes. STRING No No polling.column The column name that is polled to capture the change data. It is recommended to have a TIMESTAMP field as the 'polling.column' in order to capture the inserts and updates. Numeric auto-incremental fields and char fields can also be used as 'polling.column'. However, note that fields of these types only support insert change capturing, and the possibility of using a char field also depends on how the data is input. It is required to enter a value for this parameter when the mode is 'polling'. STRING Yes No polling.interval The time interval (specified in seconds) to poll the given table for changes. This parameter is applicable only when the mode is set to 'polling'. 1 INT Yes No operation The change event operation you want to carry out. Possible values are 'insert', 'update' or 'delete'. It is required to specify a value when the mode is 'listening'. This parameter is not case sensitive. STRING No No connector.properties Here, you can specify Debezium connector properties as a comma-separated string. The properties specified here are given more priority over the parameters. This parameter is applicable only for the 'listening' mode. Empty_String STRING Yes No database.server.id An ID to be used when joining MySQL database cluster to read the bin log. This should be a unique integer between 1 to 2^32. This parameter is applicable only when the mode is 'listening'. Random integer between 5400 and 6400 STRING Yes No database.server.name A logical name that identifies and provides a namespace for the database server. This parameter is applicable only when the mode is 'listening'. {host}_{port} STRING Yes No Examples EXAMPLE 1 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'insert', @map(type='keyvalue', @attributes(id = 'id', name = 'name'))) define stream inputStream (id string, name string); In this example, the CDC source listens to the row insertions that are made in the 'students' table with the column name, and the ID. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 2 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'update', @map(type='keyvalue', @attributes(id = 'id', name = 'name', before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, id string, before_name string , name string); In this example, the CDC source listens to the row updates that are made in the 'students' table. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 3 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'delete', @map(type='keyvalue', @attributes(before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, before_name string); In this example, the CDC source listens to the row deletions made in the 'students' table. This table belongs to the 'SimpleDB' database that can be accessed via the given URL. EXAMPLE 4 @source(type = 'cdc', mode='polling', polling.column = 'id', jdbc.driver.name = 'com.mysql.jdbc.Driver', url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. 'id' that is specified as the polling colum' is an auto incremental field. The connection to the database is made via the URL, username, password, and the JDBC driver name. EXAMPLE 5 @source(type = 'cdc', mode='polling', polling.column = 'id', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The given polling column is a char column with the 'S001, S002, ... .' pattern. The connection to the database is made via a data source named 'SimpleDB'. Note that the 'datasource.name' parameter works only with the Stream Processor. EXAMPLE 6 @source(type = 'cdc', mode='polling', polling.column = 'last_updated', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue')) define stream inputStream (name string); In this example, the CDC source polls the 'students' table for inserts and updates. The polling column is a timestamp field. email (Source) The 'Email' source allows you to receive events via emails. An 'Email' source can be configured using the 'imap' or 'pop3' server to receive events. This allows you to filter the messages that satisfy the criteria specified under the 'search term' option. The email source parameters can be defined in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or the stream definition. If the parameter configurations are not available in either place, the default values are considered (i.e., if default values are available). If you need to configure server system parameters that are not provided as options in the stream definition, they need to be defined in the 'deployment yaml' file under 'email source properties'. For more information about 'imap' and 'pop3' server system parameters, see the following. [JavaMail Reference Implementation - IMAP Store](https://javaee.github.io/javamail/IMAP-Store) [JavaMail Reference Implementation - POP3 Store Store](https://javaee.github.io/javamail/POP3-Store) Origin: siddhi-io-email:2.0.1 Syntax @source(type=\"email\", username=\" STRING \", password=\" STRING \", store=\" STRING \", host=\" STRING \", port=\" INT \", folder=\" STRING \", search.term=\" STRING \", polling.interval=\" LONG \", action.after.processed=\" STRING \", folder.to.move=\" STRING \", content.type=\" STRING \", ssl.enable=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The user name of the email account. e.g., 'wso2mail' is the username of the 'wso2mail@gmail.com' mail account. STRING No No password The password of the email account STRING No No store The store type that used to receive emails. Possible values are 'imap' and 'pop3'. imap STRING Yes No host The host name of the server (e.g., 'imap.gmail.com' is the host name for a gmail account with an IMAP store.). The default value 'imap.gmail.com' is only valid if the email account is a gmail account with IMAP enabled. If store type is 'imap', then the default value is 'imap.gmail.com'. If the store type is 'pop3', then thedefault value is 'pop3.gmail.com'. STRING Yes No port The port that is used to create the connection. '993', the default value is valid only if the store is 'imap' and ssl-enabled. INT Yes No folder The name of the folder to which the emails should be fetched. INBOX STRING Yes No search.term The option that includes conditions such as key-value pairs to search for emails. In a string search term, the key and the value should be separated by a semicolon (';'). Each key-value pair must be within inverted commas (' '). The string search term can define multiple comma-separated key-value pairs. This string search term currently supports only the 'subject', 'from', 'to', 'bcc', and 'cc' keys. e.g., if you enter 'subject:DAS, from:carbon, bcc:wso2', the search term creates a search term instance that filters emails that contain 'DAS' in the subject, 'carbon' in the 'from' address, and 'wso2' in one of the 'bcc' addresses. The string search term carries out sub string matching that is case-sensitive. If '@' in included in the value for any key other than the 'subject' key, it checks for an address that is equal to the value given. e.g., If you search for 'abc@', the string search terms looks for an address that contains 'abc' before the '@' symbol. None STRING Yes No polling.interval This defines the time interval in seconds at which th email source should poll the account to check for new mail arrivals.in seconds. 600 LONG Yes No action.after.processed The action to be performed by the email source for the processed mail. Possible values are as follows: 'FLAGGED': Sets the flag as 'flagged'. 'SEEN': Sets the flag as 'read'. 'ANSWERED': Sets the flag as 'answered'. 'DELETE': Deletes tha mail after the polling cycle. 'MOVE': Moves the mail to the folder specified in the 'folder.to.move' parameter. If the folder specified is 'pop3', then the only option available is 'DELETE'. NONE STRING Yes No folder.to.move The name of the folder to which the mail must be moved once it is processed. If the action after processing is 'MOVE', it is required to specify a value for this parameter. STRING No No content.type The content type of the email. It can be either 'text/plain' or 'text/html.' text/plain STRING Yes No ssl.enable If this is set to 'true', a secure port is used to establish the connection. The possible values are 'true' and 'false'. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters mail.imap.partialfetch This determines whether the IMAP partial-fetch capability should be used. true true or false mail.imap.fetchsize The partial fetch size in bytes. 16K value in bytes mail.imap.peek If this is set to 'true', the IMAP PEEK option should be used when fetching body parts to avoid setting the 'SEEN' flag on messages. The default value is 'false'. This can be overridden on a per-message basis by the 'setPeek method' in 'IMAPMessage'. false true or false mail.imap.connectiontimeout The socket connection timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.timeout The socket read timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.writetimeout The socket write timeout value in milliseconds. This timeout is implemented by using a 'java.util.concurrent.ScheduledExecutorService' per connection that schedules a thread to close the socket if the timeout period elapses. Therefore, the overhead of using this timeout is one thread per connection. infinity timeout Any Integer value mail.imap.statuscachetimeout The timeout value in milliseconds for the cache of 'STATUS' command response. 1000ms Time out in miliseconds mail.imap.appendbuffersize The maximum size of a message to buffer in memory when appending to an IMAP folder. None Any Integer value mail.imap.connectionpoolsize The maximum number of available connections in the connection pool. 1 Any Integer value mail.imap.connectionpooltimeout The timeout value in milliseconds for connection pool connections. 45000ms Any Integer mail.imap.separatestoreconnection If this parameter is set to 'true', it indicates that a dedicated store connection needs to be used for store commands. true true or false mail.imap.auth.login.disable If this is set to 'true', it is not possible to use the non-standard 'AUTHENTICATE LOGIN' command instead of the plain 'LOGIN' command. false true or false mail.imap.auth.plain.disable If this is set to 'true', the 'AUTHENTICATE PLAIN' command cannot be used. false true or false mail.imap.auth.ntlm.disable If true, prevents use of the AUTHENTICATE NTLM command. false true or false mail.imap.proxyauth.user If the server supports the PROXYAUTH extension, this property specifies the name of the user to act as. Authentication to log in to the server is carried out using the administrator's credentials. After authentication, the IMAP provider issues the 'PROXYAUTH' command with the user name specified in this property. None Valid string value mail.imap.localaddress The local address (host name) to bind to when creating the IMAP socket. Defaults to the address picked by the Socket class. Valid string value mail.imap.localport The local port number to bind to when creating the IMAP socket. Defaults to the port number picked by the Socket class. Valid String value mail.imap.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.imap.sasl.mechanisms A list of SASL mechanism names that the system should to try to use. The names can be separated by spaces or commas. None Valid string value mail.imap.sasl.authorizationid The authorization ID to use in the SASL authentication. If this parameter is not set, the authentication ID (username) is used. Valid string value mail.imap.sasl.realm The realm to use with SASL authentication mechanisms that require a realm, such as 'DIGEST-MD5'. None Valid string value mail.imap.auth.ntlm.domain The NTLM authentication domain. None Valid string value The NTLM authentication domain. NTLM protocol-specific flags. None Valid integer value mail.imap.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create IMAP sockets. None Valid SocketFactory mail.imap.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create IMAP sockets. None Valid string mail.imap.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. true true or false mail.imap.socketFactory.port This specifies the port to connect to when using the specified socket factory. If this parameter is not set, the default port is used. 143 Valid Integer mail.imap.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by RFC 2595. false true or false mail.imap.ssl.trust If this parameter is set and a socket factory has not been specified, it enables the use of a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If this parameter specifies list of hosts separated by white spaces, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.imap.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class this class is used to create IMAP SSL sockets. None SSL Socket Factory mail.imap.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create IMAP SSL sockets. None Valid String mail.imap.ssl.socketFactory.port This specifies the port to connect to when using the specified socket factory. the default port 993 is used. valid port number mail.imap.ssl.protocols This specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid string mail.imap.starttls.enable If this parameter is set to 'true', it is possible to use the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.imap.socks.host This specifies the host name of a 'SOCKS5' proxy server that is used to connect to the mail server. None Valid String mail.imap.socks.port This specifies the port number for the 'SOCKS5' proxy server. This is needed if the proxy server is not using the standard port number 1080. 1080 Valid String mail.imap.minidletime This property sets the delay in milliseconds. 10 milliseconds time in seconds (Integer) mail.imap.enableimapevents If this property is set to 'true', it enables special IMAP-specific events to be delivered to the 'ConnectionListener' of the store. The unsolicited responses received during the idle method of the store are sent as connection events with 'IMAPStore.RESPONSE' as the type. The event's message is the raw IMAP response string. false true or false mail.imap.folder.class The class name of a subclass of 'com.sun.mail.imap.IMAPFolder'. The subclass can be used to provide support for additional IMAP commands. The subclass must have public constructors of the form 'public MyIMAPFolder'(String fullName, char separator, IMAPStore store, Boolean isNamespace) and public 'MyIMAPFolder'(ListInfo li, IMAPStore store) None Valid String mail.pop3.connectiontimeout The socket connection timeout value in milliseconds. Infinite timeout Integer value mail.pop3.timeout The socket I/O timeout value in milliseconds. Infinite timeout Integer value mail.pop3.message.class The class name of a subclass of 'com.sun.mail.pop3.POP3Message'. None Valid String mail.pop3.localaddress The local address (host name) to bind to when creating the POP3 socket. Defaults to the address picked by the Socket class. Valid String mail.pop3.localport The local port number to bind to when creating the POP3 socket. Defaults to the port number picked by the Socket class. Valid port number mail.pop3.apop.enable If this parameter is set to 'true', use 'APOP' instead of 'USER/PASS' to log in to the 'POP3' server (if the 'POP3' server supports 'APOP'). APOP sends a digest of the password instead of clearing the text password. false true or false mail.pop3.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create 'POP3' sockets. None Socket Factory mail.pop3.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create 'POP3' sockets. None Valid String mail.pop3.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. false true or false mail.pop3.socketFactory.port This specifies the port to connect to when using the specified socket factory. Default port Valid port number mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', check the server identity as specified by RFC 2595. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3' SSL sockets. None SSL Socket Factory mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by 'RFC 2595'. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to '*', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. Trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3 SSL' sockets. None SSL Socket Factory mail.pop3.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create 'POP3 SSL' sockets. None Valid String mail.pop3.ssl.socketFactory.p This parameter pecifies the port to connect to when using the specified socket factory. 995 Valid Integer mail.pop3.ssl.protocols This parameter specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid String mail.pop3.starttls.enable If this parameter is set to 'true', it is possible to use the 'STLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.pop3.starttls.required If this parameter is set to 'true', it is required to use the 'STLS' command. The connect method fails if the server does not support the 'STLS' command or if the command fails. false true or false mail.pop3.socks.host This parameter specifies the host name of a 'SOCKS5' proxy server that can be used to connect to the mail server. None Valid String mail.pop3.socks.port This parameter specifies the port number for the 'SOCKS5' proxy server. None Valid String mail.pop3.disabletop If this parameter is set to 'true', the 'POP3 TOP' command is not used to fetch message headers. false true or false mail.pop3.forgettopheaders If this parameter is set to 'true', the headers that might have been retrieved using the 'POP3 TOP' command is forgotten and replaced by the headers retrieved when the 'POP3 RETR' command is executed. false true or false mail.pop3.filecache.enable If this parameter is set to 'true', the 'POP3' provider caches message data in a temporary file instead of caching them in memory. Messages are only added to the cache when accessing the message content. Message headers are always cached in memory (on demand). The file cache is removed when the folder is closed or the JVM terminates. false true or false mail.pop3.filecache.dir If the file cache is enabled, this property is used to override the default directory used by the JDK for temporary files. None Valid String mail.pop3.cachewriteto This parameter controls the behavior of the 'writeTo' method on a 'POP3' message object. If the parameter is set to 'true', the message content has not been cached yet, and the 'ignoreList' is null, the message is cached before being written. If not, the message is streamed directly to the output stream without being cached. false true or false mail.pop3.keepmessagecontent If this property is set to 'true', a hard reference to the cached content is retained, preventing the memory from being reused until the folder is closed, or until the cached content is explicitly invalidated (using the 'invalidate' method). false true or false Examples EXAMPLE 1 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. In this example, only the required parameters are defined in the stream definition. The default values are taken for the other parameters. The search term is not defined, and therefore, all the new messages in the inbox folder are polled and taken. EXAMPLE 2 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',store = 'imap',host = 'imap.gmail.com',port = '993',searchTerm = 'subject:Stream Processor, from: from.account@ , cc: cc.account',polling.interval='500',action.after.processed='DELETE',content.type='text/html,)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. The email source polls the mail account every 500 seconds to check whether any new mails have arrived. It processes new mails only if they satisfy the conditions specified for the email search term (the value for 'from' of the email message should be 'from.account@. host name ', and the message should contain 'cc.account' in the cc receipient list and the word 'Stream Processor' in the mail subject). in this example, the action after processing is 'DELETE'. Therefore,after processing the event, corresponding mail is deleted from the mail folder. http (Source) The HTTP source receives POST requests via HTTP or HTTPS in format such as text , XML and JSON . In WSO2 SP, if required, you can enable basic authentication to ensure that events are received only from users who are authorized to access the service. Origin: siddhi-io-http:2.0.4 Syntax @source(type=\"http\", receiver.url=\" STRING \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", server.enable.session.creation=\" STRING \", server.supported.snimatchers=\" STRING \", server.suported.server.names=\" STRING \", request.size.validation.configuration=\" STRING \", request.size.validation=\" STRING \", request.size.validation.maximum.value=\" STRING \", request.size.validation.reject.status.code=\" STRING \", request.size.validation.reject.message=\" STRING \", request.size.validation.reject.message.content.type=\" STRING \", header.size.validation=\" STRING \", header.validation.maximum.request.line=\" STRING \", header.validation.maximum.size=\" STRING \", header.validation.maximum.chunk.size=\" STRING \", header.validation.reject.status.code=\" STRING \", header.validation.reject.message=\" STRING \", header.validation.reject.message.content.type=\" STRING \", server.bootstrap.configuration=\" OBJECT \", server.bootstrap.nodelay=\" BOOL \", server.bootstrap.keepalive=\" BOOL \", server.bootstrap.sendbuffersize=\" INT \", server.bootstrap.recievebuffersize=\" INT \", server.bootstrap.connect.timeout=\" INT \", server.bootstrap.socket.reuse=\" BOOL \", server.bootstrap.socket.timeout=\" BOOL \", server.bootstrap.socket.backlog=\" BOOL \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL to which the events should be received. User can provide any valid url and if the url is not provided the system will use the following format http://0.0.0.0:9763/ appNAme / streamName If the user want to use SSL the url should be given in following format https://localhost:8080/ streamName http://0.0.0.0:9763/ / STRING Yes No basic.auth.enabled This works only in WSO2 SP. If this is set to true , basic authentication is enabled for incoming events, and the credentials with which each event is sent are verified to ensure that the user is authorized to access the service. If basic authentication fails, the event is not authenticated and an authentication error is logged in the CLI. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. The value is 1 by default. This will ensure that the events are directed to the event stream in the same order in which they arrive. By increasing this value the performance might increase at the cost of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection. 120000 INT Yes No ssl.verify.client The type of client certificate verification. null STRING Yes No ssl.protocol ssl/tls related options TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No server.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No server.supported.snimatchers Http SNIMatcher to be added. This parameter should include under parameters Ex: 'server.supported.snimatchers:SNIMatcher' null STRING Yes No server.suported.server.names Http supported servers. This parameter should include under parameters Ex: 'server.suported.server.names:server' null STRING Yes No request.size.validation.configuration Parameters that responsible for validating the http request and request headers. Expected format of these parameters is as follows: \"'request.size.validation:xxx','request.size.validation.maximum.value:xxx'\" null STRING Yes No request.size.validation To enable the request size validation. false STRING Yes No request.size.validation.maximum.value If request size is validated then maximum size. Integer.MAX_VALUE STRING Yes No request.size.validation.reject.status.code If request is exceed maximum size and request.size.validation is enabled then status code to be send as response. 401 STRING Yes No request.size.validation.reject.message If request is exceed maximum size and request.size.validation is enabled then status message to be send as response. Message is bigger than the valid size STRING Yes No request.size.validation.reject.message.content.type If request is exceed maximum size and request.size.validation is enabled then content type to be send as response. plain/text STRING Yes No header.size.validation To enable the header size validation. false STRING Yes No header.validation.maximum.request.line If header header validation is enabled then the maximum request line. 4096 STRING Yes No header.validation.maximum.size If header header validation is enabled then the maximum expected header size. 8192 STRING Yes No header.validation.maximum.chunk.size If header header validation is enabled then the maximum expected chunk size. 8192 STRING Yes No header.validation.reject.status.code 401 If header is exceed maximum size and header.size.validation is enabled then status code to be send as response. STRING Yes No header.validation.reject.message If header is exceed maximum size and header.size.validation is enabled then message to be send as response. Message header is bigger than the valid size STRING Yes No header.validation.reject.message.content.type If header is exceed maximum size and header.size.validation is enabled then content type to be send as response. plain/text STRING Yes No server.bootstrap.configuration Parameters that for bootstrap configurations of the server. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null OBJECT Yes No server.bootstrap.nodelay Http server no delay. true BOOL Yes No server.bootstrap.keepalive Http server keep alive. true BOOL Yes No server.bootstrap.sendbuffersize Http server send buffer size. 1048576 INT Yes No server.bootstrap.recievebuffersize Http server receive buffer size. 1048576 INT Yes No server.bootstrap.connect.timeout Http server connection timeout. 15000 INT Yes No server.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No server.bootstrap.socket.timeout Http server socket timeout. 15 BOOL Yes No server.bootstrap.socket.backlog THttp server socket backlog. 100 BOOL Yes No trace.log.enabled Http traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize property to configure number of boss threads, which accepts incoming connections until the ports are unbound. Once connection accepts successfully, boss thread passes the accepted channel to one of the worker threads. Number of available processors Any integer serverBootstrapWorkerGroupSize property to configure number of worker threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer serverBootstrapClientGroupSize property to configure number of client threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultHttpPort The default port if the default scheme is 'http'. 8280 Any valid port defaultHttpsPort The default port if the default scheme is 'https'. 8243 Any valid port defaultScheme The default protocol. http http https keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to wso2carbon.jks file keyStorePassword The default keystore password. wso2carbon String of keystore password Examples EXAMPLE 1 @source(type='http', receiver.url='http://localhost:9055/endpoints/RecPro', socketIdleTimeout='150000', parameters=\"'ciphers : TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256', 'sslEnabledProtocols:TLSv1.1,TLSv1.2'\",request.size.validation.configuration=\"request.size.validation:true\",server.bootstrap.configuration=\"server.bootstrap.socket.timeout:25\" @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above source listenerConfiguration performs a default XML input mapping. The expected input is as follows: events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events If basic authentication is enabled via the basic.auth.enabled='true setting, each input event is also expected to contain the Authorization:'Basic encodeBase64(username:Password)' header. http-request (Source) The HTTP request is correlated with the HTTP response sink, through a unique source.id , and for each POST requests it receives via HTTP or HTTPS in format such as text , XML and JSON it sends the response via the HTTP response sink. The individual request and response messages are correlated at the sink using the message.id of the events. If required, you can enable basic authentication at the source to ensure that events are received only from users who are authorized to access the service. Origin: siddhi-io-http:2.0.4 Syntax @source(type=\"http-request\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", server.enable.session.creation=\" STRING \", server.supported.snimatchers=\" STRING \", server.suported.server.names=\" STRING \", request.size.validation.configuration=\" STRING \", request.size.validation=\" STRING \", request.size.validation.maximum.value=\" STRING \", request.size.validation.reject.status.code=\" STRING \", request.size.validation.reject.message=\" STRING \", request.size.validation.reject.message.content.type=\" STRING \", header.size.validation=\" STRING \", header.validation.maximum.request.line=\" STRING \", header.validation.maximum.size=\" STRING \", header.validation.maximum.chunk.size=\" STRING \", header.validation.reject.status.code=\" STRING \", header.validation.reject.message=\" STRING \", header.validation.reject.message.content.type=\" STRING \", server.bootstrap.configuration=\" OBJECT \", server.bootstrap.nodelay=\" BOOL \", server.bootstrap.keepalive=\" BOOL \", server.bootstrap.sendbuffersize=\" INT \", server.bootstrap.recievebuffersize=\" INT \", server.bootstrap.connect.timeout=\" INT \", server.bootstrap.socket.reuse=\" BOOL \", server.bootstrap.socket.timeout=\" BOOL \", server.bootstrap.socket.backlog=\" BOOL \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL to which the events should be received. User can provide any valid url and if the url is not provided the system will use the following format http://0.0.0.0:9763/ appNAme / streamName If the user want to use SSL the url should be given in following format https://localhost:8080/ streamName http://0.0.0.0:9763/ / STRING Yes No source.id Identifier need to map the source to sink. STRING No No connection.timeout Connection timeout in milliseconds. If the mapped http-response sink does not get a correlated message, after this timeout value, a timeout response is sent 120000 INT Yes No basic.auth.enabled If this is set to true , basic authentication is enabled for incoming events, and the credentials with which each event is sent are verified to ensure that the user is authorized to access the service. If basic authentication fails, the event is not authenticated and an authentication error is logged in the CLI. By default this values 'false' false STRING Yes No worker.count The number of active worker threads to serve the incoming events. The value is 1 by default. This will ensure that the events are directed to the event stream in the same order in which they arrive. By increasing this value the performance might increase at the cost of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection. 120000 INT Yes No ssl.verify.client The type of client certificate verification. null STRING Yes No ssl.protocol ssl/tls related options TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No server.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No server.supported.snimatchers Http SNIMatcher to be added. This parameter should include under parameters Ex: 'server.supported.snimatchers:SNIMatcher' null STRING Yes No server.suported.server.names Http supported servers. This parameter should include under parameters Ex: 'server.suported.server.names:server' null STRING Yes No request.size.validation.configuration Parameters that responsible for validating the http request and request headers. Expected format of these parameters is as follows: \"'request.size.validation:xxx','request.size.validation.maximum.value:xxx'\" null STRING Yes No request.size.validation To enable the request size validation. false STRING Yes No request.size.validation.maximum.value If request size is validated then maximum size. Integer.MAX_VALUE STRING Yes No request.size.validation.reject.status.code If request is exceed maximum size and request.size.validation is enabled then status code to be send as response. 401 STRING Yes No request.size.validation.reject.message If request is exceed maximum size and request.size.validation is enabled then status message to be send as response. Message is bigger than the valid size STRING Yes No request.size.validation.reject.message.content.type If request is exceed maximum size and request.size.validation is enabled then content type to be send as response. plain/text STRING Yes No header.size.validation To enable the header size validation. false STRING Yes No header.validation.maximum.request.line If header header validation is enabled then the maximum request line. 4096 STRING Yes No header.validation.maximum.size If header header validation is enabled then the maximum expected header size. 8192 STRING Yes No header.validation.maximum.chunk.size If header header validation is enabled then the maximum expected chunk size. 8192 STRING Yes No header.validation.reject.status.code 401 If header is exceed maximum size and header.size.validation is enabled then status code to be send as response. STRING Yes No header.validation.reject.message If header is exceed maximum size and header.size.validation is enabled then message to be send as response. Message header is bigger than the valid size STRING Yes No header.validation.reject.message.content.type If header is exceed maximum size and header.size.validation is enabled then content type to be send as response. plain/text STRING Yes No server.bootstrap.configuration Parameters that for bootstrap configurations of the server. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null OBJECT Yes No server.bootstrap.nodelay Http server no delay. true BOOL Yes No server.bootstrap.keepalive Http server keep alive. true BOOL Yes No server.bootstrap.sendbuffersize Http server send buffer size. 1048576 INT Yes No server.bootstrap.recievebuffersize Http server receive buffer size. 1048576 INT Yes No server.bootstrap.connect.timeout Http server connection timeout. 15000 INT Yes No server.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No server.bootstrap.socket.timeout Http server socket timeout. 15 BOOL Yes No server.bootstrap.socket.backlog THttp server socket backlog. 100 BOOL Yes No trace.log.enabled Http traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize property to configure number of boss threads, which accepts incoming connections until the ports are unbound. Once connection accepts successfully, boss thread passes the accepted channel to one of the worker threads. Number of available processors Any integer serverBootstrapWorkerGroupSize property to configure number of worker threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer serverBootstrapClientGroupSize property to configure number of client threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultHttpPort The default port if the default scheme is 'http'. 8280 Any valid port defaultHttpsPort The default port if the default scheme is 'https'. 8243 Any valid port defaultScheme The default protocol. http http https keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to wso2carbon.jks file keyStorePassword The default keystore password. wso2carbon String of keystore password certPassword The default cert password. wso2carbon String of cert password Examples EXAMPLE 1 @source(type='http-request', source.id='sampleSourceId, receiver.url='http://localhost:9055/endpoints/RecPro', connection.timeout='150000', parameters=\"'ciphers : TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256', 'sslEnabledProtocols:TLSv1.1,TLSv1.2'\", request.size.validation.configuration=\"request.size.validation:true\", server.bootstrap.configuration=\"server.bootstrap.socket.timeout:25\", @map(type='json, @attributes(messageId='trp:messageId', symbol='$.events.event.symbol', price='$.events.event.price', volume='$.events.event.volume'))) define stream FooStream (messageId string, symbol string, price float, volume long); The expected input is as follows: {\"events\": {\"event\": \"symbol\":WSO2, \"price\":55.6, \"volume\":100, } } If basic authentication is enabled via the basic.auth.enabled='true setting, each input event is also expected to contain the Authorization:'Basic encodeBase64(username:Password)' header. http-response (Source) The http-response source co-relates with http-request sink with the parameter 'sink.id'. This receives responses for the requests sent by the http-request sink which has the same sink id. Response messages can be in formats such as TEXT, JSON and XML. In order to handle the responses with different http status codes, user is allowed to defined the acceptable response source code using the parameter 'http.status.code' Origin: siddhi-io-http:2.0.4 Syntax @source(type=\"http-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id This parameter is used to map the http-response source to a http-request sink. Then this source will accepts the response messages for the requests sent by corresponding http-request sink. STRING No No http.status.code Acceptable http status code for the responses. This can be a complete string or a regex. Only the responses with matching status codes to the defined value, will be received by the http-response source. Eg: 'http.status.code = '200', http.status.code = '2\\d+'' 200 STRING Yes No allow.streaming.responses If responses can be received multiple times for a single request, this option should be enabled. If this is not enabled, for every request, response will be extracted only once. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-request', downloading.enabled='true', publisher.url='http://localhost:8005/registry/employee', method='POST', headers='{{headers}}',sink.id='employee-info', @map(type='json')) define stream BarStream (name String, id int, headers String, downloadPath string); @source(type='http-response' , sink.id='employee-info', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(message='A[1]'))) define stream responseStream2xx(message string);@source(type='http-response' , sink.id='employee-info', http.status.code='4\\\\d+' , @map(type='text', regex.A='((.|\\n)*)', @attributes(message='A[1]'))) define stream responseStream4xx(message string); In above example, the defined http-request sink will send a POST requests to the endpoint defined by 'publisher.url'. Then for those requests, the source with the response code '2\\d+' and sink.id 'employee-info' will receive the responses with 2xx status codes. The http-response source which has 'employee-info' as the 'sink.id' and '4\\d+' as the http.response.code will receive all the responses with 4xx status codes. . Then the body of the response message will be extracted using text mapper and converted into siddhi events. . inMemory (Source) In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Origin: siddhi-core:5.0.0 Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport. kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Origin: siddhi-io-kafka:5.0.0 Syntax @source(type=\"kafka\", bootstrap.servers=\" STRING \", topic.list=\" STRING \", group.id=\" STRING \", threading.option=\" STRING \", partition.no.list=\" STRING \", seq.enabled=\" BOOL \", is.binary.message=\" BOOL \", topic.offset.map=\" STRING \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51 st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Origin: siddhi-io-kafka:5.0.0 Syntax @source(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream. nats (Source) NATS Source allows users to subscribe to a NATS broker and receive messages. It has the ability to receive all the message types supported by NATS. Origin: siddhi-io-nats:2.0.1 Syntax @source(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", queue.group.name=\" STRING \", durable.name=\" STRING \", subscription.sequence=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS Source should subscribe to. STRING No No bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client subscribing/connecting to the NATS broker. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No queue.group.name This can be used when there is a requirement to share the load of a NATS subject. Clients belongs to the same queue group share the subscription load. None STRING Yes No durable.name This can be used to subscribe to a subject from the last acknowledged message when a client or connection failure happens. The client can be uniquely identified using the tuple (client.id, durable.name). None STRING Yes No subscription.sequence This can be used to subscribe to a subject from a given number of message sequence. All the messages from the given point of sequence number will be passed to the client. If not provided then the either the persisted value or 0 will be used. None STRING Yes No Examples EXAMPLE 1 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with all supporting configurations.With the following configuration the source identified as 'nats-client' will subscribes to a subject named as 'SP_NATS_INPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This subscription will receive all the messages from 100 th in the subject. EXAMPLE 2 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', ) define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with mandatory configurations.With the following configuration the source identified with an auto generated client id will subscribes to a subject named as 'SP_NATS_INTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This will receive all available messages in the subject tcp (Source) A Siddhi application can be configured to receive events via the TCP transport by adding the @Source(type = 'tcp') annotation at the top of an event stream definition. When this is defined the associated stream will receive events from the TCP transport on the host and port defined in the system. Origin: siddhi-io-tcp:3.0.1 Syntax @source(type=\"tcp\", context=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic context The URL 'context' that should be used to receive the events. / STRING Yes No System Parameters Name Description Default Value Possible Parameters host Tcp server host. 0.0.0.0 Any valid host or IP port Tcp server port. 9892 Any integer representing valid port receiver.threads Number of threads to receive connections. 10 Any positive integer worker.threads Number of threads to serve events. 10 Any positive integer tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true true false keep.alive This property defines whether the server should be kept alive when there are no connections available. true true false Examples EXAMPLE 1 @Source(type = 'tcp', context='abc', @map(type='binary')) define stream Foo (attribute1 string, attribute2 int ); Under this configuration, events are received via the TCP transport on default host,port, abc context, and they are passed to Foo stream for processing. Sourcemapper binary (Source Mapper) This extension is a binary input mapper that converts events received in binary format to Siddhi events before they are processed. Origin: siddhi-map-binary:2.0.0 Syntax @source(..., @map(type=\"binary\") Examples EXAMPLE 1 @source(type='inMemory', topic='WSO2', @map(type='binary'))define stream FooStream (symbol string, price float, volume long); This query performs a mapping to convert an event of the binary format to a Siddhi event. csv (Source Mapper) This extension is used to convert CSV message to Siddhi event input mapper. You can either receive pre-defined CSV message where event conversion takes place without extra configurations,or receive custom CSV message where a custom place order to map from custom CSV message. Origin: siddhi-map-csv:2.0.0 Syntax @source(..., @map(type=\"csv\", delimiter=\" STRING \", header.present=\" BOOL \", fail.on.unknown.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter When converting a CSV format message to Siddhi event, this parameter indicatesinput CSV message's data should be split by this parameter , STRING Yes No header.present When converting a CSV format message to Siddhi event, this parameter indicates whether CSV message has header or not. This can either have value true or false.If it's set to false then it indicates that CSV message has't header. false BOOL Yes No fail.on.unknown.attribute This parameter specifies how unknown attributes should be handled. If it's set to true and one or more attributes don't havevalues, then SP will drop that message. If this parameter is set to false , the Stream Processor adds the required attribute's values to such events with a null value and the event is converted to a Siddhi event. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='csv')) define stream FooStream (symbol string, price float, volume int); Above configuration will do a default CSV input mapping. Expected input will look like below: WSO2 ,55.6 , 100OR \"WSO2,No10,Palam Groove Rd,Col-03\" ,55.6 , 100If header.present is true and delimiter is \"-\", then the input is as follows: symbol-price-volumeWSO2-55.6-100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='csv',header='true', @attributes(symbol = \"2\", price = \"0\", volume = \"1\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom CSV mapping. Here, user can add place order of each attribute in the @attribute. The place order indicates where the attribute name's value has appeared in the input.Expected input will look like below: 55.6,100,WSO2 OR55.6,100,\"WSO2,No10,Palm Groove Rd,Col-03\" If header is true and delimiter is \"-\", then the output is as follows: price-volume-symbol 55.6-100-WSO2 If group events is enabled then input should be as follows: price-volume-symbol 55.6-100-WSO2System.lineSeparator() 55.6-100-IBMSystem.lineSeparator() 55.6-100-IFSSystem.lineSeparator() json (Source Mapper) This extension is a JSON-to-Event input mapper. Transports that accept JSON messages can utilize this extension to convert an incoming JSON message into a Siddhi event. Users can either send a pre-defined JSON format, where event conversion happens without any configurations, or use the JSON path to map from a custom JSON message. In default mapping, the JSON string of the event can be enclosed by the element \"event\", though optional. Origin: siddhi-map-json:5.0.1 Syntax @source(..., @map(type=\"json\", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic enclosing.element This is used to specify the enclosing element when sending multiple events in the same JSON message. Mapper treats the child elements of a given enclosing element as events and executes the JSON path expressions on these child elements. If the enclosing.element is not provided then the multiple-event scenario is disregarded and the JSON path is evaluated based on the root element. $ STRING Yes No fail.on.missing.attribute This parameter allows users to handle unknown attributes.The value of this can either be true or false. By default it is true. If a JSON execution fails or returns null, mapper drops that message. However, setting this property to false prompts mapper to send an event with a null value to Siddhi, where users can handle it as required, ie., assign a default value.) true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For a single event, the input is required to be in one of the following formats: { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } or { \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For multiple events, the input is required to be in one of the following formats: [ {\"event\":{\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80}} ] or [ {\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}, {\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}, {\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80} ] EXAMPLE 3 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"company.symbol\", price = \"price\", volume = \"volume\"))) This configuration performs a custom JSON mapping. For a single event, the expected input is similar to the one shown below: .{ \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"WSO2\" }, \"price\":55.6 } } EXAMPLE 4 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream FooStream (symbol string, price float, volume long); The configuration performs a custom JSON mapping. For multiple events, expected input looks as follows. .{\"portfolio\": [ {\"stock\":{\"volume\":100,\"company\":{\"symbol\":\"wso2\"},\"price\":56.6}}, {\"stock\":{\"volume\":200,\"company\":{\"symbol\":\"wso2\"},\"price\":57.6}} ] } keyvalue (Source Mapper) Key-Value Map to Event input mapper extension allows transports that accept events as key value maps to convert those events to Siddhi events. You can either receive pre-defined keys where conversion takes place without extra configurations, or use custom keys to map from the message. Origin: siddhi-map-keyvalue:2.0.0 Syntax @source(..., @map(type=\"keyvalue\", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic fail.on.missing.attribute If this parameter is set to true , if an event arrives without a matching key for a specific attribute in the connected stream, it is dropped and not processed by the Stream Processor. If this parameter is set to false the Stream Processor adds the required key to such events with a null value, and the event is converted to a Siddhi event so that you could handle them as required before they are further processed. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default key value input mapping. The expected input is a map similar to the following: symbol: 'WSO2' price: 55.6f volume: 100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='keyvalue', fail.on.missing.attribute='true', @attributes(symbol = 's', price = 'p', volume = 'v')))define stream FooStream (symbol string, price float, volume long); This query performs a custom key value input mapping. The matching keys for the symbol , price and volume attributes are be s , p, and v` respectively. The expected input is a map similar to the following: s: 'WSO2' p: 55.6 v: 100 passThrough (Source Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.0.0 Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source. text (Source Mapper) This extension is a text to Siddhi event input mapper. Transports that accept text messages can utilize this extension to convert the incoming text message to Siddhi event. Users can either use a pre-defined text format where event conversion happens without any additional configurations, or specify a regex to map a text message using custom configurations. Origin: siddhi-map-text:2.0.0 Syntax @source(..., @map(type=\"text\", regex.groupid=\" STRING \", fail.on.missing.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex.groupid This parameter specifies a regular expression group. The groupid can be any capital letter (e.g., regex.A,regex.B .. etc). You can specify any number of regular expression groups. In the attribute annotation, you need to map all attributes to the regular expression group with the matching group index. If you need to to enable custom mapping, it is required to specifythe matching group for each and every attribute. STRING No No fail.on.missing.attribute This parameter specifies how unknown attributes should be handled. If it is set to true a message is dropped if its execution fails, or if one or more attributes do not have values. If this parameter is set to false , null values are assigned to attributes with missing values, and messages with such attributes are not dropped. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No delimiter This parameter specifies how events must be separated when multiple events are received. This must be whole line and not a single character. ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n as the end of line character whereas windows uses \\r\\n . \\n STRING Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected input is as follows: symbol:\"WSO2\", price:55.6, volume:100 OR symbol:'WSO2', price:55.6, volume:100 If group events is enabled then input should be as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='text', fail.on.unknown.attribute = 'true', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected input is as follows: wos2 550 volume 100 If group events is enabled then input should be as follows: wos2 550 volume 100 ~ wos2 550 volume 100 ~ wos2 550 volume 100 xml (Source Mapper) This mapper converts XML input to Siddhi event. Transports which accepts XML messages can utilize this extension to convert the incoming XML message to Siddhi event. Users can either send a pre-defined XML format where event conversion will happen without any configs or can use xpath to map from a custom XML message. Origin: siddhi-map-xml:5.0.0 Syntax @source(..., @map(type=\"xml\", namespaces=\" STRING \", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic namespaces Used to provide namespaces used in the incoming XML message beforehand to configure xpath expressions. User can provide a comma separated list. If these are not provided xpath evaluations will fail None STRING Yes No enclosing.element Used to specify the enclosing element in case of sending multiple events in same XML message. WSO2 DAS will treat the child element of given enclosing element as events and execute xpath expressions on child elements. If enclosing.element is not provided multiple event scenario is disregarded and xpaths will be evaluated with respect to root element. Root element STRING Yes No fail.on.missing.attribute This can either have value true or false. By default it will be true. This attribute allows user to handle unknown attributes. By default if an xpath execution fails or returns null DAS will drop that message. However setting this property to false will prompt DAS to send and event with null value to Siddhi where user can handle it accordingly(ie. Assign a default value) True BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping. Expected input will look like below. events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='xml', namespaces = \"dt=urn:schemas-microsoft-com:datatypes\", enclosing.element=\"//portfolio\", @attributes(symbol = \"company/symbol\", price = \"price\", volume = \"volume\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. In the custom mapping user can add xpath expressions representing each event attribute using @attribute annotation. Expected input will look like below. portfolio xmlns:dt=\"urn:schemas-microsoft-com:datatypes\" stock exchange=\"nasdaq\" volume 100 /volume company symbol WSO2 /symbol /company price dt:type=\"number\" 55.6 /price /stock /portfolio Store rdbms (Store) This extension assigns data sources and connection instructions to event tables. It also implements read-write operations on connected datasources. Origin: siddhi-store-rdbms:6.0.0 Syntax @Store(type=\"rdbms\", jdbc.url=\" STRING \", username=\" STRING \", password=\" STRING \", jdbc.driver.name=\" STRING \", pool.properties=\" STRING \", jndi.resource=\" STRING \", datasource=\" STRING \", table.name=\" STRING \", field.length=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic jdbc.url The JDBC URL via which the RDBMS data store is accessed. STRING No No username The username to be used to access the RDBMS data store. STRING No No password The password to be used to access the RDBMS data store. STRING No No jdbc.driver.name The driver class name for connecting the RDBMS data store. STRING No No pool.properties Any pool parameters for the database connection must be specified as key-value pairs. null STRING Yes No jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account and the connection is attempted via JNDI lookup instead. null STRING Yes No datasource The name of the Carbon datasource that should be used for creating the connection with the database. If this is found, neither the pool properties nor the JNDI resource name described above are taken into account and the connection is attempted via Carbon datasources instead. null STRING Yes No table.name The name with which the event table should be persisted in the store. If no name is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The table name defined in the Siddhi App query. STRING Yes No field.length The number of characters that the values for fields of the 'STRING' type in the table definition must contain. Each required field must be provided as a comma-separated list of key-value pairs in the ' field.name : length ' format. If this is not specified, the default number of characters specific to the database type is considered. null STRING Yes No System Parameters Name Description Default Value Possible Parameters {{RDBMS-Name}}.maxVersion The latest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.minVersion The earliest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.tableCheckQuery The template query for the 'check table' operation in {{RDBMS-Name}}. H2 : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) MySQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Oracle : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Microsoft SQL Server : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) PostgreSQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) DB2. : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) N/A {{RDBMS-Name}}.tableCreateQuery The template query for the 'create table' operation in {{RDBMS-Name}}. H2 : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 MySQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 Oracle : SELECT 1 FROM {{TABLE_NAME}} WHERE rownum=1 Microsoft SQL Server : SELECT TOP 1 1 from {{TABLE_NAME}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.indexCreateQuery The template query for the 'create index' operation in {{RDBMS-Name}}. H2 : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) MySQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Oracle : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Microsoft SQL Server : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) {{TABLE_NAME}} ({{INDEX_COLUMNS}}) PostgreSQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) DB2. : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) N/A {{RDBMS-Name}}.recordInsertQuery The template query for the 'insert record' operation in {{RDBMS-Name}}. H2 : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) MySQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Oracle : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Microsoft SQL Server : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) PostgreSQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) DB2. : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) N/A {{RDBMS-Name}}.recordUpdateQuery The template query for the 'update record' operation in {{RDBMS-Name}}. H2 : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} MySQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Oracle : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Microsoft SQL Server : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} PostgreSQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} DB2. : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} N/A {{RDBMS-Name}}.recordSelectQuery The template query for the 'select record' operation in {{RDBMS-Name}}. H2 : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} DB2. : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.recordExistsQuery The template query for the 'check record existence' operation in {{RDBMS-Name}}. H2 : SELECT TOP 1 1 FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT COUNT(1) INTO existence FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT TOP 1 FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.recordDeleteQuery The query for the 'delete record' operation in {{RDBMS-Name}}. H2 : DELETE FROM {{TABLE_NAME}} {{CONDITION}} MySQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Oracle : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : DELETE FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} DB2. : DELETE FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.stringSize This defines the length for the string fields in {{RDBMS-Name}}. H2 : 254 MySQL : 254 Oracle : 254 Microsoft SQL Server : 254 PostgreSQL : 254 DB2. : 254 N/A {{RDBMS-Name}}.fieldSizeLimit This defines the field size limit for select/switch to big string type from the default string type if the 'bigStringType' is available in field type list. H2 : N/A MySQL : N/A Oracle : 2000 Microsoft SQL Server : N/A PostgreSQL : N/A DB2. : N/A 0 = n = INT_MAX {{RDBMS-Name}}.batchSize This defines the batch size when operations are performed for batches of events. H2 : 1000 MySQL : 1000 Oracle : 1000 Microsoft SQL Server : 1000 PostgreSQL : 1000 DB2. : 1000 N/A {{RDBMS-Name}}.batchEnable This specifies whether 'Update' and 'Insert' operations can be performed for batches of events or not. H2 : true MySQL : true Oracle (versions 12.0 and less) : false Oracle (versions 12.1 and above) : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.transactionSupported This is used to specify whether the JDBC connection that is used supports JDBC transactions or not. H2 : true MySQL : true Oracle : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.typeMapping.binaryType This is used to specify the binary data type. An attribute defines as 'object' type in Siddhi stream will be stored into RDBMS with this type. H2 : BLOB MySQL : BLOB Oracle : BLOB Microsoft SQL Server : VARBINARY(max) PostgreSQL : BYTEA DB2. : BLOB(64000) N/A {{RDBMS-Name}}.typeMapping.booleanType This is used to specify the boolean data type. An attribute defines as 'bool' type in Siddhi stream will be stored into RDBMS with this type. H2 : TINYINT(1) MySQL : TINYINT(1) Oracle : NUMBER(1) Microsoft SQL Server : BIT PostgreSQL : BOOLEAN DB2. : SMALLINT N/A {{RDBMS-Name}}.typeMapping.doubleType This is used to specify the double data type. An attribute defines as 'double' type in Siddhi stream will be stored into RDBMS with this type. H2 : DOUBLE MySQL : DOUBLE Oracle : NUMBER(19,4) Microsoft SQL Server : FLOAT(32) PostgreSQL : DOUBLE PRECISION DB2. : DOUBLE N/A {{RDBMS-Name}}.typeMapping.floatType This is used to specify the float data type. An attribute defines as 'float' type in Siddhi stream will be stored into RDBMS with this type. H2 : FLOAT MySQL : FLOAT Oracle : NUMBER(19,4) Microsoft SQL Server : REAL PostgreSQL : REAL DB2. : REAL N/A {{RDBMS-Name}}.typeMapping.integerType This is used to specify the integer data type. An attribute defines as 'int' type in Siddhi stream will be stored into RDBMS with this type. H2 : INTEGER MySQL : INTEGER Oracle : NUMBER(10) Microsoft SQL Server : INTEGER PostgreSQL : INTEGER DB2. : INTEGER N/A {{RDBMS-Name}}.typeMapping.longType This is used to specify the long data type. An attribute defines as 'long' type in Siddhi stream will be stored into RDBMS with this type. H2 : BIGINT MySQL : BIGINT Oracle : NUMBER(19) Microsoft SQL Server : BIGINT PostgreSQL : BIGINT DB2. : BIGINT N/A {{RDBMS-Name}}.typeMapping.stringType This is used to specify the string data type. An attribute defines as 'string' type in Siddhi stream will be stored into RDBMS with this type. H2 : VARCHAR(stringSize) MySQL : VARCHAR(stringSize) Oracle : VARCHAR(stringSize) Microsoft SQL Server : VARCHAR(stringSize) PostgreSQL : VARCHAR(stringSize) DB2. : VARCHAR(stringSize) N/A {{RDBMS-Name}}.typeMapping.bigStringType This is used to specify the big string data type. An attribute defines as 'string' type in Siddhi stream and field.length define in the annotation is greater than the fieldSizeLimit, will be stored into RDBMS with this type. H2 : N/A MySQL : N/A Oracle : CLOB Microsoft SQL Server : N/A PostgreSQL : N/A DB2.* : N/A N/A Examples EXAMPLE 1 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/stocks\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"volume\") define table StockTable (symbol string, price float, volume long); The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float', and 'long' respectively). The connection is made as specified by the parameters configured for the '@Store' annotation. The 'symbol' attribute is considered a unique field, and a DB index is created for it. EXAMPLE 2 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)] Str groupConcat (Aggregate Function) This function aggregates the received events by concatenating the keys in those events using a separator, e.g.,a comma (,) or a hyphen (-), and returns the concatenated key string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:groupConcat( STRING key, STRING separator, STRING distinct, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key The string that needs to be aggregated. STRING No No separator The separator that separates each string key after concatenating the keys. , STRING Yes No distinct This is used to only have distinct values in the concatenated string that is returned. false STRING Yes No order This parameter accepts 'ASC' or 'DESC' strings to sort the string keys in either ascending or descending order respectively. No order STRING Yes No Examples EXAMPLE 1 from InputStream#window.time(5 min) select str:groupConcat(\"key\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , it returns \"A,B,S,C,A\" to the 'OutputStream'. EXAMPLE 2 from InputStream#window.time(5 min) select groupConcat(\"key\",\"-\",true,\"ASC\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , specify the seperator as hyphen and choose the order to be ascending, the function returns \"A-B-C-S\" to the 'OutputStream'. charAt (Function) This function returns the 'char' value that is present at the given index position. of the input string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:charAt( STRING input.value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.value The input string of which the char value at the given position needs to be returned. STRING No No index The variable that specifies the index of the char value that needs to be returned. INT No No Examples EXAMPLE 1 charAt(\"WSO2\", 1) In this case, the functiion returns the character that exists at index 1. Hence, it returns 'S'. coalesce (Function) This returns the first input parameter value of the given argument, that is not null. Origin: siddhi-execution-string:5.0.1 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT str:coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT argn) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic argn It can have one or more input parameters in any data type. However, all the specified parameters are required to be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 coalesce(null, \"BBB\", \"CCC\") This returns the first input parameter that is not null. In this example, it returns \"BBB\". concat (Function) This function returns a string value that is obtained as a result of concatenating two or more input string values. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:concat( STRING argn) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic argn This can have two or more string type input parameters. STRING No No Examples EXAMPLE 1 concat(\"D533\", \"8JU^\", \"XYZ\") This returns a string value by concatenating two or more given arguments. In the example shown above, it returns \"D5338JU^XYZ\". contains (Function) This function returns true if the input.string contains the specified sequence of char values in the search.string . Origin: siddhi-execution-string:5.0.1 Syntax BOOL str:contains( STRING input.string, STRING search.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string Input string value. STRING No No search.string The string value to be searched for in the input.string . STRING No No Examples EXAMPLE 1 contains(\"21 products are produced by WSO2 currently\", \"WSO2\") This returns a boolean value as the output. In this case, it returns true . equalsIgnoreCase (Function) This returns a boolean value by comparing two strings lexicographically without considering the letter case. Origin: siddhi-execution-string:5.0.1 Syntax BOOL str:equalsIgnoreCase( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No No arg2 The second input string argument. This is compared with the first argument. STRING No No Examples EXAMPLE 1 equalsIgnoreCase(\"WSO2\", \"wso2\") This returns a boolean value as the output. In this scenario, it returns \"true\". fillTemplate (Function) This extension replaces the templated positions that are marked with an index value in a specified template with the strings provided. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:fillTemplate( STRING template, STRING|INT|LONG|DOUBLE|FLOAT|BOOL replacement.strings) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic template The string with templated fields that needs to be filled with the given strings. The format of the templated fields should be as follows: {{INDEX}} where 'INDEX' is an integer. This index is used to map the strings that are used to replace the templated fields. STRING No No replacement.strings The strings with which the templated positions in the template need to be replaced. The minimum of two arguments need to be included in the execution string. There is no upper limit on the number of arguments allowed to be included. STRING INT LONG DOUBLE FLOAT BOOL No No Examples EXAMPLE 1 str:fillTemplate(\"This is {{1}} for the {{2}} function\", 'an example', 'fillTemplate') In this example, the template is 'This is {{1}} for the {{2}} function'.Here, the templated string {{1}} is replaced with the 1 st string value provided, which is 'an example'. {{2}} is replaced with the 2 nd string provided, which is 'fillTemplate' The complete return string is 'This is an example for the fillTemplate function'. hex (Function) This function returns a hexadecimal string by converting each byte of each character in the input string to two hexadecimal digits. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:hex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the hexadecimal value. STRING No No Examples EXAMPLE 1 hex(\"MySQL\") This returns the hexadecimal value of the input.string. In this scenario, the output is \"4d7953514c\". length (Function) Returns the length of the input string. Origin: siddhi-execution-string:5.0.1 Syntax INT str:length( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the length. STRING No No Examples EXAMPLE 1 length(\"Hello World\") This outputs the length of the provided string. In this scenario, the, output is 11 . lower (Function) Converts the capital letters in the input string to the equivalent simple letters. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:lower( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to convert to the lower case (i.e., equivalent simple letters). STRING No No Examples EXAMPLE 1 lower(\"WSO2 cep \") This converts the capital letters in the input.string to the equivalent simple letters. In this scenario, the output is \"wso2 cep \". regexp (Function) Returns a boolean value based on the matchability of the input string and the given regular expression. Origin: siddhi-execution-string:5.0.1 Syntax BOOL str:regexp( STRING input.string, STRING regex) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to match with the given regular expression. STRING No No regex The regular expression to be matched with the input string. STRING No No Examples EXAMPLE 1 regexp(\"WSO2 abcdh\", \"WSO(.*h)\") This returns a boolean value after matching regular expression with the given string. In this scenario, it returns \"true\" as the output. repeat (Function) Repeats the input string for a specified number of times. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:repeat( STRING input.string, INT times) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that is repeated the number of times as defined by the user. STRING No No times The number of times the input.string needs to be repeated . INT No No Examples EXAMPLE 1 repeat(\"StRing 1\", 3) This returns a string value by repeating the string for a specified number of times. In this scenario, the output is \"StRing 1StRing 1StRing 1\". replaceAll (Function) Finds all the substrings of the input string that matches with the given expression, and replaces them with the given replacement string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:replaceAll( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No No regex The regular expression to be matched with the input string. STRING No No replacement.string The string with which each substring that matches the given expression should be replaced. STRING No No Examples EXAMPLE 1 replaceAll(\"hello hi hello\", 'hello', 'test') This returns a string after replacing the substrings of the input string with the replacement string. In this scenario, the output is \"test hi test\" . replaceFirst (Function) Finds the first substring of the input string that matches with the given regular expression, and replaces itwith the given replacement string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:replaceFirst( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be replaced. STRING No No regex The regular expression with which the input string should be matched. STRING No No replacement.string The string with which the first substring of input string that matches the regular expression should be replaced. STRING No No Examples EXAMPLE 1 replaceFirst(\"hello WSO2 A hello\", 'WSO2(.*)A', 'XXXX') This returns a string after replacing the first substring with the given replacement string. In this scenario, the output is \"hello XXXX hello\". reverse (Function) Returns the input string in the reverse order character-wise and string-wise. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:reverse( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be reversed. STRING No No Examples EXAMPLE 1 reverse(\"Hello World\") This outputs a string value by reversing the incoming input.string . In this scenario, the output is \"dlroW olleH\". split (Function) Splits the input.string into substrings using the value parsed in the split.string and returns the substring at the position specified in the group.number . Origin: siddhi-execution-string:5.0.1 Syntax STRING str:split( STRING input.string, STRING split.string, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No No split.string The string value to be used to split the input.string . STRING No No group.number The index of the split group INT No No Examples EXAMPLE 1 split(\"WSO2,ABM,NSFT\", \",\", 0) This splits the given input.string by given split.string and returns the string in the index given by group.number. In this scenario, the output will is \"WSO2\". strcmp (Function) Compares two strings lexicographically and returns an integer value. If both strings are equal, 0 is returned. If the first string is lexicographically greater than the second string, a positive value is returned. If the first string is lexicographically greater than the second string, a negative value is returned. Origin: siddhi-execution-string:5.0.1 Syntax INT str:strcmp( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No No arg2 The second input string argument that should be compared with the first argument lexicographically. STRING No No Examples EXAMPLE 1 strcmp(\"AbCDefghiJ KLMN\", 'Hello') This compares two strings lexicographically and outputs an integer value. substr (Function) Returns a substring of the input string by considering a subset or all of the following factors: starting index, length, regular expression, and regex group number. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:substr( STRING input.string, INT begin.index, INT length, STRING regex, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be processed. STRING No No begin.index Starting index to consider for the substring. INT No No length The length of the substring. INT No No regex The regular expression that should be matched with the input string. STRING No No group.number The regex group number INT No No Examples EXAMPLE 1 substr(\"AbCDefghiJ KLMN\", 4) This outputs the substring based on the given begin.index . In this scenario, the output is \"efghiJ KLMN\". EXAMPLE 2 substr(\"AbCDefghiJ KLMN\", 2, 4) This outputs the substring based on the given begin.index and length. In this scenario, the output is \"CDef\". EXAMPLE 3 substr(\"WSO2D efghiJ KLMN\", '^WSO2(.*)') This outputs the substring by applying the regex. In this scenario, the output is \"WSO2D efghiJ KLMN\". EXAMPLE 4 substr(\"WSO2 cep WSO2 XX E hi hA WSO2 heAllo\", 'WSO2(.*)A(.*)', 2) This outputs the substring by applying the regex and considering the group.number . In this scenario, the output is \" ello\". trim (Function) Returns a copy of the input string without the leading and trailing whitespace (if any). Origin: siddhi-execution-string:5.0.1 Syntax STRING str:trim( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that needs to be trimmed. STRING No No Examples EXAMPLE 1 trim(\" AbCDefghiJ KLMN \") This returns a copy of the input.string with the leading and/or trailing white-spaces omitted. In this scenario, the output is \"AbCDefghiJ KLMN\". unhex (Function) Returns a string by converting the hexadecimal characters in the input string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:unhex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The hexadecimal input string that needs to be converted to string. STRING No No Examples EXAMPLE 1 unhex(\"4d7953514c\") This converts the hexadecimal value to string. upper (Function) Converts the simple letters in the input string to the equivalent capital/block letters. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:upper( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be converted to the upper case (equivalent capital/block letters). STRING No No Examples EXAMPLE 1 upper(\"Hello World\") This converts the simple letters in the input.string to theequivalent capital letters. In this scenario, the output is \"HELLO WORLD\". tokenize (Stream Processor) This function splits the input string into tokens using a given regular expression and returns the split tokens. Origin: siddhi-execution-string:5.0.1 Syntax str:tokenize( STRING input.string, STRING regex, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string which needs to be split. STRING No No regex The string value which is used to tokenize the 'input.string'. STRING No No distinct This flag is used to return only distinct values. false BOOL Yes No Extra Return Attributes Name Description Possible Types token The attribute which contains a single token. STRING Examples EXAMPLE 1 define stream inputStream (str string); @info(name = 'query1') from inputStream#str:tokenize(str , ',') select text insert into outputStream; This query performs tokenization on the given string. If the str is \"Android,Windows8,iOS\", then the string is split into 3 events containing the token attribute values, i.e., Android , Windows8 and iOS .","title":"latest"},{"location":"docs/api/latest/#api-docs-v500","text":"","title":"API Docs - v5.0.0"},{"location":"docs/api/latest/#core","text":"","title":"Core"},{"location":"docs/api/latest/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Origin: siddhi-core:5.0.0 Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"docs/api/latest/#avg-aggregate-function","text":"Calculates the average for all the events. Origin: siddhi-core:5.0.0 Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"docs/api/latest/#count-aggregate-function","text":"Returns the count of all the events. Origin: siddhi-core:5.0.0 Syntax LONG count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"docs/api/latest/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Origin: siddhi-core:5.0.0 Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"docs/api/latest/#max-aggregate-function","text":"Returns the maximum value for all the events. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"docs/api/latest/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"docs/api/latest/#min-aggregate-function","text":"Returns the minimum value for all the events. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"docs/api/latest/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"docs/api/latest/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Origin: siddhi-core:5.0.0 Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"docs/api/latest/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Origin: siddhi-core:5.0.0 Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"docs/api/latest/#sum-aggregate-function","text":"Returns the sum for all the events. Origin: siddhi-core:5.0.0 Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"docs/api/latest/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Origin: siddhi-core:5.0.0 Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"docs/api/latest/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Origin: siddhi-core:5.0.0 Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"docs/api/latest/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"docs/api/latest/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"docs/api/latest/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"docs/api/latest/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Origin: siddhi-core:5.0.0 Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"docs/api/latest/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Origin: siddhi-core:5.0.0 Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"docs/api/latest/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"docs/api/latest/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Origin: siddhi-core:5.0.0 Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"docs/api/latest/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"docs/api/latest/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"docs/api/latest/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"docs/api/latest/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"docs/api/latest/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"docs/api/latest/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"docs/api/latest/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Origin: siddhi-core:5.0.0 Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"docs/api/latest/#maximum-function","text":"Returns the maximum value of the input parameters. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"docs/api/latest/#minimum-function","text":"Returns the minimum value of the input parameters. Origin: siddhi-core:5.0.0 Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"docs/api/latest/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Origin: siddhi-core:5.0.0 Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"docs/api/latest/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Origin: siddhi-core:5.0.0 Syntax pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"docs/api/latest/#log-stream-processor","text":"The logger logs the message on the given priority with or without processed event. Origin: siddhi-core:5.0.0 Syntax log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log(\"INFO\", \"Sample Event :\", true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log(\"Sample Event :\", true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log(\"Sample Event :\", fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log(\"Sample Event :\") select * insert into barStream; This will log message and fooStream:events.","title":"log (Stream Processor)"},{"location":"docs/api/latest/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Origin: siddhi-core:5.0.0 Syntax batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"docs/api/latest/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Origin: siddhi-core:5.0.0 Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"docs/api/latest/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Origin: siddhi-core:5.0.0 Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"docs/api/latest/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Origin: siddhi-core:5.0.0 Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"docs/api/latest/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Origin: siddhi-core:5.0.0 Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/latest/#frequent-window","text":"This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Origin: siddhi-core:5.0.0 Syntax frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"docs/api/latest/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Origin: siddhi-core:5.0.0 Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"docs/api/latest/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Origin: siddhi-core:5.0.0 Syntax lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"docs/api/latest/#lossyfrequent-window","text":"This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Origin: siddhi-core:5.0.0 Syntax lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"docs/api/latest/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Origin: siddhi-core:5.0.0 Syntax session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"docs/api/latest/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Origin: siddhi-core:5.0.0 Syntax sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"docs/api/latest/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Origin: siddhi-core:5.0.0 Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"docs/api/latest/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Origin: siddhi-core:5.0.0 Syntax timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"docs/api/latest/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Origin: siddhi-core:5.0.0 Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"docs/api/latest/#json","text":"","title":"Json"},{"location":"docs/api/latest/#getbool-function","text":"This method returns a 'boolean' value, either 'true' or 'false', based on the valuespecified against the JSON element present in the given path.In case there is no valid boolean value found in the given path, the method still returns 'false'. Origin: siddhi-execution-json:2.0.0 Syntax BOOL json:getBool( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the boolean value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getBool' function fetches theboolean value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getBool(json,\"$.name\") as name insert into OutputStream; This returns the boolean value of the JSON input in the given path. The results are directed to the 'OutputStream' stream.","title":"getBool (Function)"},{"location":"docs/api/latest/#getdouble-function","text":"This method returns the double value of the JSON element present in the given path. If there is no valid double value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax DOUBLE json:getDouble( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getDouble' function fetches thedouble value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getDouble(json,\"$.name\") as name insert into OutputStream; This returns the double value of the given path. The results aredirected to the 'OutputStream' stream.","title":"getDouble (Function)"},{"location":"docs/api/latest/#getfloat-function","text":"This method returns the float value of the JSON element present in the given path.If there is no valid float value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax FLOAT json:getFloat( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getFloat' function fetches thevalue. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getFloat(json,\"$.name\") as name insert into OutputStream; This returns the float value of the JSON input in the given path. The results aredirected to the 'OutputStream' stream.","title":"getFloat (Function)"},{"location":"docs/api/latest/#getint-function","text":"This method returns the integer value of the JSON element present in the given path. If there is no valid integer value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax INT json:getInt( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getInt' function fetches theinteger value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getInt(json,\"$.name\") as name insert into OutputStream; This returns the integer value of the JSON input in the given path. The resultsare directed to the 'OutputStream' stream.","title":"getInt (Function)"},{"location":"docs/api/latest/#getlong-function","text":"This returns the long value of the JSON element present in the given path. Ifthere is no valid long value in the given path, the method returns 'null'. Origin: siddhi-execution-json:2.0.0 Syntax LONG json:getLong( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the JSON element from which the 'getLong' functionfetches the long value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getLong(json,\"$.name\") as name insert into OutputStream; This returns the long value of the JSON input in the given path. The results aredirected to 'OutputStream' stream.","title":"getLong (Function)"},{"location":"docs/api/latest/#getobject-function","text":"This returns the object of the JSON element present in the given path. Origin: siddhi-execution-json:2.0.0 Syntax OBJECT json:getObject( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the input JSON from which the 'getObject' function fetches theobject. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getObject(json,\"$.name\") as name insert into OutputStream; This returns the object of the JSON input in the given path. The results are directed to the 'OutputStream' stream.","title":"getObject (Function)"},{"location":"docs/api/latest/#getstring-function","text":"This returns the string value of the JSON element present in the given path. Origin: siddhi-execution-json:2.0.0 Syntax STRING json:getString( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that holds the value in the given path. STRING OBJECT No No path The path of the JSON input from which the 'getString' function fetches the string value. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:getString(json,\"$.name\") as name insert into OutputStream; This returns the string value of the JSON input in the given path. The results are directed to the 'OutputStream' stream.","title":"getString (Function)"},{"location":"docs/api/latest/#isexists-function","text":"This method checks whether there is a JSON element present in the given path or not.If there is a valid JSON element in the given path, it returns 'true'. If there is no valid JSON element, it returns 'false' Origin: siddhi-execution-json:2.0.0 Syntax BOOL json:isExists( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input in a given path, on which the function performs the search forJSON elements. STRING OBJECT No No path The path that contains the input JSON on which the function performs the search. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:isExists(json,\"$.name\") as name insert into OutputStream; This returns either true or false based on the existence of a JSON element in a given path. The results are directed to the 'OutputStream' stream.","title":"isExists (Function)"},{"location":"docs/api/latest/#setelement-function","text":"This method allows to insert elements into a given JSON present in a specific path. If there is no valid path given, it returns the original JSON. Otherwise, it returns the new JSON. Origin: siddhi-execution-json:2.0.0 Syntax OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT jsonelement, STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input into which is this function inserts the new value. STRING OBJECT No No path The path on the JSON input which is used to insert the given element. STRING No No jsonelement The JSON element which is inserted by the function into the input JSON. STRING BOOL DOUBLE FLOAT INT LONG OBJECT No No key The key which is used to insert the given element into the input JSON. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:setElement(json,\"$.name\") as name insert into OutputStream; This returns the JSON object present in the given path with the newly inserted JSONelement. The results are directed to the 'OutputStream' stream.","title":"setElement (Function)"},{"location":"docs/api/latest/#toobject-function","text":"This method returns the JSON object related to a given JSON string. Origin: siddhi-execution-json:2.0.0 Syntax OBJECT json:toObject( STRING json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON string from which the function generates the JSON object. STRING No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:toJson(json) as jsonObject insert into OutputStream; This returns the JSON object corresponding to the given JSON string.The results aredirected to the 'OutputStream' stream.","title":"toObject (Function)"},{"location":"docs/api/latest/#tostring-function","text":"This method returns the JSON string corresponding to a given JSON object. Origin: siddhi-execution-json:2.0.0 Syntax STRING json:toString( OBJECT json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON object from which the function generates a JSON string. OBJECT No No Examples EXAMPLE 1 define stream InputStream(json string); from InputStream select json:toString(json) as jsonString insert into OutputStream; This returns the JSON string corresponding to a given JSON object. The results are directed to the 'OutputStream' stream.","title":"toString (Function)"},{"location":"docs/api/latest/#tokenize-stream-processor","text":"This tokenizes the given json according the path provided Origin: siddhi-execution-json:2.0.0 Syntax json:tokenize( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input json that should be tokenized using the given path. STRING OBJECT No No path The path that is used to tokenize the given json STRING No No fail.on.missing.attribute If this parameter is set to 'true' and a json is not provided in the given path, the event is dropped. If the parameter is set to 'false', the unavailability of a json in the specified path results in the event being created with a 'null' value for the json element. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The json element retrieved based on the given path and the json. STRING Examples EXAMPLE 1 define stream InputStream (json string,path string); @info(name = 'query1') from InputStream#json:tokenize(json, path) select jsonElement insert into OutputStream; This query performs a tokenization for the given json using the path specified. If the specified path provides a json array, it generates events for each element in that array by adding an additional attributes as the 'jsonElement' to the stream. e.g., jsonInput - {name:\"John\",enrolledSubjects:[\"Mathematics\",\"Physics\"]}, path - \" .enrolledSubjects\" /code br If we use the configuration in this example, it generates two events with the attributes \"Mathematics\", \"Physics\". br If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream. br code e.g., jsonInput - {name:\"John\",age:25}, path - \" .enrolledSubjects\" </code><br>&nbsp;If we use the configuration in this example, it generates two events with the attributes \"Mathematics\", \"Physics\".<br>If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream. <br><code> e.g., jsonInput - {name:\"John\",age:25}, path - \" .age\"","title":"tokenize (Stream Processor)"},{"location":"docs/api/latest/#tokenizeasobject-stream-processor","text":"This tokenizes the given JSON based on the path provided and returns the response as an object. Origin: siddhi-execution-json:2.0.0 Syntax json:tokenizeAsObject( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input json that is tokenized using the given path. STRING OBJECT No No path The path of the input JSON that the function tokenizes. STRING No No fail.on.missing.attribute If this parameter is set to 'true' and a JSON is not provided in the given path, the event is dropped. If the parameter is set to 'false', the unavailability of a JSON in the specified path results in the event being created with a 'null' value for the json element. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path and the JSON. OBJECT Examples EXAMPLE 1 define stream InputStream (json string,path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select jsonElement insert into OutputStream; This query performs a tokenization for the given JSON using the path specified. If the specified path provides a JSON array, it generates events for each element in the specified json array by adding an additional attribute as the 'jsonElement' into the stream. e.g., jsonInput - {name:\"John\",enrolledSubjects:[\"Mathematics\",\"Physics\"]}, path - \" .enrolledSubjects\" /code br If we use the configuration in the above example, it generates two events with the attributes \"Mathematics\" and \"Physics\". br If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream br code e.g., jsonInput - {name:\"John\",age:25}, path - \" .enrolledSubjects\" </code><br>If we use the configuration in the above example, it generates two events with the attributes \"Mathematics\" and \"Physics\".<br>If the specified path provides a single json element, it adds the specified json element as an additional attribute named 'jsonElement' into the stream <br><code> e.g., jsonInput - {name:\"John\",age:25}, path - \" .age\"","title":"tokenizeAsObject (Stream Processor)"},{"location":"docs/api/latest/#math","text":"","title":"Math"},{"location":"docs/api/latest/#percentile-aggregate-function","text":"This functions returns the pth percentile value of a given argument. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:percentile( INT|LONG|FLOAT|DOUBLE arg, DOUBLE p) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value of the parameter whose percentile should be found. INT LONG FLOAT DOUBLE No No p Estimate of the percentile to be found (pth percentile) where p is any number greater than 0 or lesser than or equal to 100. DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (sensorId int, temperature double); from InValueStream select math:percentile(temperature, 97.0) as percentile insert into OutMediationStream; This function returns the percentile value based on the argument given. For example, math:percentile(temperature, 97.0) returns the 97 th percentile value of all the temperature events.","title":"percentile (Aggregate Function)"},{"location":"docs/api/latest/#abs-function","text":"This function returns the absolute value of the given parameter. It wraps the java.lang.Math.abs() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:abs( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The parameter whose absolute value is found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:abs(inValue) as absValue insert into OutMediationStream; Irrespective of whether the 'invalue' in the input stream holds a value of abs(3) or abs(-3),the function returns 3 since the absolute value of both 3 and -3 is 3. The result directed to OutMediationStream stream.","title":"abs (Function)"},{"location":"docs/api/latest/#acos-function","text":"If -1 = p1 = 1, this function returns the arc-cosine (inverse cosine) value of p1.If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.acos() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:acos( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-cosine (inverse cosine) value is found. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:acos(inValue) as acosValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-cosine value of it and returns the arc-cosine value to the output stream, OutMediationStream. For example, acos(0.5) returns 1.0471975511965979.","title":"acos (Function)"},{"location":"docs/api/latest/#asin-function","text":"If -1 = p1 = 1, this function returns the arc-sin (inverse sine) value of p1. If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.asin() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:asin( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-sin (inverse sine) value is found. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:asin(inValue) as asinValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-sin value of it and returns the arc-sin value to the output stream, OutMediationStream. For example, asin(0.5) returns 0.5235987755982989.","title":"asin (Function)"},{"location":"docs/api/latest/#atan-function","text":"1. If a single p1 is received, this function returns the arc-tangent (inverse tangent) value of p1 . 2. If p1 is received along with an optional p1 , it considers them as x and y coordinates and returns the arc-tangent (inverse tangent) value. The returned value is in radian scale. This function wraps the java.lang.Math.atan() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-tangent (inverse tangent) is found. If the optional second parameter is given this represents the x coordinate of the (x,y) coordinate pair. INT LONG FLOAT DOUBLE No No p1 This optional parameter represents the y coordinate of the (x,y) coordinate pair. 0D INT LONG FLOAT DOUBLE Yes No Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:atan(inValue1, inValue2) as convertedValue insert into OutMediationStream; If the 'inValue1' in the input stream is given, the function calculates the arc-tangent value of it and returns the arc-tangent value to the output stream, OutMediationStream. If both the 'inValue1' and 'inValue2' are given, then the function considers them to be x and y coordinates respectively and returns the calculated arc-tangent value to the output stream, OutMediationStream. For example, atan(12d, 5d) returns 1.1760052070951352.","title":"atan (Function)"},{"location":"docs/api/latest/#bin-function","text":"This function returns a string representation of the p1 argument, that is of either 'integer' or 'long' data type, as an unsigned integer in base 2. It wraps the java.lang.Integer.toBinaryString and java.lang.Long.toBinaryString` methods. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:bin( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value in either 'integer' or 'long', that should be converted into an unsigned integer of base 2. INT LONG No No Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:bin(inValue) as binValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function converts it into an unsigned integer in base 2 and directs the output to the output stream, OutMediationStream. For example, bin(9) returns '1001'.","title":"bin (Function)"},{"location":"docs/api/latest/#cbrt-function","text":"This function returns the cube-root of 'p1' which is in radians. It wraps the java.lang.Math.cbrt() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:cbrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cube-root should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cbrt(inValue) as cbrtValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cube-root value for the same and directs the output to the output stream, OutMediationStream. For example, cbrt(17d) returns 2.5712815906582356.","title":"cbrt (Function)"},{"location":"docs/api/latest/#ceil-function","text":"This function returns the smallest double value, i.e., the closest to the negative infinity, that is greater than or equal to the p1 argument, and is equal to a mathematical integer. It wraps the java.lang.Math.ceil() method. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:ceil( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose ceiling value is found. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ceil(inValue) as ceilingValue insert into OutMediationStream; This function calculates the ceiling value of the given 'inValue' and directs the result to 'OutMediationStream' output stream. For example, ceil(423.187d) returns 424.0.","title":"ceil (Function)"},{"location":"docs/api/latest/#conv-function","text":"This function converts a from the fromBase base to the toBase base. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:conv( STRING a, INT from.base, INT to.base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic a The value whose base should be changed. Input should be given as a 'String'. STRING No No from.base The source base of the input parameter 'a'. INT No No to.base The target base that the input parameter 'a' should be converted into. INT No No Examples EXAMPLE 1 define stream InValueStream (inValue string,fromBase int,toBase int); from InValueStream select math:conv(inValue,fromBase,toBase) as convertedValue insert into OutMediationStream; If the 'inValue' in the input stream is given, and the base in which it currently resides in and the base to which it should be converted to is specified, the function converts it into a string in the target base and directs it to the output stream, OutMediationStream. For example, conv(\"7f\", 16, 10) returns \"127\".","title":"conv (Function)"},{"location":"docs/api/latest/#copysign-function","text":"This function returns a value of an input with the received magnitude and sign of another input. It wraps the java.lang.Math.copySign() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:copySign( INT|LONG|FLOAT|DOUBLE magnitude, INT|LONG|FLOAT|DOUBLE sign) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic magnitude The magnitude of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No No sign The sign of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:copySign(inValue1,inValue2) as copysignValue insert into OutMediationStream; If two values are provided as 'inValue1' and 'inValue2', the function copies the magnitude and sign of the second argument into the first one and directs the result to the output stream, OutMediatonStream. For example, copySign(5.6d, -3.0d) returns -5.6.","title":"copySign (Function)"},{"location":"docs/api/latest/#cos-function","text":"This function returns the cosine of p1 which is in radians. It wraps the java.lang.Math.cos() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:cos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cosine value should be found.The input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cos(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cos(6d) returns 0.9601702866503661.","title":"cos (Function)"},{"location":"docs/api/latest/#cosh-function","text":"This function returns the hyperbolic cosine of p1 which is in radians. It wraps the java.lang.Math.cosh() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:cosh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic cosine should be found. The input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cosh(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the hyperbolic cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cosh (6d) returns 201.7156361224559.","title":"cosh (Function)"},{"location":"docs/api/latest/#e-function","text":"This function returns the java.lang.Math.E constant, which is the closest double value to e, where e is the base of the natural logarithms. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:e() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:e() as eValue insert into OutMediationStream; This function returns the constant, 2.7182818284590452354 which is the closest double value to e and directs the output to 'OutMediationStream' output stream.","title":"e (Function)"},{"location":"docs/api/latest/#exp-function","text":"This function returns the Euler's number e raised to the power of p1 . It wraps the java.lang.Math.exp() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:exp( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The power that the Euler's number e is raised to. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:exp(inValue) as expValue insert into OutMediationStream; If the 'inValue' in the inputstream holds a value, this function calculates the corresponding Euler's number 'e' and directs it to the output stream, OutMediationStream. For example, exp(10.23) returns 27722.51006805505.","title":"exp (Function)"},{"location":"docs/api/latest/#floor-function","text":"This function wraps the java.lang.Math.floor() function and returns the largest value, i.e., closest to the positive infinity, that is less than or equal to p1 , and is equal to a mathematical integer. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:floor( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose floor value should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:floor(inValue) as floorValue insert into OutMediationStream; This function calculates the floor value of the given 'inValue' input and directs the output to the 'OutMediationStream' output stream. For example, (10.23) returns 10.0.","title":"floor (Function)"},{"location":"docs/api/latest/#getexponent-function","text":"This function returns the unbiased exponent that is used in the representation of p1 . This function wraps the java.lang.Math.getExponent() function. Origin: siddhi-execution-math:5.0.0 Syntax INT math:getExponent( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of whose unbiased exponent representation should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:getExponent(inValue) as expValue insert into OutMediationStream; This function calculates the unbiased exponent of a given input, 'inValue' and directs the result to the 'OutMediationStream' output stream. For example, getExponent(60984.1) returns 15.","title":"getExponent (Function)"},{"location":"docs/api/latest/#hex-function","text":"This function wraps the java.lang.Double.toHexString() function. It returns a hexadecimal string representation of the input, p1`. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:hex( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hexadecimal value should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue int); from InValueStream select math:hex(inValue) as hexString insert into OutMediationStream; If the 'inValue' in the input stream is provided, the function converts this into its corresponding hexadecimal format and directs the output to the output stream, OutMediationStream. For example, hex(200) returns \"c8\".","title":"hex (Function)"},{"location":"docs/api/latest/#isinfinite-function","text":"This function wraps the java.lang.Float.isInfinite() and java.lang.Double.isInfinite() and returns true if p1 is infinitely large in magnitude and false if otherwise. Origin: siddhi-execution-math:5.0.0 Syntax BOOL math:isInfinite( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 This is the value of the parameter that the function determines to be either infinite or finite. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isInfinite(inValue1) as isInfinite insert into OutMediationStream; If the value given in the 'inValue' in the input stream is of infinitely large magnitude, the function returns the value, 'true' and directs the result to the output stream, OutMediationStream'. For example, isInfinite(java.lang.Double.POSITIVE_INFINITY) returns true.","title":"isInfinite (Function)"},{"location":"docs/api/latest/#isnan-function","text":"This function wraps the java.lang.Float.isNaN() and java.lang.Double.isNaN() functions and returns true if p1 is NaN (Not-a-Number), and returns false if otherwise. Origin: siddhi-execution-math:5.0.0 Syntax BOOL math:isNan( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter which the function determines to be either NaN or a number. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isNan(inValue1) as isNaN insert into OutMediationStream; If the 'inValue1' in the input stream has a value that is undefined, then the function considers it as an 'NaN' value and directs 'True' to the output stream, OutMediationStream. For example, isNan(java.lang.Math.log(-12d)) returns true.","title":"isNan (Function)"},{"location":"docs/api/latest/#ln-function","text":"This function returns the natural logarithm (base e) of p1 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:ln( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose natural logarithm (base e) should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ln(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates its natural logarithm (base e) and directs the results to the output stream, 'OutMeditionStream'. For example, ln(11.453) returns 2.438251704415579.","title":"ln (Function)"},{"location":"docs/api/latest/#log-function","text":"This function returns the logarithm of the received number as per the given base . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:log( INT|LONG|FLOAT|DOUBLE number, INT|LONG|FLOAT|DOUBLE base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic number The value of the parameter whose base should be changed. INT LONG FLOAT DOUBLE No No base The base value of the ouput. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (number double, base double); from InValueStream select math:log(number, base) as logValue insert into OutMediationStream; If the number and the base to which it has to be converted into is given in the input stream, the function calculates the number to the base specified and directs the result to the output stream, OutMediationStream. For example, log(34, 2f) returns 5.08746284125034.","title":"log (Function)"},{"location":"docs/api/latest/#log10-function","text":"This function returns the base 10 logarithm of p1 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:log10( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 10 logarithm should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log10(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 10 logarithm of the same and directs the result to the output stream, OutMediatioStream. For example, log10(19.234) returns 1.2840696117100832.","title":"log10 (Function)"},{"location":"docs/api/latest/#log2-function","text":"This function returns the base 2 logarithm of p1 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:log2( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 2 logarithm should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log2(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 2 logarithm of the same and returns the value to the output stream, OutMediationStream. For example log2(91d) returns 6.507794640198696.","title":"log2 (Function)"},{"location":"docs/api/latest/#max-function","text":"This function returns the greater value of p1 and p2 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:max( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values to be compared in order to find the larger value of the two INT LONG FLOAT DOUBLE No No p2 The input value to be compared with 'p1' in order to find the larger value of the two. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:max(inValue1,inValue2) as maxValue insert into OutMediationStream; If two input values 'inValue1, and 'inValue2' are given, the function compares them and directs the larger value to the output stream, OutMediationStream. For example, max(123.67d, 91) returns 123.67.","title":"max (Function)"},{"location":"docs/api/latest/#min-function","text":"This function returns the smaller value of p1 and p2 . Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:min( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values that are to be compared in order to find the smaller value. INT LONG FLOAT DOUBLE No No p2 The input value that is to be compared with 'p1' in order to find the smaller value. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:min(inValue1,inValue2) as minValue insert into OutMediationStream; If two input values, 'inValue1' and 'inValue2' are given, the function compares them and directs the smaller value of the two to the output stream, OutMediationStream. For example, min(123.67d, 91) returns 91.","title":"min (Function)"},{"location":"docs/api/latest/#oct-function","text":"This function converts the input parameter p1 to octal. Origin: siddhi-execution-math:5.0.0 Syntax STRING math:oct( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose octal representation should be found. INT LONG No No Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:oct(inValue) as octValue insert into OutMediationStream; If the 'inValue' in the input stream is given, this function calculates the octal value corresponding to the same and directs it to the output stream, OutMediationStream. For example, oct(99l) returns \"143\".","title":"oct (Function)"},{"location":"docs/api/latest/#parsedouble-function","text":"This function returns the double value of the string received. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:parseDouble( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a double value. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseDouble(inValue) as output insert into OutMediationStream; If the 'inValue' in the input stream holds a value, this function converts it into the corresponding double value and directs it to the output stream, OutMediationStream. For example, parseDouble(\"123\") returns 123.0.","title":"parseDouble (Function)"},{"location":"docs/api/latest/#parsefloat-function","text":"This function returns the float value of the received string. Origin: siddhi-execution-math:5.0.0 Syntax FLOAT math:parseFloat( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a float value. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseFloat(inValue) as output insert into OutMediationStream; The function converts the input value given in 'inValue',into its corresponding float value and directs the result into the output stream, OutMediationStream. For example, parseFloat(\"123\") returns 123.0.","title":"parseFloat (Function)"},{"location":"docs/api/latest/#parseint-function","text":"This function returns the integer value of the received string. Origin: siddhi-execution-math:5.0.0 Syntax INT math:parseInt( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to an integer. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseInt(inValue) as output insert into OutMediationStream; The function converts the 'inValue' into its corresponding integer value and directs the output to the output stream, OutMediationStream. For example, parseInt(\"123\") returns 123.","title":"parseInt (Function)"},{"location":"docs/api/latest/#parselong-function","text":"This function returns the long value of the string received. Origin: siddhi-execution-math:5.0.0 Syntax LONG math:parseLong( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to a long value. STRING No No Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseLong(inValue) as output insert into OutMediationStream; The function converts the 'inValue' to its corresponding long value and directs the result to the output stream, OutMediationStream. For example, parseLong(\"123\") returns 123.","title":"parseLong (Function)"},{"location":"docs/api/latest/#pi-function","text":"This function returns the java.lang.Math.PI constant, which is the closest value to pi, i.e., the ratio of the circumference of a circle to its diameter. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:pi() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:pi() as piValue insert into OutMediationStream; pi() always returns 3.141592653589793.","title":"pi (Function)"},{"location":"docs/api/latest/#power-function","text":"This function raises the given value to a given power. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:power( INT|LONG|FLOAT|DOUBLE value, INT|LONG|FLOAT|DOUBLE to.power) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value The value that should be raised to the power of 'to.power' input parameter. INT LONG FLOAT DOUBLE No No to.power The power to which the 'value' input parameter should be raised. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:power(inValue1,inValue2) as powerValue insert into OutMediationStream; This function raises the 'inValue1' to the power of 'inValue2' and directs the output to the output stream, 'OutMediationStream. For example, (5.6d, 3.0d) returns 175.61599999999996.","title":"power (Function)"},{"location":"docs/api/latest/#rand-function","text":"This returns a stream of pseudo-random numbers when a sequence of calls are sent to the rand() . Optionally, it is possible to define a seed, i.e., rand(seed) using which the pseudo-random numbers are generated. These functions internally use the java.util.Random class. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:rand( INT|LONG seed) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic seed An optional seed value that will be used to generate the random number sequence. defaultSeed INT LONG Yes No Examples EXAMPLE 1 define stream InValueStream (symbol string, price long, volume long); from InValueStream select symbol, math:rand() as randNumber select math:oct(inValue) as octValue insert into OutMediationStream; In the example given above, a random double value between 0 and 1 will be generated using math:rand().","title":"rand (Function)"},{"location":"docs/api/latest/#round-function","text":"This function returns the value of the input argument rounded off to the closest integer/long value. Origin: siddhi-execution-math:5.0.0 Syntax INT|LONG math:round( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be rounded off to the closest integer/long value. FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:round(inValue) as roundValue insert into OutMediationStream; The function rounds off 'inValue1' to the closest int/long value and directs the output to the output stream, 'OutMediationStream'. For example, round(3252.353) returns 3252.","title":"round (Function)"},{"location":"docs/api/latest/#signum-function","text":"This returns +1, 0, or -1 for the given positive, zero and negative values respectively. This function wraps the java.lang.Math.signum() function. Origin: siddhi-execution-math:5.0.0 Syntax INT math:signum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be checked to be positive, negative or zero. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:signum(inValue) as sign insert into OutMediationStream; The function evaluates the 'inValue' given to be positive, negative or zero and directs the result to the output stream, 'OutMediationStream'. For example, signum(-6.32d) returns -1.","title":"signum (Function)"},{"location":"docs/api/latest/#sin-function","text":"This returns the sine of the value given in radians. This function wraps the java.lang.Math.sin() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:sin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sin(inValue) as sinValue insert into OutMediationStream; The function calculates the sine value of the given 'inValue' and directs the output to the output stream, 'OutMediationStream. For example, sin(6d) returns -0.27941549819892586.","title":"sin (Function)"},{"location":"docs/api/latest/#sinh-function","text":"This returns the hyperbolic sine of the value given in radians. This function wraps the java.lang.Math.sinh() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:sinh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sinh(inValue) as sinhValue insert into OutMediationStream; This function calculates the hyperbolic sine value of 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sinh(6d) returns 201.71315737027922.","title":"sinh (Function)"},{"location":"docs/api/latest/#sqrt-function","text":"This function returns the square-root of the given value. It wraps the java.lang.Math.sqrt() s function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:sqrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose square-root value should be found. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sqrt(inValue) as sqrtValue insert into OutMediationStream; The function calculates the square-root value of the 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sqrt(4d) returns 2.","title":"sqrt (Function)"},{"location":"docs/api/latest/#tan-function","text":"This function returns the tan of the given value in radians. It wraps the java.lang.Math.tan() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:tan( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose tan value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tan(inValue) as tanValue insert into OutMediationStream; This function calculates the tan value of the 'inValue' given and directs the output to the output stream, 'OutMediationStream'. For example, tan(6d) returns -0.29100619138474915.","title":"tan (Function)"},{"location":"docs/api/latest/#tanh-function","text":"This function returns the hyperbolic tangent of the value given in radians. It wraps the java.lang.Math.tanh() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:tanh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic tangent value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tanh(inValue) as tanhValue insert into OutMediationStream; If the 'inVaue' in the input stream is given, this function calculates the hyperbolic tangent value of the same and directs the output to 'OutMediationStream' stream. For example, tanh(6d) returns 0.9999877116507956.","title":"tanh (Function)"},{"location":"docs/api/latest/#todegrees-function","text":"This function converts the value given in radians to degrees. It wraps the java.lang.Math.toDegrees() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:toDegrees( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in radians that should be converted to degrees. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toDegrees(inValue) as degreesValue insert into OutMediationStream; The function converts the 'inValue' in the input stream from radians to degrees and directs the output to 'OutMediationStream' output stream. For example, toDegrees(6d) returns 343.77467707849394.","title":"toDegrees (Function)"},{"location":"docs/api/latest/#toradians-function","text":"This function converts the value given in degrees to radians. It wraps the java.lang.Math.toRadians() function. Origin: siddhi-execution-math:5.0.0 Syntax DOUBLE math:toRadians( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in degrees that should be converted to radians. INT LONG FLOAT DOUBLE No No Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toRadians(inValue) as radiansValue insert into OutMediationStream; This function converts the input, from degrees to radians and directs the result to 'OutMediationStream' output stream. For example, toRadians(6d) returns 0.10471975511965977.","title":"toRadians (Function)"},{"location":"docs/api/latest/#rdbms","text":"","title":"Rdbms"},{"location":"docs/api/latest/#cud-stream-processor","text":"This function performs SQL CUD (INSERT, UPDATE, DELETE) queries on WSO2 datasources. Note: This function is only available when running Siddhi with WSO2 SP. Origin: siddhi-store-rdbms:6.0.0 Syntax rdbms:cud( STRING datasource.name, STRING query, STRING parameter.n) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the WSO2 datasource for which the query should be performed. STRING No No query The update, delete, or insert query(formatted according to the relevant database type) that needs to be performed. STRING No No parameter.n If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING No No System Parameters Name Description Default Value Possible Parameters perform.CUD.operations If this parameter is set to 'true', the RDBMS CUD function is enabled to perform CUD operations. false true false Extra Return Attributes Name Description Possible Types numRecords The number of records manipulated by the query. INT Examples EXAMPLE 1 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName='abc' where customerName='xyz'\") select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. EXAMPLE 2 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName=? where customerName=?\", changedName, previousName) select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. Here the values of attributes changedName and previousName in the event will be set to the query.","title":"cud (Stream Processor)"},{"location":"docs/api/latest/#query-stream-processor","text":"This function performs SQL retrieval queries on WSO2 datasources. Note: This function is only available when running Siddhi with WSO2 SP. Origin: siddhi-store-rdbms:6.0.0 Syntax rdbms:query( STRING datasource.name, STRING query, STRING parameter.n, STRING attribute.definition.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the WSO2 datasource for which the query should be performed. STRING No No query The select query(formatted according to the relevant database type) that needs to be performed STRING No No parameter.n If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING No No attribute.definition.list This is provided as a comma-separated list in the ' AttributeName AttributeType ' format. The SQL query is expected to return the attributes in the given order. e.g., If one attribute is defined here, the SQL query should return one column result set. If more than one column is returned, then the first column is processed. The Siddhi data types supported are 'STRING', 'INT', 'LONG', 'DOUBLE', 'FLOAT', and 'BOOL'. Mapping of the Siddhi data type to the database data type can be done as follows, Siddhi Datatype - Datasource Datatype STRING - CHAR , VARCHAR , LONGVARCHAR INT - INTEGER LONG - BIGINT DOUBLE - DOUBLE FLOAT - REAL BOOL - BIT STRING No No Extra Return Attributes Name Description Possible Types attributeName The return attributes will be the ones defined in the parameter attribute.definition.list . STRING INT LONG DOUBLE FLOAT BOOL Examples EXAMPLE 1 from TriggerStream#rdbms:query('SAMPLE_DB', 'select * from Transactions_Table', 'creditcardno string, country string, transaction string, amount int') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). EXAMPLE 2 from TriggerStream#rdbms:query('SAMPLE_DB', 'select * from where country=? ', countrySearchWord, 'creditcardno string, country string, transaction string, amount int') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). countrySearchWord value from the event will be set in the query when querying the datasource.","title":"query (Stream Processor)"},{"location":"docs/api/latest/#regex","text":"","title":"Regex"},{"location":"docs/api/latest/#find-function","text":"These methods attempt to find the subsequence of the 'inputSequence' that matches the given 'regex' pattern. Origin: siddhi-execution-regex:5.0.0 Syntax BOOL regex:find( STRING regex, STRING input.sequence, INT starting.index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression that is matched to a sequence in order to find the subsequence of the same. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No starting.index The starting index of the input sequence from where the input sequence ismatched with the given regex pattern. eg: 1, 2. INT No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string); from InputStream select inputSequence , regex:find(\\d\\d(.*)WSO2, 21 products are produced by WSO2 currently) as aboutWSO2 insert into OutputStream; This method attempts to find the subsequence of the 'inputSequence' that matches the regex pattern, \\d\\d(.*)WSO2. It returns true as a subsequence exists. EXAMPLE 2 define stream InputStream (inputSequence string, price long, regex string); from InputStream select inputSequence , regex:find(\\d\\d(.*)WSO2, 21 products are produced currently) as aboutWSO2 insert into OutputStream; This method attempts to find the subsequence of the 'inputSequence' that matches the regex pattern, \\d\\d(.*)WSO2 . It returns 'false' as a subsequence does not exist. EXAMPLE 3 define stream InputStream (inputSequence string, price long, regex string); from InputStream select inputSequence , regex:find(\\d\\d(.*)WSO2, 21 products are produced within 10 years by WSO2 currently by WSO2 employees, 30) as aboutWSO2 insert into OutputStream; This method attempts to find the subsequence of the 'inputSequence' that matches the regex pattern, \\d\\d(.*)WSO2 starting from index 30. It returns 'true' since a subsequence exists.","title":"find (Function)"},{"location":"docs/api/latest/#group-function","text":"This method returns the input subsequence captured by the given group during the previous match operation. Origin: siddhi-execution-regex:5.0.0 Syntax STRING regex:group( STRING regex, STRING input.sequence, INT group.id) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No group.id The given group id of the regex expression. For example, 0, 1, 2, etc. INT No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:group(\\d\\d(.*)(WSO2.*), 21 products are produced within 10 years by WSO2 currently by WSO2 employees, 3) insert into OutputStream; This function returns 'WSO2 employees', the input subsequence captured within the given groupID, 3 after grouping the 'inputSequence' according to the regex pattern, \\d\\d(. )(WSO2. ).","title":"group (Function)"},{"location":"docs/api/latest/#lookingat-function","text":"This method attempts to match the 'inputSequence', from the beginning, against the 'regex' pattern. Origin: siddhi-execution-regex:5.0.0 Syntax BOOL regex:lookingAt( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:lookingAt(\\d\\d(.*)(WSO2.*), 21 products are produced by WSO2 currently in Sri Lanka) This method attempts to match the 'inputSequence' against the regex pattern, \\d\\d(. )(WSO2. ) from the beginning. Since it matches, the function returns 'true'. EXAMPLE 2 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:lookingAt(WSO2(.*)middleware(.*), sample test string and WSO2 is situated in trace and it's a middleware company) This method attempts to match the 'inputSequence' against the regex pattern, WSO2(. )middleware(. ) from the beginning. Since it does not match, the function returns false.","title":"lookingAt (Function)"},{"location":"docs/api/latest/#matches-function","text":"This method attempts to match the entire 'inputSequence' against the 'regex' pattern. Origin: siddhi-execution-regex:5.0.0 Syntax BOOL regex:matches( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No No input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2. STRING No No Examples EXAMPLE 1 define stream InputStream (inputSequence string, price long, regex string, group int); from InputStream select inputSequence, regex:matches(WSO2(.*)middleware(.*), WSO2 is situated in trace and its a middleware company) This method attempts to match the entire 'inputSequence' against WSO2(. )middleware(. ) regex pattern. Since it matches, it returns 'true'. EXAMPLE 2 define stream inputStream (inputSequence string, price long, regex string, group int); from inputStream select inputSequence, regex:matches(WSO2(.*)middleware, WSO2 is situated in trace and its a middleware company) This method attempts to match the entire 'inputSequence' against WSO2(.*)middleware regex pattern. Since it does not match, it returns 'false'.","title":"matches (Function)"},{"location":"docs/api/latest/#sink","text":"","title":"Sink"},{"location":"docs/api/latest/#email-sink","text":"The email sink uses the 'smtp' server to publish events via emails. The events can be published in 'text', 'xml' or 'json' formats. The user can define email sink parameters in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or in the stream definition. The email sink first checks the stream definition for parameters, and if they are no configured there, it checks the 'deployment.yaml' file. If the parameters are not configured in either place, default values are considered for optional parameters. If you need to configure server system parameters that are not provided as options in the stream definition, then those parameters need to be defined them in the 'deployment.yaml' file under 'email sink properties'. For more information about the SMTP server parameters, see https://javaee.github.io/javamail/SMTP-Transport. Further, some email accounts are required to enable the 'access to less secure apps' option. For gmail accounts, you can enable this option via https://myaccount.google.com/lesssecureapps. Origin: siddhi-io-email:2.0.1 Syntax @sink(type=\"email\", username=\" STRING \", address=\" STRING \", password=\" STRING \", host=\" STRING \", port=\" INT \", ssl.enable=\" BOOL \", auth=\" BOOL \", content.type=\" STRING \", subject=\" STRING \", to=\" STRING \", cc=\" STRING \", bcc=\" STRING \", attachments=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The username of the email account that is used to send emails. e.g., 'abc' is the username of the 'abc@gmail.com' account. STRING No No address The address of the email account that is used to send emails. STRING No No password The password of the email account. STRING No No host The host name of the SMTP server. e.g., 'smtp.gmail.com' is a host name for a gmail account. The default value 'smtp.gmail.com' is only valid if the email account is a gmail account. smtp.gmail.com STRING Yes No port The port that is used to create the connection. '465' the default value is only valid is SSL is enabled. INT Yes No ssl.enable This parameter specifies whether the connection should be established via a secure connection or not. The value can be either 'true' or 'false'. If it is 'true', then the connection is establish via the 493 port which is a secure connection. true BOOL Yes No auth This parameter specifies whether to use the 'AUTH' command when authenticating or not. If the parameter is set to 'true', an attempt is made to authenticate the user using the 'AUTH' command. true BOOL Yes No content.type The content type can be either 'text/plain' or 'text/html'. text/plain STRING Yes No subject The subject of the mail to be send. STRING No Yes to The address of the 'to' recipient. If there are more than one 'to' recipients, then all the required addresses can be given as a comma-separated list. STRING No Yes cc The address of the 'cc' recipient. If there are more than one 'cc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No bcc The address of the 'bcc' recipient. If there are more than one 'bcc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No attachments File paths of the files that need to be attached to the email. These paths should be absolute paths. They can be either directories or files . If the path is to a directory, all the files located at the first level (i.e., not within another sub directory) are attached. None STRING Yes Yes System Parameters Name Description Default Value Possible Parameters mail.smtp.ssl.trust If this parameter is se, and a socket factory has not been specified, it enables the use of a MailSSLSocketFactory. If this parameter is set to \" \", all the hosts are trusted. If it is set to a whitespace-separated list of hosts, only those specified hosts are trusted. If not, the hosts trusted depends on the certificate presented by the server. String mail.smtp.connectiontimeout The socket connection timeout value in milliseconds. infinite timeout Any Integer mail.smtp.timeout The socket I/O timeout value in milliseconds. infinite timeout Any Integer mail.smtp.from The email address to use for the SMTP MAIL command. This sets the envelope return address. Defaults to msg.getFrom() or InternetAddress.getLocalAddress(). Any valid email address mail.smtp.localport The local port number to bind to when creating the SMTP socket. Defaults to the port number picked by the Socket class. Any Integer mail.smtp.ehlo If this parameter is set to 'false', you must not attempt to sign in with the EHLO command. true true or false mail.smtp.auth.login.disable If this is set to 'true', it is not allowed to use the 'AUTH LOGIN' command. false true or false mail.smtp.auth.plain.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH PLAIN' command. false true or false mail.smtp.auth.digest-md5.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH DIGEST-MD5' command. false true or false mail.smtp.auth.ntlm.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH NTLM' command false true or false mail.smtp.auth.ntlm.domain The NTLM authentication domain. None The valid NTLM authentication domain name. mail.smtp.auth.ntlm.flags NTLM protocol-specific flags. For more details, see http://curl.haxx.se/rfc/ntlm.html#theNtlmFlags. None Valid NTLM protocol-specific flags. mail.smtp.dsn.notify The NOTIFY option to the RCPT command. None Either 'NEVER', or a combination of 'SUCCESS', 'FAILURE', and 'DELAY' (separated by commas). mail.smtp.dsn.ret The 'RET' option to the 'MAIL' command. None Either 'FULL' or 'HDRS'. mail.smtp.sendpartial If this parameter is set to 'true' and a message is addressed to both valid and invalid addresses, the message is sent with a log that reports the partial failure with a 'SendFailedException' error. If this parameter is set to 'false' (which is default), the message is not sent to any of the recipients when the recipient lists contain one or more invalid addresses. false true or false mail.smtp.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.smtp.sasl.mechanisms Enter a space or a comma-separated list of SASL mechanism names that the system shouldt try to use. None mail.smtp.sasl.authorizationid The authorization ID to be used in the SASL authentication. If no value is specified, the authentication ID (i.e., username) is used. username Valid ID mail.smtp.sasl.realm The realm to be used with the 'DIGEST-MD5' authentication. None mail.smtp.quitwait If this parameter is set to 'false', the 'QUIT' command is issued and the connection is immediately closed. If this parameter is set to 'true' (which is default), the transport waits for the response to the QUIT command. false true or false mail.smtp.reportsuccess If this parameter is set to 'true', the transport to includes an 'SMTPAddressSucceededException' for each address to which the message is successfully delivered. false true or false mail.smtp.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create SMTP sockets. None Socket Factory mail.smtp.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory interface'. This class is used to create SMTP sockets. None mail.smtp.socketFactory.fallback If this parameter is set to 'true', the failure to create a socket using the specified socket factory class causes the socket to be created using the 'java.net.Socket' class. true true or false mail.smtp.socketFactory.port This specifies the port to connect to when using the specified socket factory. 25 Valid port number mail.smtp.ssl.protocols This specifies the SSL protocols that need to be enabled for the SSL connections. None This parameter specifies a whitespace separated list of tokens that are acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. mail.smtp.starttls.enable If this parameter is set to 'true', it is possible to issue the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.smtp.starttls.required If this parameter is set to 'true', it is required to use the 'STARTTLS' command. If the server does not support the 'STARTTLS' command, or if the command fails, the connection method will fail. false true or false mail.smtp.socks.host This specifies the host name of a SOCKS5 proxy server to be used for the connections to the mail server. None mail.smtp.socks.port This specifies the port number for the SOCKS5 proxy server. This needs to be used only if the proxy server is not using the standard port number 1080. 1080 valid port number mail.smtp.auth.ntlm.disable If this parameter is set to 'true', the AUTH NTLM command cannot be issued. false true or false mail.smtp.mailextension The extension string to be appended to the MAIL command. None mail.smtp.userset If this parameter is set to 'true', you should use the 'RSET' command instead of the 'NOOP' command in the 'isConnected' method. In some scenarios, 'sendmail' responds slowly after many 'NOOP' commands. This is avoided by using 'RSET' instead. false true or false Examples EXAMPLE 1 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to publish events via an email sink based on the values provided for the mandatory parameters. As shown in the example, it publishes events from the 'FooStream' in 'json' format as emails to the specified 'to' recipients via the email sink. The email is sent from the 'sender.account@gmail.com' email address via a secure connection. EXAMPLE 2 @sink(type='email', @map(type ='json'), subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to configure the query parameters and the system parameters in the 'deployment.yaml' file. Corresponding parameters need to be configured under 'email', and namespace:'sink' as follows: siddhi: extensions: - extension: name:'email' namespace:'sink' properties: username: sender's email username address: sender's email address password: sender's email password As shown in the example, events from the FooStream are published in 'json' format via the email sink as emails to the given 'to' recipients. The email is sent from the 'sender.account@gmail.com' address via a secure connection. EXAMPLE 3 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.com)define stream FooStream (name string, age int, country string); This example illustrates how to publish events via the email sink. Events from the 'FooStream' stream are published in 'xml' format via the email sink as a text/html message and sent to the specified 'to', 'cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value of the 'name' parameter in the corresponding output event. EXAMPLE 4 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.comattachments= '{{attachments}}')define stream FooStream (name string, age int, country string, attachments string); This example illustrates how to publish events via the email sink. Here, the email also contains attachments. Events from the FooStream are published in 'xml' format via the email sink as a 'text/html' message to the specified 'to','cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value for the 'name' parameter in the corresponding output event. The attachments included in the email message are the local files available in the path specified as the value for the 'attachments' attribute.","title":"email (Sink)"},{"location":"docs/api/latest/#http-sink","text":"This extension publish the HTTP events in any HTTP method POST, GET, PUT, DELETE via HTTP or https protocols. As the additional features this component can provide basic authentication as well as user can publish events using custom client truststore files when publishing events via https protocol. And also user can add any number of headers including HTTP_METHOD header for each event dynamically. Following content types will be set by default according to the type of sink mapper used. You can override them by setting the new content types in headers. - TEXT : text/plain - XML : application/xml - JSON : application/json - KEYVALUE : application/x-www-form-urlencoded Origin: siddhi-io-http:2.0.4 Syntax @sink(type=\"http\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", client.enable.session.creation=\" STRING \", follow.redirect=\" BOOL \", max.redirect.count=\" INT \", tls.store.type=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configuration=\" STRING \", client.bootstrap.nodelay=\" BOOL \", client.bootstrap.keepalive=\" BOOL \", client.bootstrap.sendbuffersize=\" INT \", client.bootstrap.recievebuffersize=\" INT \", client.bootstrap.connect.timeout=\" INT \", client.bootstrap.socket.reuse=\" BOOL \", client.bootstrap.socket.timeout=\" STRING \", client.threadpool.configurations=\" STRING \", client.connection.pool.count=\" INT \", client.max.active.connections.per.pool=\" INT \", client.min.idle.connections.per.pool=\" INT \", client.max.idle.connections.per.pool=\" INT \", client.min.eviction.idle.time=\" STRING \", sender.thread.count=\" STRING \", event.group.executor.thread.size=\" STRING \", max.wait.for.client.connection.pool=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", refresh.token=\" STRING \", token.url=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published via HTTP. This is a mandatory parameter and if this is not specified, an error is logged in the CLI. If user wants to enable SSL for the events, use https instead of http in the publisher.url.e.g., http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No basic.auth.username The username to be included in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No basic.auth.password The password to include in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No https.truststore.file The file path to the location of the truststore of the client that sends the HTTP events through 'https' protocol. A custom client-truststore can be specified if required. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified and the protocol of URL is 'https' then, the system uses default password. wso2carbon STRING Yes No headers The headers that should be included as HTTP request headers. There can be any number of headers concatenated in following format. \"'header1:value1','header2:value2'\". User can include Content-Type header if he needs to use a specific content-type for the payload. Or else, system decides the Content-Type by considering the type of sink mapper, in following way. - @map(xml):application/xml - @map(json):application/json - @map(text):plain/text ) - if user does not include any mapping type then the system gets 'plain/text' as default Content-Type header. Note that providing content-length as a header is not supported. The size of the payload will be automatically calculated and included in the content-length header. STRING Yes No method For HTTP events, HTTP_METHOD header should be included as a request header. If the parameter is null then system uses 'POST' as a default header. POST STRING Yes No socket.idle.timeout Socket timeout value in millisecond 6000 INT Yes No chunk.disabled This parameter is used to disable/enable chunked transfer encoding false BOOL Yes No ssl.protocol The SSL protocol version TLS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No client.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No follow.redirect Redirect related enabled. true BOOL Yes No max.redirect.count Maximum redirect count. 5 INT Yes No tls.store.type TLS store type to be used. JKS STRING Yes No proxy.host Proxy server host null STRING Yes No proxy.port Proxy server port null STRING Yes No proxy.username Proxy server username null STRING Yes No proxy.password Proxy server password null STRING Yes No client.bootstrap.configuration Client bootsrap configurations. Expected format of these parameters is as follows: \"'client.bootstrap.nodelay:xxx','client.bootstrap.keepalive:xxx'\" TODO STRING Yes No client.bootstrap.nodelay Http client no delay. true BOOL Yes No client.bootstrap.keepalive Http client keep alive. true BOOL Yes No client.bootstrap.sendbuffersize Http client send buffer size. 1048576 INT Yes No client.bootstrap.recievebuffersize Http client receive buffer size. 1048576 INT Yes No client.bootstrap.connect.timeout Http client connection timeout. 15000 INT Yes No client.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No client.bootstrap.socket.timeout Http client socket timeout. 15 STRING Yes No client.threadpool.configurations Thread pool configuration. Expected format of these parameters is as follows: \"'client.connection.pool.count:xxx','client.max.active.connections.per.pool:xxx'\" TODO STRING Yes No client.connection.pool.count Connection pool count. 0 INT Yes No client.max.active.connections.per.pool Active connections per pool. -1 INT Yes No client.min.idle.connections.per.pool Minimum ideal connection per pool. 0 INT Yes No client.max.idle.connections.per.pool Maximum ideal connection per pool. 100 INT Yes No client.min.eviction.idle.time Minimum eviction idle time. 5 * 60 * 1000 STRING Yes No sender.thread.count Http sender thread count. 20 STRING Yes No event.group.executor.thread.size Event group executor thread size. 15 STRING Yes No max.wait.for.client.connection.pool Maximum wait for client connection pool. 60000 STRING Yes No oauth.username The username to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No oauth.password The password to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No consumer.key consumer key for the Http request. It is only applicable for for Oauth requests STRING Yes No consumer.secret consumer secret for the Http request. It is only applicable for for Oauth requests STRING Yes No refresh.token refresh token for the Http request. It is only applicable for for Oauth requests STRING Yes No token.url token url for generate a new access token. It is only applicable for for Oauth requests STRING Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapBossGroupSize property to configure number of boss threads, which accepts incoming connections until the ports are unbound. Once connection accepts successfully, boss thread passes the accepted channel to one of the worker threads. Number of available processors Any integer clientBootstrapWorkerGroupSize property to configure number of worker threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer clientBootstrapClientGroupSize property to configure number of client threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client-truststore.jks trustStorePassword The default truststore password. wso2carbon Truststore password Examples EXAMPLE 1 @sink(type='http',publisher.url='http://localhost:8009/foo', method='{{method}}',headers=\"'content-type:xml','content-length:94'\", client.bootstrap.configuration=\"'client.bootstrap.socket.timeout:20', 'client.bootstrap.worker.group.size:10'\", client.pool.configuration=\"'client.connection.pool.count:10','client.max.active.connections.per.pool:1'\", @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody String, method string, headers string); If it is xml mapping expected input should be in following format for FooStream: { events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events , POST, Content-Length:24#Content-Location:USA#Retry-After:120 } Above event will generate output as below. ~Output http event payload events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events ~Output http event headers Content-Length:24, Content-Location:'USA', Retry-After:120, Content-Type:'application/xml', HTTP_METHOD:'POST', ~Output http event properties HTTP_METHOD:'POST', HOST:'localhost', PORT:8009, PROTOCOL:'http', TO:'/foo'","title":"http (Sink)"},{"location":"docs/api/latest/#http-request-sink","text":"This extension publish the HTTP events in any HTTP method POST, GET, PUT, DELETE via HTTP or https protocols. As the additional features this component can provide basic authentication as well as user can publish events using custom client truststore files when publishing events via https protocol. And also user can add any number of headers including HTTP_METHOD header for each event dynamically. Following content types will be set by default according to the type of sink mapper used. You can override them by setting the new content types in headers. - TEXT : text/plain - XML : application/xml - JSON : application/json - KEYVALUE : application/x-www-form-urlencoded HTTP request sink is correlated with the The HTTP reponse source, through a unique sink.id .It sends the request to the defined url and the response is received by the response source which has the same 'sink.id'. Origin: siddhi-io-http:2.0.4 Syntax @sink(type=\"http-request\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", client.enable.session.creation=\" STRING \", follow.redirect=\" BOOL \", max.redirect.count=\" INT \", tls.store.type=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configuration=\" STRING \", client.bootstrap.nodelay=\" BOOL \", client.bootstrap.keepalive=\" BOOL \", client.bootstrap.sendbuffersize=\" INT \", client.bootstrap.recievebuffersize=\" INT \", client.bootstrap.connect.timeout=\" INT \", client.bootstrap.socket.reuse=\" BOOL \", client.bootstrap.socket.timeout=\" STRING \", client.threadpool.configurations=\" STRING \", client.connection.pool.count=\" INT \", client.max.active.connections.per.pool=\" INT \", client.min.idle.connections.per.pool=\" INT \", client.max.idle.connections.per.pool=\" INT \", client.min.eviction.idle.time=\" STRING \", sender.thread.count=\" STRING \", event.group.executor.thread.size=\" STRING \", max.wait.for.client.connection.pool=\" STRING \", sink.id=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", refresh.token=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published via HTTP. This is a mandatory parameter and if this is not specified, an error is logged in the CLI. If user wants to enable SSL for the events, use https instead of http in the publisher.url. e.g., http://localhost:8080/endpoint , https://localhost:8080/endpoint This can be used as a dynamic parameter as well. STRING No Yes basic.auth.username The username to be included in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No basic.auth.password The password to include in the authentication header of the basic authentication enabled events. It is required to specify both username and password to enable basic authentication. If one of the parameter is not given by user then an error is logged in the CLI. STRING Yes No https.truststore.file The file path to the location of the truststore of the client that sends the HTTP events through 'https' protocol. A custom client-truststore can be specified if required. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified and the protocol of URL is 'https' then, the system uses default password. wso2carbon STRING Yes No headers The headers that should be included as HTTP request headers. There can be any number of headers concatenated in following format. \"'header1:value1','header2:value2'\". User can include Content-Type header if he needs to use a specific content-type for the payload. Or else, system decides the Content-Type by considering the type of sink mapper, in following way. - @map(xml):application/xml - @map(json):application/json - @map(text):plain/text ) - if user does not include any mapping type then the system gets 'plain/text' as default Content-Type header. Note that providing content-length as a header is not supported. The size of the payload will be automatically calculated and included in the content-length header. STRING Yes No method For HTTP events, HTTP_METHOD header should be included as a request header. If the parameter is null then system uses 'POST' as a default header. POST STRING Yes No socket.idle.timeout Socket timeout value in millisecond 6000 INT Yes No chunk.disabled port: Port number of the remote service false BOOL Yes No ssl.protocol The SSL protocol version TLS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No client.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No follow.redirect Redirect related enabled. true BOOL Yes No max.redirect.count Maximum redirect count. 5 INT Yes No tls.store.type TLS store type to be used. JKS STRING Yes No proxy.host Proxy server host null STRING Yes No proxy.port Proxy server port null STRING Yes No proxy.username Proxy server username null STRING Yes No proxy.password Proxy server password null STRING Yes No client.bootstrap.configuration Client bootsrap configurations. Expected format of these parameters is as follows: \"'client.bootstrap.nodelay:xxx','client.bootstrap.keepalive:xxx'\" TODO STRING Yes No client.bootstrap.nodelay Http client no delay. true BOOL Yes No client.bootstrap.keepalive Http client keep alive. true BOOL Yes No client.bootstrap.sendbuffersize Http client send buffer size. 1048576 INT Yes No client.bootstrap.recievebuffersize Http client receive buffer size. 1048576 INT Yes No client.bootstrap.connect.timeout Http client connection timeout. 15000 INT Yes No client.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No client.bootstrap.socket.timeout Http client socket timeout. 15 STRING Yes No client.threadpool.configurations Thread pool configuration. Expected format of these parameters is as follows: \"'client.connection.pool.count:xxx','client.max.active.connections.per.pool:xxx'\" TODO STRING Yes No client.connection.pool.count Connection pool count. 0 INT Yes No client.max.active.connections.per.pool Active connections per pool. -1 INT Yes No client.min.idle.connections.per.pool Minimum ideal connection per pool. 0 INT Yes No client.max.idle.connections.per.pool Maximum ideal connection per pool. 100 INT Yes No client.min.eviction.idle.time Minimum eviction idle time. 5 * 60 * 1000 STRING Yes No sender.thread.count Http sender thread count. 20 STRING Yes No event.group.executor.thread.size Event group executor thread size. 15 STRING Yes No max.wait.for.client.connection.pool Maximum wait for client connection pool. 60000 STRING Yes No sink.id Identifier of the sink. This is used to co-relate with the corresponding http-response source which needs to process the repose for the request sent by this sink. STRING No No downloading.enabled If this is set to 'true' then the response received by the response source will be written to a file. If downloading is enabled, the download.path parameter is mandatory. false BOOL Yes No download.path If downloading is enabled, the path of the file which is going to be downloaded should be specified using 'download.path' parameter. This should be an absolute path including the file name. null STRING Yes Yes oauth.username The username to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No oauth.password The password to be included in the authentication header of the oauth authentication enabled events. It is required to specify both username and password to enable oauth authentication. If one of the parameter is not given by user then an error is logged in the CLI. It is only applicable for for Oauth requests STRING Yes No consumer.key consumer key for the Http request. It is only applicable for for Oauth requests STRING Yes No consumer.secret consumer secret for the Http request. It is only applicable for for Oauth requests STRING Yes No refresh.token refresh token for the Http request. It is only applicable for for Oauth requests STRING Yes No Examples EXAMPLE 1 @sink(type='http-request', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody String, method string, headers string); @source(type='http-response', sink.id='foo', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', fileName='A[1]'))) define stream responseStream2xx(fileName string, headers string); @source(type='http-response', sink.id='foo', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream responseStream4xx(errorMsg string); In above example, the payload body for 'FooStream' will be in following format. { events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events , This message will sent as the body of a POST request with the content-type 'application/xml' to the endpoint defined as the 'publisher.url' and in order to process the responses for these requests, there should be a source of type 'http-response' defined with the same sink id 'foo' in the siddhi app. The responses with 2xx status codes will be received by the http-response source which has the http.status.code defined by the regex '2\\d+'. If the response has a 4xx status code, it will be received by the http-response source which has the http.status.code defined by the regex '4\\d+'. EXAMPLE 2 define stream FooStream (name String, id int, headers String, downloadPath string); @sink(type='http-request', downloading.enabled='true', download.path='{{downloadPath}}',publisher.url='http://localhost:8005/files', method='GET', headers='{{headers}}',sink.id='download-sink', @map(type='json')) define stream BarStream (name String, id int, headers String, downloadPath string); @source(type='http-response', sink.id='download-sink', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', fileName='A[1]'))) define stream responseStream2xx(fileName string, headers string); @source(type='http-response', sink.id='download-sink', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream responseStream4xx(errorMsg string); In above example, http-request sink will send a GET request to the publisher url and the requested file will be received as the response by a corresponding http-response source. If the http status code of the response is a successful one (2xx), it will be received by the http-response source which has the http.status.code '2\\d+' and downloaded as a local file. Then the event received to the responseStream2xx will have the headers included in the request and the downloaded file name. If the http status code of the response is a 4xx code, it will be received by the http-response source which has the http.status.code '4\\d+'. Then the event received to the responseStream4xx will have the response message body in text format.","title":"http-request (Sink)"},{"location":"docs/api/latest/#http-response-sink","text":"HTTP response sink is correlated with the The HTTP request source, through a unique source.id , and it send a response to the HTTP request source having the same source.id . The response message can be formatted in text , XML or JSON and can be sent with appropriate headers. Origin: siddhi-io-http:2.0.4 Syntax @sink(type=\"http-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier of the source. STRING No No message.id Identifier of the message. STRING No Yes headers The headers that should be included as HTTP response headers. There can be any number of headers concatenated on following format. \"'header1:value1','header2:value2'\" User can include content-type header if he/she need to have any specific type for payload. If not system get the mapping type as the content-Type header (ie. @map(xml) : application/xml , @map(json) : application/json , @map(text) : plain/text ) and if user does not include any mapping type then system gets the plain/text as default Content-Type header. If user does not include Content-Length header then system calculate the bytes size of payload and include it as content-length header. STRING Yes No Examples EXAMPLE 1 @sink(type='http-response', source.id='sampleSourceId', message.id='{{messageId}}', headers=\"'content-type:json','content-length:94'\"@map(type='json', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody String, messageId string, headers string); If it is json mapping expected input should be in following format for FooStream: { {\"events\": {\"event\": \"symbol\":WSO2, \"price\":55.6, \"volume\":100, } }, 0cf708b1-7eae-440b-a93e-e72f801b486a, Content-Length:24#Content-Location:USA } Above event will generate response for the matching source message as below. ~Output http event payload {\"events\": {\"event\": \"symbol\":WSO2, \"price\":55.6, \"volume\":100, } } ~Output http event headers Content-Length:24, Content-Location:'USA', Content-Type:'application/json'","title":"http-response (Sink)"},{"location":"docs/api/latest/#inmemory-sink","text":"In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Origin: siddhi-core:5.0.0 Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation.","title":"inMemory (Sink)"},{"location":"docs/api/latest/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Origin: siddhi-io-kafka:5.0.0 Syntax @sink(type=\"kafka\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", sequence.id=\" STRING \", key=\" STRING \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0 th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"docs/api/latest/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Origin: siddhi-io-kafka:5.0.0 Syntax @sink(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", sequence.id=\" STRING \", key=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0 th ) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"docs/api/latest/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Origin: siddhi-core:5.0.0 Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"docs/api/latest/#nats-sink","text":"NATS Sink allows users to subscribe to a NATS broker and publish messages. Origin: siddhi-io-nats:2.0.1 Syntax @sink(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS sink should publish to. STRING No Yes bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client publishing/connecting to the NATS broker. Should be unique for each client connecting to the server/cluster. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No Examples EXAMPLE 1 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with all supporting configurations. With the following configuration the sink identified as 'nats-client' will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. EXAMPLE 2 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with mandatory configurations. With the following configuration the sink identified with an auto generated client id will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection.","title":"nats (Sink)"},{"location":"docs/api/latest/#tcp-sink","text":"A Siddhi application can be configured to publish events via the TCP transport by adding the @Sink(type = 'tcp') annotation at the top of an event stream definition. Origin: siddhi-io-tcp:3.0.1 Syntax @sink(type=\"tcp\", url=\" STRING \", sync=\" STRING \", tcp.no.delay=\" BOOL \", keep.alive=\" BOOL \", worker.threads=\" INT|LONG \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The URL to which outgoing events should be published via TCP. The URL should adhere to tcp:// host : port / context format. STRING No No sync This parameter defines whether the events should be published in a synchronized manner or not. If sync = 'true', then the worker will wait for the ack after sending the message. Else it will not wait for an ack. false STRING Yes Yes tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true BOOL Yes No keep.alive This property defines whether the server should be kept alive when there are no connections available. true BOOL Yes No worker.threads Number of threads to publish events. 10 INT LONG Yes No Examples EXAMPLE 1 @Sink(type = 'tcp', url='tcp://localhost:8080/abc', sync='true' @map(type='binary')) define stream Foo (attribute1 string, attribute2 int); A sink of type 'tcp' has been defined. All events arriving at Foo stream via TCP transport will be sent to the url tcp://localhost:8080/abc in a synchronous manner.","title":"tcp (Sink)"},{"location":"docs/api/latest/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"docs/api/latest/#binary-sink-mapper","text":"This section explains how to map events processed via Siddhi in order to publish them in the binary format. Origin: siddhi-map-binary:2.0.0 Syntax @sink(..., @map(type=\"binary\") Examples EXAMPLE 1 @sink(type='inMemory', topic='WSO2', @map(type='binary')) define stream FooStream (symbol string, price float, volume long); This will publish Siddhi event in binary format.","title":"binary (Sink Mapper)"},{"location":"docs/api/latest/#csv-sink-mapper","text":"This output mapper extension allows you to convert Siddhi events processed by the WSO2 SP to CSV message before publishing them. You can either use custom placeholder to map a custom CSV message or use pre-defined CSV format where event conversion takes place without extra configurations. Origin: siddhi-map-csv:2.0.0 Syntax @sink(..., @map(type=\"csv\", delimiter=\" STRING \", header=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter This parameter used to separate the output CSV data, when converting a Siddhi event to CSV format, , STRING Yes No header This parameter specifies whether the CSV messages will be generated with header or not. If this parameter is set to true, message will be generated with header false BOOL Yes No event.grouping.enabled If this parameter is set to true , events are grouped via a line.separator when multiple events are received. It is required to specify a value for the System.lineSeparator() when the value for this parameter is true . false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv')) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a default CSV output mapping, which will generate output as follows: WSO2,55.6,100 OS supported line separator If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume OS supported line separator WSO2-55.6-100 OS supported line separator EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv',header='true',delimiter='-',@payload(symbol='0',price='2',volume='1')))define stream BarStream (symbol string, price float,volume long); Above configuration will perform a custom CSV mapping. Here, user can add custom place order in the @payload. The place order indicates that where the attribute name's value will be appear in the output message, The output will be produced output as follows: WSO2,100,55.6 If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume WSO2-55.6-100 OS supported line separator If event grouping is enabled, then the output is as follows: WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator","title":"csv (Sink Mapper)"},{"location":"docs/api/latest/#json-sink-mapper","text":"This extension is an Event to JSON output mapper. Transports that publish messages can utilize this extension to convert Siddhi events to JSON messages. You can either send a pre-defined JSON format or a custom JSON message. Origin: siddhi-map-json:5.0.1 Syntax @sink(..., @map(type=\"json\", validate.json=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.json If this property is set to true , it enables JSON validation for the JSON messages generated. When validation is carried out, messages that do not adhere to proper JSON standards are dropped. This property is set to 'false' by default. false BOOL Yes No enclosing.element This specifies the enclosing element to be used if multiple events are sent in the same JSON message. Siddhi treats the child elements of the given enclosing element as events and executes JSON expressions on them. If an enclosing.element is not provided, the multiple event scenario is disregarded and JSON path is evaluated based on the root element. $ STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Above configuration does a default JSON input mapping that generates the output given below. { \"event\":{ \"symbol\":WSO2, \"price\":55.6, \"volume\":100 } } EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='json', enclosing.element='$.portfolio', validate.json='true', @payload( \"\"\"{\"StockData\":{\"Symbol\":\"{{symbol}}\",\"Price\":{{price}}}\"\"\"))) define stream BarStream (symbol string, price float, volume long); The above configuration performs a custom JSON mapping that generates the following JSON message as the output. {\"portfolio\":{ \"StockData\":{ \"Symbol\":WSO2, \"Price\":55.6 } } }","title":"json (Sink Mapper)"},{"location":"docs/api/latest/#keyvalue-sink-mapper","text":"The Event to Key-Value Map output mapper extension allows you to convert Siddhi events processed by WSO2 SP to key-value map events before publishing them. You can either use pre-defined keys where conversion takes place without extra configurations, or use custom keys with which the messages can be published. Origin: siddhi-map-keyvalue:2.0.0 Syntax @sink(..., @map(type=\"keyvalue\") Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default Key-Value output mapping. The expected output is something similar to the following:symbol:'WSO2' price : 55.6f volume: 100L EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='symbol',b='price',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where values are passed as objects. Values for symbol , price , and volume attributes are published with the keys a , b and c respectively. The expected output is a map similar to the following: a:'WSO2' b : 55.6f c: 100L EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='{{symbol}} is here',b='`price`',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where the values of the a and b attributes are strings and c is object. The expected output should be a Map similar to the following 'WSO2 is here' b : 'price' c: 100L","title":"keyvalue (Sink Mapper)"},{"location":"docs/api/latest/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.0.0 Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"docs/api/latest/#text-sink-mapper","text":"This extension is a Event to Text output mapper. Transports that publish text messages can utilize this extension to convert the Siddhi events to text messages. Users can use a pre-defined text format where event conversion is carried out without any additional configurations, or use custom placeholder(using {{ and }} or {{{ and }}} ) to map custom text messages. All variables are HTML escaped by default. For example: & is replaced with amp; \" is replaced with quot; = is replaced with #61; If you want to return unescaped HTML, use the triple mustache {{{ instead of double {{ . Origin: siddhi-map-text:2.0.0 Syntax @sink(..., @map(type=\"text\", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.grouping.enabled If this parameter is set to true , events are grouped via a delimiter when multiple events are received. It is required to specify a value for the delimiter parameter when the value for this parameter is true . false BOOL Yes No delimiter This parameter specifies how events are separated when a grouped event is received. This must be a whole line and not a single character. ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n whereas Windows uses \\r\\n as the end of line character. \\n STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping with event grouping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{symbol}}/{{volume}}, SensorPrice : Rs{{price}}/=, Value : {{volume}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected output is as follows: SensorID : wso2/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {wso2,1000,100} EXAMPLE 4 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true', @payload(\"Stock price of {{symbol}} is {{price}}\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping with event grouping. The expected output is as follows: Stock price of WSO2 is 55.6 ~ Stock price of WSO2 is 55.6 ~ Stock price of WSO2 is 55.6 for the following siddhi event. {WSO2,55.6,10} EXAMPLE 5 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{{symbol}}}/{{{volume}}}, SensorPrice : Rs{{{price}}}/=, Value : {{{volume}}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping to return unescaped HTML. The expected output is as follows: SensorID : a b/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {a b,1000,100}","title":"text (Sink Mapper)"},{"location":"docs/api/latest/#xml-sink-mapper","text":"This mapper converts Siddhi output events to XML before they are published via transports that publish in XML format. Users can either send a pre-defined XML format or a custom XML message containing event data. Origin: siddhi-map-xml:5.0.0 Syntax @sink(..., @map(type=\"xml\", validate.xml=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.xml This parameter specifies whether the XML messages generated should be validated or not. If this parameter is set to true, messages that do not adhere to proper XML standards are dropped. false BOOL Yes No enclosing.element When an enclosing element is specified, the child elements (e.g., the immediate child elements) of that element are considered as events. This is useful when you need to send multiple events in a single XML message. When an enclosing element is not specified, one XML message per every event will be emitted without enclosing. None in custom mapping and events in default mapping STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping which will generate below output events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='xml', enclosing.element=' portfolio ', validate.xml='true', @payload( \" StockData Symbol {{symbol}} /Symbol Price {{price}} /Price /StockData \"))) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. Inside @payload you can specify the custom template that you want to send the messages out and addd placeholders to places where you need to add event attributes.Above config will produce below output XML message portfolio StockData Symbol WSO2 /Symbol Price 55.6 /Price /StockData /portfolio","title":"xml (Sink Mapper)"},{"location":"docs/api/latest/#source","text":"","title":"Source"},{"location":"docs/api/latest/#cdc-source","text":"The CDC source receives events when change events (i.e., INSERT, UPDATE, DELETE) are triggered for a database table. Events are received in the 'key-value' format. The key values of the map of a CDC change event are as follows. For insert: Keys are specified as columns of the table. For delete: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For update: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For 'polling' mode: Keys are specified as the coloumns of the table. See parameter: mode for supported databases and change events. Origin: siddhi-io-cdc:2.0.0 Syntax @source(type=\"cdc\", url=\" STRING \", mode=\" STRING \", jdbc.driver.name=\" STRING \", username=\" STRING \", password=\" STRING \", pool.properties=\" STRING \", datasource.name=\" STRING \", table.name=\" STRING \", polling.column=\" STRING \", polling.interval=\" INT \", operation=\" STRING \", connector.properties=\" STRING \", database.server.id=\" STRING \", database.server.name=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The connection URL to the database. F=The format used is: 'jdbc:mysql:// host : port / database_name ' STRING No No mode Mode to capture the change data. The type of events that can be received, and the required parameters differ based on the mode. The mode can be one of the following: 'polling': This mode uses a column named 'polling.column' to monitor the given table. It captures change events of the 'RDBMS', 'INSERT, and 'UPDATE' types. 'listening': This mode uses logs to monitor the given table. It currently supports change events only of the 'MySQL', 'INSERT', 'UPDATE', and 'DELETE' types. listening STRING Yes No jdbc.driver.name The driver class name for connecting the database. It is required to specify a value for this parameter when the mode is 'polling'. STRING Yes No username The username to be used for accessing the database. This user needs to have the 'SELECT', 'RELOAD', 'SHOW DATABASES', 'REPLICATION SLAVE', and 'REPLICATION CLIENT'privileges for the change data capturing table (specified via the 'table.name' parameter). To operate in the polling mode, the user needs 'SELECT' privileges. STRING No No password The password of the username you specified for accessing the database. STRING No No pool.properties The pool parameters for the database connection can be specified as key-value pairs. STRING Yes No datasource.name Name of the wso2 datasource to connect to the database. When datasource name is provided, the URL, username and password are not needed. A datasource based connection is given more priority over the URL based connection. This parameter is applicable only when the mode is set to 'polling', and it can be applied only when you use this extension with WSO2 Stream Processor. STRING Yes No table.name The name of the table that needs to be monitored for data changes. STRING No No polling.column The column name that is polled to capture the change data. It is recommended to have a TIMESTAMP field as the 'polling.column' in order to capture the inserts and updates. Numeric auto-incremental fields and char fields can also be used as 'polling.column'. However, note that fields of these types only support insert change capturing, and the possibility of using a char field also depends on how the data is input. It is required to enter a value for this parameter when the mode is 'polling'. STRING Yes No polling.interval The time interval (specified in seconds) to poll the given table for changes. This parameter is applicable only when the mode is set to 'polling'. 1 INT Yes No operation The change event operation you want to carry out. Possible values are 'insert', 'update' or 'delete'. It is required to specify a value when the mode is 'listening'. This parameter is not case sensitive. STRING No No connector.properties Here, you can specify Debezium connector properties as a comma-separated string. The properties specified here are given more priority over the parameters. This parameter is applicable only for the 'listening' mode. Empty_String STRING Yes No database.server.id An ID to be used when joining MySQL database cluster to read the bin log. This should be a unique integer between 1 to 2^32. This parameter is applicable only when the mode is 'listening'. Random integer between 5400 and 6400 STRING Yes No database.server.name A logical name that identifies and provides a namespace for the database server. This parameter is applicable only when the mode is 'listening'. {host}_{port} STRING Yes No Examples EXAMPLE 1 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'insert', @map(type='keyvalue', @attributes(id = 'id', name = 'name'))) define stream inputStream (id string, name string); In this example, the CDC source listens to the row insertions that are made in the 'students' table with the column name, and the ID. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 2 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'update', @map(type='keyvalue', @attributes(id = 'id', name = 'name', before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, id string, before_name string , name string); In this example, the CDC source listens to the row updates that are made in the 'students' table. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 3 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'delete', @map(type='keyvalue', @attributes(before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, before_name string); In this example, the CDC source listens to the row deletions made in the 'students' table. This table belongs to the 'SimpleDB' database that can be accessed via the given URL. EXAMPLE 4 @source(type = 'cdc', mode='polling', polling.column = 'id', jdbc.driver.name = 'com.mysql.jdbc.Driver', url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. 'id' that is specified as the polling colum' is an auto incremental field. The connection to the database is made via the URL, username, password, and the JDBC driver name. EXAMPLE 5 @source(type = 'cdc', mode='polling', polling.column = 'id', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The given polling column is a char column with the 'S001, S002, ... .' pattern. The connection to the database is made via a data source named 'SimpleDB'. Note that the 'datasource.name' parameter works only with the Stream Processor. EXAMPLE 6 @source(type = 'cdc', mode='polling', polling.column = 'last_updated', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue')) define stream inputStream (name string); In this example, the CDC source polls the 'students' table for inserts and updates. The polling column is a timestamp field.","title":"cdc (Source)"},{"location":"docs/api/latest/#email-source","text":"The 'Email' source allows you to receive events via emails. An 'Email' source can be configured using the 'imap' or 'pop3' server to receive events. This allows you to filter the messages that satisfy the criteria specified under the 'search term' option. The email source parameters can be defined in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or the stream definition. If the parameter configurations are not available in either place, the default values are considered (i.e., if default values are available). If you need to configure server system parameters that are not provided as options in the stream definition, they need to be defined in the 'deployment yaml' file under 'email source properties'. For more information about 'imap' and 'pop3' server system parameters, see the following. [JavaMail Reference Implementation - IMAP Store](https://javaee.github.io/javamail/IMAP-Store) [JavaMail Reference Implementation - POP3 Store Store](https://javaee.github.io/javamail/POP3-Store) Origin: siddhi-io-email:2.0.1 Syntax @source(type=\"email\", username=\" STRING \", password=\" STRING \", store=\" STRING \", host=\" STRING \", port=\" INT \", folder=\" STRING \", search.term=\" STRING \", polling.interval=\" LONG \", action.after.processed=\" STRING \", folder.to.move=\" STRING \", content.type=\" STRING \", ssl.enable=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The user name of the email account. e.g., 'wso2mail' is the username of the 'wso2mail@gmail.com' mail account. STRING No No password The password of the email account STRING No No store The store type that used to receive emails. Possible values are 'imap' and 'pop3'. imap STRING Yes No host The host name of the server (e.g., 'imap.gmail.com' is the host name for a gmail account with an IMAP store.). The default value 'imap.gmail.com' is only valid if the email account is a gmail account with IMAP enabled. If store type is 'imap', then the default value is 'imap.gmail.com'. If the store type is 'pop3', then thedefault value is 'pop3.gmail.com'. STRING Yes No port The port that is used to create the connection. '993', the default value is valid only if the store is 'imap' and ssl-enabled. INT Yes No folder The name of the folder to which the emails should be fetched. INBOX STRING Yes No search.term The option that includes conditions such as key-value pairs to search for emails. In a string search term, the key and the value should be separated by a semicolon (';'). Each key-value pair must be within inverted commas (' '). The string search term can define multiple comma-separated key-value pairs. This string search term currently supports only the 'subject', 'from', 'to', 'bcc', and 'cc' keys. e.g., if you enter 'subject:DAS, from:carbon, bcc:wso2', the search term creates a search term instance that filters emails that contain 'DAS' in the subject, 'carbon' in the 'from' address, and 'wso2' in one of the 'bcc' addresses. The string search term carries out sub string matching that is case-sensitive. If '@' in included in the value for any key other than the 'subject' key, it checks for an address that is equal to the value given. e.g., If you search for 'abc@', the string search terms looks for an address that contains 'abc' before the '@' symbol. None STRING Yes No polling.interval This defines the time interval in seconds at which th email source should poll the account to check for new mail arrivals.in seconds. 600 LONG Yes No action.after.processed The action to be performed by the email source for the processed mail. Possible values are as follows: 'FLAGGED': Sets the flag as 'flagged'. 'SEEN': Sets the flag as 'read'. 'ANSWERED': Sets the flag as 'answered'. 'DELETE': Deletes tha mail after the polling cycle. 'MOVE': Moves the mail to the folder specified in the 'folder.to.move' parameter. If the folder specified is 'pop3', then the only option available is 'DELETE'. NONE STRING Yes No folder.to.move The name of the folder to which the mail must be moved once it is processed. If the action after processing is 'MOVE', it is required to specify a value for this parameter. STRING No No content.type The content type of the email. It can be either 'text/plain' or 'text/html.' text/plain STRING Yes No ssl.enable If this is set to 'true', a secure port is used to establish the connection. The possible values are 'true' and 'false'. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters mail.imap.partialfetch This determines whether the IMAP partial-fetch capability should be used. true true or false mail.imap.fetchsize The partial fetch size in bytes. 16K value in bytes mail.imap.peek If this is set to 'true', the IMAP PEEK option should be used when fetching body parts to avoid setting the 'SEEN' flag on messages. The default value is 'false'. This can be overridden on a per-message basis by the 'setPeek method' in 'IMAPMessage'. false true or false mail.imap.connectiontimeout The socket connection timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.timeout The socket read timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.writetimeout The socket write timeout value in milliseconds. This timeout is implemented by using a 'java.util.concurrent.ScheduledExecutorService' per connection that schedules a thread to close the socket if the timeout period elapses. Therefore, the overhead of using this timeout is one thread per connection. infinity timeout Any Integer value mail.imap.statuscachetimeout The timeout value in milliseconds for the cache of 'STATUS' command response. 1000ms Time out in miliseconds mail.imap.appendbuffersize The maximum size of a message to buffer in memory when appending to an IMAP folder. None Any Integer value mail.imap.connectionpoolsize The maximum number of available connections in the connection pool. 1 Any Integer value mail.imap.connectionpooltimeout The timeout value in milliseconds for connection pool connections. 45000ms Any Integer mail.imap.separatestoreconnection If this parameter is set to 'true', it indicates that a dedicated store connection needs to be used for store commands. true true or false mail.imap.auth.login.disable If this is set to 'true', it is not possible to use the non-standard 'AUTHENTICATE LOGIN' command instead of the plain 'LOGIN' command. false true or false mail.imap.auth.plain.disable If this is set to 'true', the 'AUTHENTICATE PLAIN' command cannot be used. false true or false mail.imap.auth.ntlm.disable If true, prevents use of the AUTHENTICATE NTLM command. false true or false mail.imap.proxyauth.user If the server supports the PROXYAUTH extension, this property specifies the name of the user to act as. Authentication to log in to the server is carried out using the administrator's credentials. After authentication, the IMAP provider issues the 'PROXYAUTH' command with the user name specified in this property. None Valid string value mail.imap.localaddress The local address (host name) to bind to when creating the IMAP socket. Defaults to the address picked by the Socket class. Valid string value mail.imap.localport The local port number to bind to when creating the IMAP socket. Defaults to the port number picked by the Socket class. Valid String value mail.imap.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.imap.sasl.mechanisms A list of SASL mechanism names that the system should to try to use. The names can be separated by spaces or commas. None Valid string value mail.imap.sasl.authorizationid The authorization ID to use in the SASL authentication. If this parameter is not set, the authentication ID (username) is used. Valid string value mail.imap.sasl.realm The realm to use with SASL authentication mechanisms that require a realm, such as 'DIGEST-MD5'. None Valid string value mail.imap.auth.ntlm.domain The NTLM authentication domain. None Valid string value The NTLM authentication domain. NTLM protocol-specific flags. None Valid integer value mail.imap.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create IMAP sockets. None Valid SocketFactory mail.imap.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create IMAP sockets. None Valid string mail.imap.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. true true or false mail.imap.socketFactory.port This specifies the port to connect to when using the specified socket factory. If this parameter is not set, the default port is used. 143 Valid Integer mail.imap.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by RFC 2595. false true or false mail.imap.ssl.trust If this parameter is set and a socket factory has not been specified, it enables the use of a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If this parameter specifies list of hosts separated by white spaces, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.imap.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class this class is used to create IMAP SSL sockets. None SSL Socket Factory mail.imap.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create IMAP SSL sockets. None Valid String mail.imap.ssl.socketFactory.port This specifies the port to connect to when using the specified socket factory. the default port 993 is used. valid port number mail.imap.ssl.protocols This specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid string mail.imap.starttls.enable If this parameter is set to 'true', it is possible to use the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.imap.socks.host This specifies the host name of a 'SOCKS5' proxy server that is used to connect to the mail server. None Valid String mail.imap.socks.port This specifies the port number for the 'SOCKS5' proxy server. This is needed if the proxy server is not using the standard port number 1080. 1080 Valid String mail.imap.minidletime This property sets the delay in milliseconds. 10 milliseconds time in seconds (Integer) mail.imap.enableimapevents If this property is set to 'true', it enables special IMAP-specific events to be delivered to the 'ConnectionListener' of the store. The unsolicited responses received during the idle method of the store are sent as connection events with 'IMAPStore.RESPONSE' as the type. The event's message is the raw IMAP response string. false true or false mail.imap.folder.class The class name of a subclass of 'com.sun.mail.imap.IMAPFolder'. The subclass can be used to provide support for additional IMAP commands. The subclass must have public constructors of the form 'public MyIMAPFolder'(String fullName, char separator, IMAPStore store, Boolean isNamespace) and public 'MyIMAPFolder'(ListInfo li, IMAPStore store) None Valid String mail.pop3.connectiontimeout The socket connection timeout value in milliseconds. Infinite timeout Integer value mail.pop3.timeout The socket I/O timeout value in milliseconds. Infinite timeout Integer value mail.pop3.message.class The class name of a subclass of 'com.sun.mail.pop3.POP3Message'. None Valid String mail.pop3.localaddress The local address (host name) to bind to when creating the POP3 socket. Defaults to the address picked by the Socket class. Valid String mail.pop3.localport The local port number to bind to when creating the POP3 socket. Defaults to the port number picked by the Socket class. Valid port number mail.pop3.apop.enable If this parameter is set to 'true', use 'APOP' instead of 'USER/PASS' to log in to the 'POP3' server (if the 'POP3' server supports 'APOP'). APOP sends a digest of the password instead of clearing the text password. false true or false mail.pop3.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create 'POP3' sockets. None Socket Factory mail.pop3.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create 'POP3' sockets. None Valid String mail.pop3.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. false true or false mail.pop3.socketFactory.port This specifies the port to connect to when using the specified socket factory. Default port Valid port number mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', check the server identity as specified by RFC 2595. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3' SSL sockets. None SSL Socket Factory mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by 'RFC 2595'. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to '*', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. Trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3 SSL' sockets. None SSL Socket Factory mail.pop3.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create 'POP3 SSL' sockets. None Valid String mail.pop3.ssl.socketFactory.p This parameter pecifies the port to connect to when using the specified socket factory. 995 Valid Integer mail.pop3.ssl.protocols This parameter specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid String mail.pop3.starttls.enable If this parameter is set to 'true', it is possible to use the 'STLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.pop3.starttls.required If this parameter is set to 'true', it is required to use the 'STLS' command. The connect method fails if the server does not support the 'STLS' command or if the command fails. false true or false mail.pop3.socks.host This parameter specifies the host name of a 'SOCKS5' proxy server that can be used to connect to the mail server. None Valid String mail.pop3.socks.port This parameter specifies the port number for the 'SOCKS5' proxy server. None Valid String mail.pop3.disabletop If this parameter is set to 'true', the 'POP3 TOP' command is not used to fetch message headers. false true or false mail.pop3.forgettopheaders If this parameter is set to 'true', the headers that might have been retrieved using the 'POP3 TOP' command is forgotten and replaced by the headers retrieved when the 'POP3 RETR' command is executed. false true or false mail.pop3.filecache.enable If this parameter is set to 'true', the 'POP3' provider caches message data in a temporary file instead of caching them in memory. Messages are only added to the cache when accessing the message content. Message headers are always cached in memory (on demand). The file cache is removed when the folder is closed or the JVM terminates. false true or false mail.pop3.filecache.dir If the file cache is enabled, this property is used to override the default directory used by the JDK for temporary files. None Valid String mail.pop3.cachewriteto This parameter controls the behavior of the 'writeTo' method on a 'POP3' message object. If the parameter is set to 'true', the message content has not been cached yet, and the 'ignoreList' is null, the message is cached before being written. If not, the message is streamed directly to the output stream without being cached. false true or false mail.pop3.keepmessagecontent If this property is set to 'true', a hard reference to the cached content is retained, preventing the memory from being reused until the folder is closed, or until the cached content is explicitly invalidated (using the 'invalidate' method). false true or false Examples EXAMPLE 1 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. In this example, only the required parameters are defined in the stream definition. The default values are taken for the other parameters. The search term is not defined, and therefore, all the new messages in the inbox folder are polled and taken. EXAMPLE 2 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',store = 'imap',host = 'imap.gmail.com',port = '993',searchTerm = 'subject:Stream Processor, from: from.account@ , cc: cc.account',polling.interval='500',action.after.processed='DELETE',content.type='text/html,)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. The email source polls the mail account every 500 seconds to check whether any new mails have arrived. It processes new mails only if they satisfy the conditions specified for the email search term (the value for 'from' of the email message should be 'from.account@. host name ', and the message should contain 'cc.account' in the cc receipient list and the word 'Stream Processor' in the mail subject). in this example, the action after processing is 'DELETE'. Therefore,after processing the event, corresponding mail is deleted from the mail folder.","title":"email (Source)"},{"location":"docs/api/latest/#http-source","text":"The HTTP source receives POST requests via HTTP or HTTPS in format such as text , XML and JSON . In WSO2 SP, if required, you can enable basic authentication to ensure that events are received only from users who are authorized to access the service. Origin: siddhi-io-http:2.0.4 Syntax @source(type=\"http\", receiver.url=\" STRING \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", server.enable.session.creation=\" STRING \", server.supported.snimatchers=\" STRING \", server.suported.server.names=\" STRING \", request.size.validation.configuration=\" STRING \", request.size.validation=\" STRING \", request.size.validation.maximum.value=\" STRING \", request.size.validation.reject.status.code=\" STRING \", request.size.validation.reject.message=\" STRING \", request.size.validation.reject.message.content.type=\" STRING \", header.size.validation=\" STRING \", header.validation.maximum.request.line=\" STRING \", header.validation.maximum.size=\" STRING \", header.validation.maximum.chunk.size=\" STRING \", header.validation.reject.status.code=\" STRING \", header.validation.reject.message=\" STRING \", header.validation.reject.message.content.type=\" STRING \", server.bootstrap.configuration=\" OBJECT \", server.bootstrap.nodelay=\" BOOL \", server.bootstrap.keepalive=\" BOOL \", server.bootstrap.sendbuffersize=\" INT \", server.bootstrap.recievebuffersize=\" INT \", server.bootstrap.connect.timeout=\" INT \", server.bootstrap.socket.reuse=\" BOOL \", server.bootstrap.socket.timeout=\" BOOL \", server.bootstrap.socket.backlog=\" BOOL \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL to which the events should be received. User can provide any valid url and if the url is not provided the system will use the following format http://0.0.0.0:9763/ appNAme / streamName If the user want to use SSL the url should be given in following format https://localhost:8080/ streamName http://0.0.0.0:9763/ / STRING Yes No basic.auth.enabled This works only in WSO2 SP. If this is set to true , basic authentication is enabled for incoming events, and the credentials with which each event is sent are verified to ensure that the user is authorized to access the service. If basic authentication fails, the event is not authenticated and an authentication error is logged in the CLI. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. The value is 1 by default. This will ensure that the events are directed to the event stream in the same order in which they arrive. By increasing this value the performance might increase at the cost of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection. 120000 INT Yes No ssl.verify.client The type of client certificate verification. null STRING Yes No ssl.protocol ssl/tls related options TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No server.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No server.supported.snimatchers Http SNIMatcher to be added. This parameter should include under parameters Ex: 'server.supported.snimatchers:SNIMatcher' null STRING Yes No server.suported.server.names Http supported servers. This parameter should include under parameters Ex: 'server.suported.server.names:server' null STRING Yes No request.size.validation.configuration Parameters that responsible for validating the http request and request headers. Expected format of these parameters is as follows: \"'request.size.validation:xxx','request.size.validation.maximum.value:xxx'\" null STRING Yes No request.size.validation To enable the request size validation. false STRING Yes No request.size.validation.maximum.value If request size is validated then maximum size. Integer.MAX_VALUE STRING Yes No request.size.validation.reject.status.code If request is exceed maximum size and request.size.validation is enabled then status code to be send as response. 401 STRING Yes No request.size.validation.reject.message If request is exceed maximum size and request.size.validation is enabled then status message to be send as response. Message is bigger than the valid size STRING Yes No request.size.validation.reject.message.content.type If request is exceed maximum size and request.size.validation is enabled then content type to be send as response. plain/text STRING Yes No header.size.validation To enable the header size validation. false STRING Yes No header.validation.maximum.request.line If header header validation is enabled then the maximum request line. 4096 STRING Yes No header.validation.maximum.size If header header validation is enabled then the maximum expected header size. 8192 STRING Yes No header.validation.maximum.chunk.size If header header validation is enabled then the maximum expected chunk size. 8192 STRING Yes No header.validation.reject.status.code 401 If header is exceed maximum size and header.size.validation is enabled then status code to be send as response. STRING Yes No header.validation.reject.message If header is exceed maximum size and header.size.validation is enabled then message to be send as response. Message header is bigger than the valid size STRING Yes No header.validation.reject.message.content.type If header is exceed maximum size and header.size.validation is enabled then content type to be send as response. plain/text STRING Yes No server.bootstrap.configuration Parameters that for bootstrap configurations of the server. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null OBJECT Yes No server.bootstrap.nodelay Http server no delay. true BOOL Yes No server.bootstrap.keepalive Http server keep alive. true BOOL Yes No server.bootstrap.sendbuffersize Http server send buffer size. 1048576 INT Yes No server.bootstrap.recievebuffersize Http server receive buffer size. 1048576 INT Yes No server.bootstrap.connect.timeout Http server connection timeout. 15000 INT Yes No server.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No server.bootstrap.socket.timeout Http server socket timeout. 15 BOOL Yes No server.bootstrap.socket.backlog THttp server socket backlog. 100 BOOL Yes No trace.log.enabled Http traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize property to configure number of boss threads, which accepts incoming connections until the ports are unbound. Once connection accepts successfully, boss thread passes the accepted channel to one of the worker threads. Number of available processors Any integer serverBootstrapWorkerGroupSize property to configure number of worker threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer serverBootstrapClientGroupSize property to configure number of client threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultHttpPort The default port if the default scheme is 'http'. 8280 Any valid port defaultHttpsPort The default port if the default scheme is 'https'. 8243 Any valid port defaultScheme The default protocol. http http https keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to wso2carbon.jks file keyStorePassword The default keystore password. wso2carbon String of keystore password Examples EXAMPLE 1 @source(type='http', receiver.url='http://localhost:9055/endpoints/RecPro', socketIdleTimeout='150000', parameters=\"'ciphers : TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256', 'sslEnabledProtocols:TLSv1.1,TLSv1.2'\",request.size.validation.configuration=\"request.size.validation:true\",server.bootstrap.configuration=\"server.bootstrap.socket.timeout:25\" @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above source listenerConfiguration performs a default XML input mapping. The expected input is as follows: events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events If basic authentication is enabled via the basic.auth.enabled='true setting, each input event is also expected to contain the Authorization:'Basic encodeBase64(username:Password)' header.","title":"http (Source)"},{"location":"docs/api/latest/#http-request-source","text":"The HTTP request is correlated with the HTTP response sink, through a unique source.id , and for each POST requests it receives via HTTP or HTTPS in format such as text , XML and JSON it sends the response via the HTTP response sink. The individual request and response messages are correlated at the sink using the message.id of the events. If required, you can enable basic authentication at the source to ensure that events are received only from users who are authorized to access the service. Origin: siddhi-io-http:2.0.4 Syntax @source(type=\"http-request\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", parameters=\" STRING \", ciphers=\" STRING \", ssl.enabled.protocols=\" STRING \", server.enable.session.creation=\" STRING \", server.supported.snimatchers=\" STRING \", server.suported.server.names=\" STRING \", request.size.validation.configuration=\" STRING \", request.size.validation=\" STRING \", request.size.validation.maximum.value=\" STRING \", request.size.validation.reject.status.code=\" STRING \", request.size.validation.reject.message=\" STRING \", request.size.validation.reject.message.content.type=\" STRING \", header.size.validation=\" STRING \", header.validation.maximum.request.line=\" STRING \", header.validation.maximum.size=\" STRING \", header.validation.maximum.chunk.size=\" STRING \", header.validation.reject.status.code=\" STRING \", header.validation.reject.message=\" STRING \", header.validation.reject.message.content.type=\" STRING \", server.bootstrap.configuration=\" OBJECT \", server.bootstrap.nodelay=\" BOOL \", server.bootstrap.keepalive=\" BOOL \", server.bootstrap.sendbuffersize=\" INT \", server.bootstrap.recievebuffersize=\" INT \", server.bootstrap.connect.timeout=\" INT \", server.bootstrap.socket.reuse=\" BOOL \", server.bootstrap.socket.timeout=\" BOOL \", server.bootstrap.socket.backlog=\" BOOL \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL to which the events should be received. User can provide any valid url and if the url is not provided the system will use the following format http://0.0.0.0:9763/ appNAme / streamName If the user want to use SSL the url should be given in following format https://localhost:8080/ streamName http://0.0.0.0:9763/ / STRING Yes No source.id Identifier need to map the source to sink. STRING No No connection.timeout Connection timeout in milliseconds. If the mapped http-response sink does not get a correlated message, after this timeout value, a timeout response is sent 120000 INT Yes No basic.auth.enabled If this is set to true , basic authentication is enabled for incoming events, and the credentials with which each event is sent are verified to ensure that the user is authorized to access the service. If basic authentication fails, the event is not authenticated and an authentication error is logged in the CLI. By default this values 'false' false STRING Yes No worker.count The number of active worker threads to serve the incoming events. The value is 1 by default. This will ensure that the events are directed to the event stream in the same order in which they arrive. By increasing this value the performance might increase at the cost of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection. 120000 INT Yes No ssl.verify.client The type of client certificate verification. null STRING Yes No ssl.protocol ssl/tls related options TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No parameters Parameters other than basics such as ciphers,sslEnabledProtocols,client.enable.session.creation. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null STRING Yes No ciphers List of ciphers to be used. This parameter should include under parameters Ex: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' null STRING Yes No ssl.enabled.protocols SSL/TLS protocols to be enabled. This parameter should be in camel case format(sslEnabledProtocols) under parameters. Ex 'sslEnabledProtocols:true' null STRING Yes No server.enable.session.creation Enable HTTP session creation.This parameter should include under parameters Ex: 'client.enable.session.creation:true' null STRING Yes No server.supported.snimatchers Http SNIMatcher to be added. This parameter should include under parameters Ex: 'server.supported.snimatchers:SNIMatcher' null STRING Yes No server.suported.server.names Http supported servers. This parameter should include under parameters Ex: 'server.suported.server.names:server' null STRING Yes No request.size.validation.configuration Parameters that responsible for validating the http request and request headers. Expected format of these parameters is as follows: \"'request.size.validation:xxx','request.size.validation.maximum.value:xxx'\" null STRING Yes No request.size.validation To enable the request size validation. false STRING Yes No request.size.validation.maximum.value If request size is validated then maximum size. Integer.MAX_VALUE STRING Yes No request.size.validation.reject.status.code If request is exceed maximum size and request.size.validation is enabled then status code to be send as response. 401 STRING Yes No request.size.validation.reject.message If request is exceed maximum size and request.size.validation is enabled then status message to be send as response. Message is bigger than the valid size STRING Yes No request.size.validation.reject.message.content.type If request is exceed maximum size and request.size.validation is enabled then content type to be send as response. plain/text STRING Yes No header.size.validation To enable the header size validation. false STRING Yes No header.validation.maximum.request.line If header header validation is enabled then the maximum request line. 4096 STRING Yes No header.validation.maximum.size If header header validation is enabled then the maximum expected header size. 8192 STRING Yes No header.validation.maximum.chunk.size If header header validation is enabled then the maximum expected chunk size. 8192 STRING Yes No header.validation.reject.status.code 401 If header is exceed maximum size and header.size.validation is enabled then status code to be send as response. STRING Yes No header.validation.reject.message If header is exceed maximum size and header.size.validation is enabled then message to be send as response. Message header is bigger than the valid size STRING Yes No header.validation.reject.message.content.type If header is exceed maximum size and header.size.validation is enabled then content type to be send as response. plain/text STRING Yes No server.bootstrap.configuration Parameters that for bootstrap configurations of the server. Expected format of these parameters is as follows: \"'ciphers:xxx','sslEnabledProtocols,client.enable:xxx'\" null OBJECT Yes No server.bootstrap.nodelay Http server no delay. true BOOL Yes No server.bootstrap.keepalive Http server keep alive. true BOOL Yes No server.bootstrap.sendbuffersize Http server send buffer size. 1048576 INT Yes No server.bootstrap.recievebuffersize Http server receive buffer size. 1048576 INT Yes No server.bootstrap.connect.timeout Http server connection timeout. 15000 INT Yes No server.bootstrap.socket.reuse To enable http socket reuse. false BOOL Yes No server.bootstrap.socket.timeout Http server socket timeout. 15 BOOL Yes No server.bootstrap.socket.backlog THttp server socket backlog. 100 BOOL Yes No trace.log.enabled Http traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize property to configure number of boss threads, which accepts incoming connections until the ports are unbound. Once connection accepts successfully, boss thread passes the accepted channel to one of the worker threads. Number of available processors Any integer serverBootstrapWorkerGroupSize property to configure number of worker threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer serverBootstrapClientGroupSize property to configure number of client threads, which performs non blocking read and write for one or more channels in non-blocking mode. (Number of available processors)*2 Any integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultHttpPort The default port if the default scheme is 'http'. 8280 Any valid port defaultHttpsPort The default port if the default scheme is 'https'. 8243 Any valid port defaultScheme The default protocol. http http https keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to wso2carbon.jks file keyStorePassword The default keystore password. wso2carbon String of keystore password certPassword The default cert password. wso2carbon String of cert password Examples EXAMPLE 1 @source(type='http-request', source.id='sampleSourceId, receiver.url='http://localhost:9055/endpoints/RecPro', connection.timeout='150000', parameters=\"'ciphers : TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256', 'sslEnabledProtocols:TLSv1.1,TLSv1.2'\", request.size.validation.configuration=\"request.size.validation:true\", server.bootstrap.configuration=\"server.bootstrap.socket.timeout:25\", @map(type='json, @attributes(messageId='trp:messageId', symbol='$.events.event.symbol', price='$.events.event.price', volume='$.events.event.volume'))) define stream FooStream (messageId string, symbol string, price float, volume long); The expected input is as follows: {\"events\": {\"event\": \"symbol\":WSO2, \"price\":55.6, \"volume\":100, } } If basic authentication is enabled via the basic.auth.enabled='true setting, each input event is also expected to contain the Authorization:'Basic encodeBase64(username:Password)' header.","title":"http-request (Source)"},{"location":"docs/api/latest/#http-response-source","text":"The http-response source co-relates with http-request sink with the parameter 'sink.id'. This receives responses for the requests sent by the http-request sink which has the same sink id. Response messages can be in formats such as TEXT, JSON and XML. In order to handle the responses with different http status codes, user is allowed to defined the acceptable response source code using the parameter 'http.status.code' Origin: siddhi-io-http:2.0.4 Syntax @source(type=\"http-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id This parameter is used to map the http-response source to a http-request sink. Then this source will accepts the response messages for the requests sent by corresponding http-request sink. STRING No No http.status.code Acceptable http status code for the responses. This can be a complete string or a regex. Only the responses with matching status codes to the defined value, will be received by the http-response source. Eg: 'http.status.code = '200', http.status.code = '2\\d+'' 200 STRING Yes No allow.streaming.responses If responses can be received multiple times for a single request, this option should be enabled. If this is not enabled, for every request, response will be extracted only once. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-request', downloading.enabled='true', publisher.url='http://localhost:8005/registry/employee', method='POST', headers='{{headers}}',sink.id='employee-info', @map(type='json')) define stream BarStream (name String, id int, headers String, downloadPath string); @source(type='http-response' , sink.id='employee-info', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(message='A[1]'))) define stream responseStream2xx(message string);@source(type='http-response' , sink.id='employee-info', http.status.code='4\\\\d+' , @map(type='text', regex.A='((.|\\n)*)', @attributes(message='A[1]'))) define stream responseStream4xx(message string); In above example, the defined http-request sink will send a POST requests to the endpoint defined by 'publisher.url'. Then for those requests, the source with the response code '2\\d+' and sink.id 'employee-info' will receive the responses with 2xx status codes. The http-response source which has 'employee-info' as the 'sink.id' and '4\\d+' as the http.response.code will receive all the responses with 4xx status codes. . Then the body of the response message will be extracted using text mapper and converted into siddhi events. .","title":"http-response (Source)"},{"location":"docs/api/latest/#inmemory-source","text":"In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Origin: siddhi-core:5.0.0 Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport.","title":"inMemory (Source)"},{"location":"docs/api/latest/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Origin: siddhi-io-kafka:5.0.0 Syntax @source(type=\"kafka\", bootstrap.servers=\" STRING \", topic.list=\" STRING \", group.id=\" STRING \", threading.option=\" STRING \", partition.no.list=\" STRING \", seq.enabled=\" BOOL \", is.binary.message=\" BOOL \", topic.offset.map=\" STRING \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51 st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"docs/api/latest/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Origin: siddhi-io-kafka:5.0.0 Syntax @source(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"docs/api/latest/#nats-source","text":"NATS Source allows users to subscribe to a NATS broker and receive messages. It has the ability to receive all the message types supported by NATS. Origin: siddhi-io-nats:2.0.1 Syntax @source(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", queue.group.name=\" STRING \", durable.name=\" STRING \", subscription.sequence=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS Source should subscribe to. STRING No No bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client subscribing/connecting to the NATS broker. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No queue.group.name This can be used when there is a requirement to share the load of a NATS subject. Clients belongs to the same queue group share the subscription load. None STRING Yes No durable.name This can be used to subscribe to a subject from the last acknowledged message when a client or connection failure happens. The client can be uniquely identified using the tuple (client.id, durable.name). None STRING Yes No subscription.sequence This can be used to subscribe to a subject from a given number of message sequence. All the messages from the given point of sequence number will be passed to the client. If not provided then the either the persisted value or 0 will be used. None STRING Yes No Examples EXAMPLE 1 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with all supporting configurations.With the following configuration the source identified as 'nats-client' will subscribes to a subject named as 'SP_NATS_INPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This subscription will receive all the messages from 100 th in the subject. EXAMPLE 2 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', ) define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with mandatory configurations.With the following configuration the source identified with an auto generated client id will subscribes to a subject named as 'SP_NATS_INTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This will receive all available messages in the subject","title":"nats (Source)"},{"location":"docs/api/latest/#tcp-source","text":"A Siddhi application can be configured to receive events via the TCP transport by adding the @Source(type = 'tcp') annotation at the top of an event stream definition. When this is defined the associated stream will receive events from the TCP transport on the host and port defined in the system. Origin: siddhi-io-tcp:3.0.1 Syntax @source(type=\"tcp\", context=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic context The URL 'context' that should be used to receive the events. / STRING Yes No System Parameters Name Description Default Value Possible Parameters host Tcp server host. 0.0.0.0 Any valid host or IP port Tcp server port. 9892 Any integer representing valid port receiver.threads Number of threads to receive connections. 10 Any positive integer worker.threads Number of threads to serve events. 10 Any positive integer tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true true false keep.alive This property defines whether the server should be kept alive when there are no connections available. true true false Examples EXAMPLE 1 @Source(type = 'tcp', context='abc', @map(type='binary')) define stream Foo (attribute1 string, attribute2 int ); Under this configuration, events are received via the TCP transport on default host,port, abc context, and they are passed to Foo stream for processing.","title":"tcp (Source)"},{"location":"docs/api/latest/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"docs/api/latest/#binary-source-mapper","text":"This extension is a binary input mapper that converts events received in binary format to Siddhi events before they are processed. Origin: siddhi-map-binary:2.0.0 Syntax @source(..., @map(type=\"binary\") Examples EXAMPLE 1 @source(type='inMemory', topic='WSO2', @map(type='binary'))define stream FooStream (symbol string, price float, volume long); This query performs a mapping to convert an event of the binary format to a Siddhi event.","title":"binary (Source Mapper)"},{"location":"docs/api/latest/#csv-source-mapper","text":"This extension is used to convert CSV message to Siddhi event input mapper. You can either receive pre-defined CSV message where event conversion takes place without extra configurations,or receive custom CSV message where a custom place order to map from custom CSV message. Origin: siddhi-map-csv:2.0.0 Syntax @source(..., @map(type=\"csv\", delimiter=\" STRING \", header.present=\" BOOL \", fail.on.unknown.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter When converting a CSV format message to Siddhi event, this parameter indicatesinput CSV message's data should be split by this parameter , STRING Yes No header.present When converting a CSV format message to Siddhi event, this parameter indicates whether CSV message has header or not. This can either have value true or false.If it's set to false then it indicates that CSV message has't header. false BOOL Yes No fail.on.unknown.attribute This parameter specifies how unknown attributes should be handled. If it's set to true and one or more attributes don't havevalues, then SP will drop that message. If this parameter is set to false , the Stream Processor adds the required attribute's values to such events with a null value and the event is converted to a Siddhi event. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='csv')) define stream FooStream (symbol string, price float, volume int); Above configuration will do a default CSV input mapping. Expected input will look like below: WSO2 ,55.6 , 100OR \"WSO2,No10,Palam Groove Rd,Col-03\" ,55.6 , 100If header.present is true and delimiter is \"-\", then the input is as follows: symbol-price-volumeWSO2-55.6-100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='csv',header='true', @attributes(symbol = \"2\", price = \"0\", volume = \"1\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom CSV mapping. Here, user can add place order of each attribute in the @attribute. The place order indicates where the attribute name's value has appeared in the input.Expected input will look like below: 55.6,100,WSO2 OR55.6,100,\"WSO2,No10,Palm Groove Rd,Col-03\" If header is true and delimiter is \"-\", then the output is as follows: price-volume-symbol 55.6-100-WSO2 If group events is enabled then input should be as follows: price-volume-symbol 55.6-100-WSO2System.lineSeparator() 55.6-100-IBMSystem.lineSeparator() 55.6-100-IFSSystem.lineSeparator()","title":"csv (Source Mapper)"},{"location":"docs/api/latest/#json-source-mapper","text":"This extension is a JSON-to-Event input mapper. Transports that accept JSON messages can utilize this extension to convert an incoming JSON message into a Siddhi event. Users can either send a pre-defined JSON format, where event conversion happens without any configurations, or use the JSON path to map from a custom JSON message. In default mapping, the JSON string of the event can be enclosed by the element \"event\", though optional. Origin: siddhi-map-json:5.0.1 Syntax @source(..., @map(type=\"json\", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic enclosing.element This is used to specify the enclosing element when sending multiple events in the same JSON message. Mapper treats the child elements of a given enclosing element as events and executes the JSON path expressions on these child elements. If the enclosing.element is not provided then the multiple-event scenario is disregarded and the JSON path is evaluated based on the root element. $ STRING Yes No fail.on.missing.attribute This parameter allows users to handle unknown attributes.The value of this can either be true or false. By default it is true. If a JSON execution fails or returns null, mapper drops that message. However, setting this property to false prompts mapper to send an event with a null value to Siddhi, where users can handle it as required, ie., assign a default value.) true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For a single event, the input is required to be in one of the following formats: { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } or { \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For multiple events, the input is required to be in one of the following formats: [ {\"event\":{\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80}} ] or [ {\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}, {\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}, {\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80} ] EXAMPLE 3 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"company.symbol\", price = \"price\", volume = \"volume\"))) This configuration performs a custom JSON mapping. For a single event, the expected input is similar to the one shown below: .{ \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"WSO2\" }, \"price\":55.6 } } EXAMPLE 4 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream FooStream (symbol string, price float, volume long); The configuration performs a custom JSON mapping. For multiple events, expected input looks as follows. .{\"portfolio\": [ {\"stock\":{\"volume\":100,\"company\":{\"symbol\":\"wso2\"},\"price\":56.6}}, {\"stock\":{\"volume\":200,\"company\":{\"symbol\":\"wso2\"},\"price\":57.6}} ] }","title":"json (Source Mapper)"},{"location":"docs/api/latest/#keyvalue-source-mapper","text":"Key-Value Map to Event input mapper extension allows transports that accept events as key value maps to convert those events to Siddhi events. You can either receive pre-defined keys where conversion takes place without extra configurations, or use custom keys to map from the message. Origin: siddhi-map-keyvalue:2.0.0 Syntax @source(..., @map(type=\"keyvalue\", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic fail.on.missing.attribute If this parameter is set to true , if an event arrives without a matching key for a specific attribute in the connected stream, it is dropped and not processed by the Stream Processor. If this parameter is set to false the Stream Processor adds the required key to such events with a null value, and the event is converted to a Siddhi event so that you could handle them as required before they are further processed. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default key value input mapping. The expected input is a map similar to the following: symbol: 'WSO2' price: 55.6f volume: 100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='keyvalue', fail.on.missing.attribute='true', @attributes(symbol = 's', price = 'p', volume = 'v')))define stream FooStream (symbol string, price float, volume long); This query performs a custom key value input mapping. The matching keys for the symbol , price and volume attributes are be s , p, and v` respectively. The expected input is a map similar to the following: s: 'WSO2' p: 55.6 v: 100","title":"keyvalue (Source Mapper)"},{"location":"docs/api/latest/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.0.0 Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"docs/api/latest/#text-source-mapper","text":"This extension is a text to Siddhi event input mapper. Transports that accept text messages can utilize this extension to convert the incoming text message to Siddhi event. Users can either use a pre-defined text format where event conversion happens without any additional configurations, or specify a regex to map a text message using custom configurations. Origin: siddhi-map-text:2.0.0 Syntax @source(..., @map(type=\"text\", regex.groupid=\" STRING \", fail.on.missing.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex.groupid This parameter specifies a regular expression group. The groupid can be any capital letter (e.g., regex.A,regex.B .. etc). You can specify any number of regular expression groups. In the attribute annotation, you need to map all attributes to the regular expression group with the matching group index. If you need to to enable custom mapping, it is required to specifythe matching group for each and every attribute. STRING No No fail.on.missing.attribute This parameter specifies how unknown attributes should be handled. If it is set to true a message is dropped if its execution fails, or if one or more attributes do not have values. If this parameter is set to false , null values are assigned to attributes with missing values, and messages with such attributes are not dropped. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No delimiter This parameter specifies how events must be separated when multiple events are received. This must be whole line and not a single character. ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n as the end of line character whereas windows uses \\r\\n . \\n STRING Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected input is as follows: symbol:\"WSO2\", price:55.6, volume:100 OR symbol:'WSO2', price:55.6, volume:100 If group events is enabled then input should be as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='text', fail.on.unknown.attribute = 'true', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected input is as follows: wos2 550 volume 100 If group events is enabled then input should be as follows: wos2 550 volume 100 ~ wos2 550 volume 100 ~ wos2 550 volume 100","title":"text (Source Mapper)"},{"location":"docs/api/latest/#xml-source-mapper","text":"This mapper converts XML input to Siddhi event. Transports which accepts XML messages can utilize this extension to convert the incoming XML message to Siddhi event. Users can either send a pre-defined XML format where event conversion will happen without any configs or can use xpath to map from a custom XML message. Origin: siddhi-map-xml:5.0.0 Syntax @source(..., @map(type=\"xml\", namespaces=\" STRING \", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic namespaces Used to provide namespaces used in the incoming XML message beforehand to configure xpath expressions. User can provide a comma separated list. If these are not provided xpath evaluations will fail None STRING Yes No enclosing.element Used to specify the enclosing element in case of sending multiple events in same XML message. WSO2 DAS will treat the child element of given enclosing element as events and execute xpath expressions on child elements. If enclosing.element is not provided multiple event scenario is disregarded and xpaths will be evaluated with respect to root element. Root element STRING Yes No fail.on.missing.attribute This can either have value true or false. By default it will be true. This attribute allows user to handle unknown attributes. By default if an xpath execution fails or returns null DAS will drop that message. However setting this property to false will prompt DAS to send and event with null value to Siddhi where user can handle it accordingly(ie. Assign a default value) True BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping. Expected input will look like below. events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='xml', namespaces = \"dt=urn:schemas-microsoft-com:datatypes\", enclosing.element=\"//portfolio\", @attributes(symbol = \"company/symbol\", price = \"price\", volume = \"volume\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. In the custom mapping user can add xpath expressions representing each event attribute using @attribute annotation. Expected input will look like below. portfolio xmlns:dt=\"urn:schemas-microsoft-com:datatypes\" stock exchange=\"nasdaq\" volume 100 /volume company symbol WSO2 /symbol /company price dt:type=\"number\" 55.6 /price /stock /portfolio","title":"xml (Source Mapper)"},{"location":"docs/api/latest/#store","text":"","title":"Store"},{"location":"docs/api/latest/#rdbms-store","text":"This extension assigns data sources and connection instructions to event tables. It also implements read-write operations on connected datasources. Origin: siddhi-store-rdbms:6.0.0 Syntax @Store(type=\"rdbms\", jdbc.url=\" STRING \", username=\" STRING \", password=\" STRING \", jdbc.driver.name=\" STRING \", pool.properties=\" STRING \", jndi.resource=\" STRING \", datasource=\" STRING \", table.name=\" STRING \", field.length=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic jdbc.url The JDBC URL via which the RDBMS data store is accessed. STRING No No username The username to be used to access the RDBMS data store. STRING No No password The password to be used to access the RDBMS data store. STRING No No jdbc.driver.name The driver class name for connecting the RDBMS data store. STRING No No pool.properties Any pool parameters for the database connection must be specified as key-value pairs. null STRING Yes No jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account and the connection is attempted via JNDI lookup instead. null STRING Yes No datasource The name of the Carbon datasource that should be used for creating the connection with the database. If this is found, neither the pool properties nor the JNDI resource name described above are taken into account and the connection is attempted via Carbon datasources instead. null STRING Yes No table.name The name with which the event table should be persisted in the store. If no name is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The table name defined in the Siddhi App query. STRING Yes No field.length The number of characters that the values for fields of the 'STRING' type in the table definition must contain. Each required field must be provided as a comma-separated list of key-value pairs in the ' field.name : length ' format. If this is not specified, the default number of characters specific to the database type is considered. null STRING Yes No System Parameters Name Description Default Value Possible Parameters {{RDBMS-Name}}.maxVersion The latest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.minVersion The earliest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.tableCheckQuery The template query for the 'check table' operation in {{RDBMS-Name}}. H2 : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) MySQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Oracle : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Microsoft SQL Server : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) PostgreSQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) DB2. : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) N/A {{RDBMS-Name}}.tableCreateQuery The template query for the 'create table' operation in {{RDBMS-Name}}. H2 : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 MySQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 Oracle : SELECT 1 FROM {{TABLE_NAME}} WHERE rownum=1 Microsoft SQL Server : SELECT TOP 1 1 from {{TABLE_NAME}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.indexCreateQuery The template query for the 'create index' operation in {{RDBMS-Name}}. H2 : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) MySQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Oracle : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Microsoft SQL Server : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) {{TABLE_NAME}} ({{INDEX_COLUMNS}}) PostgreSQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) DB2. : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) N/A {{RDBMS-Name}}.recordInsertQuery The template query for the 'insert record' operation in {{RDBMS-Name}}. H2 : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) MySQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Oracle : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Microsoft SQL Server : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) PostgreSQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) DB2. : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) N/A {{RDBMS-Name}}.recordUpdateQuery The template query for the 'update record' operation in {{RDBMS-Name}}. H2 : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} MySQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Oracle : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Microsoft SQL Server : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} PostgreSQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} DB2. : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} N/A {{RDBMS-Name}}.recordSelectQuery The template query for the 'select record' operation in {{RDBMS-Name}}. H2 : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} DB2. : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.recordExistsQuery The template query for the 'check record existence' operation in {{RDBMS-Name}}. H2 : SELECT TOP 1 1 FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT COUNT(1) INTO existence FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT TOP 1 FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.recordDeleteQuery The query for the 'delete record' operation in {{RDBMS-Name}}. H2 : DELETE FROM {{TABLE_NAME}} {{CONDITION}} MySQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Oracle : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : DELETE FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} DB2. : DELETE FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.stringSize This defines the length for the string fields in {{RDBMS-Name}}. H2 : 254 MySQL : 254 Oracle : 254 Microsoft SQL Server : 254 PostgreSQL : 254 DB2. : 254 N/A {{RDBMS-Name}}.fieldSizeLimit This defines the field size limit for select/switch to big string type from the default string type if the 'bigStringType' is available in field type list. H2 : N/A MySQL : N/A Oracle : 2000 Microsoft SQL Server : N/A PostgreSQL : N/A DB2. : N/A 0 = n = INT_MAX {{RDBMS-Name}}.batchSize This defines the batch size when operations are performed for batches of events. H2 : 1000 MySQL : 1000 Oracle : 1000 Microsoft SQL Server : 1000 PostgreSQL : 1000 DB2. : 1000 N/A {{RDBMS-Name}}.batchEnable This specifies whether 'Update' and 'Insert' operations can be performed for batches of events or not. H2 : true MySQL : true Oracle (versions 12.0 and less) : false Oracle (versions 12.1 and above) : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.transactionSupported This is used to specify whether the JDBC connection that is used supports JDBC transactions or not. H2 : true MySQL : true Oracle : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.typeMapping.binaryType This is used to specify the binary data type. An attribute defines as 'object' type in Siddhi stream will be stored into RDBMS with this type. H2 : BLOB MySQL : BLOB Oracle : BLOB Microsoft SQL Server : VARBINARY(max) PostgreSQL : BYTEA DB2. : BLOB(64000) N/A {{RDBMS-Name}}.typeMapping.booleanType This is used to specify the boolean data type. An attribute defines as 'bool' type in Siddhi stream will be stored into RDBMS with this type. H2 : TINYINT(1) MySQL : TINYINT(1) Oracle : NUMBER(1) Microsoft SQL Server : BIT PostgreSQL : BOOLEAN DB2. : SMALLINT N/A {{RDBMS-Name}}.typeMapping.doubleType This is used to specify the double data type. An attribute defines as 'double' type in Siddhi stream will be stored into RDBMS with this type. H2 : DOUBLE MySQL : DOUBLE Oracle : NUMBER(19,4) Microsoft SQL Server : FLOAT(32) PostgreSQL : DOUBLE PRECISION DB2. : DOUBLE N/A {{RDBMS-Name}}.typeMapping.floatType This is used to specify the float data type. An attribute defines as 'float' type in Siddhi stream will be stored into RDBMS with this type. H2 : FLOAT MySQL : FLOAT Oracle : NUMBER(19,4) Microsoft SQL Server : REAL PostgreSQL : REAL DB2. : REAL N/A {{RDBMS-Name}}.typeMapping.integerType This is used to specify the integer data type. An attribute defines as 'int' type in Siddhi stream will be stored into RDBMS with this type. H2 : INTEGER MySQL : INTEGER Oracle : NUMBER(10) Microsoft SQL Server : INTEGER PostgreSQL : INTEGER DB2. : INTEGER N/A {{RDBMS-Name}}.typeMapping.longType This is used to specify the long data type. An attribute defines as 'long' type in Siddhi stream will be stored into RDBMS with this type. H2 : BIGINT MySQL : BIGINT Oracle : NUMBER(19) Microsoft SQL Server : BIGINT PostgreSQL : BIGINT DB2. : BIGINT N/A {{RDBMS-Name}}.typeMapping.stringType This is used to specify the string data type. An attribute defines as 'string' type in Siddhi stream will be stored into RDBMS with this type. H2 : VARCHAR(stringSize) MySQL : VARCHAR(stringSize) Oracle : VARCHAR(stringSize) Microsoft SQL Server : VARCHAR(stringSize) PostgreSQL : VARCHAR(stringSize) DB2. : VARCHAR(stringSize) N/A {{RDBMS-Name}}.typeMapping.bigStringType This is used to specify the big string data type. An attribute defines as 'string' type in Siddhi stream and field.length define in the annotation is greater than the fieldSizeLimit, will be stored into RDBMS with this type. H2 : N/A MySQL : N/A Oracle : CLOB Microsoft SQL Server : N/A PostgreSQL : N/A DB2.* : N/A N/A Examples EXAMPLE 1 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/stocks\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"volume\") define table StockTable (symbol string, price float, volume long); The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float', and 'long' respectively). The connection is made as specified by the parameters configured for the '@Store' annotation. The 'symbol' attribute is considered a unique field, and a DB index is created for it. EXAMPLE 2 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)]","title":"rdbms (Store)"},{"location":"docs/api/latest/#str","text":"","title":"Str"},{"location":"docs/api/latest/#groupconcat-aggregate-function","text":"This function aggregates the received events by concatenating the keys in those events using a separator, e.g.,a comma (,) or a hyphen (-), and returns the concatenated key string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:groupConcat( STRING key, STRING separator, STRING distinct, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key The string that needs to be aggregated. STRING No No separator The separator that separates each string key after concatenating the keys. , STRING Yes No distinct This is used to only have distinct values in the concatenated string that is returned. false STRING Yes No order This parameter accepts 'ASC' or 'DESC' strings to sort the string keys in either ascending or descending order respectively. No order STRING Yes No Examples EXAMPLE 1 from InputStream#window.time(5 min) select str:groupConcat(\"key\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , it returns \"A,B,S,C,A\" to the 'OutputStream'. EXAMPLE 2 from InputStream#window.time(5 min) select groupConcat(\"key\",\"-\",true,\"ASC\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , specify the seperator as hyphen and choose the order to be ascending, the function returns \"A-B-C-S\" to the 'OutputStream'.","title":"groupConcat (Aggregate Function)"},{"location":"docs/api/latest/#charat-function","text":"This function returns the 'char' value that is present at the given index position. of the input string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:charAt( STRING input.value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.value The input string of which the char value at the given position needs to be returned. STRING No No index The variable that specifies the index of the char value that needs to be returned. INT No No Examples EXAMPLE 1 charAt(\"WSO2\", 1) In this case, the functiion returns the character that exists at index 1. Hence, it returns 'S'.","title":"charAt (Function)"},{"location":"docs/api/latest/#coalesce-function_1","text":"This returns the first input parameter value of the given argument, that is not null. Origin: siddhi-execution-string:5.0.1 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT str:coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT argn) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic argn It can have one or more input parameters in any data type. However, all the specified parameters are required to be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 coalesce(null, \"BBB\", \"CCC\") This returns the first input parameter that is not null. In this example, it returns \"BBB\".","title":"coalesce (Function)"},{"location":"docs/api/latest/#concat-function","text":"This function returns a string value that is obtained as a result of concatenating two or more input string values. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:concat( STRING argn) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic argn This can have two or more string type input parameters. STRING No No Examples EXAMPLE 1 concat(\"D533\", \"8JU^\", \"XYZ\") This returns a string value by concatenating two or more given arguments. In the example shown above, it returns \"D5338JU^XYZ\".","title":"concat (Function)"},{"location":"docs/api/latest/#contains-function","text":"This function returns true if the input.string contains the specified sequence of char values in the search.string . Origin: siddhi-execution-string:5.0.1 Syntax BOOL str:contains( STRING input.string, STRING search.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string Input string value. STRING No No search.string The string value to be searched for in the input.string . STRING No No Examples EXAMPLE 1 contains(\"21 products are produced by WSO2 currently\", \"WSO2\") This returns a boolean value as the output. In this case, it returns true .","title":"contains (Function)"},{"location":"docs/api/latest/#equalsignorecase-function","text":"This returns a boolean value by comparing two strings lexicographically without considering the letter case. Origin: siddhi-execution-string:5.0.1 Syntax BOOL str:equalsIgnoreCase( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No No arg2 The second input string argument. This is compared with the first argument. STRING No No Examples EXAMPLE 1 equalsIgnoreCase(\"WSO2\", \"wso2\") This returns a boolean value as the output. In this scenario, it returns \"true\".","title":"equalsIgnoreCase (Function)"},{"location":"docs/api/latest/#filltemplate-function","text":"This extension replaces the templated positions that are marked with an index value in a specified template with the strings provided. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:fillTemplate( STRING template, STRING|INT|LONG|DOUBLE|FLOAT|BOOL replacement.strings) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic template The string with templated fields that needs to be filled with the given strings. The format of the templated fields should be as follows: {{INDEX}} where 'INDEX' is an integer. This index is used to map the strings that are used to replace the templated fields. STRING No No replacement.strings The strings with which the templated positions in the template need to be replaced. The minimum of two arguments need to be included in the execution string. There is no upper limit on the number of arguments allowed to be included. STRING INT LONG DOUBLE FLOAT BOOL No No Examples EXAMPLE 1 str:fillTemplate(\"This is {{1}} for the {{2}} function\", 'an example', 'fillTemplate') In this example, the template is 'This is {{1}} for the {{2}} function'.Here, the templated string {{1}} is replaced with the 1 st string value provided, which is 'an example'. {{2}} is replaced with the 2 nd string provided, which is 'fillTemplate' The complete return string is 'This is an example for the fillTemplate function'.","title":"fillTemplate (Function)"},{"location":"docs/api/latest/#hex-function_1","text":"This function returns a hexadecimal string by converting each byte of each character in the input string to two hexadecimal digits. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:hex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the hexadecimal value. STRING No No Examples EXAMPLE 1 hex(\"MySQL\") This returns the hexadecimal value of the input.string. In this scenario, the output is \"4d7953514c\".","title":"hex (Function)"},{"location":"docs/api/latest/#length-function","text":"Returns the length of the input string. Origin: siddhi-execution-string:5.0.1 Syntax INT str:length( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the length. STRING No No Examples EXAMPLE 1 length(\"Hello World\") This outputs the length of the provided string. In this scenario, the, output is 11 .","title":"length (Function)"},{"location":"docs/api/latest/#lower-function","text":"Converts the capital letters in the input string to the equivalent simple letters. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:lower( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to convert to the lower case (i.e., equivalent simple letters). STRING No No Examples EXAMPLE 1 lower(\"WSO2 cep \") This converts the capital letters in the input.string to the equivalent simple letters. In this scenario, the output is \"wso2 cep \".","title":"lower (Function)"},{"location":"docs/api/latest/#regexp-function","text":"Returns a boolean value based on the matchability of the input string and the given regular expression. Origin: siddhi-execution-string:5.0.1 Syntax BOOL str:regexp( STRING input.string, STRING regex) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to match with the given regular expression. STRING No No regex The regular expression to be matched with the input string. STRING No No Examples EXAMPLE 1 regexp(\"WSO2 abcdh\", \"WSO(.*h)\") This returns a boolean value after matching regular expression with the given string. In this scenario, it returns \"true\" as the output.","title":"regexp (Function)"},{"location":"docs/api/latest/#repeat-function","text":"Repeats the input string for a specified number of times. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:repeat( STRING input.string, INT times) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that is repeated the number of times as defined by the user. STRING No No times The number of times the input.string needs to be repeated . INT No No Examples EXAMPLE 1 repeat(\"StRing 1\", 3) This returns a string value by repeating the string for a specified number of times. In this scenario, the output is \"StRing 1StRing 1StRing 1\".","title":"repeat (Function)"},{"location":"docs/api/latest/#replaceall-function","text":"Finds all the substrings of the input string that matches with the given expression, and replaces them with the given replacement string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:replaceAll( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No No regex The regular expression to be matched with the input string. STRING No No replacement.string The string with which each substring that matches the given expression should be replaced. STRING No No Examples EXAMPLE 1 replaceAll(\"hello hi hello\", 'hello', 'test') This returns a string after replacing the substrings of the input string with the replacement string. In this scenario, the output is \"test hi test\" .","title":"replaceAll (Function)"},{"location":"docs/api/latest/#replacefirst-function","text":"Finds the first substring of the input string that matches with the given regular expression, and replaces itwith the given replacement string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:replaceFirst( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be replaced. STRING No No regex The regular expression with which the input string should be matched. STRING No No replacement.string The string with which the first substring of input string that matches the regular expression should be replaced. STRING No No Examples EXAMPLE 1 replaceFirst(\"hello WSO2 A hello\", 'WSO2(.*)A', 'XXXX') This returns a string after replacing the first substring with the given replacement string. In this scenario, the output is \"hello XXXX hello\".","title":"replaceFirst (Function)"},{"location":"docs/api/latest/#reverse-function","text":"Returns the input string in the reverse order character-wise and string-wise. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:reverse( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be reversed. STRING No No Examples EXAMPLE 1 reverse(\"Hello World\") This outputs a string value by reversing the incoming input.string . In this scenario, the output is \"dlroW olleH\".","title":"reverse (Function)"},{"location":"docs/api/latest/#split-function","text":"Splits the input.string into substrings using the value parsed in the split.string and returns the substring at the position specified in the group.number . Origin: siddhi-execution-string:5.0.1 Syntax STRING str:split( STRING input.string, STRING split.string, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No No split.string The string value to be used to split the input.string . STRING No No group.number The index of the split group INT No No Examples EXAMPLE 1 split(\"WSO2,ABM,NSFT\", \",\", 0) This splits the given input.string by given split.string and returns the string in the index given by group.number. In this scenario, the output will is \"WSO2\".","title":"split (Function)"},{"location":"docs/api/latest/#strcmp-function","text":"Compares two strings lexicographically and returns an integer value. If both strings are equal, 0 is returned. If the first string is lexicographically greater than the second string, a positive value is returned. If the first string is lexicographically greater than the second string, a negative value is returned. Origin: siddhi-execution-string:5.0.1 Syntax INT str:strcmp( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No No arg2 The second input string argument that should be compared with the first argument lexicographically. STRING No No Examples EXAMPLE 1 strcmp(\"AbCDefghiJ KLMN\", 'Hello') This compares two strings lexicographically and outputs an integer value.","title":"strcmp (Function)"},{"location":"docs/api/latest/#substr-function","text":"Returns a substring of the input string by considering a subset or all of the following factors: starting index, length, regular expression, and regex group number. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:substr( STRING input.string, INT begin.index, INT length, STRING regex, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be processed. STRING No No begin.index Starting index to consider for the substring. INT No No length The length of the substring. INT No No regex The regular expression that should be matched with the input string. STRING No No group.number The regex group number INT No No Examples EXAMPLE 1 substr(\"AbCDefghiJ KLMN\", 4) This outputs the substring based on the given begin.index . In this scenario, the output is \"efghiJ KLMN\". EXAMPLE 2 substr(\"AbCDefghiJ KLMN\", 2, 4) This outputs the substring based on the given begin.index and length. In this scenario, the output is \"CDef\". EXAMPLE 3 substr(\"WSO2D efghiJ KLMN\", '^WSO2(.*)') This outputs the substring by applying the regex. In this scenario, the output is \"WSO2D efghiJ KLMN\". EXAMPLE 4 substr(\"WSO2 cep WSO2 XX E hi hA WSO2 heAllo\", 'WSO2(.*)A(.*)', 2) This outputs the substring by applying the regex and considering the group.number . In this scenario, the output is \" ello\".","title":"substr (Function)"},{"location":"docs/api/latest/#trim-function","text":"Returns a copy of the input string without the leading and trailing whitespace (if any). Origin: siddhi-execution-string:5.0.1 Syntax STRING str:trim( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that needs to be trimmed. STRING No No Examples EXAMPLE 1 trim(\" AbCDefghiJ KLMN \") This returns a copy of the input.string with the leading and/or trailing white-spaces omitted. In this scenario, the output is \"AbCDefghiJ KLMN\".","title":"trim (Function)"},{"location":"docs/api/latest/#unhex-function","text":"Returns a string by converting the hexadecimal characters in the input string. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:unhex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The hexadecimal input string that needs to be converted to string. STRING No No Examples EXAMPLE 1 unhex(\"4d7953514c\") This converts the hexadecimal value to string.","title":"unhex (Function)"},{"location":"docs/api/latest/#upper-function","text":"Converts the simple letters in the input string to the equivalent capital/block letters. Origin: siddhi-execution-string:5.0.1 Syntax STRING str:upper( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be converted to the upper case (equivalent capital/block letters). STRING No No Examples EXAMPLE 1 upper(\"Hello World\") This converts the simple letters in the input.string to theequivalent capital letters. In this scenario, the output is \"HELLO WORLD\".","title":"upper (Function)"},{"location":"docs/api/latest/#tokenize-stream-processor_1","text":"This function splits the input string into tokens using a given regular expression and returns the split tokens. Origin: siddhi-execution-string:5.0.1 Syntax str:tokenize( STRING input.string, STRING regex, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string which needs to be split. STRING No No regex The string value which is used to tokenize the 'input.string'. STRING No No distinct This flag is used to return only distinct values. false BOOL Yes No Extra Return Attributes Name Description Possible Types token The attribute which contains a single token. STRING Examples EXAMPLE 1 define stream inputStream (str string); @info(name = 'query1') from inputStream#str:tokenize(str , ',') select text insert into outputStream; This query performs tokenization on the given string. If the str is \"Android,Windows8,iOS\", then the string is split into 3 events containing the token attribute values, i.e., Android , Windows8 and iOS .","title":"tokenize (Stream Processor)"},{"location":"docs/quick-start/","text":"Siddhi Quick Start Guide Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Siddhi is used by many companies including Uber, eBay, PayPal (via Apache Eagle), here Uber processed more than 20 billion events per day using Siddhi for their fraud analytics use cases. Siddhi is also used in various analytics and integration platforms such as Apache Eagle as a policy enforcement engine, WSO2 API Manager as analytics and throttling engine, WSO2 Identity Server as an adaptive authorization engine. This quick start guide contains the following six sections: Domain of Siddhi Overview of Siddhi architecture Using Siddhi for the first time Writing first Siddhi Application Testing Siddhi Application A bit of Stream Processing 1. Domain of Siddhi Siddhi is an event driven system where all the data it consumes, processes and sends are modeled as events. Therefore, Siddhi can play a vital part in any event-driven architecture. As Siddhi works with events, first let's understand what an event is through an example. If we consider transactions carried out via an ATM as a data stream, one withdrawal from it can be considered as an event . This event contains data such as amount, time, account number, etc. Many such transactions form a stream. Siddhi provides following functionalities, Streaming Data Analytics Forrester defines Streaming Analytics as: Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . Complex Event Processing (CEP) Gartner\u2019s IT Glossary defines CEP as follows: \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201d event data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" Streaming Data Integration Streaming data integration is a way of integrating several systems by processing, correlating, and analyzing the data in memory, while continuously moving data in real-time from one system to another. Alerts Notifications The system to continuously monitor event streams, and send alerts and notifications, based on defined KPIs and other analytics. Adaptive Decision Making A way to dynamically making real-time decisions based on predefined rules, the current state of the connected systems, and machine learning techniques. Basically, Siddhi receives data event-by-event and processes them in real-time to produce meaningful information. Using the above Siddhi can be used to solve may use-cases as follows: Fraud Analytics Monitoring System Integration Anomaly Detection Sentiment Analysis Processing Customer Behavior .. etc 2. Overview of Siddhi architecture As indicated above, Siddhi can: Accept event inputs from many different types of sources. Process them to transform, enrich, and generate insights. Publish them to multiple types of sinks. To use Siddhi, you need to write the processing logic as a Siddhi Application in the Siddhi Streaming SQL language which is discussed in the section 4 . Here a Siddhi Application is a script file that contains business logic for a scenario. When the Siddhi application is started, it: Consumes data one-by-one as events. Pipe the events to queries through various streams for processing. Generates new events based on the processing done at the queries. Finally, Sends newly generated events through output to streams. 3. Using Siddhi for the first time In this section, we will be using the Siddhi tooling distribution\u200a\u2014\u200aa server version of Siddhi that has a sophisticated web based editor with a GUI (referred to as \u201cSiddhi Editor\u201d ) where you can write Siddhi Apps and simulate events to test your scenario. Step 1 \u200a\u2014\u200aInstall Oracle Java SE Development Kit (JDK) version 1.8. Step 2 \u200a\u2014\u200a Set the JAVA_HOME environment variable. Step 3 \u200a\u2014\u200aDownload the latest tooling distribution from here . Step 4 \u200a\u2014\u200aExtract the downloaded zip and navigate to TOOLING_HOME /bin . ( TOOLING_HOME refers to the extracted folder) Step 5 \u200a\u2014\u200aIssue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh After successfully starting the Siddhi Editor, the terminal should look like as shown below: After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser (Google Chrome is the Recommended). http://localhost:9390/editor This takes you to the Siddhi Editor landing page. 4. Writing first Siddhi Application Siddhi Streaming SQL is a rich, compact, easy-to-use SQL-like language. As the first Siddhi Application, let's learn how to find the total of values from the incoming events and output the current running total value for each event. Siddhi has lot of in-built functions and extensions available for complex analysis, and you can find more information about the Siddhi grammar and its functions from the Siddhi Query Guide . Let's consider sample scenario where we are loading cargo boxes into a ship . Here, we need to keep track of the total weight of the cargo added, and the weight of each loaded cargo box is considered an event . We can write a Siddhi Application for the above scenario using the following 4 parts . Part 1\u200a\u2014\u200aGiving our Siddhi application a suitable name. This allows us to uniquely identity a Siddhi Application. In this example, let's name our application as \u201cHelloWorldApp\u201d @App:name(\"HelloWorldApp\") Part 2\u200a\u2014\u200aDefining the input stream. The stream needs to have a name and a schema defining the data that each incoming event should contain. The event data attributes are expressed as name and type pairs. We can also attach a \"source\" to the created stream, so that we can consume events from outside and send them to the stream. ( Source is the Siddhi way to consume streams from external systems ). For this scenario we will use an http source to consume Cargo Events. When added the http source will spin up a HTTP endpoint and keep on listening for messages. To learn more about sources, refer source ) In this scenario: The name of the input stream\u200a\u2014\u200a \u201cCargoStream\u201d This contains only one data attribute: The name of the data in each event\u200a\u2014\u200a \u201cweight\u201d Type of the data \u201cweight\u201d \u200a\u2014\u200aint Type of source - HTTP HTTP endpoint address - http://0.0.0.0:8006/cargo Accepted input data format - JSON @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); Part 3 - Defining the output stream. This has the same info as the input \u201cCargoStream\u201d stream\u200adefinition with an additional totalWeight attribute containing the total weight calculated so far. In addition we also need to add a log \"sink\" to log the OutputStream so that we can observe the output produced by the stream. ( Sink is the Siddhi way to publish streams to external systems ). This particular log type sink simply logs the stream events. To learn more about sinks, refer sink ) @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); Part 4\u200a\u2014\u200aWriting the Siddhi query. As part of the query we need to specify the following: A name for the query\u200a\u2014\u200a \u201cHelloWorldQuery\u201d The input stream from which the query consumes events \u2014\u200a \u201cCargoStream\u201d How the output to be calculated - by calculating the sum of the *weight**s The data outputted to the output stream\u200a\u2014\u200a \u201cweight\u201d , \u201ctotalWeight\u201d The output stream to which the event should be outputted\u200a\u2014\u200a \u201cOutputStream\u201d @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; This query will calculate the sum of weights from the start of the Siddhi application. For more complex use cases refer Siddhi Query Guild ) Final Siddhi application in the editor will look like following. You can copy the final Siddhi app from below. @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; 5. Testing Siddhi Application In this section first we will test the logical accuracy of Siddhi query using in-built functions of Siddhi Editor. In a later section we will invoke the HTTP endpoint and perform an end to end test. The Siddhi Editor has in-built support to simulate events. You can do it via the \u201cEvent Simulator\u201d panel at the left of the Siddhi Editor. Before running the event simulation, you should save your HelloWorldApp by browsing to File menu - and clicking Save . To simulate events, click Event Simulator and configure Single Simulation as shown below. Step 1\u200a\u2014\u200aConfigurations: Siddhi App Name\u200a\u2014\u200a \u201cHelloWorldApp\u201d Stream Name\u200a\u2014\u200a \u201cCargoStream\u201d Timestamp\u200a\u2014\u200a(Leave it blank) weight\u200a\u2014\u200a2 (or some integer) Step 2\u200a\u2014\u200aClick \u201cRun\u201d mode and then click \u201cStart and Send\u201d . This starts the Siddhi Application and send the event. If the Siddhi application is successfully started, the following message is printed in the Stream Processor Studio console: HelloWorldApp.siddhi Started Successfully! Step 3\u200a\u2014\u200aClick \u201cSend\u201d and observe the terminal . This will send a new event for each click. You can see a logs containing outputData=[2, 2] and outputData=[2, 4] , etc. You can change the value of the weight and send it to see how the sum of the weight is updated. Bravo! You have successfully completed building and testing your first Siddhi Application! 6. A bit of Stream Processing This section will improve our Siddhi app to demonstrates how to carry out temporal window processing with Siddhi. Up to this point, we are calculating the sum of weights from the start of the Siddhi app, and now let's improve it to consider only the last three events for the calculation. For this scenario, let's imagine that when we are loading cargo boxes into the ship and we need to keep track of the average weight of the last three loaded boxes so that we can balance the weight across the ship. For this purpose, let's try to find the average weight of last three boxes of each event. For window processing, we need to modify our query as follows: @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; from CargoStream#window.length(3) - Specifies that we need to consider the last three events in a sliding manner. avg(weight) as averageWeight - Specifies calculating the average of events stored in the window and producing the results as \"averageWeight\" (Note: Similarly the sum also calculates the totalWeight based on the last three events). We also need to modify the \"OutputStream\" definition to accommodate the new \"averageWeight\" . define stream OutputStream(weight int, totalWeight long, averageWeight double); The updated Siddhi Application is given below: @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\",@map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long, averageWeight double); @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; Now you can send events using the Event Simulator and observe the log to see the sum and average of the weights based on the last three cargo events. In the earlier scenario when the window is not used, the system only stored the running sum in its memory, and it did not store any events. But for length based window processing the system will retain the events that fall into the window to perform aggregation operations such as average, maximum, etc. In this case when the 4 th event arrives, the first event in the window is removed ensuring the memory usage does not grow beyond a specific limit. Note: some window types in Siddhi are even more optimized to perform the operations with minimal or no event retention. 7. Running Siddhi Application as a Docker microservice In this step we will run above developed Siddhi application as a microservice utilizing Docker. For other available options please refer here . Here we will use siddhi-runner docker distribution. Follow the below steps to obtain the docker. Install docker in your machine and start the daemon ( https://docs.docker.com/install/ ). Pull the latest siddhi-runner image by executing below command. docker pull siddhiio/siddhi-runner-alpine:latest * Navigate to Siddhi Editor and choose File - Export File for download above Siddhi application as a file. * Move downloaded Siddhi file( HelloWorldApp.siddhi ) to a desired location (e.g. /home/me/siddhi-apps ) * Execute below command to start the Siddhi Application as a microservice. docker run -it -p 8006:8006 -v /home/me/siddhi-apps:/apps siddhiio/siddhi-runner-alpine -Dapps=/apps/HelloWorldApp.siddhi Note: Make sure to update the /home/me/siddhi-apps with the folder path you have stored the HelloWorldApp.siddhi app. * Once container is started use below curl command to send events into \"CargoStream\" curl -X POST http://localhost:8006/cargo \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"weight\":2}}' * You will be able to observe outputs via logs as shown below. [2019-04-24 08:54:51,755] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096091751, data=[2, 2, 2.0], isExpired=false} [2019-04-24 08:56:25,307] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096185307, data=[2, 4, 2.0], isExpired=false} To learn more about the Siddhi functionality, see Siddhi Documentation . If you have questions please post them on Stackoverflow with \"Siddhi\" tag.","title":"Quick Start"},{"location":"docs/quick-start/#siddhi-quick-start-guide","text":"Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Siddhi is used by many companies including Uber, eBay, PayPal (via Apache Eagle), here Uber processed more than 20 billion events per day using Siddhi for their fraud analytics use cases. Siddhi is also used in various analytics and integration platforms such as Apache Eagle as a policy enforcement engine, WSO2 API Manager as analytics and throttling engine, WSO2 Identity Server as an adaptive authorization engine. This quick start guide contains the following six sections: Domain of Siddhi Overview of Siddhi architecture Using Siddhi for the first time Writing first Siddhi Application Testing Siddhi Application A bit of Stream Processing","title":"Siddhi Quick Start Guide"},{"location":"docs/quick-start/#1-domain-of-siddhi","text":"Siddhi is an event driven system where all the data it consumes, processes and sends are modeled as events. Therefore, Siddhi can play a vital part in any event-driven architecture. As Siddhi works with events, first let's understand what an event is through an example. If we consider transactions carried out via an ATM as a data stream, one withdrawal from it can be considered as an event . This event contains data such as amount, time, account number, etc. Many such transactions form a stream. Siddhi provides following functionalities, Streaming Data Analytics Forrester defines Streaming Analytics as: Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . Complex Event Processing (CEP) Gartner\u2019s IT Glossary defines CEP as follows: \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201d event data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" Streaming Data Integration Streaming data integration is a way of integrating several systems by processing, correlating, and analyzing the data in memory, while continuously moving data in real-time from one system to another. Alerts Notifications The system to continuously monitor event streams, and send alerts and notifications, based on defined KPIs and other analytics. Adaptive Decision Making A way to dynamically making real-time decisions based on predefined rules, the current state of the connected systems, and machine learning techniques. Basically, Siddhi receives data event-by-event and processes them in real-time to produce meaningful information. Using the above Siddhi can be used to solve may use-cases as follows: Fraud Analytics Monitoring System Integration Anomaly Detection Sentiment Analysis Processing Customer Behavior .. etc","title":"1. Domain of Siddhi"},{"location":"docs/quick-start/#2-overview-of-siddhi-architecture","text":"As indicated above, Siddhi can: Accept event inputs from many different types of sources. Process them to transform, enrich, and generate insights. Publish them to multiple types of sinks. To use Siddhi, you need to write the processing logic as a Siddhi Application in the Siddhi Streaming SQL language which is discussed in the section 4 . Here a Siddhi Application is a script file that contains business logic for a scenario. When the Siddhi application is started, it: Consumes data one-by-one as events. Pipe the events to queries through various streams for processing. Generates new events based on the processing done at the queries. Finally, Sends newly generated events through output to streams.","title":"2. Overview of Siddhi architecture"},{"location":"docs/quick-start/#3-using-siddhi-for-the-first-time","text":"In this section, we will be using the Siddhi tooling distribution\u200a\u2014\u200aa server version of Siddhi that has a sophisticated web based editor with a GUI (referred to as \u201cSiddhi Editor\u201d ) where you can write Siddhi Apps and simulate events to test your scenario. Step 1 \u200a\u2014\u200aInstall Oracle Java SE Development Kit (JDK) version 1.8. Step 2 \u200a\u2014\u200a Set the JAVA_HOME environment variable. Step 3 \u200a\u2014\u200aDownload the latest tooling distribution from here . Step 4 \u200a\u2014\u200aExtract the downloaded zip and navigate to TOOLING_HOME /bin . ( TOOLING_HOME refers to the extracted folder) Step 5 \u200a\u2014\u200aIssue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh After successfully starting the Siddhi Editor, the terminal should look like as shown below: After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser (Google Chrome is the Recommended). http://localhost:9390/editor This takes you to the Siddhi Editor landing page.","title":"3. Using Siddhi for the first time"},{"location":"docs/quick-start/#4-writing-first-siddhi-application","text":"Siddhi Streaming SQL is a rich, compact, easy-to-use SQL-like language. As the first Siddhi Application, let's learn how to find the total of values from the incoming events and output the current running total value for each event. Siddhi has lot of in-built functions and extensions available for complex analysis, and you can find more information about the Siddhi grammar and its functions from the Siddhi Query Guide . Let's consider sample scenario where we are loading cargo boxes into a ship . Here, we need to keep track of the total weight of the cargo added, and the weight of each loaded cargo box is considered an event . We can write a Siddhi Application for the above scenario using the following 4 parts . Part 1\u200a\u2014\u200aGiving our Siddhi application a suitable name. This allows us to uniquely identity a Siddhi Application. In this example, let's name our application as \u201cHelloWorldApp\u201d @App:name(\"HelloWorldApp\") Part 2\u200a\u2014\u200aDefining the input stream. The stream needs to have a name and a schema defining the data that each incoming event should contain. The event data attributes are expressed as name and type pairs. We can also attach a \"source\" to the created stream, so that we can consume events from outside and send them to the stream. ( Source is the Siddhi way to consume streams from external systems ). For this scenario we will use an http source to consume Cargo Events. When added the http source will spin up a HTTP endpoint and keep on listening for messages. To learn more about sources, refer source ) In this scenario: The name of the input stream\u200a\u2014\u200a \u201cCargoStream\u201d This contains only one data attribute: The name of the data in each event\u200a\u2014\u200a \u201cweight\u201d Type of the data \u201cweight\u201d \u200a\u2014\u200aint Type of source - HTTP HTTP endpoint address - http://0.0.0.0:8006/cargo Accepted input data format - JSON @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); Part 3 - Defining the output stream. This has the same info as the input \u201cCargoStream\u201d stream\u200adefinition with an additional totalWeight attribute containing the total weight calculated so far. In addition we also need to add a log \"sink\" to log the OutputStream so that we can observe the output produced by the stream. ( Sink is the Siddhi way to publish streams to external systems ). This particular log type sink simply logs the stream events. To learn more about sinks, refer sink ) @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); Part 4\u200a\u2014\u200aWriting the Siddhi query. As part of the query we need to specify the following: A name for the query\u200a\u2014\u200a \u201cHelloWorldQuery\u201d The input stream from which the query consumes events \u2014\u200a \u201cCargoStream\u201d How the output to be calculated - by calculating the sum of the *weight**s The data outputted to the output stream\u200a\u2014\u200a \u201cweight\u201d , \u201ctotalWeight\u201d The output stream to which the event should be outputted\u200a\u2014\u200a \u201cOutputStream\u201d @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; This query will calculate the sum of weights from the start of the Siddhi application. For more complex use cases refer Siddhi Query Guild ) Final Siddhi application in the editor will look like following. You can copy the final Siddhi app from below. @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream;","title":"4. Writing first Siddhi Application"},{"location":"docs/quick-start/#5-testing-siddhi-application","text":"In this section first we will test the logical accuracy of Siddhi query using in-built functions of Siddhi Editor. In a later section we will invoke the HTTP endpoint and perform an end to end test. The Siddhi Editor has in-built support to simulate events. You can do it via the \u201cEvent Simulator\u201d panel at the left of the Siddhi Editor. Before running the event simulation, you should save your HelloWorldApp by browsing to File menu - and clicking Save . To simulate events, click Event Simulator and configure Single Simulation as shown below. Step 1\u200a\u2014\u200aConfigurations: Siddhi App Name\u200a\u2014\u200a \u201cHelloWorldApp\u201d Stream Name\u200a\u2014\u200a \u201cCargoStream\u201d Timestamp\u200a\u2014\u200a(Leave it blank) weight\u200a\u2014\u200a2 (or some integer) Step 2\u200a\u2014\u200aClick \u201cRun\u201d mode and then click \u201cStart and Send\u201d . This starts the Siddhi Application and send the event. If the Siddhi application is successfully started, the following message is printed in the Stream Processor Studio console: HelloWorldApp.siddhi Started Successfully! Step 3\u200a\u2014\u200aClick \u201cSend\u201d and observe the terminal . This will send a new event for each click. You can see a logs containing outputData=[2, 2] and outputData=[2, 4] , etc. You can change the value of the weight and send it to see how the sum of the weight is updated. Bravo! You have successfully completed building and testing your first Siddhi Application!","title":"5. Testing Siddhi Application"},{"location":"docs/quick-start/#6-a-bit-of-stream-processing","text":"This section will improve our Siddhi app to demonstrates how to carry out temporal window processing with Siddhi. Up to this point, we are calculating the sum of weights from the start of the Siddhi app, and now let's improve it to consider only the last three events for the calculation. For this scenario, let's imagine that when we are loading cargo boxes into the ship and we need to keep track of the average weight of the last three loaded boxes so that we can balance the weight across the ship. For this purpose, let's try to find the average weight of last three boxes of each event. For window processing, we need to modify our query as follows: @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; from CargoStream#window.length(3) - Specifies that we need to consider the last three events in a sliding manner. avg(weight) as averageWeight - Specifies calculating the average of events stored in the window and producing the results as \"averageWeight\" (Note: Similarly the sum also calculates the totalWeight based on the last three events). We also need to modify the \"OutputStream\" definition to accommodate the new \"averageWeight\" . define stream OutputStream(weight int, totalWeight long, averageWeight double); The updated Siddhi Application is given below: @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\",@map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long, averageWeight double); @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; Now you can send events using the Event Simulator and observe the log to see the sum and average of the weights based on the last three cargo events. In the earlier scenario when the window is not used, the system only stored the running sum in its memory, and it did not store any events. But for length based window processing the system will retain the events that fall into the window to perform aggregation operations such as average, maximum, etc. In this case when the 4 th event arrives, the first event in the window is removed ensuring the memory usage does not grow beyond a specific limit. Note: some window types in Siddhi are even more optimized to perform the operations with minimal or no event retention.","title":"6. A bit of Stream Processing"},{"location":"docs/quick-start/#7-running-siddhi-application-as-a-docker-microservice","text":"In this step we will run above developed Siddhi application as a microservice utilizing Docker. For other available options please refer here . Here we will use siddhi-runner docker distribution. Follow the below steps to obtain the docker. Install docker in your machine and start the daemon ( https://docs.docker.com/install/ ). Pull the latest siddhi-runner image by executing below command. docker pull siddhiio/siddhi-runner-alpine:latest * Navigate to Siddhi Editor and choose File - Export File for download above Siddhi application as a file. * Move downloaded Siddhi file( HelloWorldApp.siddhi ) to a desired location (e.g. /home/me/siddhi-apps ) * Execute below command to start the Siddhi Application as a microservice. docker run -it -p 8006:8006 -v /home/me/siddhi-apps:/apps siddhiio/siddhi-runner-alpine -Dapps=/apps/HelloWorldApp.siddhi Note: Make sure to update the /home/me/siddhi-apps with the folder path you have stored the HelloWorldApp.siddhi app. * Once container is started use below curl command to send events into \"CargoStream\" curl -X POST http://localhost:8006/cargo \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"weight\":2}}' * You will be able to observe outputs via logs as shown below. [2019-04-24 08:54:51,755] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096091751, data=[2, 2, 2.0], isExpired=false} [2019-04-24 08:56:25,307] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096185307, data=[2, 4, 2.0], isExpired=false} To learn more about the Siddhi functionality, see Siddhi Documentation . If you have questions please post them on Stackoverflow with \"Siddhi\" tag.","title":"7. Running Siddhi Application as a Docker microservice"}]}
