{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"download/","text":"Siddhi 5.2 Download Siddhi Select the appropriate Siddhi distribution for your use case. Siddhi Distribution Siddhi 5.2 ( Distribution 0.1.0 ) Siddhi Tooling Siddhi Runner Refer the user guide to use Siddhi as a Local Microservice Siddhi Docker Siddhi 5.2 (based on Distribution 0.1.0) Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu Refer the user guide to use Siddhi as a Docker Microservice Siddhi Kubernetes Siddhi 5.2 (based on Distribution 0.1.1) Siddhi CRD Refer the user guide to use Siddhi as Kubernetes Microservice Siddhi Libs Siddhi 5.2.x Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Refer the user guide to use Siddhi as a Java library For other Siddhi Versions refer the Download Archives","title":"Download"},{"location":"download/#siddhi-52-download-siddhi","text":"Select the appropriate Siddhi distribution for your use case.","title":"Siddhi 5.2 Download Siddhi"},{"location":"download/#siddhi-distribution","text":"Siddhi 5.2 ( Distribution 0.1.0 ) Siddhi Tooling Siddhi Runner Refer the user guide to use Siddhi as a Local Microservice","title":"Siddhi Distribution"},{"location":"download/#siddhi-docker","text":"Siddhi 5.2 (based on Distribution 0.1.0) Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu Refer the user guide to use Siddhi as a Docker Microservice","title":"Siddhi Docker"},{"location":"download/#siddhi-kubernetes","text":"Siddhi 5.2 (based on Distribution 0.1.1) Siddhi CRD Refer the user guide to use Siddhi as Kubernetes Microservice","title":"Siddhi Kubernetes"},{"location":"download/#siddhi-libs","text":"Siddhi 5.2.x Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Refer the user guide to use Siddhi as a Java library For other Siddhi Versions refer the Download Archives","title":"Siddhi Libs"},{"location":"introduction/","text":"Siddhi Deployment Guide This section provides information on developing and running Siddhi. Siddhi Application A self contained stream processing logic can be written as a Siddhi Application and put together in a single file with .siddhi extension. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logic that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, configure them using a common topic using In-Memory Sink and In-Memory Source . For writing Siddhi Application using Streaming SQL refer Siddhi Query Guide Execution Environments Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP) System Requirements For all execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"Siddhi Deployment Guide"},{"location":"introduction/#siddhi-deployment-guide","text":"This section provides information on developing and running Siddhi.","title":"Siddhi Deployment Guide"},{"location":"introduction/#siddhi-application","text":"A self contained stream processing logic can be written as a Siddhi Application and put together in a single file with .siddhi extension. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logic that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, configure them using a common topic using In-Memory Sink and In-Memory Source . For writing Siddhi Application using Streaming SQL refer Siddhi Query Guide","title":"Siddhi Application"},{"location":"introduction/#execution-environments","text":"Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP)","title":"Execution Environments"},{"location":"introduction/#system-requirements","text":"For all execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"System Requirements"},{"location":"release-notes/","text":"Release Notes WIP","title":"Release Notes"},{"location":"release-notes/#release-notes","text":"WIP","title":"Release Notes"},{"location":"development/","text":"Siddhi 5.2 Development Guide Obtaining and Building Project Source code Find the project source code here and the instruction to building the project repos here . Getting Involved in Project Development Siddhi design-related discussions are carried out in the Siddhi-Dev Google Group , you can subscribe to it to get notifications on the discussions and please feel free to get involved by contributing and sharing your thoughts and ideas. You can also propose changes or improvements by starting a thread in the Siddhi-Dev Google Group, and also by reporting issues in the Siddhi GitHub repository with the label type/improvement or type/new-feature . Project Architecture Find out about the architecture of Siddhi for the Siddhi Architecture documentation.","title":"Introduction"},{"location":"development/#siddhi-52-development-guide","text":"","title":"Siddhi 5.2 Development Guide"},{"location":"development/#obtaining-and-building-project-source-code","text":"Find the project source code here and the instruction to building the project repos here .","title":"Obtaining and Building Project Source code"},{"location":"development/#getting-involved-in-project-development","text":"Siddhi design-related discussions are carried out in the Siddhi-Dev Google Group , you can subscribe to it to get notifications on the discussions and please feel free to get involved by contributing and sharing your thoughts and ideas. You can also propose changes or improvements by starting a thread in the Siddhi-Dev Google Group, and also by reporting issues in the Siddhi GitHub repository with the label type/improvement or type/new-feature .","title":"Getting Involved in Project Development"},{"location":"development/#project-architecture","text":"Find out about the architecture of Siddhi for the Siddhi Architecture documentation.","title":"Project Architecture"},{"location":"development/architecture/","text":"Siddhi 5.2 Architecture Siddhi is an open source, cloud-native, stream processing and complex event processing engine. It can be utilized in any of the following ways: Run as a server on its own Run as a micro service on bare metal, VM, Docker and natively in Kubernetes Embedded into any Java or Python based application Run on an Android application Siddhi provides streaming data integration and data analytical operators. It connects multiple disparate live data sources, orchestrates data flows, calculates analytics, and also detects complex event patterns. This allows developers to build applications that collect data, perform data transformation and analytics, and publish the results to data sinks in real time. This section illustrates the architecture of the Siddhi Engine and guides you through its key functionality. We hope this article helps developers to understand Siddhi and its codebase better, and also help them to contribute and improve Siddhi. Main Design Decisions Event-by-event processing of real-time streaming data to achieve low latency. Ease of use with Streaming SQL providing an intuitive way to express stream processing logic and complex event processing constructs such as Patterns. Achieve high performance by processing events in-memory and using data stores for long term data storage. Optimize performance by enforcing a strict event stream schema and by pre-compiling the queries. Optimize memory consumption by having only the absolutely necessary information in-memory and dropping the rest as soon as possible. Supporting multiple extension points to accommodate a diverse set of functionality such as supporting multiple sources, sinks, functions, aggregation operations, windows, etc. High-Level Architecture At a high level, Siddhi consumes events from various events sources, processes them according to the defined Siddhi application, and produces results to the subscribed event sinks. Siddhi can store and consume events from in-memory tables or from external data stores such as RDBMS , MongoDB , Hazelcast in-memory grid, etc. (i.e., when configured to do so). Siddhi also allows applications and users to query Siddhi via its Store Query API to interactively retrieve data from in-memory and other stores. Main Modules in Siddhi Engine Siddhi Engine comprises four main modules, they are: Siddhi Query API : This allows users to define the execution logic of the Siddhi application as queries and definitions using POJOs (Plain Old Java Objects). Internally, Siddhi uses these objects to identify the logic that it is expected to perform. Siddhi Query Compiler : This allows users to define the Siddhi application using the Siddhi Streaming SQL, and it compiles the Streaming SQL script to Siddhi Query API POJOs so that Siddhi can execute them. Siddhi Core : This builds the execution runtime based on the defined Siddhi Application POJOs and processes the incoming events as and when they arrive. Siddhi Annotation : This is a helper module that allows all extensions to be annotated so that they can be picked by Siddhi Core for processing. This also helps Siddhi to generate the extension documentation. Siddhi Component Architecture The following diagram illustrates the main components of Siddhi and how they work together. Here the Siddhi Core module maintains the execution logic. It also interacts with the external environment and systems for consuming, processing and publishing events. It uses the following components to achieve its tasks: SiddhiManager : This is a key component of Siddhi Core that manages Siddhi Application Runtimes and facilitates their functionality via Siddhi Context with periodic state persistence, statistics reporting and extension loading. It is recommended to use one Siddhi Manager for a single JVM. SiddhiAppRuntime : Siddhi Application Runtime can be generated for each Siddhi Application through the Siddhi Manager. Siddhi Application Runtimes provide an isolated execution environment for each defined Siddhi Application. These Siddhi Application Runtimes can have their own lifecycle and they execute based on the logic defined in their Siddhi Application. SiddhiContext : This is a shared object across all the Siddhi Application Runtimes within the same Siddhi manager. It contains references to the persistence store for periodic persistence, statistics manager to report performance statistics of Siddhi Application Runtimes, and extension holders for loading Siddhi extensions. Siddhi Application Creation Execution logic of the Siddhi Engine is composed as a Siddhi Application, and this is usually passed as a string to SiddhiManager to create the SiddhiAppRuntime for execution. When a Siddhi Application is passed to the SiddhiManager.createSiddhiAppRuntime() , it is processed internally with the SiddhiCompiler . Here, the SiddhiApp String is compiled to SiddhiApp object model by the SiddhiQLBaseVisitorImpl class. This validates the syntax of the given Siddhi Application. The model is then passed to the SiddhiAppParser to create the SiddhiAppRuntime . During this phase, the semantics of the Siddhi Application is validated and the execution logic of the Siddhi Application is optimized. Siddhi App Execution Flow Following diagram depicts the execution flow within a Siddhi App Runtime. The path taken by events within Siddhi Engine is indicated in blue. The components that are involved in handling the events are the following: StreamJunction This routes events of a particular stream to various components within the Siddhi App Runtime. A stream junction is generated for each defined or inferred Stream in the Siddhi Application. A stream junction by default uses the incoming event's thread and passes all the events to its subscribed components as soon as they arrive, but this behaviour can be altered by configuring @Async annotation to buffer the events at the and stream junction and to use another one or more threads to collect the events from the buffer and process the subsequent executions. InputHandler Input handler is used to push Event and Event[] objects into stream junctions from defined event sources, and from Java/Python programmes. StreamCallback This receives Event[] s from stream junction and passes them to event sinks to publish to external endpoints, and/or passes them to subscribed Java/Python programmes for further processing. Queries Partitions These components process events by filtering, transforming, aggregating, joining, pattern matching, etc. They consume events from one or more stream junctions, process them and publish the processed events into a set of stream junctions based on the defined queries or partitions. Source Sources consume events from external sources in various data formats, convert them into Siddhi events using SourceMapper s and pass them to corresponding stream junction via their associated input handlers. A source is generated for each @Source annotation defined above a stream definition. SourceMapper A source mapper is a sub-component of source, and it needs to be configured for each source in order to convert the incoming event into Siddhi event. The source mapper type can be configured using the @Map annotation within the @Source annotation. When the @Map annotation is not defined, Siddhi uses the PassThroughSourceMapper , where it assumes that the incoming message is already in the Siddhi Event format (i.e Event or Event[] ), and therefore makes no changes to the incoming event format. Sink Sinks consumes events from its associated stream junction, convert them to various data formats via SinkMapper and publish them to external endpoints as defined in the @Sink annotation. A sink is generated for each @Sink annotation defined above a stream definition. SinkMapper A sink mapper is a sub-component of sink. and its need to be configured for each sink in order to map the Siddhi events to the specified data format so that they can be published via the sink. The sink mapper type can be configured using the @Map annotation within the @Sink annotation. When the @Map annotation is not defined, Siddhi uses PassThroughSinkMapper , where it passes the Siddhi Event (i.e Event or Event[] ) without any formatting to the Sink. Table Tables are used to store events. When tables are defined by default, Siddhi uses the InMemoryTable implementation to store events in-memory. When @Store annotation is used on top of the table definition, it loads the associated external data store connector based on the defined store type. Most table implementations are extended from either AbstractRecordTable or AbstractQueryableRecordTable abstract classes the former provides the functionality to query external data store based on a given filtering condition, and the latter queries external data store by providing projection, limits, and ordering parameters in addition to data filter condition. Window Windows store events as and when they arrive and automatically expire/clean them based on the given window constraint. Multiple types of windows are can be implemented by extending the WindowProcessor abstract class. IncrementalAggregation Long running time series aggregates defined via the aggregation definition is calculated in an incremental manner using the Incremental Aggregation Processor for the defined time periods. Incremental aggregation functions can be implemented by extending IncrementalAttributeAggregator . By default, incremental aggregations aggregate all the values in-memory, but when it is associated with a store by adding @store annotation it uses in-memory to aggregate partial results and uses data stores to persist those increments. When requested for aggregate results it retrieves data from data stores and (if needed from) in-memory, computes combined aggregate results and provides as the output. Trigger A trigger triggers events at a given interval as given in the trigger definition. The triggered events are pushed to a stream junction having the same name as the trigger. QueryCallback A query callback taps into the events that are emitted by a particular query. It notifies the event occurrence timestamp and classifies the output events into currentEvents , and expiredEvents . Siddhi Query Execution Siddhi QueryRuntimes can be categorized into three main types: SingleInputStream : Queries that consist of query types such as filters and windows. JoinInputStream : Queries that consist of joins. StateInputStream : Queries that consist of patterns and sequences. The following section explains the internals of each query type. SingleInputStream Query Runtime (Filter Windows) A single input stream query runtime is generated for filter and window queries. They consume events from a stream junction or a window and convert the incoming events according to the expected output stream format at the ProcessStreamReceiver by dropping all the unrelated incoming stream attributes. Then the converted events are passed through a few Processors such as FilterProcessor , StreamProcessor , StreamFunctionProcessor , WindowProcessor , and QuerySelector . Here, the StreamProcessor , StreamFunctionProcessor , and WindowProcessor can be extended with various stream processing capabilities. The last processor of the chain of processors must always be a QuerySelector and it can't appear anywhere else. When the query runtime consumes events from a stream, its processor chain can maximum contain one WindowProcessor , and when query runtime consumes events from a window, its chain of processors cannot contain any WindowProcessor . The FilterProcessor is implemented using expressions that return a boolean value. ExpressionExecutor is used to process conditions, mathematical operations, unary operations, constant values, variables, and functions. Expressions have a tree structure, and they are processed based using the Depth First search algorithm. To achieve high performance, Siddhi currently depends on the user to formulate the least successful case in the leftmost side of the condition, thereby increasing the chance of early false detection. The condition expression price = 100 and ( Symbol == 'IBM' or Symbol == 'MSFT' ) is represented as shown below. These expressions also support the execution of user-defined functions (UDFs), and they can be implemented by extending the FunctionExecutor class. After getting processed by all the processors, events reach the QuerySelector for transformation. At the QuerySelector , events are transformed based on the select clause of the query. The select clause produces one AttributeProcessor for each output stream attribute, and these AttributeProcessor s contain expressions defining data transformation including constant values, variables, user-defined functions, etc. They can also contain AttributeAggregatorExecutor s to process aggregation operations such as sum , count , etc. If there is a Group By clause defined, then the GroupByKeyGenerator is used to identify the composite group-by key, and then for each key, an AttributeAggregatorExecutor state is generated to maintain per group-by key aggregations. When each time AttributeProcessor is executed the AttributeAggregatorExecutor calculates per group-by aggregation results and output the values. When AttributeAggregatorExecutor group-by states become obsolete, they are destroyed and automatically cleaned. After an event is transformed to the output format through the above process, it is evaluated against the having condition executor if a having clause is provided. The succeeding events are then ordered, and limited based on order by , limit and offset clauses before they pushed to the OutputRateLimiter . At OutputRateLimiter , the event output is controlled before sending the events to the stream junction or to the query callback. When the output clause is not defined, the PassThroughOutputRateLimiter is used by passing all the events without any rate limiting. Temporal Processing with Windows The temporal event processing aspect is achieved via Window and AttributeAggregators To achieve temporal processing, Siddhi uses the following four type of events: Current Events : Events that are newly arriving to the query from streams. Expired Events : Events that have expired from a window. Timer Events : Events that inform the query about an update of execution time. These events are usually generated by schedulers. Reset Events : Events that resets the Siddhi query states. In Siddhi, when an event comes into a WindowProcessor , it creates an appropriate expired event corresponding to the incoming current event with the expiring timestamp, and stores that event in the window. At the same time, WindowProcessor also forwards the current event to the next processor for further processing. It uses a scheduler or some other counting approach to determine when to emit the events that are stored in in-memory. When the expired events meet the condition for expiry based on the window contains, it emits the expired events to the next processor. At times like in window.timeBatch() there can be cases that need emitting all the events in-memory at once and the output does not need individual expired events values, in this cases the window emits a single reset event instead of sending one expired event for each event it has stored, so that it can reset the states in one go. For the QuerySelector aggregations to work correctly the window must emit a corresponding expired event for each current event it has emitted or it must send a reset event . In the QuerySelector , the arrived current events increase the aggregation values, expired events decrease the values, and reset events reset the aggregation calculation to produce correct query output. For example, the sliding TimeWindow ( window.time() ) creates a corresponding expired event for each current event that arrives, adds the expired event s to the window, adds an entry to the scheduler to notify when that event need to be expired, and finally sends the current event to the next processor for subsequent processing. The scheduler notifies the window by sending a timer event , and when the window receives an indication that the expected expiry time has come for the oldest event in the window via a timer event or by other means, it removes the expired event from the window and passes that to the next processor. JoinInputStream Query Runtime (Join) Join input stream query runtime is generated for join queries. This can consume events from two stream junctions and perform a join operation as depicted above. It can also perform a join by consuming events from one stream junction and join against itself, or it can also join against a table, window or an aggregation. When a join is performed with a table, window or aggregation, the WindowProcessor in the above image is replaced with the corresponding table, window or aggregation and no basic processors are used on their side. The joining operation is triggered by the events that arrive from the stream junction. Here, when an event from one stream reaches the pre JoinProcessor , it matches against all the available events of the other stream's WindowProcessor . When a match is found, those matched events are sent to the QuerySelector as current events , and at the same time, the original event is added to the WindowProcessor where it remains until it expires. Similarly, when an event expires from the WindowProcessor , it matches against all the available events of the other stream's WindowProcessor , and when a match is found, those matched events are sent to the QuerySelector as expired events . Note Despite the optimizations, a join query is quite expensive when it comes to performance. This is because the WindowProcessor is locked during the matching process to avoid race conditions and to achieve accuracy while joining. Therefore, when possible avoid matching large (time or length) windows in high volume streams. StateInputStream Query Runtime (Pattern Sequence) The state input stream query runtime is generated for pattern and sequence queries. This consumes events from one or more stream junctions via ProcessStreamReceiver s and checks whether the events match each pattern or sequence condition by processing the set of basic processors associated with each ProcessStreamReceiver . The PreStateProcessor s usually contains lists of state events that are already matched by previous conditions, and if its the first condition then it will have an empty state event in its list. When ProcessStreamReceiver consumes an event, it passes the event to the PreStateProcessor which updates the list of state events it has with the incoming event and executes the condition by passing the events to the basic processors. The state events that match the conditions reach the PostStateProcessor which will then stores the events to the state event list of the following PreStateProcessor . If it is the final condition's PostStateProcessor , then it will pass the state event to the QuerySelector to generate and emit the output. Siddhi Partition Execution A partition is a wrapper around one or more Siddhi queries and inner streams that connect them. A partition is implemented in Siddhi as a PartitionRuntime which contains multiple QueryRuntime s and inner stream junctions. Each partitioned stream entering the partition goes through a designated PartitionStreamReceiver . The PartitionExecutor of PartitionStreamReceiver evaluates the incoming events to identify their associated partition-key using either RangePartitionExecutor or ValuePartitionExecutor . The identified partition-key is then set as thread local variable and the event is passed to the QueryRuntime s of processing. The QueryRuntime s process events by maintaining separate states for each partition-key such that producing separate output per partition. When a partition query consumes a non-partitioned global stream, the QueryRuntime s are executed for each available partition-key in the system such that allowing all partitions to receive the same event. When the partitions are obsolete PartitionRuntime deletes all the partition states from its QueryRuntime s. Siddhi Aggregation Siddhi supports long duration time series aggregations via its aggregation definition. AggregationRuntime implements this by the use of streaming lambda architecture , where it processes part of the data in-memory and gets part of the data from data stores. AggregationRuntime creates an in-memory table or external store for each time granularity (i.e seconds, minutes, days, etc) it has to process the events, and when events enter it calculates the aggregations in-memory for its least granularity (usually seconds) using the IncrementalExecutor and maintains the running aggregation values in its BaseIncrementalValueStore . At each clock end time of the granularity (end of each second) IncrementalExecutor stores the summarized values to the associated granularity table and also passes the summarized values to the IncrementalExecutor of the next granularity level, which also follows the same methodology in processing the events. Through this approach each time granularities, the current time duration will be in-memory and all the historical time durations will be in stored in the tables. The aggregations results are calculated by IncrementalAttributeAggregator s and stored in such a way that allows proper data composition upon retrial, for example, avg() is stored as sum and count . This allows data composition across various granularity time durations when retrieving, for example, results for avg() composed by returning sum of sum s divided by the sum of count s. Aggregation can also work in a distributed manner and across system restarts. This is done by storing node specific IDs and granularity time duration information in the tables. To make sure tables do not go out of memory IncrementalDataPurger is used to purge old data. When aggregation is queried through join or store query for a given time granularity it reads the data from the in-memory BaseIncrementalValueStore and from the tables computes the composite results as described, and presents the results. Siddhi Event Formats Siddhi has three event formats. Event This is the format exposed to external systems when they send events via Input Handler and consume events via Stream Callback or Query Callback. This consists of a timestamp and an Object[] that contains all the values in accordance to the corresponding stream. StreamEvent (Subtype of ComplexEvent ) This is used within queries. This contains a timestamp and the following three Object[] s: beforeWindowData : This contains values that are only used in processors that are executed before the WindowProcessor . onAfterWindowData : This contains values that are only used by the WindowProcessor and the other processors that follow it, but not sent as output. outputData : This contains the values that are sent via the output stream of the query. In order to optimize the amount of data that is stored in the in-memory at windows, the content in beforeWindowData is cleared before the event enters the WindowProcessor . StreamEvents can also be chained by linking each other via the next property in them. StateEvent (Subtype of ComplexEvent ) This is used in joins, patterns and sequences queries when we need to associate events of multiple streams, tables, windows or aggregations together. This contains a timestamp , a collection of StreamEvent s representing different streams, tables, etc, that are used in the query, and an Object[] to contain outputData values that are needed for query output. The StreamEvent s within the StateEvent and the StateEvent themselves can be chained by linking each other with the next property in them. Event Chunks Event Chunks provide an easier way of manipulating the chain of StreamEvent s and StateEvent s so that they are be easily iterated, inserted and removed. Summary This article focuses on describing the architecture of Siddhi and rationalizing some of the architectural decisions made when implementing the system. It also explains the key features of Siddhi. We hope this will be a good starting point for new developers to understand Siddhi and to start contributing to it.","title":"Architecture"},{"location":"development/architecture/#siddhi-52-architecture","text":"Siddhi is an open source, cloud-native, stream processing and complex event processing engine. It can be utilized in any of the following ways: Run as a server on its own Run as a micro service on bare metal, VM, Docker and natively in Kubernetes Embedded into any Java or Python based application Run on an Android application Siddhi provides streaming data integration and data analytical operators. It connects multiple disparate live data sources, orchestrates data flows, calculates analytics, and also detects complex event patterns. This allows developers to build applications that collect data, perform data transformation and analytics, and publish the results to data sinks in real time. This section illustrates the architecture of the Siddhi Engine and guides you through its key functionality. We hope this article helps developers to understand Siddhi and its codebase better, and also help them to contribute and improve Siddhi.","title":"Siddhi 5.2 Architecture"},{"location":"development/architecture/#main-design-decisions","text":"Event-by-event processing of real-time streaming data to achieve low latency. Ease of use with Streaming SQL providing an intuitive way to express stream processing logic and complex event processing constructs such as Patterns. Achieve high performance by processing events in-memory and using data stores for long term data storage. Optimize performance by enforcing a strict event stream schema and by pre-compiling the queries. Optimize memory consumption by having only the absolutely necessary information in-memory and dropping the rest as soon as possible. Supporting multiple extension points to accommodate a diverse set of functionality such as supporting multiple sources, sinks, functions, aggregation operations, windows, etc.","title":"Main Design Decisions"},{"location":"development/architecture/#high-level-architecture","text":"At a high level, Siddhi consumes events from various events sources, processes them according to the defined Siddhi application, and produces results to the subscribed event sinks. Siddhi can store and consume events from in-memory tables or from external data stores such as RDBMS , MongoDB , Hazelcast in-memory grid, etc. (i.e., when configured to do so). Siddhi also allows applications and users to query Siddhi via its Store Query API to interactively retrieve data from in-memory and other stores.","title":"High-Level Architecture"},{"location":"development/architecture/#main-modules-in-siddhi-engine","text":"Siddhi Engine comprises four main modules, they are: Siddhi Query API : This allows users to define the execution logic of the Siddhi application as queries and definitions using POJOs (Plain Old Java Objects). Internally, Siddhi uses these objects to identify the logic that it is expected to perform. Siddhi Query Compiler : This allows users to define the Siddhi application using the Siddhi Streaming SQL, and it compiles the Streaming SQL script to Siddhi Query API POJOs so that Siddhi can execute them. Siddhi Core : This builds the execution runtime based on the defined Siddhi Application POJOs and processes the incoming events as and when they arrive. Siddhi Annotation : This is a helper module that allows all extensions to be annotated so that they can be picked by Siddhi Core for processing. This also helps Siddhi to generate the extension documentation.","title":"Main Modules in Siddhi Engine"},{"location":"development/architecture/#siddhi-component-architecture","text":"The following diagram illustrates the main components of Siddhi and how they work together. Here the Siddhi Core module maintains the execution logic. It also interacts with the external environment and systems for consuming, processing and publishing events. It uses the following components to achieve its tasks: SiddhiManager : This is a key component of Siddhi Core that manages Siddhi Application Runtimes and facilitates their functionality via Siddhi Context with periodic state persistence, statistics reporting and extension loading. It is recommended to use one Siddhi Manager for a single JVM. SiddhiAppRuntime : Siddhi Application Runtime can be generated for each Siddhi Application through the Siddhi Manager. Siddhi Application Runtimes provide an isolated execution environment for each defined Siddhi Application. These Siddhi Application Runtimes can have their own lifecycle and they execute based on the logic defined in their Siddhi Application. SiddhiContext : This is a shared object across all the Siddhi Application Runtimes within the same Siddhi manager. It contains references to the persistence store for periodic persistence, statistics manager to report performance statistics of Siddhi Application Runtimes, and extension holders for loading Siddhi extensions.","title":"Siddhi Component Architecture"},{"location":"development/architecture/#siddhi-application-creation","text":"Execution logic of the Siddhi Engine is composed as a Siddhi Application, and this is usually passed as a string to SiddhiManager to create the SiddhiAppRuntime for execution. When a Siddhi Application is passed to the SiddhiManager.createSiddhiAppRuntime() , it is processed internally with the SiddhiCompiler . Here, the SiddhiApp String is compiled to SiddhiApp object model by the SiddhiQLBaseVisitorImpl class. This validates the syntax of the given Siddhi Application. The model is then passed to the SiddhiAppParser to create the SiddhiAppRuntime . During this phase, the semantics of the Siddhi Application is validated and the execution logic of the Siddhi Application is optimized.","title":"Siddhi Application Creation"},{"location":"development/architecture/#siddhi-app-execution-flow","text":"Following diagram depicts the execution flow within a Siddhi App Runtime. The path taken by events within Siddhi Engine is indicated in blue. The components that are involved in handling the events are the following: StreamJunction This routes events of a particular stream to various components within the Siddhi App Runtime. A stream junction is generated for each defined or inferred Stream in the Siddhi Application. A stream junction by default uses the incoming event's thread and passes all the events to its subscribed components as soon as they arrive, but this behaviour can be altered by configuring @Async annotation to buffer the events at the and stream junction and to use another one or more threads to collect the events from the buffer and process the subsequent executions. InputHandler Input handler is used to push Event and Event[] objects into stream junctions from defined event sources, and from Java/Python programmes. StreamCallback This receives Event[] s from stream junction and passes them to event sinks to publish to external endpoints, and/or passes them to subscribed Java/Python programmes for further processing. Queries Partitions These components process events by filtering, transforming, aggregating, joining, pattern matching, etc. They consume events from one or more stream junctions, process them and publish the processed events into a set of stream junctions based on the defined queries or partitions. Source Sources consume events from external sources in various data formats, convert them into Siddhi events using SourceMapper s and pass them to corresponding stream junction via their associated input handlers. A source is generated for each @Source annotation defined above a stream definition. SourceMapper A source mapper is a sub-component of source, and it needs to be configured for each source in order to convert the incoming event into Siddhi event. The source mapper type can be configured using the @Map annotation within the @Source annotation. When the @Map annotation is not defined, Siddhi uses the PassThroughSourceMapper , where it assumes that the incoming message is already in the Siddhi Event format (i.e Event or Event[] ), and therefore makes no changes to the incoming event format. Sink Sinks consumes events from its associated stream junction, convert them to various data formats via SinkMapper and publish them to external endpoints as defined in the @Sink annotation. A sink is generated for each @Sink annotation defined above a stream definition. SinkMapper A sink mapper is a sub-component of sink. and its need to be configured for each sink in order to map the Siddhi events to the specified data format so that they can be published via the sink. The sink mapper type can be configured using the @Map annotation within the @Sink annotation. When the @Map annotation is not defined, Siddhi uses PassThroughSinkMapper , where it passes the Siddhi Event (i.e Event or Event[] ) without any formatting to the Sink. Table Tables are used to store events. When tables are defined by default, Siddhi uses the InMemoryTable implementation to store events in-memory. When @Store annotation is used on top of the table definition, it loads the associated external data store connector based on the defined store type. Most table implementations are extended from either AbstractRecordTable or AbstractQueryableRecordTable abstract classes the former provides the functionality to query external data store based on a given filtering condition, and the latter queries external data store by providing projection, limits, and ordering parameters in addition to data filter condition. Window Windows store events as and when they arrive and automatically expire/clean them based on the given window constraint. Multiple types of windows are can be implemented by extending the WindowProcessor abstract class. IncrementalAggregation Long running time series aggregates defined via the aggregation definition is calculated in an incremental manner using the Incremental Aggregation Processor for the defined time periods. Incremental aggregation functions can be implemented by extending IncrementalAttributeAggregator . By default, incremental aggregations aggregate all the values in-memory, but when it is associated with a store by adding @store annotation it uses in-memory to aggregate partial results and uses data stores to persist those increments. When requested for aggregate results it retrieves data from data stores and (if needed from) in-memory, computes combined aggregate results and provides as the output. Trigger A trigger triggers events at a given interval as given in the trigger definition. The triggered events are pushed to a stream junction having the same name as the trigger. QueryCallback A query callback taps into the events that are emitted by a particular query. It notifies the event occurrence timestamp and classifies the output events into currentEvents , and expiredEvents .","title":"Siddhi App Execution Flow"},{"location":"development/architecture/#siddhi-query-execution","text":"Siddhi QueryRuntimes can be categorized into three main types: SingleInputStream : Queries that consist of query types such as filters and windows. JoinInputStream : Queries that consist of joins. StateInputStream : Queries that consist of patterns and sequences. The following section explains the internals of each query type.","title":"Siddhi Query Execution"},{"location":"development/architecture/#singleinputstream-query-runtime-filter-windows","text":"A single input stream query runtime is generated for filter and window queries. They consume events from a stream junction or a window and convert the incoming events according to the expected output stream format at the ProcessStreamReceiver by dropping all the unrelated incoming stream attributes. Then the converted events are passed through a few Processors such as FilterProcessor , StreamProcessor , StreamFunctionProcessor , WindowProcessor , and QuerySelector . Here, the StreamProcessor , StreamFunctionProcessor , and WindowProcessor can be extended with various stream processing capabilities. The last processor of the chain of processors must always be a QuerySelector and it can't appear anywhere else. When the query runtime consumes events from a stream, its processor chain can maximum contain one WindowProcessor , and when query runtime consumes events from a window, its chain of processors cannot contain any WindowProcessor . The FilterProcessor is implemented using expressions that return a boolean value. ExpressionExecutor is used to process conditions, mathematical operations, unary operations, constant values, variables, and functions. Expressions have a tree structure, and they are processed based using the Depth First search algorithm. To achieve high performance, Siddhi currently depends on the user to formulate the least successful case in the leftmost side of the condition, thereby increasing the chance of early false detection. The condition expression price = 100 and ( Symbol == 'IBM' or Symbol == 'MSFT' ) is represented as shown below. These expressions also support the execution of user-defined functions (UDFs), and they can be implemented by extending the FunctionExecutor class. After getting processed by all the processors, events reach the QuerySelector for transformation. At the QuerySelector , events are transformed based on the select clause of the query. The select clause produces one AttributeProcessor for each output stream attribute, and these AttributeProcessor s contain expressions defining data transformation including constant values, variables, user-defined functions, etc. They can also contain AttributeAggregatorExecutor s to process aggregation operations such as sum , count , etc. If there is a Group By clause defined, then the GroupByKeyGenerator is used to identify the composite group-by key, and then for each key, an AttributeAggregatorExecutor state is generated to maintain per group-by key aggregations. When each time AttributeProcessor is executed the AttributeAggregatorExecutor calculates per group-by aggregation results and output the values. When AttributeAggregatorExecutor group-by states become obsolete, they are destroyed and automatically cleaned. After an event is transformed to the output format through the above process, it is evaluated against the having condition executor if a having clause is provided. The succeeding events are then ordered, and limited based on order by , limit and offset clauses before they pushed to the OutputRateLimiter . At OutputRateLimiter , the event output is controlled before sending the events to the stream junction or to the query callback. When the output clause is not defined, the PassThroughOutputRateLimiter is used by passing all the events without any rate limiting.","title":"SingleInputStream Query Runtime (Filter &amp; Windows)"},{"location":"development/architecture/#temporal-processing-with-windows","text":"The temporal event processing aspect is achieved via Window and AttributeAggregators To achieve temporal processing, Siddhi uses the following four type of events: Current Events : Events that are newly arriving to the query from streams. Expired Events : Events that have expired from a window. Timer Events : Events that inform the query about an update of execution time. These events are usually generated by schedulers. Reset Events : Events that resets the Siddhi query states. In Siddhi, when an event comes into a WindowProcessor , it creates an appropriate expired event corresponding to the incoming current event with the expiring timestamp, and stores that event in the window. At the same time, WindowProcessor also forwards the current event to the next processor for further processing. It uses a scheduler or some other counting approach to determine when to emit the events that are stored in in-memory. When the expired events meet the condition for expiry based on the window contains, it emits the expired events to the next processor. At times like in window.timeBatch() there can be cases that need emitting all the events in-memory at once and the output does not need individual expired events values, in this cases the window emits a single reset event instead of sending one expired event for each event it has stored, so that it can reset the states in one go. For the QuerySelector aggregations to work correctly the window must emit a corresponding expired event for each current event it has emitted or it must send a reset event . In the QuerySelector , the arrived current events increase the aggregation values, expired events decrease the values, and reset events reset the aggregation calculation to produce correct query output. For example, the sliding TimeWindow ( window.time() ) creates a corresponding expired event for each current event that arrives, adds the expired event s to the window, adds an entry to the scheduler to notify when that event need to be expired, and finally sends the current event to the next processor for subsequent processing. The scheduler notifies the window by sending a timer event , and when the window receives an indication that the expected expiry time has come for the oldest event in the window via a timer event or by other means, it removes the expired event from the window and passes that to the next processor.","title":"Temporal Processing with Windows"},{"location":"development/architecture/#joininputstream-query-runtime-join","text":"Join input stream query runtime is generated for join queries. This can consume events from two stream junctions and perform a join operation as depicted above. It can also perform a join by consuming events from one stream junction and join against itself, or it can also join against a table, window or an aggregation. When a join is performed with a table, window or aggregation, the WindowProcessor in the above image is replaced with the corresponding table, window or aggregation and no basic processors are used on their side. The joining operation is triggered by the events that arrive from the stream junction. Here, when an event from one stream reaches the pre JoinProcessor , it matches against all the available events of the other stream's WindowProcessor . When a match is found, those matched events are sent to the QuerySelector as current events , and at the same time, the original event is added to the WindowProcessor where it remains until it expires. Similarly, when an event expires from the WindowProcessor , it matches against all the available events of the other stream's WindowProcessor , and when a match is found, those matched events are sent to the QuerySelector as expired events . Note Despite the optimizations, a join query is quite expensive when it comes to performance. This is because the WindowProcessor is locked during the matching process to avoid race conditions and to achieve accuracy while joining. Therefore, when possible avoid matching large (time or length) windows in high volume streams.","title":"JoinInputStream Query Runtime (Join)"},{"location":"development/architecture/#stateinputstream-query-runtime-pattern-sequence","text":"The state input stream query runtime is generated for pattern and sequence queries. This consumes events from one or more stream junctions via ProcessStreamReceiver s and checks whether the events match each pattern or sequence condition by processing the set of basic processors associated with each ProcessStreamReceiver . The PreStateProcessor s usually contains lists of state events that are already matched by previous conditions, and if its the first condition then it will have an empty state event in its list. When ProcessStreamReceiver consumes an event, it passes the event to the PreStateProcessor which updates the list of state events it has with the incoming event and executes the condition by passing the events to the basic processors. The state events that match the conditions reach the PostStateProcessor which will then stores the events to the state event list of the following PreStateProcessor . If it is the final condition's PostStateProcessor , then it will pass the state event to the QuerySelector to generate and emit the output.","title":"StateInputStream Query Runtime (Pattern &amp; Sequence)"},{"location":"development/architecture/#siddhi-partition-execution","text":"A partition is a wrapper around one or more Siddhi queries and inner streams that connect them. A partition is implemented in Siddhi as a PartitionRuntime which contains multiple QueryRuntime s and inner stream junctions. Each partitioned stream entering the partition goes through a designated PartitionStreamReceiver . The PartitionExecutor of PartitionStreamReceiver evaluates the incoming events to identify their associated partition-key using either RangePartitionExecutor or ValuePartitionExecutor . The identified partition-key is then set as thread local variable and the event is passed to the QueryRuntime s of processing. The QueryRuntime s process events by maintaining separate states for each partition-key such that producing separate output per partition. When a partition query consumes a non-partitioned global stream, the QueryRuntime s are executed for each available partition-key in the system such that allowing all partitions to receive the same event. When the partitions are obsolete PartitionRuntime deletes all the partition states from its QueryRuntime s.","title":"Siddhi Partition Execution"},{"location":"development/architecture/#siddhi-aggregation","text":"Siddhi supports long duration time series aggregations via its aggregation definition. AggregationRuntime implements this by the use of streaming lambda architecture , where it processes part of the data in-memory and gets part of the data from data stores. AggregationRuntime creates an in-memory table or external store for each time granularity (i.e seconds, minutes, days, etc) it has to process the events, and when events enter it calculates the aggregations in-memory for its least granularity (usually seconds) using the IncrementalExecutor and maintains the running aggregation values in its BaseIncrementalValueStore . At each clock end time of the granularity (end of each second) IncrementalExecutor stores the summarized values to the associated granularity table and also passes the summarized values to the IncrementalExecutor of the next granularity level, which also follows the same methodology in processing the events. Through this approach each time granularities, the current time duration will be in-memory and all the historical time durations will be in stored in the tables. The aggregations results are calculated by IncrementalAttributeAggregator s and stored in such a way that allows proper data composition upon retrial, for example, avg() is stored as sum and count . This allows data composition across various granularity time durations when retrieving, for example, results for avg() composed by returning sum of sum s divided by the sum of count s. Aggregation can also work in a distributed manner and across system restarts. This is done by storing node specific IDs and granularity time duration information in the tables. To make sure tables do not go out of memory IncrementalDataPurger is used to purge old data. When aggregation is queried through join or store query for a given time granularity it reads the data from the in-memory BaseIncrementalValueStore and from the tables computes the composite results as described, and presents the results.","title":"Siddhi Aggregation"},{"location":"development/architecture/#siddhi-event-formats","text":"Siddhi has three event formats. Event This is the format exposed to external systems when they send events via Input Handler and consume events via Stream Callback or Query Callback. This consists of a timestamp and an Object[] that contains all the values in accordance to the corresponding stream. StreamEvent (Subtype of ComplexEvent ) This is used within queries. This contains a timestamp and the following three Object[] s: beforeWindowData : This contains values that are only used in processors that are executed before the WindowProcessor . onAfterWindowData : This contains values that are only used by the WindowProcessor and the other processors that follow it, but not sent as output. outputData : This contains the values that are sent via the output stream of the query. In order to optimize the amount of data that is stored in the in-memory at windows, the content in beforeWindowData is cleared before the event enters the WindowProcessor . StreamEvents can also be chained by linking each other via the next property in them. StateEvent (Subtype of ComplexEvent ) This is used in joins, patterns and sequences queries when we need to associate events of multiple streams, tables, windows or aggregations together. This contains a timestamp , a collection of StreamEvent s representing different streams, tables, etc, that are used in the query, and an Object[] to contain outputData values that are needed for query output. The StreamEvent s within the StateEvent and the StateEvent themselves can be chained by linking each other with the next property in them. Event Chunks Event Chunks provide an easier way of manipulating the chain of StreamEvent s and StateEvent s so that they are be easily iterated, inserted and removed.","title":"Siddhi Event Formats"},{"location":"development/architecture/#summary","text":"This article focuses on describing the architecture of Siddhi and rationalizing some of the architectural decisions made when implementing the system. It also explains the key features of Siddhi. We hope this will be a good starting point for new developers to understand Siddhi and to start contributing to it.","title":"Summary"},{"location":"development/build/","text":"Building Siddhi 5.2 Repos Building Java Repos Prerequisites Oracle JDK 8 , OpenJDK 8 , or JDK 11 (Java 8 should be used for building in order to support both Java 8 and Java 11 at runtime) Maven 3.5.x or later version Steps to Build Get a clone or download source from Github repo, E.g. git clone https://github.com/siddhi-io/siddhi.git Run the Maven command mvn clean install from the root directory Command Description mvn clean install Build and install the artifacts into the local repository. mvn clean install -Dmaven.test.skip=true Build and install the artifacts into the local repository, without running any of the unit tests.","title":"Build"},{"location":"development/build/#building-siddhi-52-repos","text":"","title":"Building Siddhi 5.2 Repos"},{"location":"development/build/#building-java-repos","text":"","title":"Building Java Repos"},{"location":"development/build/#prerequisites","text":"Oracle JDK 8 , OpenJDK 8 , or JDK 11 (Java 8 should be used for building in order to support both Java 8 and Java 11 at runtime) Maven 3.5.x or later version","title":"Prerequisites"},{"location":"development/build/#steps-to-build","text":"Get a clone or download source from Github repo, E.g. git clone https://github.com/siddhi-io/siddhi.git Run the Maven command mvn clean install from the root directory Command Description mvn clean install Build and install the artifacts into the local repository. mvn clean install -Dmaven.test.skip=true Build and install the artifacts into the local repository, without running any of the unit tests.","title":"Steps to Build"},{"location":"development/roadmap/","text":"Siddhi Roadmap The Siddhi road map shows the key features and improvements that are in the pipeline for future releases. We have only listed the high level features and issues in below; we will certainly work on other minor improvements, bug fixes and etc\u2026 as well in future releases. Latest Siddhi Core 5.0.0 Localized state management partition support Fault Stream support for error handling Multiple levels of Metrics (OFF, BASIC, DETAIL) support for Siddhi Extension APIs are improved/changed thus custom extensions that you have written with Siddhi 4.x.x, no longer works in Siddhi 5.0.0. Siddhi documentation upgrades - https://siddhi.io/en/v5.0/docs/ 2019-Q3 Siddhi Core 5.1.x Introduce RESET processing mode to preserve memory optimization. Support to create a Sandbox SiddhiAppRuntime for testing purposes Support error handling (log/wait/fault-stream) when event sinks publish data asynchronously. Siddhi Extension gRPC IO connector S3 IO connector GCS IO connector Execution List Connector Deduplicate support in Unique Extension Siddhi Tooling Support K8s/Docker artifacts export in Siddhi editor Overload parameter support in Siddhi source design editor CRDs to support Kubernetes deployments natively Support High-Available, Fault Tolerant Siddhi deployment with NATS Siddhi Test Framework: Provides the capability to write integration tests using Docker containers CI/CD deployment story for Siddhi Siddhi use case guides - https://siddhi.io/en/v5.1/docs/ 2019-Q4 Prometheus for metrics collection Support distributed Siddhi deployment with NATS Siddhi plugin for VSCode JDBC driver for Siddhi Query APIs Support templating Siddhi apps and configurations in Tooling 2020 + Kafka support for Siddhi K8s deployment Siddhi support in Golang Enhance management of secrets with vault services Evaluate Istio integration Allow to specify dependencies in the Siddhi Custom resource Cloud Foundry installation support If you have any queries or comments on the roadmap, please let us know via GitHub here . You can also always communicate through Google Group or Slack with us on our community page . You feedback and contribution is always welcome.","title":"Roadmap"},{"location":"development/roadmap/#siddhi-roadmap","text":"The Siddhi road map shows the key features and improvements that are in the pipeline for future releases. We have only listed the high level features and issues in below; we will certainly work on other minor improvements, bug fixes and etc\u2026 as well in future releases.","title":"Siddhi Roadmap"},{"location":"development/roadmap/#latest","text":"Siddhi Core 5.0.0 Localized state management partition support Fault Stream support for error handling Multiple levels of Metrics (OFF, BASIC, DETAIL) support for Siddhi Extension APIs are improved/changed thus custom extensions that you have written with Siddhi 4.x.x, no longer works in Siddhi 5.0.0. Siddhi documentation upgrades - https://siddhi.io/en/v5.0/docs/","title":"Latest"},{"location":"development/roadmap/#2019-q3","text":"Siddhi Core 5.1.x Introduce RESET processing mode to preserve memory optimization. Support to create a Sandbox SiddhiAppRuntime for testing purposes Support error handling (log/wait/fault-stream) when event sinks publish data asynchronously. Siddhi Extension gRPC IO connector S3 IO connector GCS IO connector Execution List Connector Deduplicate support in Unique Extension Siddhi Tooling Support K8s/Docker artifacts export in Siddhi editor Overload parameter support in Siddhi source design editor CRDs to support Kubernetes deployments natively Support High-Available, Fault Tolerant Siddhi deployment with NATS Siddhi Test Framework: Provides the capability to write integration tests using Docker containers CI/CD deployment story for Siddhi Siddhi use case guides - https://siddhi.io/en/v5.1/docs/","title":"2019-Q3"},{"location":"development/roadmap/#2019-q4","text":"Prometheus for metrics collection Support distributed Siddhi deployment with NATS Siddhi plugin for VSCode JDBC driver for Siddhi Query APIs Support templating Siddhi apps and configurations in Tooling","title":"2019-Q4"},{"location":"development/roadmap/#2020","text":"Kafka support for Siddhi K8s deployment Siddhi support in Golang Enhance management of secrets with vault services Evaluate Istio integration Allow to specify dependencies in the Siddhi Custom resource Cloud Foundry installation support If you have any queries or comments on the roadmap, please let us know via GitHub here . You can also always communicate through Google Group or Slack with us on our community page . You feedback and contribution is always welcome.","title":"2020 +"},{"location":"development/source/","text":"Siddhi 5.2 Source Code Project Source Code Siddhi Core Java Library https://github.com/siddhi-io/siddhi (Java) Siddhi repo, containing the core Java libraries of Siddhi. PySiddhi https://github.com/siddhi-io/pysiddhi (Python) The Python wrapper for Siddhi core Java libraries. This depends on the siddhi-io/siddhi repo. Siddhi Local Microservice Distribution https://github.com/siddhi-io/distribution (Java) The Microservice distribution of the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi repo. Siddhi Docker Microservice Distribution https://github.com/siddhi-io/docker-siddhi (Docker) The Docker wrapper for the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi and siddhi-io/distribution repos. Siddhi Kubernetes Operator https://github.com/siddhi-io/siddhi-operator (Go) The Siddhi Kubernetes CRD repo deploying Siddhi on Kubernetes. This depends on the siddhi-io/siddhi , siddhi-io/distribution and siddhi-io/docker-siddhi repos. Siddhi Extensions Find the supported Siddhi extensions and source here","title":"Source"},{"location":"development/source/#siddhi-52-source-code","text":"","title":"Siddhi 5.2 Source Code"},{"location":"development/source/#project-source-code","text":"","title":"Project Source Code"},{"location":"development/source/#siddhi-core-java-library","text":"https://github.com/siddhi-io/siddhi (Java) Siddhi repo, containing the core Java libraries of Siddhi.","title":"Siddhi Core Java Library"},{"location":"development/source/#pysiddhi","text":"https://github.com/siddhi-io/pysiddhi (Python) The Python wrapper for Siddhi core Java libraries. This depends on the siddhi-io/siddhi repo.","title":"PySiddhi"},{"location":"development/source/#siddhi-local-microservice-distribution","text":"https://github.com/siddhi-io/distribution (Java) The Microservice distribution of the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi repo.","title":"Siddhi Local Microservice Distribution"},{"location":"development/source/#siddhi-docker-microservice-distribution","text":"https://github.com/siddhi-io/docker-siddhi (Docker) The Docker wrapper for the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi and siddhi-io/distribution repos.","title":"Siddhi Docker Microservice Distribution"},{"location":"development/source/#siddhi-kubernetes-operator","text":"https://github.com/siddhi-io/siddhi-operator (Go) The Siddhi Kubernetes CRD repo deploying Siddhi on Kubernetes. This depends on the siddhi-io/siddhi , siddhi-io/distribution and siddhi-io/docker-siddhi repos.","title":"Siddhi Kubernetes Operator"},{"location":"development/source/#siddhi-extensions","text":"Find the supported Siddhi extensions and source here","title":"Siddhi Extensions"},{"location":"docs/","text":"Siddhi 5.2 User Guide This section provides information on using and running Siddhi. Checkout the Siddhi features to get and idea on what it can do in brief. Writing Siddhi Applications Writing steam processing logic in Siddhi is all about building Siddhi Applications. A Siddhi Application is a script with .siddhi file extension having self-contained stream processing logic. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logics that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, users can configure common topic using In-Memory Sink and In-Memory Source in order to communicate between them. To write Siddhi Applications using Siddhi Streaming SQL refer Siddhi Query Guide for details. For specific API information on Siddhi functions and features refer Siddhi API Guide . Find out about the supported Siddhi extensions and their versions here . Executing Siddhi Applications Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP) Siddhi Configurations Refer the Siddhi Config Guide for information on advance Siddhi execution configurations. System Requirements For all Siddhi execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"Introduction"},{"location":"docs/#siddhi-52-user-guide","text":"This section provides information on using and running Siddhi. Checkout the Siddhi features to get and idea on what it can do in brief.","title":"Siddhi 5.2 User Guide"},{"location":"docs/#writing-siddhi-applications","text":"Writing steam processing logic in Siddhi is all about building Siddhi Applications. A Siddhi Application is a script with .siddhi file extension having self-contained stream processing logic. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logics that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, users can configure common topic using In-Memory Sink and In-Memory Source in order to communicate between them. To write Siddhi Applications using Siddhi Streaming SQL refer Siddhi Query Guide for details. For specific API information on Siddhi functions and features refer Siddhi API Guide . Find out about the supported Siddhi extensions and their versions here .","title":"Writing Siddhi Applications"},{"location":"docs/#executing-siddhi-applications","text":"Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP)","title":"Executing Siddhi Applications"},{"location":"docs/#siddhi-configurations","text":"Refer the Siddhi Config Guide for information on advance Siddhi execution configurations.","title":"Siddhi Configurations"},{"location":"docs/#system-requirements","text":"For all Siddhi execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"System Requirements"},{"location":"docs/config-guide/","text":"Siddhi 5.2 Config Guide This section covers the following. Configuring Databases Configuring Periodic State Persistence Configuring Siddhi Sources, Sinks, Stores and Extensions Configuring Authentication Adding Extensions and Third Party Dependencies Configuring Statistics Converting Jars to OSGi Bundles Configuring Databases Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. It is recommended to configure RDBMS databases as datasources under wso2.datasources section of Siddhi configuration yaml, and pass it during startup, this will allow database to reuse connections across multiple Siddhi Apps. By default Siddhi stores product-specific data in predefined embedded H2 database located in SIDDHI_RUNNER_HOME /wso2/runner/database directory. Here, the default H2 database is only suitable for development, testing, and some production environments which do not store data. However, for most production environments we recommend using industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MSSQL. In this case users are expected to add the relevant database drivers to Siddhi's class-path. Including database drivers. The database driver corresponding to the database should be an OSGi bundle and it need to be added to SIDDHI_RUNNER_HOME /lib/ directory. If the driver is a jar then this should be converted to an OSGi bundle before adding . Converting Non OSGi drivers. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. The necessary table schemas are self generated by the features themselves, other than the tables needed for statistics reporting via databases . Below are the sample datasource configuration for each supported database types: MySQL Oracle There are two ways to configure Oracle. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE PostgreSQL MSSQL Configuring Periodic State Persistence Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. This explains how to periodically persisting the state of Siddhi either into a database system or file system, in order to prevent data losses that can result from a system failure. Persistence on Database To perform periodic state persistence on a database, the database should be configured as a datasource and the relevant jdbc drivers should be added to Siddhi's class-path. Refer Database Configuration section for more information. To configure database based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.DBPersistenceStore config datasource The datasource to be used in persisting the state. The datasource should be defined in the Siddhi configuration yaml. For detailed instructions of how to configure a datasource, see Database Configuration . SIDDHI_PERSISTENCE_DB (A datasource that is defined in wso2.datasources in Siddhi configuration yaml) config table The table that should be created and used for persisting states. PERSISTENCE_TABLE The following is a sample configuration for database based state persistence. Persistence on File System To configure file system based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample configuration for file system based state persistence. Configuring Siddhi Elements Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. You can configure some of there environment specific configurations in the Siddhi Configuration yaml rather than configuring in-line, such that your Siddhi Application can become potable between environments. Configuring Sources, Sinks and Stores Multiple sources, sinks, and stores could be defined in Siddhi Configuration yaml as ref , and referred by several Siddhi Applications as described below. The following is the syntax for the configuration. siddhi: refs: - ref: name: ' name ' type: ' type ' properties: property1 : value1 property2 : value2 For each separate refs you want to configure, add a sub-section named ref under the refs subsection. The ref configured in Siddhi Configuration yaml can be referred from a Siddhi Application Source as follows. @Source(ref=' name ', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Similarly Sinks and Store Tables can also be configured and referred from Siddhi Apps. For each separate refs you want to configure, add a sub-section named ref under the refs subsection. Example : Configuring http source using ref Following configuration defines the url and details about basic.auth , in the Siddhi Configuration yaml. siddhi: refs: - ref: name: 'http-passthrough' type: 'http' properties: receiver.url: 'http://0.0.0.0:8008/sweet-production' basic.auth.enabled: false This can be referred in the Siddhi Applications as follows. @Source(ref='http-passthrough', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Configuring Extensions Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. The following is the syntax for the configuration. siddhi: extensions: - extension: name: extension name namespace: extension namespace properties: key : value For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. Following are some examples on overriding default system properties via Siddhi Configuration yaml Example 1 : Defining service host and port for the TCP source siddhi: extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2 : Overwriting the default RDBMS extension configuration siddhi: extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: \"CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}})\" mysql.recordDeleteQuery: \"DELETE FROM {{TABLE_NAME}} {{CONDITION}}\" mysql.recordExistsQuery: \"SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1\" Configuring Authentication Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. By default, Siddhi is configured with user name admin , and password admin . This can be updated by adding related user management configuration as auth.configs to the Siddhi Configuration yaml, and pass it at startup. A sample auth.configs is as follows. # Authentication configuration auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin Adding Extensions and Third Party Dependencies Applicable for all modes. For certain use-cases, Siddhi might require extensions and/or third party dependencies to fulfill some characteristics that it does not provide by default. This section provides details on how to add or update extension and/or third party dependencies that is needed by Siddhi. Adding to Siddhi Java Program When running Siddhi as a Java library, the extension jars and/or third-party dependencies needed for Siddhi can be simply added to Siddhi class-path. When Maven is used as the build tool add them to the pom.xml file along with the other mandatory jars needed by Siddhi as given is Using Siddhi as a library guide. A sample on adding siddhi-io-http extension to the Maven pom.xml is as follows. !--HTTP extension-- dependency groupId org.wso2.extension.siddhi.io.http /groupId artifactId siddhi-io-http /artifactId version ${siddhi.io.http.version} /version /dependency Refer guide for more details on using Siddhi as a Java Library. Adding to Siddhi Local Microservice The most used Siddhi extensions are packed by default with the Siddhi Local Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, adding or replacing the relevant OSGi JARs in SIDDHI_RUNNER_HOME /lib directory. Since Local Microservice is OSGi-based, when adding libraries/drivers they need to be checked if they are OSGi bundles, and if not convert to OSGi before adding them to the SIDDHI_RUNNER_HOME /lib directory. Converting Jars to OSGi Bundles.. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Local Microservice. Adding to Siddhi Docker Microservice The most used Siddhi extensions are packed by default with the Siddhi Docker Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, a new docker image has to be built from either siddhi-runner-base-ubuntu or siddhi-runner-base-alpine images. These images contain Linux OS, JDK and the Siddhi distribution. Sample docker file using siddhi-runner-base-alpine is as follows. Find the necessary artifacts to build the docker from docker-siddhi repository. The necessary OSGi jars and extensions that need to be added to the Siddhi Docker Microservice should be placed at ${BUNDLE_JAR_DIR} ( ./files/lib ) folder as defined in the above docker file, such that they would be bundled during the docker build phase. Converting Jars to OSGi Bundles If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Docker Microservice. Adding to Siddhi Kubernetes Microservice To add or update Siddhi extensions and/or third-party dependencies, a custom docker image has to be created using the steps described in Adding to Siddhi Docker Microservice documentation including the necessary extensions and dependencies. The created image can be then referenced in the sepc.pod subsection in the SiddhiProcess Kubernetes artifact created to deploy Siddhi in Kubernetes. For details on creating the Kubernetes artifacts refer Using Siddhi as Kubernetes Microservice documentation. Configuring Statistics Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi uses dropwizard metrics library to calculate Siddhi and JVM statistics, and it can report the results via JMX Mbeans, console or database. To enable statistics, the relevant configuration should be added to the Siddhi Configuration yaml as follows, and at the same time the statistics collection should be enabled in the Siddhi Application which is being monitored. Refer Siddhi Application Statistics documentation for enabling Siddhi Application level statistics. To enable statistics the relevant matrics related configurations should be added under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. Configuring Metrics reporting level. To modify the statistics reporting, relevant metric names can be added under the wso2.metrics.levels subsection in the Siddhi Configurations yaml, along with the matrics level (i.e., OFF, INFO, DEBUG, TRACE, or ALL) as given bellow. wso2.metrics: # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO The available metrics reporting options are as follows. Reporting via JMX Mbeans JMX Mbeams is the default statistics reporting option of Siddhi. To enable stats with the default configuration add the metric-related properties under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. A sample configuration is as follows. wso2.metrics: enabled: true This will report JMX Mbeans in the name of org.wso2.carbon.metrics . However, in this default configuration the JVM metrics will not be measured. A detail JMX configuration along with the metrics reporting level is as follows. wso2.metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO Reporting via Console To enable statistics by periodically printing the metrics on console add the following configuration to the the Siddhi Configurations yaml file, and pass that during startup. # This is the main configuration for metrics wso2.metrics: # Enable Metrics enabled: false reporting: console: - # The name for the Console Reporter name: Console # Enable Console Reporter enabled: false # Polling Period in seconds. # This is the period for polling metrics from the metric registry and printing in the console pollingPeriod: 5 Reporting via Database To enable JDBC reporting and to periodically clean up the outdated statistics from the database, first a datasource should be created with the relevant database configurations and then the related metrics properties as given bellow should be added to in the Siddhi Configurations yaml file, and pass that during startup. The bellow sample is referring to the datasource with JNDI name jdbc/SiddhiMetricsDB , hence the datasource configuration in yaml should have jndiConfig.name as jdbc/SiddhiMetricsDB . For detailed instructions on configuring a datasource, refer Configuring Databases . . The scripts to create these tables are provided in the SIDDHI_RUNNER_HOME /wso2/runner/dbscripts directory. Sample configuration of reporting via database. wso2.metrics: # Enable Metrics enabled: true jdbc: # Data Source Configurations for JDBC Reporters dataSource: # Default Data Source Configuration - JDBC01 # JNDI name of the data source to be used by the JDBC Reporter. # This data source should be defined under wso2.datasources. dataSourceName: java:comp/env/jdbc/SiddhiMetricsDB # Schedule regular deletion of metrics data older than a set number of days. # It is recommended that you enable this job to ensure your metrics tables do not get extremely large. # Deleting data older than seven days should be sufficient. scheduledCleanup: # Enable scheduled cleanup to delete Metrics data in the database. enabled: false # The scheduled job will cleanup all data older than the specified days daysToKeep: 7 # This is the period for each cleanup operation in seconds. scheduledCleanupPeriod: 86400 reporting: jdbc: - # The name for the JDBC Reporter name: JDBC # Enable JDBC Reporter enabled: true # Source of Metrics, which will be used to identify each metric in database -- # Commented to use the hostname by default # source: Siddhi # Alias referring to the Data Source configuration dataSource: *JDBC01 # Polling Period in seconds. # This is the period for polling metrics from the metric registry and updating the database with the values pollingPeriod: 60 Metrics history and reporting interval If the wso2.metrics.reporting.jdbc subsection is not enabled, the information relating to metrics history will not be persisted for future references. Also note the that the reporting will only start to update the database after the given pollingPeriod time has elapsed. Information about the parameters configured under the jdbc.dataSource subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description dataSourceName java:comp/env/jdbc/SiddhiMetricsDB java:comp/env/ datasource JNDI name . The JNDI name of the datasource used to store metric data. scheduledCleanup.enabled false If this is set to true, metrics data stored in the database is cleared periodically based on scheduled time interval. scheduledCleanup.daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. scheduledCleanup.scheduledCleanupPeriod 86400 The parameter specifies the time interval in seconds at which metric data should be cleaned. Converting Jars to OSGi Bundles To convert jar files to OSGi bundles, first download and save the non-OSGi jar it in a preferred directory in your machine. Then from the CLI, navigate to the SIDDHI_RUNNER_HOME /bin directory, and issue the following command. ./jartobundle.sh path to non OSGi jar ../lib This converts the Jar to OSGi bundles and place it in SIDDHI_RUNNER_HOME /lib directory.","title":"Configuration Guide"},{"location":"docs/config-guide/#siddhi-52-config-guide","text":"This section covers the following. Configuring Databases Configuring Periodic State Persistence Configuring Siddhi Sources, Sinks, Stores and Extensions Configuring Authentication Adding Extensions and Third Party Dependencies Configuring Statistics Converting Jars to OSGi Bundles","title":"Siddhi 5.2 Config Guide"},{"location":"docs/config-guide/#configuring-databases","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. It is recommended to configure RDBMS databases as datasources under wso2.datasources section of Siddhi configuration yaml, and pass it during startup, this will allow database to reuse connections across multiple Siddhi Apps. By default Siddhi stores product-specific data in predefined embedded H2 database located in SIDDHI_RUNNER_HOME /wso2/runner/database directory. Here, the default H2 database is only suitable for development, testing, and some production environments which do not store data. However, for most production environments we recommend using industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MSSQL. In this case users are expected to add the relevant database drivers to Siddhi's class-path. Including database drivers. The database driver corresponding to the database should be an OSGi bundle and it need to be added to SIDDHI_RUNNER_HOME /lib/ directory. If the driver is a jar then this should be converted to an OSGi bundle before adding . Converting Non OSGi drivers. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. The necessary table schemas are self generated by the features themselves, other than the tables needed for statistics reporting via databases . Below are the sample datasource configuration for each supported database types: MySQL Oracle There are two ways to configure Oracle. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE PostgreSQL MSSQL","title":"Configuring Databases"},{"location":"docs/config-guide/#configuring-periodic-state-persistence","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. This explains how to periodically persisting the state of Siddhi either into a database system or file system, in order to prevent data losses that can result from a system failure.","title":"Configuring Periodic State Persistence"},{"location":"docs/config-guide/#persistence-on-database","text":"To perform periodic state persistence on a database, the database should be configured as a datasource and the relevant jdbc drivers should be added to Siddhi's class-path. Refer Database Configuration section for more information. To configure database based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.DBPersistenceStore config datasource The datasource to be used in persisting the state. The datasource should be defined in the Siddhi configuration yaml. For detailed instructions of how to configure a datasource, see Database Configuration . SIDDHI_PERSISTENCE_DB (A datasource that is defined in wso2.datasources in Siddhi configuration yaml) config table The table that should be created and used for persisting states. PERSISTENCE_TABLE The following is a sample configuration for database based state persistence.","title":"Persistence on Database"},{"location":"docs/config-guide/#persistence-on-file-system","text":"To configure file system based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample configuration for file system based state persistence.","title":"Persistence on File System"},{"location":"docs/config-guide/#configuring-siddhi-elements","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. You can configure some of there environment specific configurations in the Siddhi Configuration yaml rather than configuring in-line, such that your Siddhi Application can become potable between environments.","title":"Configuring Siddhi Elements"},{"location":"docs/config-guide/#configuring-sources-sinks-and-stores","text":"Multiple sources, sinks, and stores could be defined in Siddhi Configuration yaml as ref , and referred by several Siddhi Applications as described below. The following is the syntax for the configuration. siddhi: refs: - ref: name: ' name ' type: ' type ' properties: property1 : value1 property2 : value2 For each separate refs you want to configure, add a sub-section named ref under the refs subsection. The ref configured in Siddhi Configuration yaml can be referred from a Siddhi Application Source as follows. @Source(ref=' name ', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Similarly Sinks and Store Tables can also be configured and referred from Siddhi Apps. For each separate refs you want to configure, add a sub-section named ref under the refs subsection. Example : Configuring http source using ref Following configuration defines the url and details about basic.auth , in the Siddhi Configuration yaml. siddhi: refs: - ref: name: 'http-passthrough' type: 'http' properties: receiver.url: 'http://0.0.0.0:8008/sweet-production' basic.auth.enabled: false This can be referred in the Siddhi Applications as follows. @Source(ref='http-passthrough', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double);","title":"Configuring Sources, Sinks and Stores"},{"location":"docs/config-guide/#configuring-extensions","text":"Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. The following is the syntax for the configuration. siddhi: extensions: - extension: name: extension name namespace: extension namespace properties: key : value For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. Following are some examples on overriding default system properties via Siddhi Configuration yaml Example 1 : Defining service host and port for the TCP source siddhi: extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2 : Overwriting the default RDBMS extension configuration siddhi: extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: \"CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}})\" mysql.recordDeleteQuery: \"DELETE FROM {{TABLE_NAME}} {{CONDITION}}\" mysql.recordExistsQuery: \"SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1\"","title":"Configuring Extensions"},{"location":"docs/config-guide/#configuring-authentication","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. By default, Siddhi is configured with user name admin , and password admin . This can be updated by adding related user management configuration as auth.configs to the Siddhi Configuration yaml, and pass it at startup. A sample auth.configs is as follows. # Authentication configuration auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin","title":"Configuring Authentication"},{"location":"docs/config-guide/#adding-extensions-and-third-party-dependencies","text":"Applicable for all modes. For certain use-cases, Siddhi might require extensions and/or third party dependencies to fulfill some characteristics that it does not provide by default. This section provides details on how to add or update extension and/or third party dependencies that is needed by Siddhi.","title":"Adding Extensions and Third Party Dependencies"},{"location":"docs/config-guide/#adding-to-siddhi-java-program","text":"When running Siddhi as a Java library, the extension jars and/or third-party dependencies needed for Siddhi can be simply added to Siddhi class-path. When Maven is used as the build tool add them to the pom.xml file along with the other mandatory jars needed by Siddhi as given is Using Siddhi as a library guide. A sample on adding siddhi-io-http extension to the Maven pom.xml is as follows. !--HTTP extension-- dependency groupId org.wso2.extension.siddhi.io.http /groupId artifactId siddhi-io-http /artifactId version ${siddhi.io.http.version} /version /dependency Refer guide for more details on using Siddhi as a Java Library.","title":"Adding to Siddhi Java Program"},{"location":"docs/config-guide/#adding-to-siddhi-local-microservice","text":"The most used Siddhi extensions are packed by default with the Siddhi Local Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, adding or replacing the relevant OSGi JARs in SIDDHI_RUNNER_HOME /lib directory. Since Local Microservice is OSGi-based, when adding libraries/drivers they need to be checked if they are OSGi bundles, and if not convert to OSGi before adding them to the SIDDHI_RUNNER_HOME /lib directory. Converting Jars to OSGi Bundles.. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Local Microservice.","title":"Adding to Siddhi Local Microservice"},{"location":"docs/config-guide/#adding-to-siddhi-docker-microservice","text":"The most used Siddhi extensions are packed by default with the Siddhi Docker Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, a new docker image has to be built from either siddhi-runner-base-ubuntu or siddhi-runner-base-alpine images. These images contain Linux OS, JDK and the Siddhi distribution. Sample docker file using siddhi-runner-base-alpine is as follows. Find the necessary artifacts to build the docker from docker-siddhi repository. The necessary OSGi jars and extensions that need to be added to the Siddhi Docker Microservice should be placed at ${BUNDLE_JAR_DIR} ( ./files/lib ) folder as defined in the above docker file, such that they would be bundled during the docker build phase. Converting Jars to OSGi Bundles If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Docker Microservice.","title":"Adding to Siddhi Docker Microservice"},{"location":"docs/config-guide/#adding-to-siddhi-kubernetes-microservice","text":"To add or update Siddhi extensions and/or third-party dependencies, a custom docker image has to be created using the steps described in Adding to Siddhi Docker Microservice documentation including the necessary extensions and dependencies. The created image can be then referenced in the sepc.pod subsection in the SiddhiProcess Kubernetes artifact created to deploy Siddhi in Kubernetes. For details on creating the Kubernetes artifacts refer Using Siddhi as Kubernetes Microservice documentation.","title":"Adding to Siddhi Kubernetes Microservice"},{"location":"docs/config-guide/#configuring-statistics","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi uses dropwizard metrics library to calculate Siddhi and JVM statistics, and it can report the results via JMX Mbeans, console or database. To enable statistics, the relevant configuration should be added to the Siddhi Configuration yaml as follows, and at the same time the statistics collection should be enabled in the Siddhi Application which is being monitored. Refer Siddhi Application Statistics documentation for enabling Siddhi Application level statistics. To enable statistics the relevant matrics related configurations should be added under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. Configuring Metrics reporting level. To modify the statistics reporting, relevant metric names can be added under the wso2.metrics.levels subsection in the Siddhi Configurations yaml, along with the matrics level (i.e., OFF, INFO, DEBUG, TRACE, or ALL) as given bellow. wso2.metrics: # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO The available metrics reporting options are as follows.","title":"Configuring Statistics"},{"location":"docs/config-guide/#reporting-via-jmx-mbeans","text":"JMX Mbeams is the default statistics reporting option of Siddhi. To enable stats with the default configuration add the metric-related properties under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. A sample configuration is as follows. wso2.metrics: enabled: true This will report JMX Mbeans in the name of org.wso2.carbon.metrics . However, in this default configuration the JVM metrics will not be measured. A detail JMX configuration along with the metrics reporting level is as follows. wso2.metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO","title":"Reporting via JMX Mbeans"},{"location":"docs/config-guide/#reporting-via-console","text":"To enable statistics by periodically printing the metrics on console add the following configuration to the the Siddhi Configurations yaml file, and pass that during startup. # This is the main configuration for metrics wso2.metrics: # Enable Metrics enabled: false reporting: console: - # The name for the Console Reporter name: Console # Enable Console Reporter enabled: false # Polling Period in seconds. # This is the period for polling metrics from the metric registry and printing in the console pollingPeriod: 5","title":"Reporting via Console"},{"location":"docs/config-guide/#reporting-via-database","text":"To enable JDBC reporting and to periodically clean up the outdated statistics from the database, first a datasource should be created with the relevant database configurations and then the related metrics properties as given bellow should be added to in the Siddhi Configurations yaml file, and pass that during startup. The bellow sample is referring to the datasource with JNDI name jdbc/SiddhiMetricsDB , hence the datasource configuration in yaml should have jndiConfig.name as jdbc/SiddhiMetricsDB . For detailed instructions on configuring a datasource, refer Configuring Databases . . The scripts to create these tables are provided in the SIDDHI_RUNNER_HOME /wso2/runner/dbscripts directory. Sample configuration of reporting via database. wso2.metrics: # Enable Metrics enabled: true jdbc: # Data Source Configurations for JDBC Reporters dataSource: # Default Data Source Configuration - JDBC01 # JNDI name of the data source to be used by the JDBC Reporter. # This data source should be defined under wso2.datasources. dataSourceName: java:comp/env/jdbc/SiddhiMetricsDB # Schedule regular deletion of metrics data older than a set number of days. # It is recommended that you enable this job to ensure your metrics tables do not get extremely large. # Deleting data older than seven days should be sufficient. scheduledCleanup: # Enable scheduled cleanup to delete Metrics data in the database. enabled: false # The scheduled job will cleanup all data older than the specified days daysToKeep: 7 # This is the period for each cleanup operation in seconds. scheduledCleanupPeriod: 86400 reporting: jdbc: - # The name for the JDBC Reporter name: JDBC # Enable JDBC Reporter enabled: true # Source of Metrics, which will be used to identify each metric in database -- # Commented to use the hostname by default # source: Siddhi # Alias referring to the Data Source configuration dataSource: *JDBC01 # Polling Period in seconds. # This is the period for polling metrics from the metric registry and updating the database with the values pollingPeriod: 60 Metrics history and reporting interval If the wso2.metrics.reporting.jdbc subsection is not enabled, the information relating to metrics history will not be persisted for future references. Also note the that the reporting will only start to update the database after the given pollingPeriod time has elapsed. Information about the parameters configured under the jdbc.dataSource subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description dataSourceName java:comp/env/jdbc/SiddhiMetricsDB java:comp/env/ datasource JNDI name . The JNDI name of the datasource used to store metric data. scheduledCleanup.enabled false If this is set to true, metrics data stored in the database is cleared periodically based on scheduled time interval. scheduledCleanup.daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. scheduledCleanup.scheduledCleanupPeriod 86400 The parameter specifies the time interval in seconds at which metric data should be cleaned.","title":"Reporting via Database"},{"location":"docs/config-guide/#converting-jars-to-osgi-bundles","text":"To convert jar files to OSGi bundles, first download and save the non-OSGi jar it in a preferred directory in your machine. Then from the CLI, navigate to the SIDDHI_RUNNER_HOME /bin directory, and issue the following command. ./jartobundle.sh path to non OSGi jar ../lib This converts the Jar to OSGi bundles and place it in SIDDHI_RUNNER_HOME /lib directory.","title":"Converting Jars to OSGi Bundles"},{"location":"docs/extensions/","text":"Siddhi Extensions Following are some supported Siddhi extensions; Extensions released under Apache 2.0 License Execution Extensions Name Description Latest Tested Version execution-string Provides basic string handling capabilities such as concat, length, replace all, etc. 5.0.3 execution-regex Provides basic RegEx execution capabilities. 5.0.3 execution-math Provides useful mathematical functions. 5.0.1 execution-time Provides time related functionality such as getting current time, current date, manipulating/formatting dates, etc. 5.0.2 execution-map Provides the capability to generate and manipulate map data objects. 5.0.2 execution-json Provides the capability to retrieve, insert, and modify JSON elements. 2.0.1 execution-unitconversion Converts various units such as length, mass, time and volume. 2.0.1 execution-reorder Orders out-of-order event arrivals using algorithms such as K-Slack and alpha K-Stack. 5.0.0 execution-unique Retains and process unique events based on the given parameters. 5.0.0 execution-streamingml Performs streaming machine learning (clustering, classification and regression) on event streams. 2.0.1 execution-tensorflow provides support for running pre-built TensorFlow models. 2.0.1 Input/Output Extensions Name Description Latest Tested Version io-http Receives and publishes events via http and https transports, calls external services, and serves incoming requests and provide synchronous responses. 2.0.6 io-nats Receives and publishes events from/to NATS. 2.0.3 io-kafka Receives and publishes events from/to Kafka. 5.0.1 io-email Receives and publishes events via email using smtp , pop3 and imap protocols. 2.0.2 io-cdc Captures change data from databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 2.0.1 io-tcp Receives and publishes events through TCP transport. 3.0.2 io-googlepubsub Receives and publishes events through Google Pub/Sub. 2.0.1 io-file Receives and publishes event data from/to files. 2.0.1 io-jms Receives and publishes events via Java Message Service (JMS), supporting Message brokers such as ActiveMQ 2.0.2 io-prometheus Consumes and expose Prometheus metrics from/to Prometheus server. 2.0.1 Data Mapping Extensions Name Description Latest Tested Version map-json Converts JSON messages to/from Siddhi events. 5.0.3 map-xml Converts XML messages to/from Siddhi events. 5.0.3 map-text Converts text messages to/from Siddhi events. 2.0.2 map-avro Converts AVRO messages to/from Siddhi events. 2.0.1 map-keyvalue Converts events having Key-Value maps to/from Siddhi events. 2.0.2 map-csv Converts messages with CSV format to/from Siddhi events. 2.0.2 map-binary Converts binary events that adheres to Siddhi format to/from Siddhi events. 2.0.2 Store Extensions Name Description Latest Tested Version store-rdbms Persist and retrieve events to/from RDBMS databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 6.0.1 store-mongodb Persist and retrieve events to/from MongoDB. 2.0.1 store-redis Persist and retrieve events to/from Redis. 3.1.0 store-elasticsearch Persist and retrieve events to/from Elasticsearch. 3.0.0 Script Extensions Name Description Latest Tested Version script-js Allows writing user defined JavaScript functions within Siddhi Applications to process events. 5.0.1","title":"Extensions"},{"location":"docs/extensions/#siddhi-extensions","text":"Following are some supported Siddhi extensions;","title":"Siddhi Extensions"},{"location":"docs/extensions/#extensions-released-under-apache-20-license","text":"","title":"Extensions released under Apache 2.0 License"},{"location":"docs/extensions/#execution-extensions","text":"Name Description Latest Tested Version execution-string Provides basic string handling capabilities such as concat, length, replace all, etc. 5.0.3 execution-regex Provides basic RegEx execution capabilities. 5.0.3 execution-math Provides useful mathematical functions. 5.0.1 execution-time Provides time related functionality such as getting current time, current date, manipulating/formatting dates, etc. 5.0.2 execution-map Provides the capability to generate and manipulate map data objects. 5.0.2 execution-json Provides the capability to retrieve, insert, and modify JSON elements. 2.0.1 execution-unitconversion Converts various units such as length, mass, time and volume. 2.0.1 execution-reorder Orders out-of-order event arrivals using algorithms such as K-Slack and alpha K-Stack. 5.0.0 execution-unique Retains and process unique events based on the given parameters. 5.0.0 execution-streamingml Performs streaming machine learning (clustering, classification and regression) on event streams. 2.0.1 execution-tensorflow provides support for running pre-built TensorFlow models. 2.0.1","title":"Execution Extensions"},{"location":"docs/extensions/#inputoutput-extensions","text":"Name Description Latest Tested Version io-http Receives and publishes events via http and https transports, calls external services, and serves incoming requests and provide synchronous responses. 2.0.6 io-nats Receives and publishes events from/to NATS. 2.0.3 io-kafka Receives and publishes events from/to Kafka. 5.0.1 io-email Receives and publishes events via email using smtp , pop3 and imap protocols. 2.0.2 io-cdc Captures change data from databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 2.0.1 io-tcp Receives and publishes events through TCP transport. 3.0.2 io-googlepubsub Receives and publishes events through Google Pub/Sub. 2.0.1 io-file Receives and publishes event data from/to files. 2.0.1 io-jms Receives and publishes events via Java Message Service (JMS), supporting Message brokers such as ActiveMQ 2.0.2 io-prometheus Consumes and expose Prometheus metrics from/to Prometheus server. 2.0.1","title":"Input/Output Extensions"},{"location":"docs/extensions/#data-mapping-extensions","text":"Name Description Latest Tested Version map-json Converts JSON messages to/from Siddhi events. 5.0.3 map-xml Converts XML messages to/from Siddhi events. 5.0.3 map-text Converts text messages to/from Siddhi events. 2.0.2 map-avro Converts AVRO messages to/from Siddhi events. 2.0.1 map-keyvalue Converts events having Key-Value maps to/from Siddhi events. 2.0.2 map-csv Converts messages with CSV format to/from Siddhi events. 2.0.2 map-binary Converts binary events that adheres to Siddhi format to/from Siddhi events. 2.0.2","title":"Data Mapping Extensions"},{"location":"docs/extensions/#store-extensions","text":"Name Description Latest Tested Version store-rdbms Persist and retrieve events to/from RDBMS databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 6.0.1 store-mongodb Persist and retrieve events to/from MongoDB. 2.0.1 store-redis Persist and retrieve events to/from Redis. 3.1.0 store-elasticsearch Persist and retrieve events to/from Elasticsearch. 3.0.0","title":"Store Extensions"},{"location":"docs/extensions/#script-extensions","text":"Name Description Latest Tested Version script-js Allows writing user defined JavaScript functions within Siddhi Applications to process events. 5.0.1","title":"Script Extensions"},{"location":"docs/features/","text":"Siddhi 5.2 Features Retrieving Events From various data sources supporting multiple message formats Mapping Events Mapping events with various data formats to Stream for processing Mapping streams to multiple data formats for publishing Processing Streams Filter Filtering stream based on conditions Window Support for sliding and batch (tumbling) and many other type of windows Aggregation Supporting Avg , Sum , Min , Max , etc For long running aggregations and aggregation over windows Ability to perform aggregate processing with Group by and filter aggregated data with Having conditions Incremental Aggregation Support for processing and retrieving long running Aggregation Supports data processing in seconds, minutes, hours, days, months, and years granularity Table and Stores For storing events for future processing and retrieving them on demand Supporting storage in in-memory, RDBMs, Solr, mongoDb, etc Join Joining two streams, two windows based on conditions Joining stream/window with table or incremental aggregation based on conditions Supports inner joins, and left, right full outer joins Pattern Identifies event occurrence patterns among streams over time Identify non occurrence of events Supports repetitive matches of event pattern occurrences with logical conditions and time bound Sequence processing Identifies continuous sequence of events from streams Supports zero to many, one to many, and zero to one event matching conditions Partitions Grouping queries and based on keywords or value ranges for isolated parallel processing Scripting Support writing scripts like JavaScript, Scala and R within Siddhi Queries Process Based on event time Whole execution driven by the event time Publishing Events To various data sources with various message formats Supporting load balancing and failover data publishing Error handling Support errors and exceptions through error streams Automatic backoff retries to external data stores, sources and sinks. Parallel processing Support parallel processing through asynchronous multithreading at streams Snapshot and restore Support for periodic state persistence and restore capabilities to allow state restore during failures","title":"Features"},{"location":"docs/features/#siddhi-52-features","text":"Retrieving Events From various data sources supporting multiple message formats Mapping Events Mapping events with various data formats to Stream for processing Mapping streams to multiple data formats for publishing Processing Streams Filter Filtering stream based on conditions Window Support for sliding and batch (tumbling) and many other type of windows Aggregation Supporting Avg , Sum , Min , Max , etc For long running aggregations and aggregation over windows Ability to perform aggregate processing with Group by and filter aggregated data with Having conditions Incremental Aggregation Support for processing and retrieving long running Aggregation Supports data processing in seconds, minutes, hours, days, months, and years granularity Table and Stores For storing events for future processing and retrieving them on demand Supporting storage in in-memory, RDBMs, Solr, mongoDb, etc Join Joining two streams, two windows based on conditions Joining stream/window with table or incremental aggregation based on conditions Supports inner joins, and left, right full outer joins Pattern Identifies event occurrence patterns among streams over time Identify non occurrence of events Supports repetitive matches of event pattern occurrences with logical conditions and time bound Sequence processing Identifies continuous sequence of events from streams Supports zero to many, one to many, and zero to one event matching conditions Partitions Grouping queries and based on keywords or value ranges for isolated parallel processing Scripting Support writing scripts like JavaScript, Scala and R within Siddhi Queries Process Based on event time Whole execution driven by the event time Publishing Events To various data sources with various message formats Supporting load balancing and failover data publishing Error handling Support errors and exceptions through error streams Automatic backoff retries to external data stores, sources and sinks. Parallel processing Support parallel processing through asynchronous multithreading at streams Snapshot and restore Support for periodic state persistence and restore capabilities to allow state restore during failures","title":"Siddhi 5.2 Features"},{"location":"docs/query-guide/","text":"Siddhi 5.2 Streaming SQL Guide Introduction Siddhi Streaming SQL is designed to process streams of events. It can be used to implement streaming data integration, streaming analytics, rule based and adaptive decision making use cases. It is an evolution of Complex Event Processing (CEP) and Stream Processing systems, hence it can also be used to process stateful computations, detecting of complex event patterns, and sending notifications in real-time. Siddhi Streaming SQL uses SQL like syntax, and annotations to consume events from diverse event sources with various data formats, process then using stateful and stateless operators and send outputs to multiple endpoints according to their accepted event formats. It also supports exposing rule based and adaptive decision making as service endpoints such that external programs and systems can synchronously get decision support form Siddhi. The following sections explains how to write processing logic using Siddhi Streaming SQL. Siddhi Application The processing logic for your program can be written using the Streaming SQL and put together as a single file with .siddhi extension. This file is called as the Siddhi Application or the SiddhiApp . SiddhiApps are named by adding @app:name(' name ') annotation on the top of the SiddhiApp file. When the annotation is not added Siddhi assigns a random UUID as the name of the SiddhiApp. Purpose SiddhiApp provides an isolated execution environment for your processing logic that allows you to deploy and execute processing logic independent of other SiddhiApp in the system. Therefore it's always recommended to have a processing logic related to single use case in a single SiddhiApp. This will help you to group processing logic and easily manage addition and removal of various use cases. The following diagram depicts some of the key Siddhi Streaming SQL elements of Siddhi Application and how event flows through the elements. Below table provides brief description of a few key elements in the Siddhi Streaming SQL Language. Elements Description Stream A logical series of events ordered in time with a uniquely identifiable name, and a defined set of typed attributes defining its schema. Event An event is a single event object associated with a stream. All events of a stream contains a timestamp and an identical set of typed attributes based on the schema of the stream they belong to. Table A structured representation of data stored with a defined schema. Stored data can be backed by In-Memory , or external data stores such as RDBMS , MongoDB , etc. The tables can be accessed and manipulated at runtime. Named Window A structured representation of data stored with a defined schema and eviction policy. Window data is stored In-Memory and automatically cleared by the named window constrain. Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Named Aggregation A structured representation of data that's incrementally aggregated and stored with a defined schema and aggregation granularity such as seconds, minutes, hours, etc. Aggregation data is stored both In-Memory and in external data stores such as RDBMS . Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Query A logical construct that processes events in streaming manner by by consuming data from one or more streams, tables, windows and aggregations, and publishes output events into a stream, table or a window. Source A construct that consumes data from external sources (such as TCP , Kafka , HTTP , etc) with various event formats such as XML , JSON , binary , etc, convert then to Siddhi events, and passes into streams for processing. Sink A construct that consumes events arriving at a stream, maps them to a predefined data format (such as XML , JSON , binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Input Handler A mechanism to programmatically inject events into streams. Stream/Query Callback A mechanism to programmatically consume output events from streams or queries. Partition A logical container that isolates the processing of queries based on the partition keys derived from the events. Inner Stream A positionable stream that connects portioned queries with each other within the partition. Grammar SiddhiApp is a collection of Siddhi Streaming SQL elements composed together as a script. Here each Siddhi element must be separated by a semicolon ; . Hight level syntax of SiddhiApp is as follows. siddhi app : app annotation * ( stream definition | table definition | ... ) + ( query | partition ) + ; Example Siddhi Application with name Temperature-Analytics defined with a stream named TempStream and a query named 5minAvgQuery . @app:name('Temperature-Analytics') define stream TempStream (deviceID long, roomNo int, temp double); @name('5minAvgQuery') from TempStream#window.time(5 min) select roomNo, avg(temp) as avgTemp group by roomNo insert into OutputStream; Stream A stream is a logical series of events ordered in time. Its schema is defined via the stream definition . A stream definition contains the stream name and a set of attributes with specific types and uniquely identifiable names within the stream. All events associated to the stream will have the same schema (i.e., have the same attributes in the same order). Purpose Stream groups common types of events together with a schema. This helps in various ways such as, processing all events together in queries and performing data format transformations together when they are consumed and published via sources and sinks. Syntax The syntax for defining a new stream is as follows. define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure a stream definition. Parameter Description stream name The name of the stream created. (It is recommended to define a stream name in PascalCase .) attribute name Uniquely identifiable name of the stream attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer stream and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . To make the stream process events in multi-threading and asynchronous way use the @Async annotation as shown in Multi-threading and Asynchronous Processing configuration section. Example define stream TempStream (deviceID long, roomNo int, temp double); The above creates a stream with name TempStream having the following attributes. deviceID of type long roomNo of type int temp of type double Source Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. A source configuration allows to define a mapping in order to convert each incoming event from its native data format to a Siddhi event. When customizations to such mappings are not provided, Siddhi assumes that the arriving event adheres to the predefined format based on the stream definition and the configured message mapping type. Purpose Source provides a way to consume events from external systems and convert them to be processed by the associated stream. Syntax To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as follows: @source(type=' source type ', static.key =' value ', static.key =' value ', @map(type=' map type ', static.key =' value ', static.key =' value ', @attributes( attribute1 =' attribute mapping ', attributeN =' attribute mapping ') ) ) define stream stream name ( attribute1 type , attributeN type ); This syntax includes the following annotations. Source The type parameter of @source annotation defines the source type that receives events. The other parameters of @source annotation depends upon the selected source type, and here some of its parameters can be optional. For detailed information about the supported parameters see the documentation of the relevant source. The following is the list of source types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to consume events from other SiddhiApps running on the same JVM. HTTP Expose an HTTP service to consume messages. Kafka Subscribe to Kafka topic to consume events. TCP Expose a TCP service to consume messages. Email Consume emails via POP3 and IMAP protocols. JMS Subscribe to JMS topic or queue to consume events. File Reads files by tailing or as a whole to extract events out of them. CDC Perform change data capture on databases. Prometheus Consume data from Prometheus agent. In-memory is the only source inbuilt in Siddhi, and all other source types are implemented as extensions. Source Mapper Each @source configuration can have a mapping denoted by the @map annotation that defines how to convert the incoming event format to Siddhi events. The type parameter of the @map defines the map type to be used in converting the incoming events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional. For detailed information about the parameters see the documentation of the relevant mapper. Map Attributes @attributes is an optional annotation used with @map to define custom mapping. When @attributes is not provided, each mapper assumes that the incoming events adheres to its own default message format and attempt to convert the events from that format. By adding the @attributes annotation, users can selectively extract data from the incoming message and assign them to the attributes. There are two ways to configure @attributes . Define attribute names as keys, and mapping configurations as values: @attributes( attribute1 =' mapping ', attributeN =' mapping ') Define the mapping configurations in the same order as the attributes defined in stream definition: @attributes( ' mapping for attribute1 ', ' mapping for attributeN ') Supported Source Mapping Types The following is the list of source mapping types supported by Siddhi: Source mapping type Description PassThrough Omits data conversion on Siddhi events. JSON Converts JSON messages to Siddhi events. XML Converts XML messages to Siddhi events. TEXT Converts plain text messages to Siddhi events. Avro Converts Avro events to Siddhi events. Binary Converts Siddhi specific binary events to Siddhi events. Key Value Converts key-value HashMaps to Siddhi events. CSV Converts CSV like delimiter separated events to Siddhi events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the consumed Siddhi events directly to the streams without any data conversion. PassThrough is the only source mapper inbuilt in Siddhi, and all other source mappers are implemented as extensions. Example 1 Receive JSON messages by exposing an HTTP service, and direct them to InputStream stream for processing. Here the HTTP service will be secured with basic authentication, receives events on all network interfaces on port 8080 and context /foo . The service expects the JSON messages to be on the default data format that's supported by the JSON mapper as follows. { \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } The configuration of the HTTP source and JSON source mapper to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json')) define stream InputStream (name string, age int, country string); Example 2 Receive JSON messages by exposing an HTTP service, and direct them to StockStream stream for processing. Here the incoming JSON , as given bellow, do not adhere to the default data format that's supported by the JSON mapper. { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"FB\" }, \"price\":55.6 } } } The configuration of the HTTP source and the custom JSON source mapping to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); The same can also be configured by omitting the attribute names as bellow. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(\"stock.company.symbol\", \"stock.price\", \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); Sink Sinks consumes events from streams and publish them via multiple transports to external endpoints in various data formats. A sink configuration allows users to define a mapping to convert the Siddhi events in to the required output data format (such as JSON , TEXT , XML , etc.) and publish the events to the configured endpoints. When customizations to such mappings are not provided, Siddhi converts events to the predefined event format based on the stream definition and the configured message mapper type before publishing the events. Purpose Sink provides a way to publish Siddhi events of a stream to external systems by converting events to their supported format. Syntax To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as follows: @sink(type=' sink type ', static.key =' value ', dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) ) define stream stream name ( attribute1 type , attributeN type ); Dynamic Properties The sink and sink mapper properties that are categorized as dynamic have the ability to absorb attribute values dynamically from the Siddhi events of their associated streams. This can be configured by enclosing the relevant attribute names in double curly braces as {{...}} , and using it within the property values. Some valid dynamic properties values are: '{{attribute1}}' 'This is {{attribute1}}' {{attribute1}} {{attributeN}} Here the attribute names in the double curly braces will be replaced with the values from the events before they are published. This syntax includes the following annotations. Sink The type parameter of the @sink annotation defines the sink type that publishes the events. The other parameters of the @sink annotation depends upon the selected sink type, and here some of its parameters can be optional and/or dynamic. For detailed information about the supported parameters see documentation of the relevant sink. The following is a list of sink types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to publish events to other SiddhiApps running on the same JVM. Log Logs the events appearing on the streams. HTTP Publish events to an HTTP endpoint. Kafka Publish events to Kafka topic. TCP Publish events to a TCP service. Email Send emails via SMTP protocols. JMS Publish events to JMS topics or queues. File Writes events to files. Prometheus Expose data through Prometheus agent. Distributed Sink Distributed Sinks publish events from a defined stream to multiple endpoints using load balancing or partitioning strategies. Any sink can be used as a distributed sink. A distributed sink configuration allows users to define a common mapping to convert and send the Siddhi events for all its destination endpoints. Purpose Distributed sink provides a way to publish Siddhi events to multiple endpoints in the configured event format. Syntax To configure distributed sink add the sink configuration to a stream definition by adding the @sink annotation and add the configuration parameters that are common of all the destination endpoints inside it, along with the common parameters also add the @distribution annotation specifying the distribution strategy (i.e. roundRobin or partitioned ) and @destination annotations providing each endpoint specific configurations. The distributed sink syntax is as follows: RoundRobin Distributed Sink Publishes events to defined destinations in a round robin manner. @sink(type=' sink type ', common.static.key =' value ', common.dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) @distribution(strategy='roundRobin', @destination( destination.specific.key =' value '), @destination( destination.specific.key =' value '))) ) define stream stream name ( attribute1 type , attributeN type ); Partitioned Distributed Sink Publishes events to defined destinations by partitioning them based on the partitioning key. @sink(type=' sink type ', common.static.key =' value ', common.dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) @distribution(strategy='partitioned', partitionKey=' partition key ', @destination( destination.specific.key =' value '), @destination( destination.specific.key =' value '))) ) define stream stream name ( attribute1 type , attributeN type ); Sink Mapper Each @sink configuration can have a mapping denoted by the @map annotation that defines how to convert Siddhi events to outgoing messages with the defined format. The type parameter of the @map defines the map type to be used in converting the outgoing events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional and/or dynamic. For detailed information about the parameters see the documentation of the relevant mapper. Map Payload @payload is an optional annotation used with @map to define custom mapping. When the @payload annotation is not provided, each mapper maps the outgoing events to its own default event format. The @payload annotation allow users to configure mappers to produce the output payload of their choice, and by using dynamic properties within the payload they can selectively extract and add data from the published Siddhi events. There are two ways you to configure @payload annotation. Some mappers such as XML , JSON , and Test only accept one output payload: @payload( 'This is a test message from {{user}}.') Some mappers such key-value accept series of mapping values: @payload( key1='mapping_1', 'key2'='user : {{user}}') Here, the keys of payload mapping can be defined using the dot notation as a.b.c , or using any constant string value as '$abc' . Supported Sink Mapping Types The following is a list of sink mapping types supported by Siddhi: Sink mapping type Description PassThrough Omits data conversion on outgoing Siddhi events. JSON Converts Siddhi events to JSON messages. XML Converts Siddhi events to XML messages. TEXT Converts Siddhi events to plain text messages. Avro Converts Siddhi events to Avro Events. Binary Converts Siddhi events to Siddhi specific binary events. Key Value Converts Siddhi events to key-value HashMaps. CSV Converts Siddhi events to CSV like delimiter separated events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the outgoing Siddhi events directly to the sinks without any data conversion. PassThrough is the only sink mapper inbuilt in Siddhi, and all other sink mappers are implemented as extensions. Example 1 Publishes OutputStream events by converting them to JSON messages with the default format, and by sending to an HTTP endpoint http://localhost:8005/endpoint1 , using POST method, Accept header, and basic authentication having admin is both username and password. The configuration of the HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/endpoint', method='POST', headers='Accept-Date:20/02/2017', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json')) define stream OutputStream (name string, age int, country string); This will publish a JSON message on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } } Example 2 Publishes StockStream events by converting them to user defined JSON messages, and by sending to an HTTP endpoint http://localhost:8005/stocks . The configuration of the HTTP sink and custom JSON sink mapping to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/stocks', @map(type='json', validate.json='true', enclosing.element='$.Portfolio', @payload(\"\"\"{\"StockData\":{ \"Symbol\":\"{{symbol}}\", \"Price\":{{price}} }}\"\"\"))) define stream StockStream (symbol string, price float, volume long); This will publish a single event as the JSON message on the following format: { \"Portfolio\":{ \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } } } This can also publish multiple events together as a JSON message on the following format: { \"Portfolio\":[ { \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } }, { \"StockData\":{ \"Symbol\":\"FB\", \"Price\":57.0 } } ] } Example 3 Publishes events from the OutputStream stream to multiple the HTTP endpoints using a partitioning strategy. Here the events are sent to either http://localhost:8005/endpoint1 or http://localhost:8006/endpoint2 based on the partitioning key country . It uses default JSON mapping, POST method, and used admin as both the username and the password when publishing to both the endpoints. The configuration of the distributed HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', method='POST', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json'), @distribution(strategy='partitioned', partitionKey='country', @destination(publisher.url='http://localhost:8005/endpoint1'), @destination(publisher.url='http://localhost:8006/endpoint2'))) define stream OutputStream (name string, age int, country string); This will partition the outgoing events and publish all events with the same country attribute value to the same endpoint. The JSON message published will be on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } } Error Handling Errors in Siddhi can be handled at the Streams and in Sinks. Error Handling at Stream When errors are thrown by Siddhi elements subscribed to the stream, the error gets propagated up to the stream that delivered the event to those Siddhi elements. By default the error is logged and dropped at the stream, but this behavior can be altered by by adding @OnError annotation to the corresponding stream definition. @OnError annotation can help users to capture the error and the associated event, and handle them gracefully by sending them to a fault stream. The @OnError annotation and the required action to be specified as bellow. @OnError(action='on error action') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The action parameter of the @OnError annotation defines the action to be executed during failure scenarios. The following actions can be specified to @OnError annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when @OnError annotation is not defined. STREAM : Creates a fault stream and redirects the event and the error to it. The created fault stream will have all the attributes defined in the base stream to capture the error causing event, and in addition it also contains _error attribute of type object to containing the error information. The fault stream can be referred by adding ! in front of the base stream name as ! stream name . Example Handle errors in TempStream by redirecting the errors to a fault stream. The configuration of TempStream stream and @OnError annotation is as follows. @OnError(action='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); Siddhi will infer and automatically defines the fault stream of TempStream as given bellow. define stream !TempStream (deviceID long, roomNo int, temp double, _error object); The SiddhiApp extending the above the use-case by adding failure generation and error handling with the use of queries is as follows. Note: Details on writing processing logics via queries will be explained in later sections. -- Define fault stream to handle error occurred at TempStream subscribers @OnError(action='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); -- Error generation through a custom function `createError()` @name('error-generation') from TempStream#custom:createError() insert into IgnoreStream1; -- Handling error by simply logging the event and error. @name('handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream2; Error Handling at Sink There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. The on.error parameter of the @sink annotation can be specified as bellow. @sink(type=' sink type ', on.error=' on error action ', key =' value ', ...) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following actions can be specified to on.error parameter of @sink annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when on.error parameter is not defined on the @sink annotation. WAIT : Publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publishes to it. STREAM : Pushes the failed events with the corresponding error to the associated fault stream the sink belongs to. Example 1 Introduce back pressure on the threads who bring events via TempStream when the system cannot connect to Kafka. The configuration of TempStream stream and @sink Kafka annotation with on.error property is as follows. @sink(type='kafka', on.error='WAIT', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); Example 2 Send events to the fault stream of TempStream when the system cannot connect to Kafka. The configuration of TempStream stream with associated fault stream, @sink Kafka annotation with on.error property and a queries to handle the error is as follows. Note: Details on writing processing logics via queries will be explained in later sections. @OnError(action='STREAM') @sink(type='kafka', on.error='STREAM', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); -- Handling error by simply logging the event and error. @name('handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream; Query Query defines the processing logic in Siddhi. It consumes events from one or more streams, named-windows , tables , and/or named-aggregations , process the events in a streaming manner, and generate output events into a stream , named-window , or table . Purpose A query provides a way to process the events in the order they arrive and produce output using both stateful and stateless complex event processing and stream processing operations. Syntax The high level query syntax for defining processing logics is as follows: @name(' query name ') from input projection output action The following parameters are used to configure a stream definition. Parameter Description query name The name of the query. Since naming the query (i.e the @name(' query name ') annotation) is optional, when the name is not provided Siddhi assign a system generated name for the query. input Defines the means of event consumption via streams , named-windows , tables , and/or named-aggregations , and defines the processing logic using filters , windows , stream-functions , joins , patterns and sequences . projection Generates output event attributes using select , functions , aggregation-functions , and group by operations, and filters the generated the output using having , limit offset , order by , and output rate limiting operations before sending them out. Here the projection is optional and when it is omitted all the input events will be sent to the output as it is. output action Defines output action (such as insert into , update , delete , etc) that needs to be performed by the generated events on a stream , named-window , or table Example A query consumes events from the TempStream stream and output only the roomNo and temp attributes to the RoomTempStream stream, from which another query consumes the events and sends all its attributes to AnotherRoomTempStream stream. define stream TempStream (deviceID long, roomNo int, temp double); from TempStream select roomNo, temp insert into RoomTempStream; from RoomTempStream insert into AnotherRoomTempStream; Inferred Stream Here, the RoomTempStream and AnotherRoomTempStream streams are an inferred streams, which means their stream definitions are inferred from the queries and they can be used same as any other defined stream without any restrictions. Value Values are typed data, that can be manipulated, transferred and stored. Values can be referred by the attributes defined in definitions such as streams, and tables. Siddhi supports values of type STRING , INT (Integer), LONG , DOUBLE , FLOAT , BOOL (Boolean) and OBJECT . The syntax of each type and their example use as a constant value is as follows, Attribute Type Format Example int + 123 , -75 , +95 long +L 123000L , -750l , +154L float ( +)?('.' *)? (E(-|+)? +)?F 123.0f , -75.0e-10F , +95.789f double ( +)?('.' *)? (E(-|+)? +)?D? 123.0 , 123.0D , -75.0e-10D , +95.789d bool (true|false) true , false , TRUE , FALSE string '( char * !('|\"|\"\"\"| line ))' or \"( char * !(\"|\"\"\"| line ))\" or \"\"\"( char * !(\"\"\"))\"\"\" 'Any text.' , \"Text with 'single' quotes.\" , \"\"\" Text with 'single' quotes, \"double\" quotes, and new lines. \"\"\" Time Time is a special type of LONG value that denotes time using digits and their unit in the format ( digit + unit )+ . At execution, the time gets converted into milliseconds and returns a LONG value. Unit Syntax Year year | years Month month | months Week week | weeks Day day | days Hour hour | hours Minutes minute | minutes | min Seconds second | seconds | sec Milliseconds millisecond | milliseconds Example 1 hour and 25 minutes can by written as 1 hour and 25 minutes which is equal to the LONG value 5100000 . Select The select clause in Siddhi query defines the output event attributes of the query. Following are some basic query projection operations supported by select. Action Description Select specific attributes for projection Only select some of the input attributes as query output attributes. E.g., Select and output only roomNo and temp attributes from the TempStream stream. from TempStream select roomNo, temp insert into RoomTempStream; Select all attributes for projection Select all input attributes as query output attributes. This can be done by using asterisk ( * ) or by omitting the select clause itself. E.g., Both following queries select all attributes of TempStream input stream and output all attributes to NewTempStream stream. from TempStream select * insert into NewTempStream; or from TempStream insert into NewTempStream; Name attribute Provide a unique name for each output attribute generated by the query. This can help to rename the selected input attributes or assign an attribute name to a projection operation such as function, aggregate-function, mathematical operation, etc, using as keyword. E.g., Query that renames input attribute temp to temperature and function currentTimeMillis() as time . from TempStream select roomNo, temp as temperature, currentTimeMillis() as time insert into RoomTempStream; Constant values as attributes Creates output attributes with a constant value. Any constant value of type STRING , INT , LONG , DOUBLE , FLOAT , BOOL , and time as given in the values section can be defined. E.g., Query specifying 'C' as the constant value for the scale attribute. from TempStream select roomNo, temp, 'C' as scale insert into RoomTempStream; Mathematical and logical expressions in attributes Defines the mathematical and logical operations that need to be performed to generating output attribute values. These expressions are executed in the precedence order given below. Operator precedence Operator Distribution Example () Scope (cost + tax) * 0.05 IS NULL Null check deviceID is null NOT Logical NOT not (price > 10) * , / , % Multiplication, division, modulus temp * 9/5 + 32 + , - Addition, subtraction temp * 9/5 - 32 < , < = , > , >= Comparators: less-than, greater-than-equal, greater-than, less-than-equal totalCost >= price * quantity == , != Comparisons: equal, not equal totalCost != price * quantity IN Checks if value exist in the table roomNo in ServerRoomsTable AND Logical AND temp < 40 and humidity < 40 OR Logical OR humidity < 40 or humidity >= 60 E.g., Query converts temperature from Celsius to Fahrenheit, and identifies rooms with room number between 10 and 15 as server rooms. from TempStream select roomNo, temp * 9/5 + 32 as temp, 'F' as scale, roomNo > 10 and roomNo < 15 as isServerRoom insert into RoomTempStream; Function Function are pre-configured operations that can consumes zero, or more parameters and always produce a single value as result. It can be used anywhere an attribute can be used. Purpose Functions encapsulate pre-configured reusable execution logic allowing users to execute the logic anywhere just by calling the function. This also make writing SiddhiApps simple and easy to understand. Syntax The syntax of function is as follows, function name ( parameter * ) Here function name uniquely identifies the function. The parameter defined input parameters the function can accept. The input parameters can be attributes, constant values, results of other functions, results of mathematical or logical expressions, or time values. The number and type of parameters a function accepts depend on the function itself. Note Functions, mathematical expressions, and logical expressions can be used in a nested manner. Example 1 Function name add accepting two input parameters, is called with an attribute named input and a constant value 75 . add(input, 75) Example 2 Function name alertAfter accepting two input parameters, is called with a time value of 1 hour and 25 minutes and a mathematical addition operation of startTime + 56 . add(1 hour and 25 minutes, startTime + 56) Inbuilt functions Following are some inbuilt Siddhi functions, for more functions refer execution extensions . Inbuilt function Description eventTimestamp Returns event's timestamp. currentTimeMillis Returns current time of SiddhiApp runtime. default Returns a default value if the parameter is null. ifThenElse Returns parameters based on a conditional parameter. UUID Generates a UUID. cast Casts parameter type. convert Converts parameter type. coalesce Returns first not null input parameter. maximum Returns the maximum value of all parameters. minimum Returns the minimum value of all parameters. instanceOfBoolean Checks if the parameter is an instance of Boolean. instanceOfDouble Checks if the parameter is an instance of Double. instanceOfFloat Checks if the parameter is an instance of Float. instanceOfInteger Checks if the parameter is an instance of Integer. instanceOfLong Checks if the parameter is an instance of Long. instanceOfString Checks if the parameter is an instance of String. createSet Creates HashSet with given input parameters. sizeOfSet Returns number of items in the HashSet, that's passed as a parameter. Example Query that converts the roomNo to string using convert function, finds the maximum temperature reading with maximum function, and adds a unique messageID using the UUID function. from TempStream select convert(roomNo, 'string') as roomNo, maximum(tempReading1, tempReading2) as temp, UUID() as messageID insert into RoomTempStream; Filter Filters provide a way of filtering input stream events based on a specified condition. It accepts any type of condition including a combination of functions and/or attributes that produces a Boolean result. Filters allow events to passthrough if the condition results in true , and drops if it results in a false . Purpose Filter helps to select the events that are relevant for the processing and omit the ones that are not. Syntax Filter conditions should be defined in square brackets ( [] ) next to the input stream as shown below. from input stream [ filter condition ] select attribute name , attribute name , ... insert into output stream Example Query to filter TempStream stream events, having roomNo within the range of 100-210 and temperature greater than 40 degrees, and insert them into HighTempStream stream. from TempStream[(roomNo = 100 and roomNo 210) and temp 40] select roomNo, temp insert into HighTempStream; Window Window provides a way to capture a subset of events from an input stream and retain them for a period of time based on a specified criterion. The criterion defines when and how the events should be evicted from the windows. Such as events getting evicted from the window based on the time duration, or number of events and they events are evicted in a sliding (one by one) or tumbling (batch) manner. Within a query, each input stream can at most have only one window associated with it. Purpose Windows help to retain events based on a criterion, such that the values of those events can be aggregated, or checked if an event of interest is within the window or not. Syntax Window should be defined by using the #window prefix next to the input stream as shown below. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert ouput event type ? into output stream Note Filter conditions can be applied both before and/or after the window. Inbuilt windows Following are some inbuilt Siddhi windows, for more windows refer execution extensions . Inbuilt function Description time Retains events based on time in a sliding manner. timeBatch Retains events based on time in a tumbling/batch manner. length Retains events based on number of events in a sliding manner. lengthBatch Retains events based on number of events in a tumbling/batch manner. timeLength Retains events based on time and number of events in a sliding manner. session Retains events for each session based on session key. batch Retains events of last arrived event chunk. sort Retains top-k or bottom-k events based on a parameter value. cron Retains events based on cron time in a tumbling/batch manner. externalTime Retains events based on event time value passed as a parameter in a sliding manner. externalTimeBatch Retains events based on event time value passed as a parameter in a a tumbling/batch manner. delay Retains events and delays the output by the given time period in a sliding manner. Example 1 Query to find out the maximum temperature out of the last 10 events , using the window of length 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.length(10) select max(temp) as maxTemp insert into MaxTempStream; Here, the length window operates in a sliding manner where the following 3 event subsets are calculated and outputted when a list of 12 events are received in sequential order. Subset Event Range 1 1 - 10 2 2 - 11 3 3 - 12 Example 2 Query to find out the maximum temperature out of the every 10 events , using the window of lengthBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.lengthBatch(10) select max(temp) as maxTemp insert into MaxTempStream; Here, the window operates in a batch/tumbling manner where the following 3 event subsets are calculated and outputted when a list of 30 events are received in a sequential order. Subset Event Range 1 1 - 10 2 11 - 20 3 21 - 30 Example 3 Query to find out the maximum temperature out of the events arrived during last 10 minutes , using the window of time 10 minutes and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.time(10 min) select max(temp) as maxTemp insert into MaxTempStream; Here, the time window operates in a sliding manner with millisecond accuracy, where it will process events in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:00:01.001 - 1:10:01.000 3 1:00:01.033 - 1:10:01.034 Example 4 Query to find out the maximum temperature out of the events arriving every 10 minutes , using the window of timeBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.timeBatch(10 min) select max(temp) as maxTemp insert into MaxTempStream; Here, the window operates in a batch/tumbling manner where the window will process evetns in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:10:00.001 - 1:20:00.000 3 1:20:00.001 - 1:30:00.000 Event Type Query output depends on the current and expired event types it produces based on its internal processing state. By default all queries produce current events upon event arrival to the query. The queries containing windows additionally produce expired events when events expire from the windows. Purpose Event type helps to specify when a query should output events to the stream, such as output upon current events, expired events or upon both current and expired events. Syntax Event type should be defined in between insert and into keywords for insert queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert event type into output stream Event type should be defined next to the for keyword for delete queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... delete table (for event type )? on condition Event type should be defined next to the for keyword for update queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... update table (for event type )? set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition Event type should be defined next to the for keyword for update or insert queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... update or insert into table (for event type )? set table . attribute name = expression , table . attribute name = expression , ... on condition Note Controlling query output based on the event types neither alters query execution nor its accuracy. The event types can be defined using the following keywords to manipulate query output. Event types Description current events Outputs events only when incoming events arrive to be processed by the query. This is default behavior when no specific event type is specified. expired events Outputs events only when events expires from the window. all events Outputs events when incoming events arrive to be processed by the query as well as when events expire from the window. Example Query to output only the expired events from a 1 minute time window to the DelayedTempStream stream. This can be used for delaying the events by a minute. from TempStream#window.time(1 min) select * insert expired events into DelayedTempStream Note This is just to illustrate how expired events work, it is recommended to use delay window for usecases where we need to delay events by a given time period. Aggregate Function Aggregate functions are pre-configured aggregation operations that can consumes zero, or more parameters from multiple events and always produce a single value as result. They can be only used in the query projection (as part of the select clause). When a query comprises a window, the aggregation will be contained to the events in the window, and when it does not have a window, the aggregation is performed from the first event the query has received. Purpose Aggregate functions encapsulate pre-configured reusable aggregate logic allowing users to aggregate values of multiple events together. When used with batch/tumbling windows this can also help to reduce the number of output events produced. Syntax Aggregate function can be used in query projection (as part of the select clause) alone or as a part of another expression. In all cases, the output produced by the query should be properly mapped to the output stream attribute using the as keyword. The syntax of aggregate function is as follows, from input stream #window. window name ( parameter , parameter , ... ) select aggregate function ( parameter , parameter , ... ) as attribute name , attribute2 name , ... insert into output stream ; Here aggregate function uniquely identifies the aggregate function. The parameter defined input parameters the aggregate function can accept. The input parameters can be attributes, constant values, results of other functions or aggregate functions, results of mathematical or logical expressions, or time values. The number and type of parameters an aggregate function accepts depend on the function itself. Inbuilt aggregate functions Following are some inbuilt aggregation functions, for more functions refer execution extensions . Inbuilt aggregate function Description sum Calculates the sum from a set of values. count Calculates the count from a set of values. distinctCount Calculates the distinct count based on a parameter from a set of values. avg Calculates the average from a set of values. max Finds the maximum value from a set of values. max Finds the minimum value from a set of values. | maxForever | Finds the maximum value from all events throughout its lifetime irrespective of the windows. | | minForever | Finds the minimum value from all events throughout its lifetime irrespective of the windows. | | stdDev | Calculates the standard deviation from a set of values. | | and | Calculates boolean and from a set of values. | | or | Calculates boolean or from a set of values. | | unionSet | Calculates union as a Set from a set of values. | Example Query to calculate average, maximum, and minimum values on temp attribute of the TempStream stream in a sliding manner, from the events arrived over the last 10 minutes and to produce outputs avgTemp , maxTemp and minTemp respectively to the AvgTempStream output stream. from TempStream#window.time(10 min) select avg(temp) as avgTemp, max(temp) as maxTemp, min(temp) as minTemp insert into AvgTempStream; Group By Group By provides a way of grouping events based on one or more specified attributes to perform aggregate operations. Purpose Group By allows users to aggregate values of multiple events based on the given group-by fields. Syntax The syntax for the Group By with aggregate function is as follows. from input stream #window. window name (...) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name , ... insert into output stream ; Here the group by attributes should be defined next to the group by keyword separating each attribute by a comma. Example Query to calculate the average temp per roomNo and deviceID combination, from the events arrived from TempStream stream, during the last 10 minutes time-window in a sliding manner. from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID insert into AvgTempStream; Having Having provide a way of filtering events based on a specified condition of the query output stream attributes. It accepts any type of condition including a combination of functions and/or attributes that produces a Boolean result. Having, allow events to passthrough if the condition results in true , and drops if it results in a false . Purpose Having helps to select the events that are relevant for the output based on the attributes those are produced by the select clause and omit the ones that are not. Syntax The syntax for the Having clause is as follows. from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition insert into output stream ; Here the having condition should be defined next to the having keyword and having can be used with or without group by clause. Example Query to calculate the average temp per roomNo for the last 10 minutes, and alerts if the avgTemp exceeds 30 degrees. from TempStream#window.time(10 min) select roomNo, avg(temp) as avgTemp group by roomNo having avgTemp 30 insert into AlertStream; Order By Order By, orders the query results in ascending and or descending order based on one or more specified attributes. When an attribute is used for order by, by default Siddhi orders the events in ascending order of that attribute's value, and by adding desc keyword, the events can be ordered in descending order. When more than one attribute is defined the attributes defined towards the left will have more precedence in ordering than the ones defined in right. Purpose Order By helps to sort the events in the outputs chunks produced by the query. Order By will be more helpful for batch windows, and queries where they output many of event together then for sliding window use cases where the output will be one or few events at a time. Syntax The syntax for the Order By clause is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name (asc|desc)?, attribute2 name (asc|desc)?, ... insert into output stream ; Here the order by attributes should be defined next to the order by keyword separating each by a comma, and optionally specifying the event ordering using asc (default) or desc keywords. Example Query to calculate the average temp per roomNo and deviceID combination on every 10 minutes batches, and order the generated output events in ascending order by avgTemp and then by descending order of roomNo (if the more than one event have the same avgTemp value). from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp, roomNo desc insert into AvgTempStream; Limit Offset These provide a way to select the number of events (via limit) from the desired index (by specifying an offset) from the output event chunks produced by the query. Purpose Limit Offset helps to output only the selected set of events from large event batches. This will be more useful with Order By clause where one can order the output for topK, bottomK, or even to paginate through the dataset by obtaining a set of events from the middle. Syntax The syntax for the Limit Offset clauses is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name (asc | desc)?, attribute2 name ( ascend/descend )?, ... limit positive integer ? offset positive integer ? insert into output stream ; Here both limit and offset are optional, when limit is omitted the query will output all the events, and when offset is omitted 0 is taken as the default offset value. Example 1 Query to calculate the average temp per roomNo and deviceID combination for every 10 minutes batches, from the events arriving at the TempStream stream, and emit only two events having the highest avgTemp value. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp desc limit 2 insert into HighestAvgTempStream; Example 2 Query to calculate the average temp per roomNo and deviceID combination for every 10 minutes batches, for events that arriving at the TempStream stream, and emits only the third, forth and fifth events when sorted in descending order based on their avgTemp value. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp desc limit 3 offset 2 insert into HighestAvgTempStream; Join (Stream) Joins allow you to get a combined result from two streams in real-time based on a specified condition. Purpose Streams are stateless. Therefore, in order to join two streams, they need to be connected to a window so that there is a pool of events that can be used for joining. Joins also accept conditions to join the appropriate events from each stream. During the joining process each incoming event of each stream is matched against all the events in the other stream's window based on the given condition, and the output events are generated for all the matching event pairs. Note Join can also be performed with stored data , aggregation or externally named windows . Syntax The syntax for a join is as follows: from input stream #window. window name ( parameter , ... ) {unidirectional} {as reference } join input stream #window. window name ( parameter , ... ) {unidirectional} {as reference } on join condition select attribute name , attribute name , ... insert into output stream Here, the join condition allows you to match the attributes from both the streams. Unidirectional join operation By default, events arriving at either stream can trigger the joining process. However, if you want to control the join execution, you can add the unidirectional keyword next to a stream in the join definition as depicted in the syntax in order to enable that stream to trigger the join operation. Here, events arriving at other stream only update the window of that stream, and this stream does not trigger the join operation. Note The unidirectional keyword cannot be applied to both the input streams because the default behaviour already allows both streams to trigger the join operation. Example Assuming that the temperature of regulators are updated every minute. Following is a Siddhi App that controls the temperature regulators if they are not already on for all the rooms with a room temperature greater than 30 degrees. define stream TempStream(deviceID long, roomNo int, temp double); define stream RegulatorStream(deviceID long, roomNo int, isOn bool); from TempStream[temp 30.0]#window.time(1 min) as T join RegulatorStream[isOn == false]#window.length(1) as R on T.roomNo == R.roomNo select T.roomNo, R.deviceID, 'start' as action insert into RegulatorActionStream; Supported join types Following are the supported operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join both the streams. The output is generated only if there is a matching event in both the streams. Left outer join The left outer join operation allows you to join two streams to be merged based on a condition. left outer join is used as the keyword to join both the streams. Here, it returns all the events of left stream even if there are no matching events in the right stream by having null values for the attributes of the right stream. Example The following query generates output events for all events from the StockStream stream regardless of whether a matching symbol exists in the TwitterStream stream or not. from StockStream#window.time(1 min) as S left outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ; Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join both the streams. It returns all the events of the right stream even if there are no matching events in the left stream. Full outer join The full outer join combines the results of left outer join and right outer join. full outer join is used as the keyword to join both the streams. Here, output event are generated for each incoming event even if there are no matching events in the other stream. Example The following query generates output events for all the incoming events of each stream regardless of whether there is a match for the symbol attribute in the other stream or not. from StockStream#window.time(1 min) as S full outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ; Pattern This is a state machine implementation that allows you to detect patterns in the events that arrive over time. This can correlate events within a single stream or between multiple streams. Purpose Patterns allow you to identify trends in events over a time period. Syntax The following is the syntax for a pattern query: from (every)? event reference = input stream [ filter condition ] - (every)? event reference = input stream [ filter condition ] - ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description - This is used to indicate an event that should be following another event. The subsequent event does not necessarily have to occur immediately after the preceding event. The condition to be met by the preceding event should be added before the sign, and the condition to be met by the subsequent event should be added after the sign. event reference This allows you to add a reference to the the matching event so that it can be accessed later for further processing. (within time gap )? The within clause is optional. It defines the time duration within which all the matching events should occur. every every is an optional keyword. This defines whether the event matching should be triggered for every event arrival in the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. Siddhi also supports pattern matching with counting events and matching events in a logical order such as ( and , or , and not ). These are described in detail further below in this guide. Example This query sends an alert if the temperature of a room increases by 5 degrees within 10 min. from every( e1=TempStream ) - e2=TempStream[ e1.roomNo == roomNo and (e1.temp + 5) = temp ] within 10 min select e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Here, the matching process begins for each event in the TempStream stream (because every is used with e1=TempStream ), and if another event arrives within 10 minutes with a value for the temp attribute that is greater than or equal to e1.temp + 5 of the event e1, an output is generated via the AlertStream . Counting Pattern Counting patterns allow you to match multiple events that may have been received for the same matching condition. The number of events matched per condition can be limited via condition postfixes. Syntax Each matching condition can contain a collection of events with the minimum and maximum number of events to be matched as shown in the syntax below. from (every)? event reference = input stream [ filter condition ] ( min count : max count )? - ... (within time gap )? select event reference ([event index])?. attribute name , ... insert into output stream Postfix Description Example n1:n2 This matches n1 to n2 events (including n1 and not more than n2 ). 1:4 matches 1 to 4 events. n: This matches n or more events (including n ). 2: matches 2 or more events. :n This matches up to n events (excluding n ). :5 matches up to 5 events. n This matches exactly n events. 5 matches exactly 5 events. Specific occurrences of the event in a collection can be retrieved by using an event index with its reference. Square brackets can be used to indicate the event index where 1 can be used as the index of the first event and last can be used as the index for the last available event in the event collection. If you provide an index greater then the last event index, the system returns null . The following are some valid examples. e1[3] refers to the 3 rd event. e1[last] refers to the last event. e1[last - 1] refers to the event before the last event. Example The following Siddhi App calculates the temperature difference between two regulator events. define stream TempStream (deviceID long, roomNo int, temp double); define stream RegulatorStream (deviceID long, roomNo int, tempSet double, isOn bool); from every( e1=RegulatorStream) - e2=TempStream[e1.roomNo==roomNo] 1: - e3=RegulatorStream[e1.roomNo==roomNo] select e1.roomNo, e2[0].temp - e2[last].temp as tempDiff insert into TempDiffStream; Logical Patterns Logical patterns match events that arrive in temporal order and correlate them with logical relationships such as and , or and not . Syntax from (every)? (not)? event reference = input stream [ filter condition ] ((and|or) event reference = input stream [ filter condition ])? (within time gap )? - ... select event reference ([event index])?. attribute name , ... insert into output stream Keywords such as and , or , or not can be used to illustrate the logical relationship. Key Word Description and This allows both conditions of and to be matched by two events in any order. or The state succeeds if either condition of or is satisfied. Here the event reference of the other condition is null . not condition1 and condition2 When not is included with and , it identifies the events that match arriving before any event that match . not condition for time period When not is included with for , it allows you to identify a situation where no event that matches condition1 arrives during the specified time period . e.g., from not TemperatureStream[temp 60] for 5 sec . Here the not pattern can be followed by either an and clause or the effective period of not can be concluded after a given time period . Further in Siddhi more than two streams cannot be matched with logical conditions using and , or , or not clauses at this point. Detecting Non-occurring Events Siddhi allows you to detect non-occurring events via multiple combinations of the key words specified above as shown in the table below. In the patterns listed, P* can be either a regular event pattern, an absent event pattern or a logical pattern. Pattern Detected Scenario not A for time period The non-occurrence of event A within time period after system start up. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, to indicate that the passenger might be in danger. not A for time period and B After system start up, event A does not occur within time period , but event B occurs at some point in time. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, and the passenger marked that he/she is in danger at some point in time. not A for time period 1 and not B for time period 2 After system start up, event A doess not occur within time period 1 , and event B also does not occur within time period 2 . e.g., Generating an alert if the driver of a taxi has not reached the destination within 30 minutes, and the passenger has not marked himself/herself to be in danger within that same time period. not A for time period or B After system start up, either event A does not occur within time period , or event B occurs at some point in time. e.g., Generating an alert if the taxi has not reached its destination within 30 minutes, or if the passenger has marked that he/she is in danger at some point in time. not A for time period 1 or not B for time period 2 After system start up, either event A does not occur within time period 1 , or event B occurs within time period 2 . e.g., Generating an alert to indicate that the driver is not on an expected route if the taxi has not reached destination A within 20 minutes, or reached destination B within 30 minutes. A \u2192 not B for time period Event B does not occur within time period after the occurrence of event A. e.g., Generating an alert if the taxi has reached its destination, but this was not followed by a payment record. P* \u2192 not A for time period and B After the occurrence of P*, event A does not occur within time period , and event B occurs at some point in time. P* \u2192 not A for time period 1 and not B for time period 2 After the occurrence of P*, event A does not occur within time period 1 , and event B does not occur within time period 2 . P* \u2192 not A for time period or B After the occurrence of P*, either event A does not occur within time period , or event B occurs at some point in time. P* \u2192 not A for time period 1 or not B for time period 2 After the occurrence of P*, either event A does not occur within time period 1 , or event B does not occur within time period 2 . not A for time period \u2192 B Event A does occur within time period after the system start up, but event B occurs after that time period has elapsed. not A for time period and B \u2192 P* Event A does not occur within time period , and event B occurs at some point in time. Then P* occurs after the time period has elapsed, and after B has occurred. not A for time period 1 and not B for time period 2 \u2192 P* After system start up, event A does not occur within time period 1 , and event B does not occur within time period 2 . However, P* occurs after both A and B. not A for time period or B \u2192 P* After system start up, event A does not occur within time period or event B occurs at some point in time. The P* occurs after time period has elapsed, or after B has occurred. not A for time period 1 or not B for time period 2 \u2192 P* After system start up, either event A does not occur within time period 1 , or event B does not occur within time period 2 . Then P* occurs after both time period 1 and time period 2 have elapsed. not A and B Event A does not occur before event B. A and not B Event B does not occur before event A. Example Following Siddhi App, sends the stop control action to the regulator when the key is removed from the hotel room. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream RoomKeyStream(deviceID long, roomNo int, action string); from every( e1=RegulatorStateChangeStream[ action == 'on' ] ) - e2=RoomKeyStream[ e1.roomNo == roomNo and action == 'removed' ] or e3=RegulatorStateChangeStream[ e1.roomNo == roomNo and action == 'off'] select e1.roomNo, ifThenElse( e2 is null, 'none', 'stop' ) as action having action != 'none' insert into RegulatorActionStream; This Siddhi Application generates an alert if we have switch off the regulator before the temperature reaches 12 degrees. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] - not TempStream[e1.roomNo == roomNo and temp 12] and e2=RegulatorStateChangeStream[action == 'off'] select e1.roomNo as roomNo insert into AlertStream; This Siddhi Application generates an alert if the temperature does not reduce to 12 degrees within 5 minutes of switching on the regulator. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] - not TempStream[e1.roomNo == roomNo and temp 12] for '5 min' select e1.roomNo as roomNo insert into AlertStream; Sequence Sequence is a state machine implementation that allows you to detect the sequence of event occurrences over time. Here all matching events need to arrive consecutively to match the sequence condition, and there cannot be any non-matching events arriving within a matching sequence of events. This can correlate events within a single stream or between multiple streams. Purpose This allows you to detect a specified event sequence over a specified time period. Syntax The syntax for a sequence query is as follows: from (every)? event reference = input stream [ filter condition ], event reference = input stream [ filter condition ], ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description , This represents the immediate next event i.e., when an event that matches the first condition arrives, the event that arrives immediately after it should match the second condition. event reference This allows you to add a reference to the the matching event so that it can be accessed later for further processing. (within time gap )? The within clause is optional. It defines the time duration within which all the matching events should occur. every every is an optional keyword. This defines whether the matching event should be triggered for every event that arrives at the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. Example This query generates an alert if the increase in the temperature between two consecutive temperature events exceeds one degree. from every e1=TempStream, e2=TempStream[e1.temp + 1 temp] select e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Counting Sequence Counting sequences allow you to match multiple events for the same matching condition. The number of events matched per condition can be limited via condition postfixes such as Counting Patterns , or by using the * , + , and ? operators. The matching events can also be retrieved using event indexes, similar to how it is done in Counting Patterns . Syntax Each matching condition in a sequence can contain a collection of events as shown below. from (every)? event reference = input stream [ filter condition ](+|*|?)?, event reference = input stream [ filter condition ](+|*|?)?, ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Postfix symbol Required/Optional Description + Optional This matches one or more events to the given condition. * Optional This matches zero or more events to the given condition. ? Optional This matches zero or one events to the given condition. Example This Siddhi application identifies temperature peeks. define stream TempStream(deviceID long, roomNo int, temp double); from every e1=TempStream, e2=TempStream[e1.temp = temp]+, e3=TempStream[e2[last].temp temp] select e1.temp as initialTemp, e2[last].temp as peakTemp insert into PeekTempStream; Logical Sequence Logical sequences identify logical relationships using and , or and not on consecutively arriving events. Syntax The syntax for a logical sequence is as follows: from (every)? (not)? event reference = input stream [ filter condition ] ((and|or) event reference = input stream [ filter condition ])? (within time gap )?, ... select event reference ([event index])?. attribute name , ... insert into output stream Keywords such as and , or , or not can be used to illustrate the logical relationship, similar to how it is done in Logical Patterns . Example This Siddhi application notifies the state when a regulator event is immediately followed by both temperature and humidity events. define stream TempStream(deviceID long, temp double); define stream HumidStream(deviceID long, humid double); define stream RegulatorStream(deviceID long, isOn bool); from every e1=RegulatorStream, e2=TempStream and e3=HumidStream select e2.temp, e3.humid insert into StateNotificationStream; Output rate limiting Output rate limiting allows queries to output events periodically based on a specified condition. Purpose This allows you to limit the output to avoid overloading the subsequent executions, and to remove unnecessary information. Syntax The syntax of an output rate limiting configuration is as follows: from input stream ... select attribute name , attribute name , ... output rate limiting configuration insert into output stream Siddhi supports three types of output rate limiting configurations as explained in the following table: Rate limiting configuration Syntax Description Based on time output event every time interval This outputs output event every time interval time interval. Based on number of events output event every event interval events This outputs output event for every event interval number of events. Snapshot based output snapshot every time interval This outputs all events in the window (or the last event if no window is defined in the query) for every given time interval time interval. Here the output event specifies the event(s) that should be returned as the output of the query. The possible values are as follows: * first : Only the first event processed by the query during the specified time interval/sliding window is emitted. * last : Only the last event processed by the query during the specified time interval/sliding window is emitted. * all : All the events processed by the query during the specified time interval/sliding window are emitted. When no output event is defined, all is used by default. Examples Returning events based on the number of events Here, events are emitted every time the specified number of events arrive. You can also specify whether to emit only the first event/last event, or all the events out of the events that arrived. In this example, the last temperature per sensor is emitted for every 10 events. from TempStreamselect select temp, deviceID group by deviceID output last every 10 events insert into LowRateTempStream; Returning events based on time Here events are emitted for every predefined time interval. You can also specify whether to emit only the first event, last event, or all events out of the events that arrived during the specified time interval. In this example, emits all temperature events every 10 seconds from TempStreamoutput output every 10 sec insert into LowRateTempStream; Returning a periodic snapshot of events This method works best with windows. When an input stream is connected to a window, snapshot rate limiting emits all the current events that have arrived and do not have corresponding expired events for every predefined time interval. If the input stream is not connected to a window, only the last current event for each predefined time interval is emitted. This query emits a snapshot of the events in a time window of 5 seconds every 1 second. from TempStream#window.time(5 sec) output snapshot every 1 sec insert into SnapshotTempStream; Partition Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. A partition can contain one or more queries and there can be multiple instances where the same queries and streams are replicated for each partition. Each partition is tagged with a partition key. Those partitions only process the events that match the corresponding partition key. Purpose Partitions allow you to process the events groups in isolation so that event processing can be performed using the same set of queries for each group. Partition key generation A partition key can be generated in the following two methods: Partition by value This is created by generating unique values using input stream attributes. Syntax partition with ( expression of stream name , expression of stream name , ... ) begin query query ... end; Example This query calculates the maximum temperature recorded within the last 10 events per deviceID . partition with ( deviceID of TempStream ) begin from TempStream#window.length(10) select roomNo, deviceID, max(temp) as maxTemp insert into DeviceTempStream; end; Partition by range This is created by mapping each partition key to a range condition of the input streams numerical attribute. Syntax partition with ( condition as partition key or condition as partition key or ... of stream name , ... ) begin query query ... end; Example This query calculates the average temperature for the last 10 minutes per office area. partition with ( roomNo = 1030 as 'serverRoom' or roomNo 1030 and roomNo = 330 as 'officeRoom' or roomNo 330 as 'lobby' of TempStream) begin from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp insert into AreaTempStream end; Inner Stream Queries inside a partition block can use inner streams to communicate with each other while preserving partition isolation. Inner streams are denoted by a \"#\" placed before the stream name, and these streams cannot be accessed outside a partition block. Purpose Inner streams allow you to connect queries within the partition block so that the output of a query can be used as an input only by another query within the same partition. Therefore, you do not need to repartition the streams if they are communicating within the partition. Example This partition calculates the average temperature of every 10 events for each sensor, and sends an output to the DeviceTempIncreasingStream stream if the consecutive average temperature values increase by more than 5 degrees. partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 < avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end; Purge Partition Based on the partition key used for the partition, multiple instances of streams and queries will be generated. When an extremely large number of unique partition keys are used there is a possibility of very high instances of streams and queries getting generated and eventually system going out of memory. In order to overcome this, users can define a purge interval to clean partitions that will not be used anymore. Purpose @purge allows you to clean the partition instances that will not be used anymore. Syntax The syntax of partition purge configuration is as follows: @purge(enable='true', interval=' purge interval ', idle.period=' idle period of partition instance ') partition with ( partition key of input stream ) begin from input stream ... select attribute name , attribute name , ... insert into output stream end; Partition purge configuration Description Purge interval The periodic time interval to purge the purgeable partition instances. Idle period of partition instance The period, a particular partition instance (for a given partition key) needs to be idle before it becomes purgeable. Examples Mark partition instances eligible for purging, if there are no events from a particular deviceID for 15 seconds, and periodically purge those partition instances every 1 second. @purge(enable='true', interval='1 sec', idle.period='15 sec') partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end; Table A table is a stored version of an stream or a table of events. Its schema is defined via the table definition that is similar to a stream definition. These events are by default stored in-memory , but Siddhi also provides store extensions to work with data/events stored in various data stores through the table abstraction. Purpose Tables allow Siddhi to work with stored events. By defining a schema for tables Siddhi enables them to be processed by queries using their defined attributes with the streaming data. You can also interactively query the state of the stored events in the table. Syntax The syntax for a new table definition is as follows: define table table name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are configured in a table definition: Parameter Description table name The name of the table defined. ( PascalCase is used for table name as a convention.) attribute name The schema of the table is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . Example The following defines a table named RoomTypeTable with roomNo and type attributes of data types int and string respectively. define table RoomTypeTable ( roomNo int, type string ); Primary Keys Tables can be configured with primary keys to avoid the duplication of data. Primary keys are configured by including the @PrimaryKey( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have only one @PrimaryKey annotation. The number of attributes supported differ based on the table implementations. When more than one attribute is used for the primary key, the uniqueness of the events stored in the table is determined based on the combination of values for those attributes. Examples This query creates an event table with the symbol attribute as the primary key. Therefore each entry in this table must have a unique value for symbol attribute. @PrimaryKey('symbol') define table StockTable (symbol string, price float, volume long); Indexes Indexes allow tables to be searched/modified much faster. Indexes are configured by including the @Index( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have 0-1 @Index annotations. Support for the @Index annotation and the number of attributes supported differ based on the table implementations. When more then one attribute is used for index, each one of them is used to index the table for fast access of the data. Indexes can be configured together with primary keys. Examples This query creates an indexed event table named RoomTypeTable with the roomNo attribute as the index key. @Index('roomNo') define table RoomTypeTable (roomNo int, type string); Store Store is a table that refers to data/events stored in data stores outside of Siddhi such as RDBMS, Cassandra, etc. Store is defined via the @store annotation, and the store schema is defined via a table definition associated with it. Purpose Store allows Siddhi to search, retrieve and manipulate data stored in external data stores through Siddhi queries. Syntax The syntax for a defining store and it's associated table definition is as follows: @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN') define table TableName (attribute1 Type1, attributeN TypeN); Example The following defines a RDBMS data store pointing to a MySQL database with name hotel hosted in loacalhost:3306 having a table RoomTypeTable with columns roomNo of INTEGER and type of VARCHAR(255) mapped to Siddhi data types int and string respectively. @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/hotel\", username=\"siddhi\", password=\"123\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table RoomTypeTable ( roomNo int, type string ); Supported Store Types The following is a list of currently supported store types: RDBMS (MySQL, Oracle, SQL Server, PostgreSQL, DB2, H2) MongoDB Caching in Memory Store tables are persisted in high i/o latency storage. Hence, it is beneficial to maintain a cache of store tables in memory which has low latency. Siddhi supports caching of store tables through @cache annotation. It should be used within @store annotation in a nested fashion as shown below. @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN', @cache(size=10, cache.policy=FIFO)) define table TableName (attribute1 Type1, attributeN TypeN); In the above example we have defined a cache with a maximum size of 10 rows with first-in first-out cache policy. The following table contains the cache parameters. Parameter Mandatory/Optional Default Value Description size Mandatory - maximum number of rows to be cached cache.policy Optional FIFO policy to free up cache when cache miss occurs. There are 3 allowed policies. 1. FIFO - First-In, First-Out 2. LRU - Least Recently Used 3. LFU - Least Frequently Used retention.period Optional - If user specifies this parameter then cache expiry is enabled. For example if this is 5 min, rows older than 5 mins will be removed and in some cases reloaded from store purge.interval optional equal to retention period When cache expiry is enabled, a thread will be created for every purge.interval which will check for expired rows and remove them. The following is an example of caching with expiry. @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN', @cache(size=10, retention.period=5 min, purge.interval=1 min)) define table TableName (attribute1 Type1, attributeN TypeN); The above query will define and create a store table of given type and a cache with a max size of 10. A thread will be created every 1 minute which will check the entire cache table for rows added earlier than 5 minutes and expire them. Cache Behaviour Cache behaviour changes profoundly based on the size of store table relative to maximum cache size defined. Since memory is a limited resource we don't allow cache to grow more than the user specified maximum size. Case 1 \\ When store table is smaller than maximum cache size defined we keep the entire content of store table in memory in cache table. All types of queries are routed to cache and cache results are directly sent out to the user. Every time the expiry thread finds that cache events were loaded earlier than retention period entire cache table will be deleted and reloaded from store. In addition, when siddhi app starts, the entire store table, if it exists, will be loaded into cache. Case 2 \\ When store table is bigger than maximum cache size only the queries satisfying the following 2 conditions are sent to cache. 1. the query contains all the primary keys of the table 2. the query contains only == type of comparison. Only for the above types of queries we can establish if the cache is hit or missed. Subject to these conditions if the cache is hit the results from cache is sent out. If the cache is missed then store is checked. If the above conditions are not met by a query it is directly sent to the store table. In addition, please note that if the store table is pre existing when siddhi app is started and it is bigger than max cache size, cache preloading will take only upto max size and put it in cache. For example if store table has 50 entries when the siddhi app is defined with cache size of 10, only the first 10 rows will be cached. When cache miss occurs we look for the answer in the store table. If there is a result from the store table it is added to cache. One element from cache is removed using the user given cache policy prior to adding. When it comes to cache expiry, since not all rows are loaded at once in this case there may be some expired rows and some unexpired rows at any time. So for every purge interval a thread will be generated which looks for rows that were loaded earlier than retention period and delete only those rows. No reloading is done. Operators on Table (and Store) The following operators can be performed on tables (and stores). Insert This allows events to be inserted into tables. This is similar to inserting events into streams. Warning If the table is defined with primary keys, and if you insert duplicate data, primary key constrain violations can occur. In such cases use the update or insert into operation. Syntax from input stream select attribute name , attribute name , ... insert into table Similar to streams, you need to use the current events , expired events or the all events keyword between insert and into keywords in order to insert only the specific event types. For more information, see Event Type Example This query inserts all the events from the TempStream stream to the TempTable table. from TempStream select * insert into TempTable; Join (Table) This allows a stream to retrieve information from a table in a streaming manner. Note Joins can also be performed with two streams , aggregation or against externally named windows . Syntax from input stream join table on condition select ( input stream | table ). attribute name , ( input stream | table ). attribute name , ... insert into output stream Note A table can only be joint with a stream. Two tables cannot be joint because there must be at least one active entity to trigger the join operation. Example This Siddhi App performs a join to retrieve the room type from RoomTypeTable table based on the room number, so that it can filter the events related to server-room s. define table RoomTypeTable (roomNo int, type string); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream join RoomTypeTable on RoomTypeTable.roomNo == TempStream.roomNo select deviceID, RoomTypeTable.type as roomType, type, temp having roomType == 'server-room' insert into ServerRoomTempStream; Supported join types Table join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the table. The output is generated only if there is a matching event in both the stream and the table. Left outer join The left outer join operation allows you to join a stream on left side with a table on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right table by having null values for the attributes of the right table. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a table on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left table. Delete To delete selected events that are stored in a table. Syntax from input stream select attribute name , attribute name , ... delete table (for event type )? on condition The condition element specifies the basis on which events are selected to be deleted. When specifying the condition, table attributes should be referred to with the table name. To execute delete for specific event types, use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see Event Type Note Table attributes must be always referred to with the table name as follows: table name . attibute name Example In this example, the script deletes a record in the RoomTypeTable table if it has a value for the roomNo attribute that matches the value for the roomNumber attribute of an event in the DeleteStream stream. define table RoomTypeTable (roomNo int, type string); define stream DeleteStream (roomNumber int); from DeleteStream delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; Update This operator updates selected event attributes stored in a table based on a condition. Syntax from input stream select attribute name , attribute name , ... update table (for event type )? set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition The condition element specifies the basis on which events are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. To execute an update for specific event types use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see Event Type . Note Table attributes must be always referred to with the table name as shown below: table name . attibute name . Example This Siddhi application updates the room occupancy in the RoomOccupancyTable table for each room number based on new arrivals and exits from the UpdateStream stream. define table RoomOccupancyTable (roomNo int, people int); define stream UpdateStream (roomNumber int, arrival int, exit int); from UpdateStream select * update RoomOccupancyTable set RoomOccupancyTable.people = RoomOccupancyTable.people + arrival - exit on RoomOccupancyTable.roomNo == roomNumber; Update or Insert This allows you update if the event attributes already exist in the table based on a condition, or else insert the entry as a new attribute. Syntax from input stream select attribute name , attribute name , ... update or insert into table (for event type )? set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which events are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note When the attribute to the right is a table attribute, the operations supported differ based on the database type. To execute update upon specific event types use the current events , expired events or the all events keyword with for as shown in the syntax. To understand more see Event Type . Note Table attributes should be always referred to with the table name as table name . attibute name . Example The following query update for events in the UpdateTable event table that have room numbers that match the same in the UpdateStream stream. When such events are found in the event table, they are updated. When a room number available in the stream is not found in the event table, it is inserted from the stream. define table RoomAssigneeTable (roomNo int, type string, assignee string); define stream RoomAssigneeStream (roomNumber int, type string, assignee string); from RoomAssigneeStream select roomNumber as roomNo, type, assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo; In This allows the stream to check whether the expected value exists in the table as a part of a conditional operation. Syntax from input stream [ condition in table ] select attribute name , attribute name , ... insert into output stream The condition element specifies the basis on which events are selected to be compared. When constructing the condition , the table attribute must be always referred to with the table name as shown below: table . attibute name . Example This Siddhi application filters only room numbers that are listed in the ServerRoomTable table. define table ServerRoomTable (roomNo int); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream[ServerRoomTable.roomNo == roomNo in ServerRoomTable] insert into ServerRoomTempStream; Named Aggregation Named aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition . Purpose Named aggregation allows you to retrieve the aggregate values for different time durations. That is, it allows you to obtain aggregates such as sum , count , avg , min , max , count and distinctCount of stream attributes for durations such as sec , min , hour , etc. This is of considerable importance in many Analytics scenarios because aggregate values are often needed for several time periods. Furthermore, this ensures that the aggregations are not lost due to unexpected system failures because aggregates can be stored in different persistence stores . Syntax @store(type=\" store type \", ...) @purge(enable=\" true or false \",interval= purging interval ,purgeByShardIdEnabled=\" true or false \",@retentionPeriod( granularity = retention period , ...) ) define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; The above syntax includes the following: Item Description @store This annotation is used to refer to the data store where the calculated aggregate results are stored. This annotation is optional. When no annotation is provided, the data is stored in the in-memory store. @purge This annotation is used to configure purging in aggregation granularities. If this annotation is not provided, the default purging mentioned above is applied. If you want to disable automatic data purging, you can use this annotation as follows: '@purge(enable=false) /You should disable data purging if the aggregation query in included in the Siddhi application for read-only purposes. @retentionPeriod This annotation is used to specify the length of time the data needs to be retained when carrying out data purging. If this annotation is not provided, the default retention period is applied. aggregator name This specifies a unique name for the aggregation so that it can be referred when accessing aggregate results. input stream The stream that feeds the aggregation. Note! this stream should be already defined. group by attribute name The group by clause is optional. If it is included in a Siddhi application, aggregate values are calculated per each group by attribute. If it is not used, all the events are aggregated together. by timestamp attribute This clause is optional. This defines the attribute that should be used as the timestamp. If this clause is not used, the event time is used by default. The timestamp could be given as either a string or a long value. If it is a long value, the unix timestamp in milliseconds is expected (e.g. 1496289950000 ). If it is a string value, the supported formats are yyyy - MM - dd HH : mm : ss (if time is in GMT) and yyyy - MM - dd HH : mm : ss Z (if time is not in GMT), here the ISO 8601 UTC offset must be provided for Z . (e.g., +05:30 , -11:00 ). time periods Time periods can be specified as a range where the minimum and the maximum value are separated by three dots, or as comma-separated values. e.g., A range can be specified as sec...year where aggregation is done per second, minute, hour, day, month and year. Comma-separated values can be specified as min, hour. Skipping time durations (e.g., min, day where the hour duration is skipped) when specifying comma-separated values is supported only from v4.1.1 onwards Aggregation's granularity data holders are automatically purged every 15 minutes. When carrying out data purging, the retention period you have specified for each granularity in the named aggregation query is taken into account. The retention period defined for a granularity needs to be greater than or equal to its minimum retention period as specified in the table below. If no valid retention period is defined for a granularity, the default retention period (as specified in the table below) is applied. Granularity Default retention Minimum retention second 120 seconds 120 seconds minute 24 hours 120 minutes hour 30 days 25 hours day 1 year 32 days month All 13 month year All none Note Aggregation is carried out at calendar start times for each granularity with the GMT timezone Note The same aggregation can be defined in multiple Siddhi apps for joining, however, only one siddhi app should carry out the processing (i.e. the aggregation input stream should only feed events to one aggregation definition). Example This Siddhi Application defines an aggregation named TradeAggregation to calculate the average and sum for the price attribute of events arriving at the TradeStream stream. These aggregates are calculated per every time granularity in the second-year range. define stream TradeStream (symbol string, price double, volume long, timestamp long); @purge(enable='true', interval='10 sec',@retentionPeriod(sec='120 sec',min='24 hours',hours='30 days',days='1 year',months='all',years='all')) define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; Distributed Aggregation Distributed Aggregation allows you to partially process aggregations in different shards. This allows Siddhi app in one shard to be responsible only for processing a part of the aggregation. However for this, all aggregations must be based on a common physical database(@store). Syntax @store(type=\" store type \", ...) @PartitionById define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; Following table includes the annotation to be used to enable distributed aggregation, Item Description @PartitionById If the annotation is given, then the distributed aggregation is enabled. Further this can be disabled by using enable element, @PartitionById(enable='false') . Further, following system properties are also available, System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yes false purgeByShardIdEnabled This allows user to enable/disable distributed aggregation purging considering the shardID for all aggregations running in one siddhi manager .(Available from v5.1.28) true/false Yes false Note ShardIds should not be changed after the first configuration in order to keep data consistency. Join (Aggregation) This allows a stream to retrieve calculated aggregate values from the aggregation. Note A join can also be performed with two streams , with a table and a stream, or with a stream against externally named windows . Syntax A join with aggregation is similer to the join with table , but with additional within and per clauses. from input stream join aggrigation on join condition within time range per time granularity select attribute name , attribute name , ... insert into output stream ; Apart from constructs of table join this includes the following. Please note that the 'on' condition is optional : Item Description within time range This allows you to specify the time interval for which the aggregate values need to be retrieved. This can be specified by providing the start and end time separated by a comma as string or long values, or by using the wildcard string specifying the data range. For details refer examples. per time granularity This specifies the time granularity by which the aggregate values must be grouped and returned. e.g., If you specify days , the retrieved aggregate values are grouped for each day within the selected time interval. within and per clauses also accept attribute values from the stream. The timestamp of the aggregations can be accessed through the AGG_TIMESTAMP attribute. Example Following aggregation definition will be used for the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select AGG_TIMESTAMP, symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) define stream StockStream (symbol string, value int); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; This query retrieves hourly aggregations within the day 2014-02-15 . define stream StockStream (symbol string, value int); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within \"2014-02-15 **:**:** +05:30\" per \"hours\" select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; This query retrieves all aggregations per perValue stream attribute within the time period between timestamps 1496200000000 and 1596434876000 . define stream StockStream (symbol string, value int, perValue string); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within 1496200000000L, 1596434876000L per S.perValue select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; Supported join types Aggregation join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the aggregation. The output is generated only if there is a matching event in the stream and the aggregation. Left outer join The left outer join operation allows you to join a stream on left side with a aggregation on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right aggregation by having null values for the attributes of the right aggregation. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a aggregation on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left aggregation. Named Window A named window is a window that can be shared across multiple queries. Events can be inserted to a named window from one or more queries and it can produce output events based on the named window type. Syntax The syntax for a named window is as follows: define window window name ( attribute name attribute type , attribute name attribute type , ... ) window type ( parameter , parameter , \u2026) event type ; The following parameters are configured in a table definition: Parameter Description window name The name of the window defined. ( PascalCase is used for window names as a convention.) attribute name The schema of the window is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . window type ( parameter , ...) The window type associated with the window and its parameters. output event type This is optional. Keywords such as current events , expired events and all events (the default) can be used to specify when the window output should be exposed. For more information, see Event Type . Examples Returning all output when events arrive and when events expire from the window. In this query, the event type is not specified. Therefore, it returns both current and expired events as the output. define window SensorWindow (name string, value float, roomNo int, deviceID string) timeBatch(1 second); Returning an output only when events expire from the window. In this query, the event type of the window is expired events . Therefore, it only returns the events that have expired from the window as the output. define window SensorWindow (name string, value float, roomNo int, deviceID string) timeBatch(1 second) output expired events; Operators on Named Windows The following operators can be performed on named windows. Insert This allows events to be inserted into windows. This is similar to inserting events into streams. Syntax from input stream select attribute name , attribute name , ... insert into window To insert only events of a specific event type, add the current events , expired events or the all events keyword between insert and into keywords (similar to how it is done for streams). For more information, see Event Type . Example This query inserts all events from the TempStream stream to the OneMinTempWindow window. define stream TempStream(tempId string, temp double); define window OneMinTempWindow(tempId string, temp double) time(1 min); from TempStream select * insert into OneMinTempWindow; Join (Window) To allow a stream to retrieve information from a window based on a condition. Note A join can also be performed with two streams , aggregation or with tables tables . Syntax from input stream join window on condition select ( input stream | window ). attribute name , ( input stream | window ). attribute name , ... insert into output stream Example This Siddhi Application performs a join count the number of temperature events having more then 40 degrees within the last 2 minutes. define window TwoMinTempWindow (roomNo int, temp double) time(2 min); define stream CheckStream (requestId string); from CheckStream as C join TwoMinTempWindow as T on T.temp 40 select requestId, count(T.temp) as count insert into HighTempCountStream; Supported join types Window join supports following operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join two windows or a stream with a window. The output is generated only if there is a matching event in both stream/window. Left outer join The left outer join operation allows you to join two windows or a stream with a window to be merged based on a condition. Here, it returns all the events of left stream/window even if there are no matching events in the right stream/window by having null values for the attributes of the right stream/window. Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join two windows or a stream with a window. It returns all the events of the right stream/window even if there are no matching events in the left stream/window. Full outer join The full outer join combines the results of left outer join and right outer join . full outer join is used as the keyword to join two windows or a stream with a window. Here, output event are generated for each incoming event even if there are no matching events in the other stream/window. From A window can be an input to a query, similar to streams. Note !!! When window is used as an input to a query, another window cannot be applied on top of this. Syntax from window select attribute name , attribute name , ... insert into output stream Example This Siddhi Application calculates the maximum temperature within the last 5 minutes. define window FiveMinTempWindow (roomNo int, temp double) time(5 min); from FiveMinTempWindow select max(temp) as maxValue, roomNo insert into MaxSensorReadingStream; Trigger Triggers allow events to be periodically generated. Trigger definition can be used to define a trigger. A trigger also works like a stream with a predefined schema. Purpose For some use cases the system should be able to periodically generate events based on a specified time interval to perform some periodic executions. A trigger can be performed for a 'start' operation, for a given time interval , or for a given ' cron expression ' . Syntax The syntax for a trigger definition is as follows. define trigger trigger name at ('start'| every time interval | ' cron expression '); Similar to streams, triggers can be used as inputs. They adhere to the following stream definition and produce the triggered_time attribute of the long type. define stream trigger name (triggered_time long); The following types of triggeres are currently supported: Trigger type Description 'start' An event is triggered when Siddhi is started. every time interval An event is triggered periodically at the given time interval. ' cron expression ' An event is triggered periodically based on the given cron expression. For configuration details, see quartz-scheduler . Examples Triggering events regularly at specific time intervals The following query triggers events every 5 minutes. define trigger FiveMinTriggerStream at every 5 min; Triggering events at a specific time on specified days The following query triggers an event at 10.15 AM on every weekdays. define trigger FiveMinTriggerStream at '0 15 10 ? * MON-FRI'; Script Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Function definitions can be used to define these scripts. Function parameters are passed into the function logic as Object[] and with the name data . Purpose Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Syntax The syntax for a Script definition is as follows. define function function name [ language name ] return return type { operation of the function }; The following parameters are configured when defining a script. Parameter Description function name The name of the function ( camelCase is used for the function name) as a convention. language name The name of the programming language used to define the script, such as javascript , r and scala . return type The attribute type of the function\u2019s return. This can be int , long , float , double , string , bool or object . Here the function implementer should be responsible for returning the output attribute on the defined return type for proper functionality. operation of the function Here, the execution logic of the function is added. This logic should be written in the language specified under the language name , and it should return the output in the data type specified via the return type parameter. Examples This query performs concatenation using JavaScript, and returns the output as a string. define function concatFn[javascript] return string { var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var responce = str1 + str2 + str3; return responce; }; define stream TempStream(deviceID long, roomNo int, temp double); from TempStream select concatFn(roomNo,'-',deviceID) as id, temp insert into DeviceTempStream; Store Query Siddhi store queries are a set of on-demand queries that can be used to perform operations on Siddhi tables, windows, and aggregators. Purpose Store queries allow you to execute the following operations on Siddhi tables, windows, and aggregators without the intervention of streams. Queries supported for tables: SELECT INSERT DELETE UPDATE UPDATE OR INSERT Queries supported for windows and aggregators: SELECT This is be done by submitting the store query to the Siddhi application runtime using its query() method. In order to execute store queries, the Siddhi application of the Siddhi application runtime you are using, should have a store defined, which contains the table that needs to be queried. Example If you need to query the table named RoomTypeTable the it should have been defined in the Siddhi application. In order to execute a store query on RoomTypeTable , you need to submit the store query using query() method of SiddhiAppRuntime instance as below. siddhiAppRuntime.query( store query ); (Table/Window) Select The SELECT store query retrieves records from the specified table or window, based on the given condition. Syntax from table/window on condition ? select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example This query retrieves room numbers and types of the rooms starting from room no 10. from roomTypeTable on roomNo = 10; select roomNo, type (Aggregation) Select The SELECT store query retrieves records from the specified aggregation, based on the given condition, time range, and granularity. Syntax from aggregation on condition ? within time range per time granularity select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example Following aggregation definition will be used for the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) from TradeAggregation within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select symbol, total, avgPrice ; This query retrieves hourly aggregations of \"FB\" symbol within the day 2014-02-15 . from TradeAggregation on symbol == \"FB\" within \"2014-02-15 **:**:** +05:30\" per \"hours\" select symbol, total, avgPrice; Insert This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax select attribute name , attribute name , ... insert into table ; Example This store query inserts a new record to the table RoomOccupancyTable , with the specified attribute values. select 10 as roomNo, 2 as people insert into RoomOccupancyTable Delete The DELETE store query deletes selected records from a specified table. Syntax select ? delete table on conditional expresssion The condition element specifies the basis on which records are selected to be deleted. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example In this example, query deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection which has 10 as the actual value. select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; delete RoomTypeTable on RoomTypeTable.roomNo == 10; Update The UPDATE store query updates selected attributes stored in a specific table, based on a given condition. Syntax select attribute name , attribute name , ...? update table set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition The condition element specifies the basis on which records are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query updates the room occupancy by increasing the value of people by 1, in the RoomOccupancyTable table for each room number greater than 10. select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber; update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + 1 on RoomTypeTable.roomNo == 10; Update or Insert This allows you to update selected attributes if a record that meets the given conditions already exists in the specified table. If a matching record does not exist, the entry is inserted as a new record. Syntax select attribute name , attribute name , ... update or insert into table set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which records are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query tries to update the records in the RoomAssigneeTable table that have room numbers that match the same in the selection. If such records are not found, it inserts a new record based on the values provided in the selection. select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo; Extensions Siddhi supports an extension architecture to enhance its functionality by incorporating other libraries in a seamless manner. Purpose Extensions are supported because, Siddhi core cannot have all the functionality that's needed for all the use cases, mostly use cases require different type of functionality, and for some cases there can be gaps and you need to write the functionality by yourself. All extensions have a namespace. This is used to identify the relevant extensions together, and to let you specifically call the extension. Syntax Extensions follow the following syntax; namespace : function name ( parameter , parameter , ... ) The following parameters are configured when referring a script function. Parameter Description namespace Allows Siddhi to identify the extension without conflict function name The name of the function referred. parameter The function input parameter for function execution. Extension Types Siddhi supports following extension types: Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute. This can be used to manipulate existing event attributes to generate new attributes like any Function operation. This is implemented by extending io.siddhi.core.executor.function.FunctionExecutor . Example : math:sin(x) Here, the sin function of math extension returns the sin value for the x parameter. Aggregate Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute with aggregated results. This can be used in conjunction with a window in order to find the aggregated results based on the given window like any Aggregate Function operation. This is implemented by extending io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor . Example : custom:std(x) Here, the std aggregate function of custom extension returns the standard deviation of the x value based on its assigned window query. Window This allows events to be collected, generated, dropped and expired anytime without altering the event format based on the given input parameters, similar to any other Window operator. This is implemented by extending io.siddhi.core.query.processor.stream.window.WindowProcessor . Example : custom:unique(key) Here, the unique window of the custom extension retains one event for each unique key parameter. Stream Function This allows events to be generated or dropped only during event arrival and altered by adding one or more attributes to it. This is implemented by extending io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor . Example : custom:pol2cart(theta,rho) Here, the pol2cart function of the custom extension returns all the events by calculating the cartesian coordinates x y and adding them as new attributes to the events. Stream Processor This allows events to be collected, generated, dropped and expired anytime by altering the event format by adding one or more attributes to it based on the given input parameters. Implemented by extending io.siddhi.core.query.processor.stream.StreamProcessor . Example : custom:perMinResults( parameter , parameter , ...) Here, the perMinResults function of the custom extension returns all events by adding one or more attributes to the events based on the conversion logic. Altered events are output every minute regardless of event arrivals. Sink Sinks provide a way to publish Siddhi events to external systems in the preferred data format. Sinks publish events from the streams via multiple transports to external endpoints in various data formats. Implemented by extending io.siddhi.core.stream.output.sink.Sink . Example : @sink(type='sink_type', static_option_key1='static_option_value1') To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as above Source Source allows Siddhi to consume events from external systems , and map the events to adhere to the associated stream. Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. Implemented by extending io.siddhi.core.stream.input.source.Source . Example : @source(type='source_type', static.option.key1='static_option_value1') To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as above Store You can use Store extension type to work with data/events stored in various data stores through the table abstraction . You can find more information about these extension types under the heading 'Extension types' in this document. Implemented by extending io.siddhi.core.table.record.AbstractRecordTable . Script Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Implemented by extending io.siddhi.core.function.Script . Source Mapper Each @source configuration has a mapping denoted by the @map annotation that converts the incoming messages format to Siddhi events .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SourceMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Sink Mapper Each @sink configuration has a mapping denoted by the @map annotation that converts the outgoing Siddhi events to configured messages format .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SinkMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Example A window extension created with namespace foo and function name unique can be referred as follows: from StockExchangeStream[price = 20]#window.foo:unique(symbol) select symbol, price insert into StockQuote Available Extensions Siddhi currently has several pre written extensions that are available here We value your contribution on improving Siddhi and its extensions further. Writing Custom Extensions Custom extensions can be written in order to cater use case specific logic that are not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension types and the related maven archetypes are given below. You can use these archetypes to generate Maven projects for each extension type. Follow the procedure for the required archetype, based on your project: Note When using the generated archetype please make sure you complete the @Extension annotation with proper values. This annotation will be used to identify and document the extension, hence your extension will not work without @Extension annotation. siddhi-execution Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Extension Types . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DgroupId=io.siddhi.extension.execution -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfFunction Name of the custom function to be created Y - _nameSpaceOfFunction Namespace of the function, used to grouped similar custom functions Y - groupIdPostfix Namespace of the function is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-execution- classNameOfAggregateFunction Class name of the Aggregate Function N $ classNameOfFunction Class name of the Function N $ classNameOfStreamFunction Class name of the Stream Function N $ classNameOfStreamProcessor Class name of the Stream Processor N $ classNameOfWindow Class name of the Window N $ To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-io Siddhi-io provides following extension types: Sink Source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: * The Source extension type gets inputs to your Siddhi application. * The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DgroupId=io.siddhi.extension.io -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _IOType Type of IO for which Siddhi-io extension is written Y - groupIdPostfix Type of the IO is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-io- classNameOfSink Class name of the Sink N classNameOfSource Class name of the Source N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-map Siddhi-map provides following extension types, Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints (such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DgroupId=io.siddhi.extension.map -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _mapType Type of Mapper for which Siddhi-map extension is written Y - groupIdPostfix Type of the Map is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-map- classNameOfSinkMapper Class name of the Sink Mapper N classNameOfSourceMapper Class name of the Source Mapper N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-script Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Extension Types . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DgroupId=io.siddhi.extension.script -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfScript Name of Custom Script for which Siddhi-script extension is written Y - groupIdPostfix Name of the Script is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-script- classNameOfScript Class name of the Script N Eval To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-store Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Extension Types . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DgroupId=io.siddhi.extension.store -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _storeType Type of Store for which Siddhi-store extension is written Y - groupIdPostfix Type of the Store is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-store- className Class name of the Store N To confirm that all property values are correct, type Y in the console. If not, press N . Configuring and Monitoring Siddhi Applications Multi-threading and Asynchronous Processing When @Async annotation is added to the Streams it enable the Streams to introduce asynchronous and multi-threading behaviour. @Async(buffer.size='256', workers='2', batch.size.max='5') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following elements are configured with this annotation. Annotation Description Default Value buffer.size The size of the event buffer that will be used to handover the execution to other threads. - workers Number of worker threads that will be be used to process the buffered events. 1 batch.size.max The maximum number of events that will be processed together by a worker thread at a given time. buffer.size Statistics Use @app:statistics app level annotation to evaluate the performance of an application, you can enable the statistics of a Siddhi application to be published. This is done via the @app:statistics annotation that can be added to a Siddhi application as shown in the following example. @app:statistics(reporter = 'console') The following elements are configured with this annotation. Annotation Description Default Value reporter The interface in which statistics for the Siddhi application are published. Possible values are as follows: console jmx console interval The time interval (in seconds) at which the statistics for the Siddhi application are reported. 60 include If this parameter is added, only the types of metrics you specify are included in the reporting. The required metric types can be specified as a comma-separated list. It is also possible to use wild cards All ( . ) The metrics are reported in the following format. io.siddhi.SiddhiApps. SiddhiAppName .Siddhi. Component Type . Component Name . Metrics name The following table lists the types of metrics supported for different Siddhi application component types. Component Type Metrics Type Stream Throughput The size of the buffer if parallel processing is enabled via the @async annotation. Trigger Throughput (Trigger and Stream) Source Throughput Sink Throughput Mapper Latency Input/output throughput Table Memory Throughput (For all operations) Throughput (For all operations) Query Memory Latency Window Throughput (For all operations) Latency (For all operation) Partition Throughput (For all operations) Latency (For all operation) e.g., the following is a Siddhi application that includes the @app annotation to report performance statistics. @App:name('TestMetrics') @App:Statistics(reporter = 'console') define stream TestStream (message string); @info(name='logQuery') from TestSream#log(\"Message:\") insert into TempSream; Statistics are reported for this Siddhi application as shown in the extract below. Click to view the extract 11/26/17 8:01:20 PM ============================================================ -- Gauges ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.memory value = 5760 io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.size value = 0 -- Meters ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Sources.TestStream.http.throughput count = 0 mean rate = 0.00 events/second 1-minute rate = 0.00 events/second 5-minute rate = 0.00 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TempSream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second -- Timers ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.latency count = 2 mean rate = 0.11 calls/second 1-minute rate = 0.34 calls/second 5-minute rate = 0.39 calls/second 15-minute rate = 0.40 calls/second min = 0.61 milliseconds max = 1.08 milliseconds mean = 0.84 milliseconds stddev = 0.23 milliseconds median = 0.61 milliseconds 75% < = 1.08 milliseconds 95% < = 1.08 milliseconds 98% < = 1.08 milliseconds 99% < = 1.08 milliseconds 99.9% < = 1.08 milliseconds Event Playback When @app:playback annotation is added to the app, the timestamp of the event (specified via an attribute) is treated as the current time. This results in events being processed faster. The following elements are configured with this annotation. Annotation Description idle.time If no events are received during a time interval specified (in milliseconds) via this element, the Siddhi system time is incremented by a number of seconds specified via the increment element. increment The number of seconds by which the Siddhi system time must be incremented if no events are received during the time interval specified via the idle.time element. e.g., In the following example, the Siddhi system time is incremented by two seconds if no events arrive for a time interval of 100 milliseconds. @app:playback(idle.time = '100 millisecond', increment = '2 sec')","title":"Query Guide"},{"location":"docs/query-guide/#siddhi-52-streaming-sql-guide","text":"","title":"Siddhi 5.2 Streaming SQL Guide"},{"location":"docs/query-guide/#introduction","text":"Siddhi Streaming SQL is designed to process streams of events. It can be used to implement streaming data integration, streaming analytics, rule based and adaptive decision making use cases. It is an evolution of Complex Event Processing (CEP) and Stream Processing systems, hence it can also be used to process stateful computations, detecting of complex event patterns, and sending notifications in real-time. Siddhi Streaming SQL uses SQL like syntax, and annotations to consume events from diverse event sources with various data formats, process then using stateful and stateless operators and send outputs to multiple endpoints according to their accepted event formats. It also supports exposing rule based and adaptive decision making as service endpoints such that external programs and systems can synchronously get decision support form Siddhi. The following sections explains how to write processing logic using Siddhi Streaming SQL.","title":"Introduction"},{"location":"docs/query-guide/#siddhi-application","text":"The processing logic for your program can be written using the Streaming SQL and put together as a single file with .siddhi extension. This file is called as the Siddhi Application or the SiddhiApp . SiddhiApps are named by adding @app:name(' name ') annotation on the top of the SiddhiApp file. When the annotation is not added Siddhi assigns a random UUID as the name of the SiddhiApp. Purpose SiddhiApp provides an isolated execution environment for your processing logic that allows you to deploy and execute processing logic independent of other SiddhiApp in the system. Therefore it's always recommended to have a processing logic related to single use case in a single SiddhiApp. This will help you to group processing logic and easily manage addition and removal of various use cases. The following diagram depicts some of the key Siddhi Streaming SQL elements of Siddhi Application and how event flows through the elements. Below table provides brief description of a few key elements in the Siddhi Streaming SQL Language. Elements Description Stream A logical series of events ordered in time with a uniquely identifiable name, and a defined set of typed attributes defining its schema. Event An event is a single event object associated with a stream. All events of a stream contains a timestamp and an identical set of typed attributes based on the schema of the stream they belong to. Table A structured representation of data stored with a defined schema. Stored data can be backed by In-Memory , or external data stores such as RDBMS , MongoDB , etc. The tables can be accessed and manipulated at runtime. Named Window A structured representation of data stored with a defined schema and eviction policy. Window data is stored In-Memory and automatically cleared by the named window constrain. Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Named Aggregation A structured representation of data that's incrementally aggregated and stored with a defined schema and aggregation granularity such as seconds, minutes, hours, etc. Aggregation data is stored both In-Memory and in external data stores such as RDBMS . Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Query A logical construct that processes events in streaming manner by by consuming data from one or more streams, tables, windows and aggregations, and publishes output events into a stream, table or a window. Source A construct that consumes data from external sources (such as TCP , Kafka , HTTP , etc) with various event formats such as XML , JSON , binary , etc, convert then to Siddhi events, and passes into streams for processing. Sink A construct that consumes events arriving at a stream, maps them to a predefined data format (such as XML , JSON , binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Input Handler A mechanism to programmatically inject events into streams. Stream/Query Callback A mechanism to programmatically consume output events from streams or queries. Partition A logical container that isolates the processing of queries based on the partition keys derived from the events. Inner Stream A positionable stream that connects portioned queries with each other within the partition. Grammar SiddhiApp is a collection of Siddhi Streaming SQL elements composed together as a script. Here each Siddhi element must be separated by a semicolon ; . Hight level syntax of SiddhiApp is as follows. siddhi app : app annotation * ( stream definition | table definition | ... ) + ( query | partition ) + ; Example Siddhi Application with name Temperature-Analytics defined with a stream named TempStream and a query named 5minAvgQuery . @app:name('Temperature-Analytics') define stream TempStream (deviceID long, roomNo int, temp double); @name('5minAvgQuery') from TempStream#window.time(5 min) select roomNo, avg(temp) as avgTemp group by roomNo insert into OutputStream;","title":"Siddhi Application"},{"location":"docs/query-guide/#stream","text":"A stream is a logical series of events ordered in time. Its schema is defined via the stream definition . A stream definition contains the stream name and a set of attributes with specific types and uniquely identifiable names within the stream. All events associated to the stream will have the same schema (i.e., have the same attributes in the same order). Purpose Stream groups common types of events together with a schema. This helps in various ways such as, processing all events together in queries and performing data format transformations together when they are consumed and published via sources and sinks. Syntax The syntax for defining a new stream is as follows. define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure a stream definition. Parameter Description stream name The name of the stream created. (It is recommended to define a stream name in PascalCase .) attribute name Uniquely identifiable name of the stream attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer stream and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . To make the stream process events in multi-threading and asynchronous way use the @Async annotation as shown in Multi-threading and Asynchronous Processing configuration section. Example define stream TempStream (deviceID long, roomNo int, temp double); The above creates a stream with name TempStream having the following attributes. deviceID of type long roomNo of type int temp of type double","title":"Stream"},{"location":"docs/query-guide/#source","text":"Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. A source configuration allows to define a mapping in order to convert each incoming event from its native data format to a Siddhi event. When customizations to such mappings are not provided, Siddhi assumes that the arriving event adheres to the predefined format based on the stream definition and the configured message mapping type. Purpose Source provides a way to consume events from external systems and convert them to be processed by the associated stream. Syntax To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as follows: @source(type=' source type ', static.key =' value ', static.key =' value ', @map(type=' map type ', static.key =' value ', static.key =' value ', @attributes( attribute1 =' attribute mapping ', attributeN =' attribute mapping ') ) ) define stream stream name ( attribute1 type , attributeN type ); This syntax includes the following annotations. Source The type parameter of @source annotation defines the source type that receives events. The other parameters of @source annotation depends upon the selected source type, and here some of its parameters can be optional. For detailed information about the supported parameters see the documentation of the relevant source. The following is the list of source types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to consume events from other SiddhiApps running on the same JVM. HTTP Expose an HTTP service to consume messages. Kafka Subscribe to Kafka topic to consume events. TCP Expose a TCP service to consume messages. Email Consume emails via POP3 and IMAP protocols. JMS Subscribe to JMS topic or queue to consume events. File Reads files by tailing or as a whole to extract events out of them. CDC Perform change data capture on databases. Prometheus Consume data from Prometheus agent. In-memory is the only source inbuilt in Siddhi, and all other source types are implemented as extensions.","title":"Source"},{"location":"docs/query-guide/#source-mapper","text":"Each @source configuration can have a mapping denoted by the @map annotation that defines how to convert the incoming event format to Siddhi events. The type parameter of the @map defines the map type to be used in converting the incoming events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional. For detailed information about the parameters see the documentation of the relevant mapper. Map Attributes @attributes is an optional annotation used with @map to define custom mapping. When @attributes is not provided, each mapper assumes that the incoming events adheres to its own default message format and attempt to convert the events from that format. By adding the @attributes annotation, users can selectively extract data from the incoming message and assign them to the attributes. There are two ways to configure @attributes . Define attribute names as keys, and mapping configurations as values: @attributes( attribute1 =' mapping ', attributeN =' mapping ') Define the mapping configurations in the same order as the attributes defined in stream definition: @attributes( ' mapping for attribute1 ', ' mapping for attributeN ') Supported Source Mapping Types The following is the list of source mapping types supported by Siddhi: Source mapping type Description PassThrough Omits data conversion on Siddhi events. JSON Converts JSON messages to Siddhi events. XML Converts XML messages to Siddhi events. TEXT Converts plain text messages to Siddhi events. Avro Converts Avro events to Siddhi events. Binary Converts Siddhi specific binary events to Siddhi events. Key Value Converts key-value HashMaps to Siddhi events. CSV Converts CSV like delimiter separated events to Siddhi events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the consumed Siddhi events directly to the streams without any data conversion. PassThrough is the only source mapper inbuilt in Siddhi, and all other source mappers are implemented as extensions. Example 1 Receive JSON messages by exposing an HTTP service, and direct them to InputStream stream for processing. Here the HTTP service will be secured with basic authentication, receives events on all network interfaces on port 8080 and context /foo . The service expects the JSON messages to be on the default data format that's supported by the JSON mapper as follows. { \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } The configuration of the HTTP source and JSON source mapper to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json')) define stream InputStream (name string, age int, country string); Example 2 Receive JSON messages by exposing an HTTP service, and direct them to StockStream stream for processing. Here the incoming JSON , as given bellow, do not adhere to the default data format that's supported by the JSON mapper. { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"FB\" }, \"price\":55.6 } } } The configuration of the HTTP source and the custom JSON source mapping to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); The same can also be configured by omitting the attribute names as bellow. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(\"stock.company.symbol\", \"stock.price\", \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long);","title":"Source Mapper"},{"location":"docs/query-guide/#sink","text":"Sinks consumes events from streams and publish them via multiple transports to external endpoints in various data formats. A sink configuration allows users to define a mapping to convert the Siddhi events in to the required output data format (such as JSON , TEXT , XML , etc.) and publish the events to the configured endpoints. When customizations to such mappings are not provided, Siddhi converts events to the predefined event format based on the stream definition and the configured message mapper type before publishing the events. Purpose Sink provides a way to publish Siddhi events of a stream to external systems by converting events to their supported format. Syntax To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as follows: @sink(type=' sink type ', static.key =' value ', dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) ) define stream stream name ( attribute1 type , attributeN type ); Dynamic Properties The sink and sink mapper properties that are categorized as dynamic have the ability to absorb attribute values dynamically from the Siddhi events of their associated streams. This can be configured by enclosing the relevant attribute names in double curly braces as {{...}} , and using it within the property values. Some valid dynamic properties values are: '{{attribute1}}' 'This is {{attribute1}}' {{attribute1}} {{attributeN}} Here the attribute names in the double curly braces will be replaced with the values from the events before they are published. This syntax includes the following annotations. Sink The type parameter of the @sink annotation defines the sink type that publishes the events. The other parameters of the @sink annotation depends upon the selected sink type, and here some of its parameters can be optional and/or dynamic. For detailed information about the supported parameters see documentation of the relevant sink. The following is a list of sink types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to publish events to other SiddhiApps running on the same JVM. Log Logs the events appearing on the streams. HTTP Publish events to an HTTP endpoint. Kafka Publish events to Kafka topic. TCP Publish events to a TCP service. Email Send emails via SMTP protocols. JMS Publish events to JMS topics or queues. File Writes events to files. Prometheus Expose data through Prometheus agent.","title":"Sink"},{"location":"docs/query-guide/#distributed-sink","text":"Distributed Sinks publish events from a defined stream to multiple endpoints using load balancing or partitioning strategies. Any sink can be used as a distributed sink. A distributed sink configuration allows users to define a common mapping to convert and send the Siddhi events for all its destination endpoints. Purpose Distributed sink provides a way to publish Siddhi events to multiple endpoints in the configured event format. Syntax To configure distributed sink add the sink configuration to a stream definition by adding the @sink annotation and add the configuration parameters that are common of all the destination endpoints inside it, along with the common parameters also add the @distribution annotation specifying the distribution strategy (i.e. roundRobin or partitioned ) and @destination annotations providing each endpoint specific configurations. The distributed sink syntax is as follows: RoundRobin Distributed Sink Publishes events to defined destinations in a round robin manner. @sink(type=' sink type ', common.static.key =' value ', common.dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) @distribution(strategy='roundRobin', @destination( destination.specific.key =' value '), @destination( destination.specific.key =' value '))) ) define stream stream name ( attribute1 type , attributeN type ); Partitioned Distributed Sink Publishes events to defined destinations by partitioning them based on the partitioning key. @sink(type=' sink type ', common.static.key =' value ', common.dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) @distribution(strategy='partitioned', partitionKey=' partition key ', @destination( destination.specific.key =' value '), @destination( destination.specific.key =' value '))) ) define stream stream name ( attribute1 type , attributeN type );","title":"Distributed Sink"},{"location":"docs/query-guide/#sink-mapper","text":"Each @sink configuration can have a mapping denoted by the @map annotation that defines how to convert Siddhi events to outgoing messages with the defined format. The type parameter of the @map defines the map type to be used in converting the outgoing events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional and/or dynamic. For detailed information about the parameters see the documentation of the relevant mapper. Map Payload @payload is an optional annotation used with @map to define custom mapping. When the @payload annotation is not provided, each mapper maps the outgoing events to its own default event format. The @payload annotation allow users to configure mappers to produce the output payload of their choice, and by using dynamic properties within the payload they can selectively extract and add data from the published Siddhi events. There are two ways you to configure @payload annotation. Some mappers such as XML , JSON , and Test only accept one output payload: @payload( 'This is a test message from {{user}}.') Some mappers such key-value accept series of mapping values: @payload( key1='mapping_1', 'key2'='user : {{user}}') Here, the keys of payload mapping can be defined using the dot notation as a.b.c , or using any constant string value as '$abc' . Supported Sink Mapping Types The following is a list of sink mapping types supported by Siddhi: Sink mapping type Description PassThrough Omits data conversion on outgoing Siddhi events. JSON Converts Siddhi events to JSON messages. XML Converts Siddhi events to XML messages. TEXT Converts Siddhi events to plain text messages. Avro Converts Siddhi events to Avro Events. Binary Converts Siddhi events to Siddhi specific binary events. Key Value Converts Siddhi events to key-value HashMaps. CSV Converts Siddhi events to CSV like delimiter separated events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the outgoing Siddhi events directly to the sinks without any data conversion. PassThrough is the only sink mapper inbuilt in Siddhi, and all other sink mappers are implemented as extensions. Example 1 Publishes OutputStream events by converting them to JSON messages with the default format, and by sending to an HTTP endpoint http://localhost:8005/endpoint1 , using POST method, Accept header, and basic authentication having admin is both username and password. The configuration of the HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/endpoint', method='POST', headers='Accept-Date:20/02/2017', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json')) define stream OutputStream (name string, age int, country string); This will publish a JSON message on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } } Example 2 Publishes StockStream events by converting them to user defined JSON messages, and by sending to an HTTP endpoint http://localhost:8005/stocks . The configuration of the HTTP sink and custom JSON sink mapping to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/stocks', @map(type='json', validate.json='true', enclosing.element='$.Portfolio', @payload(\"\"\"{\"StockData\":{ \"Symbol\":\"{{symbol}}\", \"Price\":{{price}} }}\"\"\"))) define stream StockStream (symbol string, price float, volume long); This will publish a single event as the JSON message on the following format: { \"Portfolio\":{ \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } } } This can also publish multiple events together as a JSON message on the following format: { \"Portfolio\":[ { \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } }, { \"StockData\":{ \"Symbol\":\"FB\", \"Price\":57.0 } } ] } Example 3 Publishes events from the OutputStream stream to multiple the HTTP endpoints using a partitioning strategy. Here the events are sent to either http://localhost:8005/endpoint1 or http://localhost:8006/endpoint2 based on the partitioning key country . It uses default JSON mapping, POST method, and used admin as both the username and the password when publishing to both the endpoints. The configuration of the distributed HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', method='POST', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json'), @distribution(strategy='partitioned', partitionKey='country', @destination(publisher.url='http://localhost:8005/endpoint1'), @destination(publisher.url='http://localhost:8006/endpoint2'))) define stream OutputStream (name string, age int, country string); This will partition the outgoing events and publish all events with the same country attribute value to the same endpoint. The JSON message published will be on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } }","title":"Sink Mapper"},{"location":"docs/query-guide/#error-handling","text":"Errors in Siddhi can be handled at the Streams and in Sinks. Error Handling at Stream When errors are thrown by Siddhi elements subscribed to the stream, the error gets propagated up to the stream that delivered the event to those Siddhi elements. By default the error is logged and dropped at the stream, but this behavior can be altered by by adding @OnError annotation to the corresponding stream definition. @OnError annotation can help users to capture the error and the associated event, and handle them gracefully by sending them to a fault stream. The @OnError annotation and the required action to be specified as bellow. @OnError(action='on error action') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The action parameter of the @OnError annotation defines the action to be executed during failure scenarios. The following actions can be specified to @OnError annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when @OnError annotation is not defined. STREAM : Creates a fault stream and redirects the event and the error to it. The created fault stream will have all the attributes defined in the base stream to capture the error causing event, and in addition it also contains _error attribute of type object to containing the error information. The fault stream can be referred by adding ! in front of the base stream name as ! stream name . Example Handle errors in TempStream by redirecting the errors to a fault stream. The configuration of TempStream stream and @OnError annotation is as follows. @OnError(action='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); Siddhi will infer and automatically defines the fault stream of TempStream as given bellow. define stream !TempStream (deviceID long, roomNo int, temp double, _error object); The SiddhiApp extending the above the use-case by adding failure generation and error handling with the use of queries is as follows. Note: Details on writing processing logics via queries will be explained in later sections. -- Define fault stream to handle error occurred at TempStream subscribers @OnError(action='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); -- Error generation through a custom function `createError()` @name('error-generation') from TempStream#custom:createError() insert into IgnoreStream1; -- Handling error by simply logging the event and error. @name('handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream2; Error Handling at Sink There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. The on.error parameter of the @sink annotation can be specified as bellow. @sink(type=' sink type ', on.error=' on error action ', key =' value ', ...) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following actions can be specified to on.error parameter of @sink annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when on.error parameter is not defined on the @sink annotation. WAIT : Publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publishes to it. STREAM : Pushes the failed events with the corresponding error to the associated fault stream the sink belongs to. Example 1 Introduce back pressure on the threads who bring events via TempStream when the system cannot connect to Kafka. The configuration of TempStream stream and @sink Kafka annotation with on.error property is as follows. @sink(type='kafka', on.error='WAIT', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); Example 2 Send events to the fault stream of TempStream when the system cannot connect to Kafka. The configuration of TempStream stream with associated fault stream, @sink Kafka annotation with on.error property and a queries to handle the error is as follows. Note: Details on writing processing logics via queries will be explained in later sections. @OnError(action='STREAM') @sink(type='kafka', on.error='STREAM', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); -- Handling error by simply logging the event and error. @name('handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream;","title":"Error Handling"},{"location":"docs/query-guide/#query","text":"Query defines the processing logic in Siddhi. It consumes events from one or more streams, named-windows , tables , and/or named-aggregations , process the events in a streaming manner, and generate output events into a stream , named-window , or table . Purpose A query provides a way to process the events in the order they arrive and produce output using both stateful and stateless complex event processing and stream processing operations. Syntax The high level query syntax for defining processing logics is as follows: @name(' query name ') from input projection output action The following parameters are used to configure a stream definition. Parameter Description query name The name of the query. Since naming the query (i.e the @name(' query name ') annotation) is optional, when the name is not provided Siddhi assign a system generated name for the query. input Defines the means of event consumption via streams , named-windows , tables , and/or named-aggregations , and defines the processing logic using filters , windows , stream-functions , joins , patterns and sequences . projection Generates output event attributes using select , functions , aggregation-functions , and group by operations, and filters the generated the output using having , limit offset , order by , and output rate limiting operations before sending them out. Here the projection is optional and when it is omitted all the input events will be sent to the output as it is. output action Defines output action (such as insert into , update , delete , etc) that needs to be performed by the generated events on a stream , named-window , or table Example A query consumes events from the TempStream stream and output only the roomNo and temp attributes to the RoomTempStream stream, from which another query consumes the events and sends all its attributes to AnotherRoomTempStream stream. define stream TempStream (deviceID long, roomNo int, temp double); from TempStream select roomNo, temp insert into RoomTempStream; from RoomTempStream insert into AnotherRoomTempStream; Inferred Stream Here, the RoomTempStream and AnotherRoomTempStream streams are an inferred streams, which means their stream definitions are inferred from the queries and they can be used same as any other defined stream without any restrictions.","title":"Query"},{"location":"docs/query-guide/#value","text":"Values are typed data, that can be manipulated, transferred and stored. Values can be referred by the attributes defined in definitions such as streams, and tables. Siddhi supports values of type STRING , INT (Integer), LONG , DOUBLE , FLOAT , BOOL (Boolean) and OBJECT . The syntax of each type and their example use as a constant value is as follows, Attribute Type Format Example int + 123 , -75 , +95 long +L 123000L , -750l , +154L float ( +)?('.' *)? (E(-|+)? +)?F 123.0f , -75.0e-10F , +95.789f double ( +)?('.' *)? (E(-|+)? +)?D? 123.0 , 123.0D , -75.0e-10D , +95.789d bool (true|false) true , false , TRUE , FALSE string '( char * !('|\"|\"\"\"| line ))' or \"( char * !(\"|\"\"\"| line ))\" or \"\"\"( char * !(\"\"\"))\"\"\" 'Any text.' , \"Text with 'single' quotes.\" , \"\"\" Text with 'single' quotes, \"double\" quotes, and new lines. \"\"\" Time Time is a special type of LONG value that denotes time using digits and their unit in the format ( digit + unit )+ . At execution, the time gets converted into milliseconds and returns a LONG value. Unit Syntax Year year | years Month month | months Week week | weeks Day day | days Hour hour | hours Minutes minute | minutes | min Seconds second | seconds | sec Milliseconds millisecond | milliseconds Example 1 hour and 25 minutes can by written as 1 hour and 25 minutes which is equal to the LONG value 5100000 .","title":"Value"},{"location":"docs/query-guide/#select","text":"The select clause in Siddhi query defines the output event attributes of the query. Following are some basic query projection operations supported by select. Action Description Select specific attributes for projection Only select some of the input attributes as query output attributes. E.g., Select and output only roomNo and temp attributes from the TempStream stream. from TempStream select roomNo, temp insert into RoomTempStream; Select all attributes for projection Select all input attributes as query output attributes. This can be done by using asterisk ( * ) or by omitting the select clause itself. E.g., Both following queries select all attributes of TempStream input stream and output all attributes to NewTempStream stream. from TempStream select * insert into NewTempStream; or from TempStream insert into NewTempStream; Name attribute Provide a unique name for each output attribute generated by the query. This can help to rename the selected input attributes or assign an attribute name to a projection operation such as function, aggregate-function, mathematical operation, etc, using as keyword. E.g., Query that renames input attribute temp to temperature and function currentTimeMillis() as time . from TempStream select roomNo, temp as temperature, currentTimeMillis() as time insert into RoomTempStream; Constant values as attributes Creates output attributes with a constant value. Any constant value of type STRING , INT , LONG , DOUBLE , FLOAT , BOOL , and time as given in the values section can be defined. E.g., Query specifying 'C' as the constant value for the scale attribute. from TempStream select roomNo, temp, 'C' as scale insert into RoomTempStream; Mathematical and logical expressions in attributes Defines the mathematical and logical operations that need to be performed to generating output attribute values. These expressions are executed in the precedence order given below. Operator precedence Operator Distribution Example () Scope (cost + tax) * 0.05 IS NULL Null check deviceID is null NOT Logical NOT not (price > 10) * , / , % Multiplication, division, modulus temp * 9/5 + 32 + , - Addition, subtraction temp * 9/5 - 32 < , < = , > , >= Comparators: less-than, greater-than-equal, greater-than, less-than-equal totalCost >= price * quantity == , != Comparisons: equal, not equal totalCost != price * quantity IN Checks if value exist in the table roomNo in ServerRoomsTable AND Logical AND temp < 40 and humidity < 40 OR Logical OR humidity < 40 or humidity >= 60 E.g., Query converts temperature from Celsius to Fahrenheit, and identifies rooms with room number between 10 and 15 as server rooms. from TempStream select roomNo, temp * 9/5 + 32 as temp, 'F' as scale, roomNo > 10 and roomNo < 15 as isServerRoom insert into RoomTempStream;","title":"Select"},{"location":"docs/query-guide/#function","text":"Function are pre-configured operations that can consumes zero, or more parameters and always produce a single value as result. It can be used anywhere an attribute can be used. Purpose Functions encapsulate pre-configured reusable execution logic allowing users to execute the logic anywhere just by calling the function. This also make writing SiddhiApps simple and easy to understand. Syntax The syntax of function is as follows, function name ( parameter * ) Here function name uniquely identifies the function. The parameter defined input parameters the function can accept. The input parameters can be attributes, constant values, results of other functions, results of mathematical or logical expressions, or time values. The number and type of parameters a function accepts depend on the function itself. Note Functions, mathematical expressions, and logical expressions can be used in a nested manner. Example 1 Function name add accepting two input parameters, is called with an attribute named input and a constant value 75 . add(input, 75) Example 2 Function name alertAfter accepting two input parameters, is called with a time value of 1 hour and 25 minutes and a mathematical addition operation of startTime + 56 . add(1 hour and 25 minutes, startTime + 56) Inbuilt functions Following are some inbuilt Siddhi functions, for more functions refer execution extensions . Inbuilt function Description eventTimestamp Returns event's timestamp. currentTimeMillis Returns current time of SiddhiApp runtime. default Returns a default value if the parameter is null. ifThenElse Returns parameters based on a conditional parameter. UUID Generates a UUID. cast Casts parameter type. convert Converts parameter type. coalesce Returns first not null input parameter. maximum Returns the maximum value of all parameters. minimum Returns the minimum value of all parameters. instanceOfBoolean Checks if the parameter is an instance of Boolean. instanceOfDouble Checks if the parameter is an instance of Double. instanceOfFloat Checks if the parameter is an instance of Float. instanceOfInteger Checks if the parameter is an instance of Integer. instanceOfLong Checks if the parameter is an instance of Long. instanceOfString Checks if the parameter is an instance of String. createSet Creates HashSet with given input parameters. sizeOfSet Returns number of items in the HashSet, that's passed as a parameter. Example Query that converts the roomNo to string using convert function, finds the maximum temperature reading with maximum function, and adds a unique messageID using the UUID function. from TempStream select convert(roomNo, 'string') as roomNo, maximum(tempReading1, tempReading2) as temp, UUID() as messageID insert into RoomTempStream;","title":"Function"},{"location":"docs/query-guide/#filter","text":"Filters provide a way of filtering input stream events based on a specified condition. It accepts any type of condition including a combination of functions and/or attributes that produces a Boolean result. Filters allow events to passthrough if the condition results in true , and drops if it results in a false . Purpose Filter helps to select the events that are relevant for the processing and omit the ones that are not. Syntax Filter conditions should be defined in square brackets ( [] ) next to the input stream as shown below. from input stream [ filter condition ] select attribute name , attribute name , ... insert into output stream Example Query to filter TempStream stream events, having roomNo within the range of 100-210 and temperature greater than 40 degrees, and insert them into HighTempStream stream. from TempStream[(roomNo = 100 and roomNo 210) and temp 40] select roomNo, temp insert into HighTempStream;","title":"Filter"},{"location":"docs/query-guide/#window","text":"Window provides a way to capture a subset of events from an input stream and retain them for a period of time based on a specified criterion. The criterion defines when and how the events should be evicted from the windows. Such as events getting evicted from the window based on the time duration, or number of events and they events are evicted in a sliding (one by one) or tumbling (batch) manner. Within a query, each input stream can at most have only one window associated with it. Purpose Windows help to retain events based on a criterion, such that the values of those events can be aggregated, or checked if an event of interest is within the window or not. Syntax Window should be defined by using the #window prefix next to the input stream as shown below. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert ouput event type ? into output stream Note Filter conditions can be applied both before and/or after the window. Inbuilt windows Following are some inbuilt Siddhi windows, for more windows refer execution extensions . Inbuilt function Description time Retains events based on time in a sliding manner. timeBatch Retains events based on time in a tumbling/batch manner. length Retains events based on number of events in a sliding manner. lengthBatch Retains events based on number of events in a tumbling/batch manner. timeLength Retains events based on time and number of events in a sliding manner. session Retains events for each session based on session key. batch Retains events of last arrived event chunk. sort Retains top-k or bottom-k events based on a parameter value. cron Retains events based on cron time in a tumbling/batch manner. externalTime Retains events based on event time value passed as a parameter in a sliding manner. externalTimeBatch Retains events based on event time value passed as a parameter in a a tumbling/batch manner. delay Retains events and delays the output by the given time period in a sliding manner. Example 1 Query to find out the maximum temperature out of the last 10 events , using the window of length 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.length(10) select max(temp) as maxTemp insert into MaxTempStream; Here, the length window operates in a sliding manner where the following 3 event subsets are calculated and outputted when a list of 12 events are received in sequential order. Subset Event Range 1 1 - 10 2 2 - 11 3 3 - 12 Example 2 Query to find out the maximum temperature out of the every 10 events , using the window of lengthBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.lengthBatch(10) select max(temp) as maxTemp insert into MaxTempStream; Here, the window operates in a batch/tumbling manner where the following 3 event subsets are calculated and outputted when a list of 30 events are received in a sequential order. Subset Event Range 1 1 - 10 2 11 - 20 3 21 - 30 Example 3 Query to find out the maximum temperature out of the events arrived during last 10 minutes , using the window of time 10 minutes and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.time(10 min) select max(temp) as maxTemp insert into MaxTempStream; Here, the time window operates in a sliding manner with millisecond accuracy, where it will process events in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:00:01.001 - 1:10:01.000 3 1:00:01.033 - 1:10:01.034 Example 4 Query to find out the maximum temperature out of the events arriving every 10 minutes , using the window of timeBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.timeBatch(10 min) select max(temp) as maxTemp insert into MaxTempStream; Here, the window operates in a batch/tumbling manner where the window will process evetns in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:10:00.001 - 1:20:00.000 3 1:20:00.001 - 1:30:00.000","title":"Window"},{"location":"docs/query-guide/#event-type","text":"Query output depends on the current and expired event types it produces based on its internal processing state. By default all queries produce current events upon event arrival to the query. The queries containing windows additionally produce expired events when events expire from the windows. Purpose Event type helps to specify when a query should output events to the stream, such as output upon current events, expired events or upon both current and expired events. Syntax Event type should be defined in between insert and into keywords for insert queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert event type into output stream Event type should be defined next to the for keyword for delete queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... delete table (for event type )? on condition Event type should be defined next to the for keyword for update queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... update table (for event type )? set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition Event type should be defined next to the for keyword for update or insert queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... update or insert into table (for event type )? set table . attribute name = expression , table . attribute name = expression , ... on condition Note Controlling query output based on the event types neither alters query execution nor its accuracy. The event types can be defined using the following keywords to manipulate query output. Event types Description current events Outputs events only when incoming events arrive to be processed by the query. This is default behavior when no specific event type is specified. expired events Outputs events only when events expires from the window. all events Outputs events when incoming events arrive to be processed by the query as well as when events expire from the window. Example Query to output only the expired events from a 1 minute time window to the DelayedTempStream stream. This can be used for delaying the events by a minute. from TempStream#window.time(1 min) select * insert expired events into DelayedTempStream Note This is just to illustrate how expired events work, it is recommended to use delay window for usecases where we need to delay events by a given time period.","title":"Event Type"},{"location":"docs/query-guide/#aggregate-function","text":"Aggregate functions are pre-configured aggregation operations that can consumes zero, or more parameters from multiple events and always produce a single value as result. They can be only used in the query projection (as part of the select clause). When a query comprises a window, the aggregation will be contained to the events in the window, and when it does not have a window, the aggregation is performed from the first event the query has received. Purpose Aggregate functions encapsulate pre-configured reusable aggregate logic allowing users to aggregate values of multiple events together. When used with batch/tumbling windows this can also help to reduce the number of output events produced. Syntax Aggregate function can be used in query projection (as part of the select clause) alone or as a part of another expression. In all cases, the output produced by the query should be properly mapped to the output stream attribute using the as keyword. The syntax of aggregate function is as follows, from input stream #window. window name ( parameter , parameter , ... ) select aggregate function ( parameter , parameter , ... ) as attribute name , attribute2 name , ... insert into output stream ; Here aggregate function uniquely identifies the aggregate function. The parameter defined input parameters the aggregate function can accept. The input parameters can be attributes, constant values, results of other functions or aggregate functions, results of mathematical or logical expressions, or time values. The number and type of parameters an aggregate function accepts depend on the function itself. Inbuilt aggregate functions Following are some inbuilt aggregation functions, for more functions refer execution extensions . Inbuilt aggregate function Description sum Calculates the sum from a set of values. count Calculates the count from a set of values. distinctCount Calculates the distinct count based on a parameter from a set of values. avg Calculates the average from a set of values. max Finds the maximum value from a set of values. max Finds the minimum value from a set of values. | maxForever | Finds the maximum value from all events throughout its lifetime irrespective of the windows. | | minForever | Finds the minimum value from all events throughout its lifetime irrespective of the windows. | | stdDev | Calculates the standard deviation from a set of values. | | and | Calculates boolean and from a set of values. | | or | Calculates boolean or from a set of values. | | unionSet | Calculates union as a Set from a set of values. | Example Query to calculate average, maximum, and minimum values on temp attribute of the TempStream stream in a sliding manner, from the events arrived over the last 10 minutes and to produce outputs avgTemp , maxTemp and minTemp respectively to the AvgTempStream output stream. from TempStream#window.time(10 min) select avg(temp) as avgTemp, max(temp) as maxTemp, min(temp) as minTemp insert into AvgTempStream;","title":"Aggregate Function"},{"location":"docs/query-guide/#group-by","text":"Group By provides a way of grouping events based on one or more specified attributes to perform aggregate operations. Purpose Group By allows users to aggregate values of multiple events based on the given group-by fields. Syntax The syntax for the Group By with aggregate function is as follows. from input stream #window. window name (...) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name , ... insert into output stream ; Here the group by attributes should be defined next to the group by keyword separating each attribute by a comma. Example Query to calculate the average temp per roomNo and deviceID combination, from the events arrived from TempStream stream, during the last 10 minutes time-window in a sliding manner. from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID insert into AvgTempStream;","title":"Group By"},{"location":"docs/query-guide/#having","text":"Having provide a way of filtering events based on a specified condition of the query output stream attributes. It accepts any type of condition including a combination of functions and/or attributes that produces a Boolean result. Having, allow events to passthrough if the condition results in true , and drops if it results in a false . Purpose Having helps to select the events that are relevant for the output based on the attributes those are produced by the select clause and omit the ones that are not. Syntax The syntax for the Having clause is as follows. from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition insert into output stream ; Here the having condition should be defined next to the having keyword and having can be used with or without group by clause. Example Query to calculate the average temp per roomNo for the last 10 minutes, and alerts if the avgTemp exceeds 30 degrees. from TempStream#window.time(10 min) select roomNo, avg(temp) as avgTemp group by roomNo having avgTemp 30 insert into AlertStream;","title":"Having"},{"location":"docs/query-guide/#order-by","text":"Order By, orders the query results in ascending and or descending order based on one or more specified attributes. When an attribute is used for order by, by default Siddhi orders the events in ascending order of that attribute's value, and by adding desc keyword, the events can be ordered in descending order. When more than one attribute is defined the attributes defined towards the left will have more precedence in ordering than the ones defined in right. Purpose Order By helps to sort the events in the outputs chunks produced by the query. Order By will be more helpful for batch windows, and queries where they output many of event together then for sliding window use cases where the output will be one or few events at a time. Syntax The syntax for the Order By clause is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name (asc|desc)?, attribute2 name (asc|desc)?, ... insert into output stream ; Here the order by attributes should be defined next to the order by keyword separating each by a comma, and optionally specifying the event ordering using asc (default) or desc keywords. Example Query to calculate the average temp per roomNo and deviceID combination on every 10 minutes batches, and order the generated output events in ascending order by avgTemp and then by descending order of roomNo (if the more than one event have the same avgTemp value). from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp, roomNo desc insert into AvgTempStream;","title":"Order By"},{"location":"docs/query-guide/#limit-offset","text":"These provide a way to select the number of events (via limit) from the desired index (by specifying an offset) from the output event chunks produced by the query. Purpose Limit Offset helps to output only the selected set of events from large event batches. This will be more useful with Order By clause where one can order the output for topK, bottomK, or even to paginate through the dataset by obtaining a set of events from the middle. Syntax The syntax for the Limit Offset clauses is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name (asc | desc)?, attribute2 name ( ascend/descend )?, ... limit positive integer ? offset positive integer ? insert into output stream ; Here both limit and offset are optional, when limit is omitted the query will output all the events, and when offset is omitted 0 is taken as the default offset value. Example 1 Query to calculate the average temp per roomNo and deviceID combination for every 10 minutes batches, from the events arriving at the TempStream stream, and emit only two events having the highest avgTemp value. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp desc limit 2 insert into HighestAvgTempStream; Example 2 Query to calculate the average temp per roomNo and deviceID combination for every 10 minutes batches, for events that arriving at the TempStream stream, and emits only the third, forth and fifth events when sorted in descending order based on their avgTemp value. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp desc limit 3 offset 2 insert into HighestAvgTempStream;","title":"Limit &amp; Offset"},{"location":"docs/query-guide/#join-stream","text":"Joins allow you to get a combined result from two streams in real-time based on a specified condition. Purpose Streams are stateless. Therefore, in order to join two streams, they need to be connected to a window so that there is a pool of events that can be used for joining. Joins also accept conditions to join the appropriate events from each stream. During the joining process each incoming event of each stream is matched against all the events in the other stream's window based on the given condition, and the output events are generated for all the matching event pairs. Note Join can also be performed with stored data , aggregation or externally named windows . Syntax The syntax for a join is as follows: from input stream #window. window name ( parameter , ... ) {unidirectional} {as reference } join input stream #window. window name ( parameter , ... ) {unidirectional} {as reference } on join condition select attribute name , attribute name , ... insert into output stream Here, the join condition allows you to match the attributes from both the streams. Unidirectional join operation By default, events arriving at either stream can trigger the joining process. However, if you want to control the join execution, you can add the unidirectional keyword next to a stream in the join definition as depicted in the syntax in order to enable that stream to trigger the join operation. Here, events arriving at other stream only update the window of that stream, and this stream does not trigger the join operation. Note The unidirectional keyword cannot be applied to both the input streams because the default behaviour already allows both streams to trigger the join operation. Example Assuming that the temperature of regulators are updated every minute. Following is a Siddhi App that controls the temperature regulators if they are not already on for all the rooms with a room temperature greater than 30 degrees. define stream TempStream(deviceID long, roomNo int, temp double); define stream RegulatorStream(deviceID long, roomNo int, isOn bool); from TempStream[temp 30.0]#window.time(1 min) as T join RegulatorStream[isOn == false]#window.length(1) as R on T.roomNo == R.roomNo select T.roomNo, R.deviceID, 'start' as action insert into RegulatorActionStream; Supported join types Following are the supported operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join both the streams. The output is generated only if there is a matching event in both the streams. Left outer join The left outer join operation allows you to join two streams to be merged based on a condition. left outer join is used as the keyword to join both the streams. Here, it returns all the events of left stream even if there are no matching events in the right stream by having null values for the attributes of the right stream. Example The following query generates output events for all events from the StockStream stream regardless of whether a matching symbol exists in the TwitterStream stream or not. from StockStream#window.time(1 min) as S left outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ; Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join both the streams. It returns all the events of the right stream even if there are no matching events in the left stream. Full outer join The full outer join combines the results of left outer join and right outer join. full outer join is used as the keyword to join both the streams. Here, output event are generated for each incoming event even if there are no matching events in the other stream. Example The following query generates output events for all the incoming events of each stream regardless of whether there is a match for the symbol attribute in the other stream or not. from StockStream#window.time(1 min) as S full outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ;","title":"Join (Stream)"},{"location":"docs/query-guide/#pattern","text":"This is a state machine implementation that allows you to detect patterns in the events that arrive over time. This can correlate events within a single stream or between multiple streams. Purpose Patterns allow you to identify trends in events over a time period. Syntax The following is the syntax for a pattern query: from (every)? event reference = input stream [ filter condition ] - (every)? event reference = input stream [ filter condition ] - ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description - This is used to indicate an event that should be following another event. The subsequent event does not necessarily have to occur immediately after the preceding event. The condition to be met by the preceding event should be added before the sign, and the condition to be met by the subsequent event should be added after the sign. event reference This allows you to add a reference to the the matching event so that it can be accessed later for further processing. (within time gap )? The within clause is optional. It defines the time duration within which all the matching events should occur. every every is an optional keyword. This defines whether the event matching should be triggered for every event arrival in the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. Siddhi also supports pattern matching with counting events and matching events in a logical order such as ( and , or , and not ). These are described in detail further below in this guide. Example This query sends an alert if the temperature of a room increases by 5 degrees within 10 min. from every( e1=TempStream ) - e2=TempStream[ e1.roomNo == roomNo and (e1.temp + 5) = temp ] within 10 min select e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Here, the matching process begins for each event in the TempStream stream (because every is used with e1=TempStream ), and if another event arrives within 10 minutes with a value for the temp attribute that is greater than or equal to e1.temp + 5 of the event e1, an output is generated via the AlertStream .","title":"Pattern"},{"location":"docs/query-guide/#counting-pattern","text":"Counting patterns allow you to match multiple events that may have been received for the same matching condition. The number of events matched per condition can be limited via condition postfixes. Syntax Each matching condition can contain a collection of events with the minimum and maximum number of events to be matched as shown in the syntax below. from (every)? event reference = input stream [ filter condition ] ( min count : max count )? - ... (within time gap )? select event reference ([event index])?. attribute name , ... insert into output stream Postfix Description Example n1:n2 This matches n1 to n2 events (including n1 and not more than n2 ). 1:4 matches 1 to 4 events. n: This matches n or more events (including n ). 2: matches 2 or more events. :n This matches up to n events (excluding n ). :5 matches up to 5 events. n This matches exactly n events. 5 matches exactly 5 events. Specific occurrences of the event in a collection can be retrieved by using an event index with its reference. Square brackets can be used to indicate the event index where 1 can be used as the index of the first event and last can be used as the index for the last available event in the event collection. If you provide an index greater then the last event index, the system returns null . The following are some valid examples. e1[3] refers to the 3 rd event. e1[last] refers to the last event. e1[last - 1] refers to the event before the last event. Example The following Siddhi App calculates the temperature difference between two regulator events. define stream TempStream (deviceID long, roomNo int, temp double); define stream RegulatorStream (deviceID long, roomNo int, tempSet double, isOn bool); from every( e1=RegulatorStream) - e2=TempStream[e1.roomNo==roomNo] 1: - e3=RegulatorStream[e1.roomNo==roomNo] select e1.roomNo, e2[0].temp - e2[last].temp as tempDiff insert into TempDiffStream;","title":"Counting Pattern"},{"location":"docs/query-guide/#logical-patterns","text":"Logical patterns match events that arrive in temporal order and correlate them with logical relationships such as and , or and not . Syntax from (every)? (not)? event reference = input stream [ filter condition ] ((and|or) event reference = input stream [ filter condition ])? (within time gap )? - ... select event reference ([event index])?. attribute name , ... insert into output stream Keywords such as and , or , or not can be used to illustrate the logical relationship. Key Word Description and This allows both conditions of and to be matched by two events in any order. or The state succeeds if either condition of or is satisfied. Here the event reference of the other condition is null . not condition1 and condition2 When not is included with and , it identifies the events that match arriving before any event that match . not condition for time period When not is included with for , it allows you to identify a situation where no event that matches condition1 arrives during the specified time period . e.g., from not TemperatureStream[temp 60] for 5 sec . Here the not pattern can be followed by either an and clause or the effective period of not can be concluded after a given time period . Further in Siddhi more than two streams cannot be matched with logical conditions using and , or , or not clauses at this point.","title":"Logical Patterns"},{"location":"docs/query-guide/#detecting-non-occurring-events","text":"Siddhi allows you to detect non-occurring events via multiple combinations of the key words specified above as shown in the table below. In the patterns listed, P* can be either a regular event pattern, an absent event pattern or a logical pattern. Pattern Detected Scenario not A for time period The non-occurrence of event A within time period after system start up. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, to indicate that the passenger might be in danger. not A for time period and B After system start up, event A does not occur within time period , but event B occurs at some point in time. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, and the passenger marked that he/she is in danger at some point in time. not A for time period 1 and not B for time period 2 After system start up, event A doess not occur within time period 1 , and event B also does not occur within time period 2 . e.g., Generating an alert if the driver of a taxi has not reached the destination within 30 minutes, and the passenger has not marked himself/herself to be in danger within that same time period. not A for time period or B After system start up, either event A does not occur within time period , or event B occurs at some point in time. e.g., Generating an alert if the taxi has not reached its destination within 30 minutes, or if the passenger has marked that he/she is in danger at some point in time. not A for time period 1 or not B for time period 2 After system start up, either event A does not occur within time period 1 , or event B occurs within time period 2 . e.g., Generating an alert to indicate that the driver is not on an expected route if the taxi has not reached destination A within 20 minutes, or reached destination B within 30 minutes. A \u2192 not B for time period Event B does not occur within time period after the occurrence of event A. e.g., Generating an alert if the taxi has reached its destination, but this was not followed by a payment record. P* \u2192 not A for time period and B After the occurrence of P*, event A does not occur within time period , and event B occurs at some point in time. P* \u2192 not A for time period 1 and not B for time period 2 After the occurrence of P*, event A does not occur within time period 1 , and event B does not occur within time period 2 . P* \u2192 not A for time period or B After the occurrence of P*, either event A does not occur within time period , or event B occurs at some point in time. P* \u2192 not A for time period 1 or not B for time period 2 After the occurrence of P*, either event A does not occur within time period 1 , or event B does not occur within time period 2 . not A for time period \u2192 B Event A does occur within time period after the system start up, but event B occurs after that time period has elapsed. not A for time period and B \u2192 P* Event A does not occur within time period , and event B occurs at some point in time. Then P* occurs after the time period has elapsed, and after B has occurred. not A for time period 1 and not B for time period 2 \u2192 P* After system start up, event A does not occur within time period 1 , and event B does not occur within time period 2 . However, P* occurs after both A and B. not A for time period or B \u2192 P* After system start up, event A does not occur within time period or event B occurs at some point in time. The P* occurs after time period has elapsed, or after B has occurred. not A for time period 1 or not B for time period 2 \u2192 P* After system start up, either event A does not occur within time period 1 , or event B does not occur within time period 2 . Then P* occurs after both time period 1 and time period 2 have elapsed. not A and B Event A does not occur before event B. A and not B Event B does not occur before event A. Example Following Siddhi App, sends the stop control action to the regulator when the key is removed from the hotel room. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream RoomKeyStream(deviceID long, roomNo int, action string); from every( e1=RegulatorStateChangeStream[ action == 'on' ] ) - e2=RoomKeyStream[ e1.roomNo == roomNo and action == 'removed' ] or e3=RegulatorStateChangeStream[ e1.roomNo == roomNo and action == 'off'] select e1.roomNo, ifThenElse( e2 is null, 'none', 'stop' ) as action having action != 'none' insert into RegulatorActionStream; This Siddhi Application generates an alert if we have switch off the regulator before the temperature reaches 12 degrees. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] - not TempStream[e1.roomNo == roomNo and temp 12] and e2=RegulatorStateChangeStream[action == 'off'] select e1.roomNo as roomNo insert into AlertStream; This Siddhi Application generates an alert if the temperature does not reduce to 12 degrees within 5 minutes of switching on the regulator. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] - not TempStream[e1.roomNo == roomNo and temp 12] for '5 min' select e1.roomNo as roomNo insert into AlertStream;","title":"Detecting Non-occurring Events"},{"location":"docs/query-guide/#sequence","text":"Sequence is a state machine implementation that allows you to detect the sequence of event occurrences over time. Here all matching events need to arrive consecutively to match the sequence condition, and there cannot be any non-matching events arriving within a matching sequence of events. This can correlate events within a single stream or between multiple streams. Purpose This allows you to detect a specified event sequence over a specified time period. Syntax The syntax for a sequence query is as follows: from (every)? event reference = input stream [ filter condition ], event reference = input stream [ filter condition ], ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description , This represents the immediate next event i.e., when an event that matches the first condition arrives, the event that arrives immediately after it should match the second condition. event reference This allows you to add a reference to the the matching event so that it can be accessed later for further processing. (within time gap )? The within clause is optional. It defines the time duration within which all the matching events should occur. every every is an optional keyword. This defines whether the matching event should be triggered for every event that arrives at the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. Example This query generates an alert if the increase in the temperature between two consecutive temperature events exceeds one degree. from every e1=TempStream, e2=TempStream[e1.temp + 1 temp] select e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Counting Sequence Counting sequences allow you to match multiple events for the same matching condition. The number of events matched per condition can be limited via condition postfixes such as Counting Patterns , or by using the * , + , and ? operators. The matching events can also be retrieved using event indexes, similar to how it is done in Counting Patterns . Syntax Each matching condition in a sequence can contain a collection of events as shown below. from (every)? event reference = input stream [ filter condition ](+|*|?)?, event reference = input stream [ filter condition ](+|*|?)?, ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Postfix symbol Required/Optional Description + Optional This matches one or more events to the given condition. * Optional This matches zero or more events to the given condition. ? Optional This matches zero or one events to the given condition. Example This Siddhi application identifies temperature peeks. define stream TempStream(deviceID long, roomNo int, temp double); from every e1=TempStream, e2=TempStream[e1.temp = temp]+, e3=TempStream[e2[last].temp temp] select e1.temp as initialTemp, e2[last].temp as peakTemp insert into PeekTempStream; Logical Sequence Logical sequences identify logical relationships using and , or and not on consecutively arriving events. Syntax The syntax for a logical sequence is as follows: from (every)? (not)? event reference = input stream [ filter condition ] ((and|or) event reference = input stream [ filter condition ])? (within time gap )?, ... select event reference ([event index])?. attribute name , ... insert into output stream Keywords such as and , or , or not can be used to illustrate the logical relationship, similar to how it is done in Logical Patterns . Example This Siddhi application notifies the state when a regulator event is immediately followed by both temperature and humidity events. define stream TempStream(deviceID long, temp double); define stream HumidStream(deviceID long, humid double); define stream RegulatorStream(deviceID long, isOn bool); from every e1=RegulatorStream, e2=TempStream and e3=HumidStream select e2.temp, e3.humid insert into StateNotificationStream;","title":"Sequence"},{"location":"docs/query-guide/#output-rate-limiting","text":"Output rate limiting allows queries to output events periodically based on a specified condition. Purpose This allows you to limit the output to avoid overloading the subsequent executions, and to remove unnecessary information. Syntax The syntax of an output rate limiting configuration is as follows: from input stream ... select attribute name , attribute name , ... output rate limiting configuration insert into output stream Siddhi supports three types of output rate limiting configurations as explained in the following table: Rate limiting configuration Syntax Description Based on time output event every time interval This outputs output event every time interval time interval. Based on number of events output event every event interval events This outputs output event for every event interval number of events. Snapshot based output snapshot every time interval This outputs all events in the window (or the last event if no window is defined in the query) for every given time interval time interval. Here the output event specifies the event(s) that should be returned as the output of the query. The possible values are as follows: * first : Only the first event processed by the query during the specified time interval/sliding window is emitted. * last : Only the last event processed by the query during the specified time interval/sliding window is emitted. * all : All the events processed by the query during the specified time interval/sliding window are emitted. When no output event is defined, all is used by default. Examples Returning events based on the number of events Here, events are emitted every time the specified number of events arrive. You can also specify whether to emit only the first event/last event, or all the events out of the events that arrived. In this example, the last temperature per sensor is emitted for every 10 events. from TempStreamselect select temp, deviceID group by deviceID output last every 10 events insert into LowRateTempStream; Returning events based on time Here events are emitted for every predefined time interval. You can also specify whether to emit only the first event, last event, or all events out of the events that arrived during the specified time interval. In this example, emits all temperature events every 10 seconds from TempStreamoutput output every 10 sec insert into LowRateTempStream; Returning a periodic snapshot of events This method works best with windows. When an input stream is connected to a window, snapshot rate limiting emits all the current events that have arrived and do not have corresponding expired events for every predefined time interval. If the input stream is not connected to a window, only the last current event for each predefined time interval is emitted. This query emits a snapshot of the events in a time window of 5 seconds every 1 second. from TempStream#window.time(5 sec) output snapshot every 1 sec insert into SnapshotTempStream;","title":"Output rate limiting"},{"location":"docs/query-guide/#partition","text":"Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. A partition can contain one or more queries and there can be multiple instances where the same queries and streams are replicated for each partition. Each partition is tagged with a partition key. Those partitions only process the events that match the corresponding partition key. Purpose Partitions allow you to process the events groups in isolation so that event processing can be performed using the same set of queries for each group. Partition key generation A partition key can be generated in the following two methods: Partition by value This is created by generating unique values using input stream attributes. Syntax partition with ( expression of stream name , expression of stream name , ... ) begin query query ... end; Example This query calculates the maximum temperature recorded within the last 10 events per deviceID . partition with ( deviceID of TempStream ) begin from TempStream#window.length(10) select roomNo, deviceID, max(temp) as maxTemp insert into DeviceTempStream; end; Partition by range This is created by mapping each partition key to a range condition of the input streams numerical attribute. Syntax partition with ( condition as partition key or condition as partition key or ... of stream name , ... ) begin query query ... end; Example This query calculates the average temperature for the last 10 minutes per office area. partition with ( roomNo = 1030 as 'serverRoom' or roomNo 1030 and roomNo = 330 as 'officeRoom' or roomNo 330 as 'lobby' of TempStream) begin from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp insert into AreaTempStream end;","title":"Partition"},{"location":"docs/query-guide/#inner-stream","text":"Queries inside a partition block can use inner streams to communicate with each other while preserving partition isolation. Inner streams are denoted by a \"#\" placed before the stream name, and these streams cannot be accessed outside a partition block. Purpose Inner streams allow you to connect queries within the partition block so that the output of a query can be used as an input only by another query within the same partition. Therefore, you do not need to repartition the streams if they are communicating within the partition. Example This partition calculates the average temperature of every 10 events for each sensor, and sends an output to the DeviceTempIncreasingStream stream if the consecutive average temperature values increase by more than 5 degrees. partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 < avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end;","title":"Inner Stream"},{"location":"docs/query-guide/#purge-partition","text":"Based on the partition key used for the partition, multiple instances of streams and queries will be generated. When an extremely large number of unique partition keys are used there is a possibility of very high instances of streams and queries getting generated and eventually system going out of memory. In order to overcome this, users can define a purge interval to clean partitions that will not be used anymore. Purpose @purge allows you to clean the partition instances that will not be used anymore. Syntax The syntax of partition purge configuration is as follows: @purge(enable='true', interval=' purge interval ', idle.period=' idle period of partition instance ') partition with ( partition key of input stream ) begin from input stream ... select attribute name , attribute name , ... insert into output stream end; Partition purge configuration Description Purge interval The periodic time interval to purge the purgeable partition instances. Idle period of partition instance The period, a particular partition instance (for a given partition key) needs to be idle before it becomes purgeable. Examples Mark partition instances eligible for purging, if there are no events from a particular deviceID for 15 seconds, and periodically purge those partition instances every 1 second. @purge(enable='true', interval='1 sec', idle.period='15 sec') partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end;","title":"Purge Partition"},{"location":"docs/query-guide/#table","text":"A table is a stored version of an stream or a table of events. Its schema is defined via the table definition that is similar to a stream definition. These events are by default stored in-memory , but Siddhi also provides store extensions to work with data/events stored in various data stores through the table abstraction. Purpose Tables allow Siddhi to work with stored events. By defining a schema for tables Siddhi enables them to be processed by queries using their defined attributes with the streaming data. You can also interactively query the state of the stored events in the table. Syntax The syntax for a new table definition is as follows: define table table name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are configured in a table definition: Parameter Description table name The name of the table defined. ( PascalCase is used for table name as a convention.) attribute name The schema of the table is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . Example The following defines a table named RoomTypeTable with roomNo and type attributes of data types int and string respectively. define table RoomTypeTable ( roomNo int, type string ); Primary Keys Tables can be configured with primary keys to avoid the duplication of data. Primary keys are configured by including the @PrimaryKey( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have only one @PrimaryKey annotation. The number of attributes supported differ based on the table implementations. When more than one attribute is used for the primary key, the uniqueness of the events stored in the table is determined based on the combination of values for those attributes. Examples This query creates an event table with the symbol attribute as the primary key. Therefore each entry in this table must have a unique value for symbol attribute. @PrimaryKey('symbol') define table StockTable (symbol string, price float, volume long); Indexes Indexes allow tables to be searched/modified much faster. Indexes are configured by including the @Index( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have 0-1 @Index annotations. Support for the @Index annotation and the number of attributes supported differ based on the table implementations. When more then one attribute is used for index, each one of them is used to index the table for fast access of the data. Indexes can be configured together with primary keys. Examples This query creates an indexed event table named RoomTypeTable with the roomNo attribute as the index key. @Index('roomNo') define table RoomTypeTable (roomNo int, type string);","title":"Table"},{"location":"docs/query-guide/#store","text":"Store is a table that refers to data/events stored in data stores outside of Siddhi such as RDBMS, Cassandra, etc. Store is defined via the @store annotation, and the store schema is defined via a table definition associated with it. Purpose Store allows Siddhi to search, retrieve and manipulate data stored in external data stores through Siddhi queries. Syntax The syntax for a defining store and it's associated table definition is as follows: @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN') define table TableName (attribute1 Type1, attributeN TypeN); Example The following defines a RDBMS data store pointing to a MySQL database with name hotel hosted in loacalhost:3306 having a table RoomTypeTable with columns roomNo of INTEGER and type of VARCHAR(255) mapped to Siddhi data types int and string respectively. @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/hotel\", username=\"siddhi\", password=\"123\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table RoomTypeTable ( roomNo int, type string ); Supported Store Types The following is a list of currently supported store types: RDBMS (MySQL, Oracle, SQL Server, PostgreSQL, DB2, H2) MongoDB Caching in Memory Store tables are persisted in high i/o latency storage. Hence, it is beneficial to maintain a cache of store tables in memory which has low latency. Siddhi supports caching of store tables through @cache annotation. It should be used within @store annotation in a nested fashion as shown below. @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN', @cache(size=10, cache.policy=FIFO)) define table TableName (attribute1 Type1, attributeN TypeN); In the above example we have defined a cache with a maximum size of 10 rows with first-in first-out cache policy. The following table contains the cache parameters. Parameter Mandatory/Optional Default Value Description size Mandatory - maximum number of rows to be cached cache.policy Optional FIFO policy to free up cache when cache miss occurs. There are 3 allowed policies. 1. FIFO - First-In, First-Out 2. LRU - Least Recently Used 3. LFU - Least Frequently Used retention.period Optional - If user specifies this parameter then cache expiry is enabled. For example if this is 5 min, rows older than 5 mins will be removed and in some cases reloaded from store purge.interval optional equal to retention period When cache expiry is enabled, a thread will be created for every purge.interval which will check for expired rows and remove them. The following is an example of caching with expiry. @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN', @cache(size=10, retention.period=5 min, purge.interval=1 min)) define table TableName (attribute1 Type1, attributeN TypeN); The above query will define and create a store table of given type and a cache with a max size of 10. A thread will be created every 1 minute which will check the entire cache table for rows added earlier than 5 minutes and expire them. Cache Behaviour Cache behaviour changes profoundly based on the size of store table relative to maximum cache size defined. Since memory is a limited resource we don't allow cache to grow more than the user specified maximum size. Case 1 \\ When store table is smaller than maximum cache size defined we keep the entire content of store table in memory in cache table. All types of queries are routed to cache and cache results are directly sent out to the user. Every time the expiry thread finds that cache events were loaded earlier than retention period entire cache table will be deleted and reloaded from store. In addition, when siddhi app starts, the entire store table, if it exists, will be loaded into cache. Case 2 \\ When store table is bigger than maximum cache size only the queries satisfying the following 2 conditions are sent to cache. 1. the query contains all the primary keys of the table 2. the query contains only == type of comparison. Only for the above types of queries we can establish if the cache is hit or missed. Subject to these conditions if the cache is hit the results from cache is sent out. If the cache is missed then store is checked. If the above conditions are not met by a query it is directly sent to the store table. In addition, please note that if the store table is pre existing when siddhi app is started and it is bigger than max cache size, cache preloading will take only upto max size and put it in cache. For example if store table has 50 entries when the siddhi app is defined with cache size of 10, only the first 10 rows will be cached. When cache miss occurs we look for the answer in the store table. If there is a result from the store table it is added to cache. One element from cache is removed using the user given cache policy prior to adding. When it comes to cache expiry, since not all rows are loaded at once in this case there may be some expired rows and some unexpired rows at any time. So for every purge interval a thread will be generated which looks for rows that were loaded earlier than retention period and delete only those rows. No reloading is done. Operators on Table (and Store) The following operators can be performed on tables (and stores).","title":"Store"},{"location":"docs/query-guide/#insert","text":"This allows events to be inserted into tables. This is similar to inserting events into streams. Warning If the table is defined with primary keys, and if you insert duplicate data, primary key constrain violations can occur. In such cases use the update or insert into operation. Syntax from input stream select attribute name , attribute name , ... insert into table Similar to streams, you need to use the current events , expired events or the all events keyword between insert and into keywords in order to insert only the specific event types. For more information, see Event Type Example This query inserts all the events from the TempStream stream to the TempTable table. from TempStream select * insert into TempTable;","title":"Insert"},{"location":"docs/query-guide/#join-table","text":"This allows a stream to retrieve information from a table in a streaming manner. Note Joins can also be performed with two streams , aggregation or against externally named windows . Syntax from input stream join table on condition select ( input stream | table ). attribute name , ( input stream | table ). attribute name , ... insert into output stream Note A table can only be joint with a stream. Two tables cannot be joint because there must be at least one active entity to trigger the join operation. Example This Siddhi App performs a join to retrieve the room type from RoomTypeTable table based on the room number, so that it can filter the events related to server-room s. define table RoomTypeTable (roomNo int, type string); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream join RoomTypeTable on RoomTypeTable.roomNo == TempStream.roomNo select deviceID, RoomTypeTable.type as roomType, type, temp having roomType == 'server-room' insert into ServerRoomTempStream; Supported join types Table join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the table. The output is generated only if there is a matching event in both the stream and the table. Left outer join The left outer join operation allows you to join a stream on left side with a table on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right table by having null values for the attributes of the right table. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a table on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left table.","title":"Join (Table)"},{"location":"docs/query-guide/#delete","text":"To delete selected events that are stored in a table. Syntax from input stream select attribute name , attribute name , ... delete table (for event type )? on condition The condition element specifies the basis on which events are selected to be deleted. When specifying the condition, table attributes should be referred to with the table name. To execute delete for specific event types, use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see Event Type Note Table attributes must be always referred to with the table name as follows: table name . attibute name Example In this example, the script deletes a record in the RoomTypeTable table if it has a value for the roomNo attribute that matches the value for the roomNumber attribute of an event in the DeleteStream stream. define table RoomTypeTable (roomNo int, type string); define stream DeleteStream (roomNumber int); from DeleteStream delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber;","title":"Delete"},{"location":"docs/query-guide/#update","text":"This operator updates selected event attributes stored in a table based on a condition. Syntax from input stream select attribute name , attribute name , ... update table (for event type )? set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition The condition element specifies the basis on which events are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. To execute an update for specific event types use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see Event Type . Note Table attributes must be always referred to with the table name as shown below: table name . attibute name . Example This Siddhi application updates the room occupancy in the RoomOccupancyTable table for each room number based on new arrivals and exits from the UpdateStream stream. define table RoomOccupancyTable (roomNo int, people int); define stream UpdateStream (roomNumber int, arrival int, exit int); from UpdateStream select * update RoomOccupancyTable set RoomOccupancyTable.people = RoomOccupancyTable.people + arrival - exit on RoomOccupancyTable.roomNo == roomNumber;","title":"Update"},{"location":"docs/query-guide/#update-or-insert","text":"This allows you update if the event attributes already exist in the table based on a condition, or else insert the entry as a new attribute. Syntax from input stream select attribute name , attribute name , ... update or insert into table (for event type )? set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which events are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note When the attribute to the right is a table attribute, the operations supported differ based on the database type. To execute update upon specific event types use the current events , expired events or the all events keyword with for as shown in the syntax. To understand more see Event Type . Note Table attributes should be always referred to with the table name as table name . attibute name . Example The following query update for events in the UpdateTable event table that have room numbers that match the same in the UpdateStream stream. When such events are found in the event table, they are updated. When a room number available in the stream is not found in the event table, it is inserted from the stream. define table RoomAssigneeTable (roomNo int, type string, assignee string); define stream RoomAssigneeStream (roomNumber int, type string, assignee string); from RoomAssigneeStream select roomNumber as roomNo, type, assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;","title":"Update or Insert"},{"location":"docs/query-guide/#in","text":"This allows the stream to check whether the expected value exists in the table as a part of a conditional operation. Syntax from input stream [ condition in table ] select attribute name , attribute name , ... insert into output stream The condition element specifies the basis on which events are selected to be compared. When constructing the condition , the table attribute must be always referred to with the table name as shown below: table . attibute name . Example This Siddhi application filters only room numbers that are listed in the ServerRoomTable table. define table ServerRoomTable (roomNo int); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream[ServerRoomTable.roomNo == roomNo in ServerRoomTable] insert into ServerRoomTempStream;","title":"In"},{"location":"docs/query-guide/#named-aggregation","text":"Named aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition . Purpose Named aggregation allows you to retrieve the aggregate values for different time durations. That is, it allows you to obtain aggregates such as sum , count , avg , min , max , count and distinctCount of stream attributes for durations such as sec , min , hour , etc. This is of considerable importance in many Analytics scenarios because aggregate values are often needed for several time periods. Furthermore, this ensures that the aggregations are not lost due to unexpected system failures because aggregates can be stored in different persistence stores . Syntax @store(type=\" store type \", ...) @purge(enable=\" true or false \",interval= purging interval ,purgeByShardIdEnabled=\" true or false \",@retentionPeriod( granularity = retention period , ...) ) define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; The above syntax includes the following: Item Description @store This annotation is used to refer to the data store where the calculated aggregate results are stored. This annotation is optional. When no annotation is provided, the data is stored in the in-memory store. @purge This annotation is used to configure purging in aggregation granularities. If this annotation is not provided, the default purging mentioned above is applied. If you want to disable automatic data purging, you can use this annotation as follows: '@purge(enable=false) /You should disable data purging if the aggregation query in included in the Siddhi application for read-only purposes. @retentionPeriod This annotation is used to specify the length of time the data needs to be retained when carrying out data purging. If this annotation is not provided, the default retention period is applied. aggregator name This specifies a unique name for the aggregation so that it can be referred when accessing aggregate results. input stream The stream that feeds the aggregation. Note! this stream should be already defined. group by attribute name The group by clause is optional. If it is included in a Siddhi application, aggregate values are calculated per each group by attribute. If it is not used, all the events are aggregated together. by timestamp attribute This clause is optional. This defines the attribute that should be used as the timestamp. If this clause is not used, the event time is used by default. The timestamp could be given as either a string or a long value. If it is a long value, the unix timestamp in milliseconds is expected (e.g. 1496289950000 ). If it is a string value, the supported formats are yyyy - MM - dd HH : mm : ss (if time is in GMT) and yyyy - MM - dd HH : mm : ss Z (if time is not in GMT), here the ISO 8601 UTC offset must be provided for Z . (e.g., +05:30 , -11:00 ). time periods Time periods can be specified as a range where the minimum and the maximum value are separated by three dots, or as comma-separated values. e.g., A range can be specified as sec...year where aggregation is done per second, minute, hour, day, month and year. Comma-separated values can be specified as min, hour. Skipping time durations (e.g., min, day where the hour duration is skipped) when specifying comma-separated values is supported only from v4.1.1 onwards Aggregation's granularity data holders are automatically purged every 15 minutes. When carrying out data purging, the retention period you have specified for each granularity in the named aggregation query is taken into account. The retention period defined for a granularity needs to be greater than or equal to its minimum retention period as specified in the table below. If no valid retention period is defined for a granularity, the default retention period (as specified in the table below) is applied. Granularity Default retention Minimum retention second 120 seconds 120 seconds minute 24 hours 120 minutes hour 30 days 25 hours day 1 year 32 days month All 13 month year All none Note Aggregation is carried out at calendar start times for each granularity with the GMT timezone Note The same aggregation can be defined in multiple Siddhi apps for joining, however, only one siddhi app should carry out the processing (i.e. the aggregation input stream should only feed events to one aggregation definition). Example This Siddhi Application defines an aggregation named TradeAggregation to calculate the average and sum for the price attribute of events arriving at the TradeStream stream. These aggregates are calculated per every time granularity in the second-year range. define stream TradeStream (symbol string, price double, volume long, timestamp long); @purge(enable='true', interval='10 sec',@retentionPeriod(sec='120 sec',min='24 hours',hours='30 days',days='1 year',months='all',years='all')) define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year;","title":"Named Aggregation"},{"location":"docs/query-guide/#distributed-aggregation","text":"Distributed Aggregation allows you to partially process aggregations in different shards. This allows Siddhi app in one shard to be responsible only for processing a part of the aggregation. However for this, all aggregations must be based on a common physical database(@store). Syntax @store(type=\" store type \", ...) @PartitionById define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; Following table includes the annotation to be used to enable distributed aggregation, Item Description @PartitionById If the annotation is given, then the distributed aggregation is enabled. Further this can be disabled by using enable element, @PartitionById(enable='false') . Further, following system properties are also available, System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yes false purgeByShardIdEnabled This allows user to enable/disable distributed aggregation purging considering the shardID for all aggregations running in one siddhi manager .(Available from v5.1.28) true/false Yes false Note ShardIds should not be changed after the first configuration in order to keep data consistency.","title":"Distributed Aggregation"},{"location":"docs/query-guide/#join-aggregation","text":"This allows a stream to retrieve calculated aggregate values from the aggregation. Note A join can also be performed with two streams , with a table and a stream, or with a stream against externally named windows . Syntax A join with aggregation is similer to the join with table , but with additional within and per clauses. from input stream join aggrigation on join condition within time range per time granularity select attribute name , attribute name , ... insert into output stream ; Apart from constructs of table join this includes the following. Please note that the 'on' condition is optional : Item Description within time range This allows you to specify the time interval for which the aggregate values need to be retrieved. This can be specified by providing the start and end time separated by a comma as string or long values, or by using the wildcard string specifying the data range. For details refer examples. per time granularity This specifies the time granularity by which the aggregate values must be grouped and returned. e.g., If you specify days , the retrieved aggregate values are grouped for each day within the selected time interval. within and per clauses also accept attribute values from the stream. The timestamp of the aggregations can be accessed through the AGG_TIMESTAMP attribute. Example Following aggregation definition will be used for the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select AGG_TIMESTAMP, symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) define stream StockStream (symbol string, value int); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; This query retrieves hourly aggregations within the day 2014-02-15 . define stream StockStream (symbol string, value int); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within \"2014-02-15 **:**:** +05:30\" per \"hours\" select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; This query retrieves all aggregations per perValue stream attribute within the time period between timestamps 1496200000000 and 1596434876000 . define stream StockStream (symbol string, value int, perValue string); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within 1496200000000L, 1596434876000L per S.perValue select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; Supported join types Aggregation join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the aggregation. The output is generated only if there is a matching event in the stream and the aggregation. Left outer join The left outer join operation allows you to join a stream on left side with a aggregation on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right aggregation by having null values for the attributes of the right aggregation. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a aggregation on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left aggregation.","title":"Join (Aggregation)"},{"location":"docs/query-guide/#named-window","text":"A named window is a window that can be shared across multiple queries. Events can be inserted to a named window from one or more queries and it can produce output events based on the named window type. Syntax The syntax for a named window is as follows: define window window name ( attribute name attribute type , attribute name attribute type , ... ) window type ( parameter , parameter , \u2026) event type ; The following parameters are configured in a table definition: Parameter Description window name The name of the window defined. ( PascalCase is used for window names as a convention.) attribute name The schema of the window is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . window type ( parameter , ...) The window type associated with the window and its parameters. output event type This is optional. Keywords such as current events , expired events and all events (the default) can be used to specify when the window output should be exposed. For more information, see Event Type . Examples Returning all output when events arrive and when events expire from the window. In this query, the event type is not specified. Therefore, it returns both current and expired events as the output. define window SensorWindow (name string, value float, roomNo int, deviceID string) timeBatch(1 second); Returning an output only when events expire from the window. In this query, the event type of the window is expired events . Therefore, it only returns the events that have expired from the window as the output. define window SensorWindow (name string, value float, roomNo int, deviceID string) timeBatch(1 second) output expired events; Operators on Named Windows The following operators can be performed on named windows.","title":"Named Window"},{"location":"docs/query-guide/#insert_1","text":"This allows events to be inserted into windows. This is similar to inserting events into streams. Syntax from input stream select attribute name , attribute name , ... insert into window To insert only events of a specific event type, add the current events , expired events or the all events keyword between insert and into keywords (similar to how it is done for streams). For more information, see Event Type . Example This query inserts all events from the TempStream stream to the OneMinTempWindow window. define stream TempStream(tempId string, temp double); define window OneMinTempWindow(tempId string, temp double) time(1 min); from TempStream select * insert into OneMinTempWindow;","title":"Insert"},{"location":"docs/query-guide/#join-window","text":"To allow a stream to retrieve information from a window based on a condition. Note A join can also be performed with two streams , aggregation or with tables tables . Syntax from input stream join window on condition select ( input stream | window ). attribute name , ( input stream | window ). attribute name , ... insert into output stream Example This Siddhi Application performs a join count the number of temperature events having more then 40 degrees within the last 2 minutes. define window TwoMinTempWindow (roomNo int, temp double) time(2 min); define stream CheckStream (requestId string); from CheckStream as C join TwoMinTempWindow as T on T.temp 40 select requestId, count(T.temp) as count insert into HighTempCountStream; Supported join types Window join supports following operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join two windows or a stream with a window. The output is generated only if there is a matching event in both stream/window. Left outer join The left outer join operation allows you to join two windows or a stream with a window to be merged based on a condition. Here, it returns all the events of left stream/window even if there are no matching events in the right stream/window by having null values for the attributes of the right stream/window. Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join two windows or a stream with a window. It returns all the events of the right stream/window even if there are no matching events in the left stream/window. Full outer join The full outer join combines the results of left outer join and right outer join . full outer join is used as the keyword to join two windows or a stream with a window. Here, output event are generated for each incoming event even if there are no matching events in the other stream/window.","title":"Join (Window)"},{"location":"docs/query-guide/#from","text":"A window can be an input to a query, similar to streams. Note !!! When window is used as an input to a query, another window cannot be applied on top of this. Syntax from window select attribute name , attribute name , ... insert into output stream Example This Siddhi Application calculates the maximum temperature within the last 5 minutes. define window FiveMinTempWindow (roomNo int, temp double) time(5 min); from FiveMinTempWindow select max(temp) as maxValue, roomNo insert into MaxSensorReadingStream;","title":"From"},{"location":"docs/query-guide/#trigger","text":"Triggers allow events to be periodically generated. Trigger definition can be used to define a trigger. A trigger also works like a stream with a predefined schema. Purpose For some use cases the system should be able to periodically generate events based on a specified time interval to perform some periodic executions. A trigger can be performed for a 'start' operation, for a given time interval , or for a given ' cron expression ' . Syntax The syntax for a trigger definition is as follows. define trigger trigger name at ('start'| every time interval | ' cron expression '); Similar to streams, triggers can be used as inputs. They adhere to the following stream definition and produce the triggered_time attribute of the long type. define stream trigger name (triggered_time long); The following types of triggeres are currently supported: Trigger type Description 'start' An event is triggered when Siddhi is started. every time interval An event is triggered periodically at the given time interval. ' cron expression ' An event is triggered periodically based on the given cron expression. For configuration details, see quartz-scheduler . Examples Triggering events regularly at specific time intervals The following query triggers events every 5 minutes. define trigger FiveMinTriggerStream at every 5 min; Triggering events at a specific time on specified days The following query triggers an event at 10.15 AM on every weekdays. define trigger FiveMinTriggerStream at '0 15 10 ? * MON-FRI';","title":"Trigger"},{"location":"docs/query-guide/#script","text":"Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Function definitions can be used to define these scripts. Function parameters are passed into the function logic as Object[] and with the name data . Purpose Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Syntax The syntax for a Script definition is as follows. define function function name [ language name ] return return type { operation of the function }; The following parameters are configured when defining a script. Parameter Description function name The name of the function ( camelCase is used for the function name) as a convention. language name The name of the programming language used to define the script, such as javascript , r and scala . return type The attribute type of the function\u2019s return. This can be int , long , float , double , string , bool or object . Here the function implementer should be responsible for returning the output attribute on the defined return type for proper functionality. operation of the function Here, the execution logic of the function is added. This logic should be written in the language specified under the language name , and it should return the output in the data type specified via the return type parameter. Examples This query performs concatenation using JavaScript, and returns the output as a string. define function concatFn[javascript] return string { var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var responce = str1 + str2 + str3; return responce; }; define stream TempStream(deviceID long, roomNo int, temp double); from TempStream select concatFn(roomNo,'-',deviceID) as id, temp insert into DeviceTempStream;","title":"Script"},{"location":"docs/query-guide/#store-query","text":"Siddhi store queries are a set of on-demand queries that can be used to perform operations on Siddhi tables, windows, and aggregators. Purpose Store queries allow you to execute the following operations on Siddhi tables, windows, and aggregators without the intervention of streams. Queries supported for tables: SELECT INSERT DELETE UPDATE UPDATE OR INSERT Queries supported for windows and aggregators: SELECT This is be done by submitting the store query to the Siddhi application runtime using its query() method. In order to execute store queries, the Siddhi application of the Siddhi application runtime you are using, should have a store defined, which contains the table that needs to be queried. Example If you need to query the table named RoomTypeTable the it should have been defined in the Siddhi application. In order to execute a store query on RoomTypeTable , you need to submit the store query using query() method of SiddhiAppRuntime instance as below. siddhiAppRuntime.query( store query );","title":"Store Query"},{"location":"docs/query-guide/#tablewindow-select","text":"The SELECT store query retrieves records from the specified table or window, based on the given condition. Syntax from table/window on condition ? select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example This query retrieves room numbers and types of the rooms starting from room no 10. from roomTypeTable on roomNo = 10; select roomNo, type","title":"(Table/Window) Select"},{"location":"docs/query-guide/#aggregation-select","text":"The SELECT store query retrieves records from the specified aggregation, based on the given condition, time range, and granularity. Syntax from aggregation on condition ? within time range per time granularity select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example Following aggregation definition will be used for the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) from TradeAggregation within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select symbol, total, avgPrice ; This query retrieves hourly aggregations of \"FB\" symbol within the day 2014-02-15 . from TradeAggregation on symbol == \"FB\" within \"2014-02-15 **:**:** +05:30\" per \"hours\" select symbol, total, avgPrice;","title":"(Aggregation) Select"},{"location":"docs/query-guide/#insert_2","text":"This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax select attribute name , attribute name , ... insert into table ; Example This store query inserts a new record to the table RoomOccupancyTable , with the specified attribute values. select 10 as roomNo, 2 as people insert into RoomOccupancyTable","title":"Insert"},{"location":"docs/query-guide/#delete_1","text":"The DELETE store query deletes selected records from a specified table. Syntax select ? delete table on conditional expresssion The condition element specifies the basis on which records are selected to be deleted. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example In this example, query deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection which has 10 as the actual value. select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; delete RoomTypeTable on RoomTypeTable.roomNo == 10;","title":"Delete"},{"location":"docs/query-guide/#update_1","text":"The UPDATE store query updates selected attributes stored in a specific table, based on a given condition. Syntax select attribute name , attribute name , ...? update table set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition The condition element specifies the basis on which records are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query updates the room occupancy by increasing the value of people by 1, in the RoomOccupancyTable table for each room number greater than 10. select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber; update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + 1 on RoomTypeTable.roomNo == 10;","title":"Update"},{"location":"docs/query-guide/#update-or-insert_1","text":"This allows you to update selected attributes if a record that meets the given conditions already exists in the specified table. If a matching record does not exist, the entry is inserted as a new record. Syntax select attribute name , attribute name , ... update or insert into table set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which records are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query tries to update the records in the RoomAssigneeTable table that have room numbers that match the same in the selection. If such records are not found, it inserts a new record based on the values provided in the selection. select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;","title":"Update or Insert"},{"location":"docs/query-guide/#extensions","text":"Siddhi supports an extension architecture to enhance its functionality by incorporating other libraries in a seamless manner. Purpose Extensions are supported because, Siddhi core cannot have all the functionality that's needed for all the use cases, mostly use cases require different type of functionality, and for some cases there can be gaps and you need to write the functionality by yourself. All extensions have a namespace. This is used to identify the relevant extensions together, and to let you specifically call the extension. Syntax Extensions follow the following syntax; namespace : function name ( parameter , parameter , ... ) The following parameters are configured when referring a script function. Parameter Description namespace Allows Siddhi to identify the extension without conflict function name The name of the function referred. parameter The function input parameter for function execution. Extension Types Siddhi supports following extension types: Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute. This can be used to manipulate existing event attributes to generate new attributes like any Function operation. This is implemented by extending io.siddhi.core.executor.function.FunctionExecutor . Example : math:sin(x) Here, the sin function of math extension returns the sin value for the x parameter. Aggregate Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute with aggregated results. This can be used in conjunction with a window in order to find the aggregated results based on the given window like any Aggregate Function operation. This is implemented by extending io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor . Example : custom:std(x) Here, the std aggregate function of custom extension returns the standard deviation of the x value based on its assigned window query. Window This allows events to be collected, generated, dropped and expired anytime without altering the event format based on the given input parameters, similar to any other Window operator. This is implemented by extending io.siddhi.core.query.processor.stream.window.WindowProcessor . Example : custom:unique(key) Here, the unique window of the custom extension retains one event for each unique key parameter. Stream Function This allows events to be generated or dropped only during event arrival and altered by adding one or more attributes to it. This is implemented by extending io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor . Example : custom:pol2cart(theta,rho) Here, the pol2cart function of the custom extension returns all the events by calculating the cartesian coordinates x y and adding them as new attributes to the events. Stream Processor This allows events to be collected, generated, dropped and expired anytime by altering the event format by adding one or more attributes to it based on the given input parameters. Implemented by extending io.siddhi.core.query.processor.stream.StreamProcessor . Example : custom:perMinResults( parameter , parameter , ...) Here, the perMinResults function of the custom extension returns all events by adding one or more attributes to the events based on the conversion logic. Altered events are output every minute regardless of event arrivals. Sink Sinks provide a way to publish Siddhi events to external systems in the preferred data format. Sinks publish events from the streams via multiple transports to external endpoints in various data formats. Implemented by extending io.siddhi.core.stream.output.sink.Sink . Example : @sink(type='sink_type', static_option_key1='static_option_value1') To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as above Source Source allows Siddhi to consume events from external systems , and map the events to adhere to the associated stream. Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. Implemented by extending io.siddhi.core.stream.input.source.Source . Example : @source(type='source_type', static.option.key1='static_option_value1') To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as above Store You can use Store extension type to work with data/events stored in various data stores through the table abstraction . You can find more information about these extension types under the heading 'Extension types' in this document. Implemented by extending io.siddhi.core.table.record.AbstractRecordTable . Script Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Implemented by extending io.siddhi.core.function.Script . Source Mapper Each @source configuration has a mapping denoted by the @map annotation that converts the incoming messages format to Siddhi events .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SourceMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Sink Mapper Each @sink configuration has a mapping denoted by the @map annotation that converts the outgoing Siddhi events to configured messages format .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SinkMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Example A window extension created with namespace foo and function name unique can be referred as follows: from StockExchangeStream[price = 20]#window.foo:unique(symbol) select symbol, price insert into StockQuote Available Extensions Siddhi currently has several pre written extensions that are available here We value your contribution on improving Siddhi and its extensions further.","title":"Extensions"},{"location":"docs/query-guide/#writing-custom-extensions","text":"Custom extensions can be written in order to cater use case specific logic that are not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension types and the related maven archetypes are given below. You can use these archetypes to generate Maven projects for each extension type. Follow the procedure for the required archetype, based on your project: Note When using the generated archetype please make sure you complete the @Extension annotation with proper values. This annotation will be used to identify and document the extension, hence your extension will not work without @Extension annotation. siddhi-execution Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Extension Types . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DgroupId=io.siddhi.extension.execution -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfFunction Name of the custom function to be created Y - _nameSpaceOfFunction Namespace of the function, used to grouped similar custom functions Y - groupIdPostfix Namespace of the function is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-execution- classNameOfAggregateFunction Class name of the Aggregate Function N $ classNameOfFunction Class name of the Function N $ classNameOfStreamFunction Class name of the Stream Function N $ classNameOfStreamProcessor Class name of the Stream Processor N $ classNameOfWindow Class name of the Window N $ To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-io Siddhi-io provides following extension types: Sink Source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: * The Source extension type gets inputs to your Siddhi application. * The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DgroupId=io.siddhi.extension.io -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _IOType Type of IO for which Siddhi-io extension is written Y - groupIdPostfix Type of the IO is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-io- classNameOfSink Class name of the Sink N classNameOfSource Class name of the Source N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-map Siddhi-map provides following extension types, Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints (such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DgroupId=io.siddhi.extension.map -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _mapType Type of Mapper for which Siddhi-map extension is written Y - groupIdPostfix Type of the Map is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-map- classNameOfSinkMapper Class name of the Sink Mapper N classNameOfSourceMapper Class name of the Source Mapper N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-script Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Extension Types . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DgroupId=io.siddhi.extension.script -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfScript Name of Custom Script for which Siddhi-script extension is written Y - groupIdPostfix Name of the Script is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-script- classNameOfScript Class name of the Script N Eval To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-store Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Extension Types . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DgroupId=io.siddhi.extension.store -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _storeType Type of Store for which Siddhi-store extension is written Y - groupIdPostfix Type of the Store is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-store- className Class name of the Store N To confirm that all property values are correct, type Y in the console. If not, press N .","title":"Writing Custom Extensions"},{"location":"docs/query-guide/#configuring-and-monitoring-siddhi-applications","text":"","title":"Configuring and Monitoring Siddhi Applications"},{"location":"docs/query-guide/#multi-threading-and-asynchronous-processing","text":"When @Async annotation is added to the Streams it enable the Streams to introduce asynchronous and multi-threading behaviour. @Async(buffer.size='256', workers='2', batch.size.max='5') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following elements are configured with this annotation. Annotation Description Default Value buffer.size The size of the event buffer that will be used to handover the execution to other threads. - workers Number of worker threads that will be be used to process the buffered events. 1 batch.size.max The maximum number of events that will be processed together by a worker thread at a given time. buffer.size","title":"Multi-threading and Asynchronous Processing"},{"location":"docs/query-guide/#statistics","text":"Use @app:statistics app level annotation to evaluate the performance of an application, you can enable the statistics of a Siddhi application to be published. This is done via the @app:statistics annotation that can be added to a Siddhi application as shown in the following example. @app:statistics(reporter = 'console') The following elements are configured with this annotation. Annotation Description Default Value reporter The interface in which statistics for the Siddhi application are published. Possible values are as follows: console jmx console interval The time interval (in seconds) at which the statistics for the Siddhi application are reported. 60 include If this parameter is added, only the types of metrics you specify are included in the reporting. The required metric types can be specified as a comma-separated list. It is also possible to use wild cards All ( . ) The metrics are reported in the following format. io.siddhi.SiddhiApps. SiddhiAppName .Siddhi. Component Type . Component Name . Metrics name The following table lists the types of metrics supported for different Siddhi application component types. Component Type Metrics Type Stream Throughput The size of the buffer if parallel processing is enabled via the @async annotation. Trigger Throughput (Trigger and Stream) Source Throughput Sink Throughput Mapper Latency Input/output throughput Table Memory Throughput (For all operations) Throughput (For all operations) Query Memory Latency Window Throughput (For all operations) Latency (For all operation) Partition Throughput (For all operations) Latency (For all operation) e.g., the following is a Siddhi application that includes the @app annotation to report performance statistics. @App:name('TestMetrics') @App:Statistics(reporter = 'console') define stream TestStream (message string); @info(name='logQuery') from TestSream#log(\"Message:\") insert into TempSream; Statistics are reported for this Siddhi application as shown in the extract below. Click to view the extract 11/26/17 8:01:20 PM ============================================================ -- Gauges ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.memory value = 5760 io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.size value = 0 -- Meters ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Sources.TestStream.http.throughput count = 0 mean rate = 0.00 events/second 1-minute rate = 0.00 events/second 5-minute rate = 0.00 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TempSream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second -- Timers ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.latency count = 2 mean rate = 0.11 calls/second 1-minute rate = 0.34 calls/second 5-minute rate = 0.39 calls/second 15-minute rate = 0.40 calls/second min = 0.61 milliseconds max = 1.08 milliseconds mean = 0.84 milliseconds stddev = 0.23 milliseconds median = 0.61 milliseconds 75% < = 1.08 milliseconds 95% < = 1.08 milliseconds 98% < = 1.08 milliseconds 99% < = 1.08 milliseconds 99.9% < = 1.08 milliseconds","title":"Statistics"},{"location":"docs/query-guide/#event-playback","text":"When @app:playback annotation is added to the app, the timestamp of the event (specified via an attribute) is treated as the current time. This results in events being processed faster. The following elements are configured with this annotation. Annotation Description idle.time If no events are received during a time interval specified (in milliseconds) via this element, the Siddhi system time is incremented by a number of seconds specified via the increment element. increment The number of seconds by which the Siddhi system time must be incremented if no events are received during the time interval specified via the idle.time element. e.g., In the following example, the Siddhi system time is incremented by two seconds if no events arrive for a time interval of 100 milliseconds. @app:playback(idle.time = '100 millisecond', increment = '2 sec')","title":"Event Playback"},{"location":"docs/siddhi-as-a-docker-microservice/","text":"Siddhi 5.2 as a Docker Microservice This section provides information on running Siddhi Apps on Docker. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Docker Microservice is as follows. Pull the the latest Siddhi Runner image from Siddhiio Docker Hub . docker pull siddhiio/siddhi-runner-alpine:latest Start SiddhiApps with the runner config by executing the following docker command. docker run -it -v local-siddhi-file-path : siddhi-file-mount-path -v local-conf-file-path : conf-file-mount-path siddhiio/siddhi-runner-alpine:latest -Dapps= siddhi-file-mount-path -Dconfig= conf-file-mount-path E.g., docker run -it -v /home/me/siddhi-apps:/apps -v /home/me/siddhi-configs:/configs siddhiio/siddhi-runner-alpine:latest -Dapps=/apps/Foo.siddhi -Dconfig=/configs/siddhi-config.yaml Running multiple SiddhiApps in one runner instance. To run multiple SiddhiApps in one runtime instance, have all SiddhiApps in a directory, mount the directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Docker Microservice refer Siddhi Config Guide . Samples Running Siddhi App Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Always listen on 0.0.0.0 with the Siddhi Application running inside a docker container. If you listen on localhost inside the container, nothing outside the container can connect to your application. That includes blocking port forwarding from the docker host and container to container networking. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /CountOverTime.siddhi:/apps/CountOverTime.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false} Running with runner config When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can be configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following command docker run -it -p 8006:8006 -p 9443:9443 -v local-absolute-siddhi-file-path /ConsumeAndStore.siddhi:/apps/ConsumeAndStore.siddhi -v local-absolute-config-yaml-path /TestDb.yaml:/conf/TestDb.yaml siddhiio/siddhi-runner-alpine -Dapps=/apps/ConsumeAndStore.siddhi -Dconfig=/conf/TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] } Running with environmental/system variables Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set the below environment variables by passing them during the docker run command: THRESHOLD=20 TO_EMAIL= to email address EMAIL_ADDRESS= gmail address EMAIL_USERNAME= gmail username EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding them to the end of the docker run command . -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password Run the SiddhiApp by executing following command. docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /TemplatedFilterAndEmail.siddhi:/apps/TemplatedFilterAndEmail.siddhi -v local-absolute-config-yaml-path /EmailConfig.yaml:/conf/EmailConfig.yaml -e THRESHOLD=20 -e TO_EMAIL= to email address -e EMAIL_ADDRESS= gmail address -e EMAIL_USERNAME= gmail username -e EMAIL_PASSWORD= gmail password siddhiio/siddhi-runner-alpine -Dapps=/apps/TemplatedFilterAndEmail.siddhi -Dconfig=/conf/EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Siddhi Docker Microservice"},{"location":"docs/siddhi-as-a-docker-microservice/#siddhi-52-as-a-docker-microservice","text":"This section provides information on running Siddhi Apps on Docker. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Docker Microservice is as follows. Pull the the latest Siddhi Runner image from Siddhiio Docker Hub . docker pull siddhiio/siddhi-runner-alpine:latest Start SiddhiApps with the runner config by executing the following docker command. docker run -it -v local-siddhi-file-path : siddhi-file-mount-path -v local-conf-file-path : conf-file-mount-path siddhiio/siddhi-runner-alpine:latest -Dapps= siddhi-file-mount-path -Dconfig= conf-file-mount-path E.g., docker run -it -v /home/me/siddhi-apps:/apps -v /home/me/siddhi-configs:/configs siddhiio/siddhi-runner-alpine:latest -Dapps=/apps/Foo.siddhi -Dconfig=/configs/siddhi-config.yaml Running multiple SiddhiApps in one runner instance. To run multiple SiddhiApps in one runtime instance, have all SiddhiApps in a directory, mount the directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Docker Microservice refer Siddhi Config Guide .","title":"Siddhi 5.2 as a Docker Microservice"},{"location":"docs/siddhi-as-a-docker-microservice/#samples","text":"","title":"Samples"},{"location":"docs/siddhi-as-a-docker-microservice/#running-siddhi-app","text":"Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Always listen on 0.0.0.0 with the Siddhi Application running inside a docker container. If you listen on localhost inside the container, nothing outside the container can connect to your application. That includes blocking port forwarding from the docker host and container to container networking. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /CountOverTime.siddhi:/apps/CountOverTime.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false}","title":"Running Siddhi App"},{"location":"docs/siddhi-as-a-docker-microservice/#running-with-runner-config","text":"When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can be configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following command docker run -it -p 8006:8006 -p 9443:9443 -v local-absolute-siddhi-file-path /ConsumeAndStore.siddhi:/apps/ConsumeAndStore.siddhi -v local-absolute-config-yaml-path /TestDb.yaml:/conf/TestDb.yaml siddhiio/siddhi-runner-alpine -Dapps=/apps/ConsumeAndStore.siddhi -Dconfig=/conf/TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] }","title":"Running with runner config"},{"location":"docs/siddhi-as-a-docker-microservice/#running-with-environmentalsystem-variables","text":"Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set the below environment variables by passing them during the docker run command: THRESHOLD=20 TO_EMAIL= to email address EMAIL_ADDRESS= gmail address EMAIL_USERNAME= gmail username EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding them to the end of the docker run command . -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password Run the SiddhiApp by executing following command. docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /TemplatedFilterAndEmail.siddhi:/apps/TemplatedFilterAndEmail.siddhi -v local-absolute-config-yaml-path /EmailConfig.yaml:/conf/EmailConfig.yaml -e THRESHOLD=20 -e TO_EMAIL= to email address -e EMAIL_ADDRESS= gmail address -e EMAIL_USERNAME= gmail username -e EMAIL_PASSWORD= gmail password siddhiio/siddhi-runner-alpine -Dapps=/apps/TemplatedFilterAndEmail.siddhi -Dconfig=/conf/EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Running with environmental/system variables"},{"location":"docs/siddhi-as-a-java-library/","text":"Siddhi 5.2 as a Java library Siddhi can be used as a library in any Java program (including in OSGi runtimes) just by adding Siddhi and its extension jars as dependencies. Find a sample Siddhi project that's implemented as a Java program using Maven here , this can be used as a reference for any based implementation. Following are the mandatory dependencies that need to be added to the Maven pom.xml file (or to the program classpath). dependency groupId io.siddhi /groupId artifactId siddhi-core /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-api /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-compiler /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-annotations /artifactId version 5.x.x /version /dependency Sample Sample Java class using Siddhi is as follows.","title":"Siddhi Java library"},{"location":"docs/siddhi-as-a-java-library/#siddhi-52-as-a-java-library","text":"Siddhi can be used as a library in any Java program (including in OSGi runtimes) just by adding Siddhi and its extension jars as dependencies. Find a sample Siddhi project that's implemented as a Java program using Maven here , this can be used as a reference for any based implementation. Following are the mandatory dependencies that need to be added to the Maven pom.xml file (or to the program classpath). dependency groupId io.siddhi /groupId artifactId siddhi-core /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-api /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-compiler /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-annotations /artifactId version 5.x.x /version /dependency","title":"Siddhi 5.2 as a Java library"},{"location":"docs/siddhi-as-a-java-library/#sample","text":"Sample Java class using Siddhi is as follows.","title":"Sample"},{"location":"docs/siddhi-as-a-kubernetes-microservice/","text":"Siddhi 5.2 as a Kubernetes Microservice This section provides information on running Siddhi Apps natively in Kubernetes via Siddhi Kubernetes Operator. Siddhi can be configured using SiddhiProcess kind and passed to the Siddhi operator for deployment. Here, the Siddhi applications containing stream processing logic can be written inline in SiddhiProcess yaml or passed as .siddhi files via contig maps. SiddhiProcess yaml can also be configured with the necessary system configurations. Prerequisites A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster Distributed deployment of Siddhi apps need NATS operator and NATS streaming operator . Admin privileges to install Siddhi operator Minikube Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine (GKE) Cluster To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \"your-address@email.com\" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com Docker for Mac Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation . Port Forwarding for Testing Debugging Purposes Instead of creating ingress you can enable port forwarding ( kubectl port-forward ) to access the application in the Kubernetes cluster. This will help a lot for TCP connections as well. kubectl port-forward svc/mysql-db 13306:3306 For more details please refer this Kubernetes official documentation Install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-beta/00-prereqs.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-beta/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE siddhi-operator 1 1 1 1 1m Using a custom-built Siddhi runner image If you need to use a custom-built siddhi-runner image for all the SiddhiProcess deployments, you have to configure siddhiRunnerImage entry in siddhi-operator-config config map. Refer the documentation on creating custom Siddhi runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as siddhiRunnerImageSecret entry in siddhi-operator-config config map. For more details on using docker images from private registries/repositories refer this documentation . Deploy and run Siddhi App Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Here we will be creating a very simple Siddhi stream processing application that receives power consumption from several devices in a house. If the power consumption of dryer exceeds the consumption limit of 6000W then that Siddhi app sends an alert from printing a log. This can be created using a SiddhiProcess YAML file as given below. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-surge-app spec: apps: - script: | @App:name(\"PowerSurgeDetection\") @App:description(\"App consume events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='power-filter') from DevicePowerStream[deviceType == 'dryer' and power = 600] select deviceType, power insert into PowerSurgeAlertStream; container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0-beta\" Always listen on 0.0.0.0 with the Siddhi Application running inside a container environment. If you listen on localhost inside the container, nothing outside the container can connect to your application. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Kubernetes Microservice refer Siddhi Config Guide . To deploy the above Siddhi app in your Kubernetes cluster, copy above YAML to a file with name power-surge-app.yaml and execute the following command. kubectl create -f absolute-yaml-file-path /power-surge-app.yaml TLS secret Within the SiddhiProcess, a TLS secret named siddhi-tls is configured. If a Kubernetes secret with the same name does not exist in the Kubernetes cluster, the NGINX will ignore it and use a self-generated certificate. Configuring a secret will be necessary for calling HTTPS endpoints, refer deploy and run Siddhi apps with HTTPS section for more details. If the power-surge-app is deployed successfully, it should create SiddhiProcess, deployment, service, and ingress as following. $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-surge-app Running 1/1 2m $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE power-surge-app-0 1/1 1 1 2m siddhi-operator 1/1 1 1 2m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 2d power-surge-app-0 ClusterIP 10.96.44.182 none 8080/TCP 2m siddhi-operator ClusterIP 10.98.78.238 none 8383/TCP 2m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 2m Using a custom-built Siddhi runner image If you need to use a custom-built siddhi-runner image for a specific SiddhiProcess deployment, you have to configure container.image spec in the power-surge-app.yaml . Refer the documentation on creating custom Siddhi runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as imagePullSecret argument in the power-surge-app.yaml file. For more details on using docker images from private registries/repositories refer this documentation . Invoke Siddhi Applications To invoke the Siddhi App, obtain the external IP of the ingress load balancer using kubectl get ingress command as following. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 2m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Docker for Mac For Docker for Mac, you have to use 0.0.0.0 as the external IP. Use the following CURL command to send events to power-surge-app deployed in Kubernetes. curl -X POST \\ http://siddhi/power-surge-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' View Siddhi Process Logs Since the output of power-surge-app is logged, you can see the output by monitoring the associated pod's logs. To find the power-surge-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-rxzkq 1/1 Running 0 4m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 4m Here, the pod starting with the SiddhiProcess name (in this case power-surge-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs power-surge-app-0-646c4f9dd5-rxzkq ... [2019-07-12 07:12:48,925] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 07:12:48,927] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 07:12:48,941] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 6.853 sec [2019-07-12 07:17:22,219] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562915842182, data=[dryer, 60000], isExpired=false} Get Siddhi process status List Siddhi processes List the Siddhi process using the kubectl get sps or kubectl get SiddhiProcesses commands as follows. $ kubectl get sps NAME STATUS READY AGE power-surge-app Running 1/1 5m $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-surge-app Running 1/1 5m View Siddhi process configs Describe the Siddhi process configuration details using kubectl describe sp command as follows. $ kubectl describe sp power-surge-app Name: power-surge-app Namespace: default Labels: none Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"siddhi.io/v1alpha2\",\"kind\":\"SiddhiProcess\",\"metadata\":{\"annotations\":{},\"name\":\"power-surge-app\",\"namespace\":\"default\"},\"spec\":{\"apps\":[... API Version: siddhi.io/v1alpha2 Kind: SiddhiProcess Metadata: Creation Timestamp: 2019-07-12T07:12:35Z Generation: 1 Resource Version: 148205 Self Link: /apis/siddhi.io/v1alpha2/namespaces/default/siddhiprocesses/power-surge-app UID: 6c6d90a4-a474-11e9-a05b-080027f4eb25 Spec: Apps: Script: @App:name(\"PowerSurgeDetection\") @App:description(\"App consume events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='power-filter') from DevicePowerStream[deviceType == 'dryer' and power = 600] select deviceType, power insert into PowerSurgeAlertStream; Container: Env: Name: RECEIVER_URL Value: http://0.0.0.0:8080/checkPower Name: BASIC_AUTH_ENABLED Value: false Image: siddhiio/siddhi-runner-ubuntu:5.1.0-beta Status: Nodes: nil Ready: 1/1 Status: Running Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal DeploymentCreated 11m siddhiprocess-controller power-surge-app-0 deployment created successfully Normal ServiceCreated 11m siddhiprocess-controller power-surge-app-0 service created successfully View Siddhi process logs To view the Siddhi process logs, first get the Siddhi process pods using the kubectl get pods command as follows. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-rxzkq 1/1 Running 0 4m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 4m Then to retrieve the Siddhi process logs, run kubectl logs pod name command. Here pod name should be replaced with the name of the pod that starts with the relevant SiddhiProcess's name. A sample output logs are of this command is as follows. $ kubectl logs power-surge-app-0-646c4f9dd5-rxzkq ... [2019-07-12 07:12:48,925] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 07:12:48,927] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 07:12:48,941] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 6.853 sec [2019-07-12 07:17:22,219] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562915842182, data=[dryer, 60000], isExpired=false} Change the Default Configurations of Siddhi Runner Siddhi runner use SIDDHI_RUNNER_HOME /conf/runner/deployment.yaml file as the default configuration file. In the deployment.yaml the file you can configure data sources that you planned to use, add refs, and enable state persistence, etc. To change the configurations of the deployment.yaml , you can add runner YAML spec like below to your SiddhiProcess YAML file. For example, the following config change will enable file system state persistence. runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Deploy and run Siddhi App using config maps Siddhi operator allows you to deploy Siddhi app configurations via config maps instead of just adding them inline. Through this, you can also run multiple Siddhi Apps in a single SiddhiProcess. This can be done by passing the config maps containing Siddhi app files to the SiddhiProcess's apps configuration as follows. apps: - configMap: power-surge-cm1 - configMap: power-surge-cm2 Sample on deploying and running Siddhi Apps via config maps Here we will be creating a very simple Siddhi stream processing application that receives power consumption from several devices in a house. If the power consumption of dryer exceeds the consumption limit of 6000W then that Siddhi app sends an alert from printing a log. @App:name(\"PowerSurgeDetection\") @App:description(\"App consume events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='power-filter') from DevicePowerStream[deviceType == 'dryer' and power = 600] select deviceType, power insert into PowerSurgeAlertStream; Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Save the above Siddhi App file as PowerSurgeDetection.siddhi , and use this file to create a Kubernetes config map with the name power-surge-cm . This can be achieved by running the following command. kubectl create configmap power-surge-cm --from-file= absolute-file-path /PowerSurgeDetection.siddhi The created config map can be added to SiddhiProcess YAML under the apps entry as follows. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-surge-app spec: apps: - configMap: power-surge-cm container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0-beta\" Save the YAML file as power-surge-app.yaml , and use the following command to deploy the SiddhiProcess. kubectl create -f absolute-yaml-file-path /power-surge-app.yaml Using a config, created from a directory containing multiple Siddhi files SiddhiProcess's apps.configMap configuration also supports a config map that is created from a directory containing multiple Siddhi files. Use kubectl create configmap siddhi-apps --from-file= DIRECTORY_PATH command to create a config map from a directory. Invoke Siddhi Applications To invoke the Siddhi App, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 2m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to power-surge-app deployed in Kubernetes. curl -X POST \\ http://siddhi/power-surge-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -H 'cache-control: no-cache' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' View Siddhi Process Logs Since the output of power-surge-app is logged, you can see the output by monitoring the associated pod's logs. To find the power-surge-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-tns7l 1/1 Running 0 2m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 8m Here, the pod starting with the SiddhiProcess name (in this case power-surge-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs power-surge-app-0-646c4f9dd5-tns7l ... [2019-07-12 07:50:32,861] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 8.048 sec [2019-07-12 07:50:32,864] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 07:50:32,866] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 07:51:42,488] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562917902484, data=[dryer, 60000], isExpired=false} Deploy Siddhi Apps without Ingress creation By default, Siddhi operator creates an NGINX ingress and exposes your HTTP/HTTPS through that ingress. If you need to disable automatic ingress creation, you have to change the autoIngressCreation value in the Siddhi siddhi-operator-config config map to false or null as below. # This config map used to parse configurations to the Siddhi operator. apiVersion: v1 kind: ConfigMap metadata: name: siddhi-operator-config data: siddhiHome: /home/siddhi_user/siddhi-runner/ siddhiProfile: runner siddhiImage: siddhiio/siddhi-runner-alpine:5.1.0-beta autoIngressCreation: \"false\" Deploy and run Siddhi App with HTTPS Configuring TLS will allow Siddhi ingress NGINX to expose HTTPS endpoints of your Siddhi Apps. To do so, create a Kubernetes secret( siddhi-tls ) and add that to the TLS configuration in siddhi-operator-config config map as given below. ingressTLS: siddhi-tls Sample on deploying and running Siddhi App with HTTPS First, you need to create a certificate using the following commands. For more details about the certificate creation refers this . openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout siddhi.key -out siddhi.crt -subj \"/CN=siddhi/O=siddhi\" After that, create a kubernetes secret called siddhi-tls , which we intended to add to the TLS configurations using the following command. kubectl create secret tls siddhi-tls --key siddhi.key --cert siddhi.crt The created secret then need to be added to the siddhi-operator-config config map as follow. apiVersion: v1 kind: ConfigMap metadata: name: siddhi-operator-config data: siddhiHome: /home/siddhi_user/siddhi-runner/ siddhiProfile: runner siddhiImage: siddhiio/siddhi-runner-ubuntu:5.1.0-beta autoIngressCreation: \"true\" ingressTLS: siddhi-tls When this is done Siddhi operator will now enable TLS support via the NGINX ingress, and you will be able to access all the HTTPS endpoints. Invoke Siddhi Applications You can use now send the events to following HTTPS endpoint. https://siddhi/power-surge-app-0/8080/checkPower Further, you can use the following CURL command to send a request to the deployed Siddhi applications via HTTPS. curl --cacert siddhi.crt -X POST \\ https://siddhi/power-surge-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -H 'cache-control: no-cache' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' View Siddhi Process Logs The output logs show the event that you sent using the previous CURL command. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-kk5md 1/1 Running 0 2m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 10m $ kubectl logs monitor-app-667c97c898-rrtfs ... [2019-07-12 09:06:15,173] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 09:06:15,184] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 09:06:15,187] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 10.819 sec [2019-07-12 09:07:50,098] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562922470093, data=[dryer, 60000], isExpired=false} Deploy and Run Siddhi App with TCP Endpoints The default ingress creation of the Siddhi operator allows accessing HTTP/HTTPS endpoints externally. By default, it will not support TCP endpoints. Sometimes you may have some TCP endpoints to configure like NATS and Kafka sources and access those endpoints externally. @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); To access these TCP connections externally you can do it as in the following example. First, you have to disable automatic ingress creation in the Siddhi operator . Then you have to manually create ingress and enable the TCP configurations. To enable TCP configurations in NGINX ingress please refer to this documentation . To create NATS cluster you will need a NATS spec like below. apiVersion: nats.io/v1alpha2 kind: NatsCluster metadata: name: nats-siddhi spec: size: 1 Save this yaml as nats-cluster.yaml and deploy it using kubeclt . $ kubeclt apply -f nats-cluster.yaml Likewise, create a nats streaming cluster as below. apiVersion: streaming.nats.io/v1alpha1 kind: NatsStreamingCluster metadata: name: stan-siddhi spec: size: 1 natsSvc: nats-siddhi Save this yaml as stan-cluster.yaml and deploy it using kubeclt . $ kubeclt apply -f stan-cluster.yaml Now you can deploy the following Siddhi app that contained a NATS source. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-consume-app spec: apps: - script: | @App:name(\"PowerConsumptionSurgeDetection\") @App:description(\"App consumes events from NATS as a text message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power consumption in 1 minute is greater than or equal to 10000W by printing a message in the log for every 30 seconds.\") /* Input: deviceType string and powerConsuption int(Joules) Output: Alert user from printing a log, if there is a power surge in the dryer within 1 minute period. Notify the user in every 30 seconds when total power consumption is greater than or equal to 10000W in 1 minute time period. */ @source( type='nats', cluster.id='siddhi-stan', destination = 'PowerStream', bootstrap.servers='nats://siddhi-nats:4222', @map(type='text') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, powerConsumed long); @info(name='surge-detector') from DevicePowerStream#window.time(1 min) select deviceType, sum(power) as powerConsumed group by deviceType having powerConsumed 10000 output every 30 sec insert into PowerSurgeAlertStream; container: image: \"siddhiio/siddhi-runner-ubuntu:5.1.0-beta\" persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard volumeMode: Filesystem runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Save this yaml as power-consume-app.yaml and deploy it using kubeclt . $ kubeclt apply -f power-consume-app.yaml This commands will create Kubernetes artifacts like below. $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 12d power-consume-app-0 ClusterIP 10.99.148.217 none 4222/TCP 5m siddhi-nats ClusterIP 10.105.250.215 none 4222/TCP 5m siddhi-nats-mgmt ClusterIP None none 6222/TCP,8222/TCP,7777/TCP 5m siddhi-operator ClusterIP 10.102.251.237 none 8383/TCP 5m $ kubectl get pods NAME READY STATUS RESTARTS AGE nats-operator-b8f4977fc-8gnjd 1/1 Running 0 5m nats-streaming-operator-64b565bcc7-r9rpw 1/1 Running 0 5m power-consume-app-0-84f6774bd8-jl95w 1/1 Running 0 5m siddhi-nats-1 1/1 Running 0 5m siddhi-operator-6c6c5d8fcc-hvl7j 1/1 Running 0 5m siddhi-stan-1 1/1 Running 0 5m Now you have to create an ingress for the siddhi-nats service. apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: siddhi-nats annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /nats backend: serviceName: siddhi-nats servicePort: 4222 Save this yaml as siddhi-nats.yaml and deploy it using kubeclt . $ kubeclt apply -f siddhi-nats.yaml Now you can send messages directly to the NATS streaming server that running on your Kubernetes cluster. You have to send those messages to nats:// KUBERNETES_CLUSTER_IP :4222 URI. To send messages to this NATS streaming cluster you can use a Siddhi app that has NATS sink or samples provided by NATS. Minikube External TCP Access The TCP configuration change that described in the ingress NGINX documentation occurred connection refused problems in Minikube. To configure TCP external access properly in Minikube please refer to the steps described in this comment . Deploy and run Siddhi App in Distributed Mode Siddhi apps can be in two different types. Stateless Siddhi apps Stateful Siddhi apps The deployment of the stateful Siddhi apps follows distributed architecture to ensure high availability. The fully distributed scenario of Siddhi deployments handle using Siddhi distributed annotations . Without Messaging System With Messaging System Without Distributed Annotations Case 1 : The given Siddhi app will be deployed in a stateless mode in a single kubernetes deployment. Case 2 : If given Siddhi app contains stateful queries then the Siddhi app divided into two partial Siddhi apps (passthrough and process) and deployed in two kubernetes deployments. Use the configured messaging system to communicate between two apps. With Distributed Annotations Case 3 : WIP(Work In Progress) Case 4 : WIP(Work In Progress) The previously described Siddhi app deployments fall under this Case 1 category. The following sample will cover the Siddhi app deployments which fall under Case 2. Sample on deploying and running Siddhi App with a Messaging System The Siddhi operator currently supports NATS as the messaging system. Therefore it is prerequisite to deploying NATS operator and NATS streaming operator in your kubernetes cluster before you install the Siddhi app. Refer this documentation to install NATS operator and NATS streaming operator. Install the Siddhi operator . Create a persistence volume in your cluster. Now we need a NATS cluster and NATS streaming cluster to run the Siddhi app deployment. For this, there are two cases handled by the operator. User can create NATS cluster and NATS streaming cluster as described in this documentation . Specify cluster details in the YAML file like following. messagingSystem: type: nats config: bootstrapServers: - \"nats://example-nats:4222\" clusterId: example-stan If the user only specifies messaging system as NATS like below then Siddhi operator will automatically create NATS cluster( siddhi-nats ) and NATS streaming cluster( siddhi-stan ), and connect two partial apps. messagingSystem: type: nats Before installing a Siddhi app you have to check that all prerequisites(Siddhi-operator, nats-operator, and nats-streaming-operator) up and running perfectly like below. $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nats-operator 1/1 1 1 5m nats-streaming-operator 1/1 1 1 5m siddhi-operator 1/1 1 1 5m Now you need to specify a YAML file like below to create stateful Siddhi app deployment. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-consume-app spec: apps: - script: | @App:name(\"PowerConsumptionSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power consumption in 1 minute is greater than or equal to 10000W by printing a message in the log for every 30 seconds.\") /* Input: deviceType string and powerConsuption int(Joules) Output: Alert user from printing a log, if there is a power surge in the dryer within 1 minute period. Notify the user in every 30 seconds when total power consumption is greater than or equal to 10000W in 1 minute time period. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, powerConsumed long); @info(name='power-consumption-window') from DevicePowerStream#window.time(1 min) select deviceType, sum(power) as powerConsumed group by deviceType having powerConsumed 10000 output every 30 sec insert into PowerSurgeAlertStream; container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" - name: BASIC_AUTH_ENABLED value: \"false\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0\" messagingSystem: type: nats persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard volumeMode: Filesystem runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Save this YAML as power-consume-app.yaml as use kubectl to deploy the app. kubectl apply -f power-consume-app.yaml This kubectl execution in the Siddhi operator will do the following tasks. Create a NATS cluster and streaming cluster since the user did not specify it. Parse the given Siddhi app and create two partial Siddhi apps(passthrough and process). Then deploy both apps in separate deployments to distribute I/O time. Check health of the Siddhi runner and make deployments up and running. Create a service for passthrough app. Create an ingress rule that maps to passthrough service. After a successful deployment, your kubernetes cluster should have these artifacts. $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-consume-app Running 2/2 5m $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nats-operator 1/1 1 1 10m nats-streaming-operator 1/1 1 1 10m power-consume-app-0 1/1 1 1 5m power-consume-app-1 1/1 1 1 5m siddhi-operator 1/1 1 1 10m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 2d7h power-consume-app-0 ClusterIP 10.105.67.227 none 8080/TCP 5m siddhi-nats ClusterIP 10.100.205.21 none 4222/TCP 10m siddhi-nats-mgmt ClusterIP None none 6222/TCP,8222/TCP,7777/TCP 10m siddhi-operator ClusterIP 10.103.229.109 none 8383/TCP 10m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE siddhi-pv 1Gi RWO Recycle Bound default/power-consume-app-1-pvc standard 10m $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE power-consume-app-1-pvc Bound siddhi-pv 1Gi RWO standard 5m Here power-consume-app-0 is the passthrough deployment and power-consume-app-1 is the process deployment. Now you can send an HTTP request to the passthrough app. curl -X POST \\ http://siddhi/power-consume-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' The process app logs will show that event. $ kubectl get pods NAME READY STATUS RESTARTS AGE nats-operator-dd7f4945f-x4vf8 1/1 Running 0 10m nats-streaming-operator-6fbb6695ff-9rmlx 1/1 Running 0 10m power-consume-app-0-7486b87979-6tccx 1/1 Running 0 5m power-consume-app-1-588996fcfb-prncj 1/1 Running 0 5m siddhi-nats-1 1/1 Running 0 5m siddhi-operator-6698d8f69d-w2kvj 1/1 Running 0 10m siddhi-stan-1 1/1 Running 1 5m $ kubectl logs power-consume-app-1-588996fcfb-prncj JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-07-12 14:09:16,648] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner ... [2019-07-12 14:12:04,969] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562940716559, data=[dryer, 60000], isExpired=false}","title":"Siddhi Kubernetes Microservice"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#siddhi-52-as-a-kubernetes-microservice","text":"This section provides information on running Siddhi Apps natively in Kubernetes via Siddhi Kubernetes Operator. Siddhi can be configured using SiddhiProcess kind and passed to the Siddhi operator for deployment. Here, the Siddhi applications containing stream processing logic can be written inline in SiddhiProcess yaml or passed as .siddhi files via contig maps. SiddhiProcess yaml can also be configured with the necessary system configurations.","title":"Siddhi 5.2 as a Kubernetes Microservice"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#prerequisites","text":"A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster Distributed deployment of Siddhi apps need NATS operator and NATS streaming operator . Admin privileges to install Siddhi operator Minikube Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine (GKE) Cluster To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \"your-address@email.com\" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com Docker for Mac Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation . Port Forwarding for Testing Debugging Purposes Instead of creating ingress you can enable port forwarding ( kubectl port-forward ) to access the application in the Kubernetes cluster. This will help a lot for TCP connections as well. kubectl port-forward svc/mysql-db 13306:3306 For more details please refer this Kubernetes official documentation","title":"Prerequisites"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#install-siddhi-operator","text":"To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-beta/00-prereqs.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-beta/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE siddhi-operator 1 1 1 1 1m Using a custom-built Siddhi runner image If you need to use a custom-built siddhi-runner image for all the SiddhiProcess deployments, you have to configure siddhiRunnerImage entry in siddhi-operator-config config map. Refer the documentation on creating custom Siddhi runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as siddhiRunnerImageSecret entry in siddhi-operator-config config map. For more details on using docker images from private registries/repositories refer this documentation .","title":"Install Siddhi Operator"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app","text":"Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Here we will be creating a very simple Siddhi stream processing application that receives power consumption from several devices in a house. If the power consumption of dryer exceeds the consumption limit of 6000W then that Siddhi app sends an alert from printing a log. This can be created using a SiddhiProcess YAML file as given below. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-surge-app spec: apps: - script: | @App:name(\"PowerSurgeDetection\") @App:description(\"App consume events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='power-filter') from DevicePowerStream[deviceType == 'dryer' and power = 600] select deviceType, power insert into PowerSurgeAlertStream; container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0-beta\" Always listen on 0.0.0.0 with the Siddhi Application running inside a container environment. If you listen on localhost inside the container, nothing outside the container can connect to your application. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Kubernetes Microservice refer Siddhi Config Guide . To deploy the above Siddhi app in your Kubernetes cluster, copy above YAML to a file with name power-surge-app.yaml and execute the following command. kubectl create -f absolute-yaml-file-path /power-surge-app.yaml TLS secret Within the SiddhiProcess, a TLS secret named siddhi-tls is configured. If a Kubernetes secret with the same name does not exist in the Kubernetes cluster, the NGINX will ignore it and use a self-generated certificate. Configuring a secret will be necessary for calling HTTPS endpoints, refer deploy and run Siddhi apps with HTTPS section for more details. If the power-surge-app is deployed successfully, it should create SiddhiProcess, deployment, service, and ingress as following. $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-surge-app Running 1/1 2m $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE power-surge-app-0 1/1 1 1 2m siddhi-operator 1/1 1 1 2m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 2d power-surge-app-0 ClusterIP 10.96.44.182 none 8080/TCP 2m siddhi-operator ClusterIP 10.98.78.238 none 8383/TCP 2m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 2m Using a custom-built Siddhi runner image If you need to use a custom-built siddhi-runner image for a specific SiddhiProcess deployment, you have to configure container.image spec in the power-surge-app.yaml . Refer the documentation on creating custom Siddhi runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as imagePullSecret argument in the power-surge-app.yaml file. For more details on using docker images from private registries/repositories refer this documentation . Invoke Siddhi Applications To invoke the Siddhi App, obtain the external IP of the ingress load balancer using kubectl get ingress command as following. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 2m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Docker for Mac For Docker for Mac, you have to use 0.0.0.0 as the external IP. Use the following CURL command to send events to power-surge-app deployed in Kubernetes. curl -X POST \\ http://siddhi/power-surge-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' View Siddhi Process Logs Since the output of power-surge-app is logged, you can see the output by monitoring the associated pod's logs. To find the power-surge-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-rxzkq 1/1 Running 0 4m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 4m Here, the pod starting with the SiddhiProcess name (in this case power-surge-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs power-surge-app-0-646c4f9dd5-rxzkq ... [2019-07-12 07:12:48,925] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 07:12:48,927] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 07:12:48,941] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 6.853 sec [2019-07-12 07:17:22,219] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562915842182, data=[dryer, 60000], isExpired=false}","title":"Deploy and run Siddhi App"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#get-siddhi-process-status","text":"","title":"Get Siddhi process status"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#list-siddhi-processes","text":"List the Siddhi process using the kubectl get sps or kubectl get SiddhiProcesses commands as follows. $ kubectl get sps NAME STATUS READY AGE power-surge-app Running 1/1 5m $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-surge-app Running 1/1 5m","title":"List Siddhi processes"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#view-siddhi-process-configs","text":"Describe the Siddhi process configuration details using kubectl describe sp command as follows. $ kubectl describe sp power-surge-app Name: power-surge-app Namespace: default Labels: none Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"siddhi.io/v1alpha2\",\"kind\":\"SiddhiProcess\",\"metadata\":{\"annotations\":{},\"name\":\"power-surge-app\",\"namespace\":\"default\"},\"spec\":{\"apps\":[... API Version: siddhi.io/v1alpha2 Kind: SiddhiProcess Metadata: Creation Timestamp: 2019-07-12T07:12:35Z Generation: 1 Resource Version: 148205 Self Link: /apis/siddhi.io/v1alpha2/namespaces/default/siddhiprocesses/power-surge-app UID: 6c6d90a4-a474-11e9-a05b-080027f4eb25 Spec: Apps: Script: @App:name(\"PowerSurgeDetection\") @App:description(\"App consume events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='power-filter') from DevicePowerStream[deviceType == 'dryer' and power = 600] select deviceType, power insert into PowerSurgeAlertStream; Container: Env: Name: RECEIVER_URL Value: http://0.0.0.0:8080/checkPower Name: BASIC_AUTH_ENABLED Value: false Image: siddhiio/siddhi-runner-ubuntu:5.1.0-beta Status: Nodes: nil Ready: 1/1 Status: Running Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal DeploymentCreated 11m siddhiprocess-controller power-surge-app-0 deployment created successfully Normal ServiceCreated 11m siddhiprocess-controller power-surge-app-0 service created successfully","title":"View Siddhi process configs"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#view-siddhi-process-logs","text":"To view the Siddhi process logs, first get the Siddhi process pods using the kubectl get pods command as follows. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-rxzkq 1/1 Running 0 4m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 4m Then to retrieve the Siddhi process logs, run kubectl logs pod name command. Here pod name should be replaced with the name of the pod that starts with the relevant SiddhiProcess's name. A sample output logs are of this command is as follows. $ kubectl logs power-surge-app-0-646c4f9dd5-rxzkq ... [2019-07-12 07:12:48,925] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 07:12:48,927] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 07:12:48,941] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 6.853 sec [2019-07-12 07:17:22,219] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562915842182, data=[dryer, 60000], isExpired=false}","title":"View Siddhi process logs"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#change-the-default-configurations-of-siddhi-runner","text":"Siddhi runner use SIDDHI_RUNNER_HOME /conf/runner/deployment.yaml file as the default configuration file. In the deployment.yaml the file you can configure data sources that you planned to use, add refs, and enable state persistence, etc. To change the configurations of the deployment.yaml , you can add runner YAML spec like below to your SiddhiProcess YAML file. For example, the following config change will enable file system state persistence. runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence","title":"Change the Default Configurations of Siddhi Runner"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-using-config-maps","text":"Siddhi operator allows you to deploy Siddhi app configurations via config maps instead of just adding them inline. Through this, you can also run multiple Siddhi Apps in a single SiddhiProcess. This can be done by passing the config maps containing Siddhi app files to the SiddhiProcess's apps configuration as follows. apps: - configMap: power-surge-cm1 - configMap: power-surge-cm2 Sample on deploying and running Siddhi Apps via config maps Here we will be creating a very simple Siddhi stream processing application that receives power consumption from several devices in a house. If the power consumption of dryer exceeds the consumption limit of 6000W then that Siddhi app sends an alert from printing a log. @App:name(\"PowerSurgeDetection\") @App:description(\"App consume events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='power-filter') from DevicePowerStream[deviceType == 'dryer' and power = 600] select deviceType, power insert into PowerSurgeAlertStream; Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Save the above Siddhi App file as PowerSurgeDetection.siddhi , and use this file to create a Kubernetes config map with the name power-surge-cm . This can be achieved by running the following command. kubectl create configmap power-surge-cm --from-file= absolute-file-path /PowerSurgeDetection.siddhi The created config map can be added to SiddhiProcess YAML under the apps entry as follows. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-surge-app spec: apps: - configMap: power-surge-cm container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0-beta\" Save the YAML file as power-surge-app.yaml , and use the following command to deploy the SiddhiProcess. kubectl create -f absolute-yaml-file-path /power-surge-app.yaml Using a config, created from a directory containing multiple Siddhi files SiddhiProcess's apps.configMap configuration also supports a config map that is created from a directory containing multiple Siddhi files. Use kubectl create configmap siddhi-apps --from-file= DIRECTORY_PATH command to create a config map from a directory. Invoke Siddhi Applications To invoke the Siddhi App, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 2m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to power-surge-app deployed in Kubernetes. curl -X POST \\ http://siddhi/power-surge-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -H 'cache-control: no-cache' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' View Siddhi Process Logs Since the output of power-surge-app is logged, you can see the output by monitoring the associated pod's logs. To find the power-surge-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-tns7l 1/1 Running 0 2m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 8m Here, the pod starting with the SiddhiProcess name (in this case power-surge-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs power-surge-app-0-646c4f9dd5-tns7l ... [2019-07-12 07:50:32,861] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 8.048 sec [2019-07-12 07:50:32,864] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 07:50:32,866] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 07:51:42,488] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562917902484, data=[dryer, 60000], isExpired=false}","title":"Deploy and run Siddhi App using config maps"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-siddhi-apps-without-ingress-creation","text":"By default, Siddhi operator creates an NGINX ingress and exposes your HTTP/HTTPS through that ingress. If you need to disable automatic ingress creation, you have to change the autoIngressCreation value in the Siddhi siddhi-operator-config config map to false or null as below. # This config map used to parse configurations to the Siddhi operator. apiVersion: v1 kind: ConfigMap metadata: name: siddhi-operator-config data: siddhiHome: /home/siddhi_user/siddhi-runner/ siddhiProfile: runner siddhiImage: siddhiio/siddhi-runner-alpine:5.1.0-beta autoIngressCreation: \"false\"","title":"Deploy Siddhi Apps without Ingress creation"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-with-https","text":"Configuring TLS will allow Siddhi ingress NGINX to expose HTTPS endpoints of your Siddhi Apps. To do so, create a Kubernetes secret( siddhi-tls ) and add that to the TLS configuration in siddhi-operator-config config map as given below. ingressTLS: siddhi-tls Sample on deploying and running Siddhi App with HTTPS First, you need to create a certificate using the following commands. For more details about the certificate creation refers this . openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout siddhi.key -out siddhi.crt -subj \"/CN=siddhi/O=siddhi\" After that, create a kubernetes secret called siddhi-tls , which we intended to add to the TLS configurations using the following command. kubectl create secret tls siddhi-tls --key siddhi.key --cert siddhi.crt The created secret then need to be added to the siddhi-operator-config config map as follow. apiVersion: v1 kind: ConfigMap metadata: name: siddhi-operator-config data: siddhiHome: /home/siddhi_user/siddhi-runner/ siddhiProfile: runner siddhiImage: siddhiio/siddhi-runner-ubuntu:5.1.0-beta autoIngressCreation: \"true\" ingressTLS: siddhi-tls When this is done Siddhi operator will now enable TLS support via the NGINX ingress, and you will be able to access all the HTTPS endpoints. Invoke Siddhi Applications You can use now send the events to following HTTPS endpoint. https://siddhi/power-surge-app-0/8080/checkPower Further, you can use the following CURL command to send a request to the deployed Siddhi applications via HTTPS. curl --cacert siddhi.crt -X POST \\ https://siddhi/power-surge-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -H 'cache-control: no-cache' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' View Siddhi Process Logs The output logs show the event that you sent using the previous CURL command. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-kk5md 1/1 Running 0 2m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 10m $ kubectl logs monitor-app-667c97c898-rrtfs ... [2019-07-12 09:06:15,173] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 09:06:15,184] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 09:06:15,187] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 10.819 sec [2019-07-12 09:07:50,098] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562922470093, data=[dryer, 60000], isExpired=false}","title":"Deploy and run Siddhi App with HTTPS"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-with-tcp-endpoints","text":"The default ingress creation of the Siddhi operator allows accessing HTTP/HTTPS endpoints externally. By default, it will not support TCP endpoints. Sometimes you may have some TCP endpoints to configure like NATS and Kafka sources and access those endpoints externally. @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); To access these TCP connections externally you can do it as in the following example. First, you have to disable automatic ingress creation in the Siddhi operator . Then you have to manually create ingress and enable the TCP configurations. To enable TCP configurations in NGINX ingress please refer to this documentation . To create NATS cluster you will need a NATS spec like below. apiVersion: nats.io/v1alpha2 kind: NatsCluster metadata: name: nats-siddhi spec: size: 1 Save this yaml as nats-cluster.yaml and deploy it using kubeclt . $ kubeclt apply -f nats-cluster.yaml Likewise, create a nats streaming cluster as below. apiVersion: streaming.nats.io/v1alpha1 kind: NatsStreamingCluster metadata: name: stan-siddhi spec: size: 1 natsSvc: nats-siddhi Save this yaml as stan-cluster.yaml and deploy it using kubeclt . $ kubeclt apply -f stan-cluster.yaml Now you can deploy the following Siddhi app that contained a NATS source. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-consume-app spec: apps: - script: | @App:name(\"PowerConsumptionSurgeDetection\") @App:description(\"App consumes events from NATS as a text message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power consumption in 1 minute is greater than or equal to 10000W by printing a message in the log for every 30 seconds.\") /* Input: deviceType string and powerConsuption int(Joules) Output: Alert user from printing a log, if there is a power surge in the dryer within 1 minute period. Notify the user in every 30 seconds when total power consumption is greater than or equal to 10000W in 1 minute time period. */ @source( type='nats', cluster.id='siddhi-stan', destination = 'PowerStream', bootstrap.servers='nats://siddhi-nats:4222', @map(type='text') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, powerConsumed long); @info(name='surge-detector') from DevicePowerStream#window.time(1 min) select deviceType, sum(power) as powerConsumed group by deviceType having powerConsumed 10000 output every 30 sec insert into PowerSurgeAlertStream; container: image: \"siddhiio/siddhi-runner-ubuntu:5.1.0-beta\" persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard volumeMode: Filesystem runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Save this yaml as power-consume-app.yaml and deploy it using kubeclt . $ kubeclt apply -f power-consume-app.yaml This commands will create Kubernetes artifacts like below. $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 12d power-consume-app-0 ClusterIP 10.99.148.217 none 4222/TCP 5m siddhi-nats ClusterIP 10.105.250.215 none 4222/TCP 5m siddhi-nats-mgmt ClusterIP None none 6222/TCP,8222/TCP,7777/TCP 5m siddhi-operator ClusterIP 10.102.251.237 none 8383/TCP 5m $ kubectl get pods NAME READY STATUS RESTARTS AGE nats-operator-b8f4977fc-8gnjd 1/1 Running 0 5m nats-streaming-operator-64b565bcc7-r9rpw 1/1 Running 0 5m power-consume-app-0-84f6774bd8-jl95w 1/1 Running 0 5m siddhi-nats-1 1/1 Running 0 5m siddhi-operator-6c6c5d8fcc-hvl7j 1/1 Running 0 5m siddhi-stan-1 1/1 Running 0 5m Now you have to create an ingress for the siddhi-nats service. apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: siddhi-nats annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /nats backend: serviceName: siddhi-nats servicePort: 4222 Save this yaml as siddhi-nats.yaml and deploy it using kubeclt . $ kubeclt apply -f siddhi-nats.yaml Now you can send messages directly to the NATS streaming server that running on your Kubernetes cluster. You have to send those messages to nats:// KUBERNETES_CLUSTER_IP :4222 URI. To send messages to this NATS streaming cluster you can use a Siddhi app that has NATS sink or samples provided by NATS. Minikube External TCP Access The TCP configuration change that described in the ingress NGINX documentation occurred connection refused problems in Minikube. To configure TCP external access properly in Minikube please refer to the steps described in this comment .","title":"Deploy and Run Siddhi App with TCP Endpoints"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-in-distributed-mode","text":"Siddhi apps can be in two different types. Stateless Siddhi apps Stateful Siddhi apps The deployment of the stateful Siddhi apps follows distributed architecture to ensure high availability. The fully distributed scenario of Siddhi deployments handle using Siddhi distributed annotations . Without Messaging System With Messaging System Without Distributed Annotations Case 1 : The given Siddhi app will be deployed in a stateless mode in a single kubernetes deployment. Case 2 : If given Siddhi app contains stateful queries then the Siddhi app divided into two partial Siddhi apps (passthrough and process) and deployed in two kubernetes deployments. Use the configured messaging system to communicate between two apps. With Distributed Annotations Case 3 : WIP(Work In Progress) Case 4 : WIP(Work In Progress) The previously described Siddhi app deployments fall under this Case 1 category. The following sample will cover the Siddhi app deployments which fall under Case 2. Sample on deploying and running Siddhi App with a Messaging System The Siddhi operator currently supports NATS as the messaging system. Therefore it is prerequisite to deploying NATS operator and NATS streaming operator in your kubernetes cluster before you install the Siddhi app. Refer this documentation to install NATS operator and NATS streaming operator. Install the Siddhi operator . Create a persistence volume in your cluster. Now we need a NATS cluster and NATS streaming cluster to run the Siddhi app deployment. For this, there are two cases handled by the operator. User can create NATS cluster and NATS streaming cluster as described in this documentation . Specify cluster details in the YAML file like following. messagingSystem: type: nats config: bootstrapServers: - \"nats://example-nats:4222\" clusterId: example-stan If the user only specifies messaging system as NATS like below then Siddhi operator will automatically create NATS cluster( siddhi-nats ) and NATS streaming cluster( siddhi-stan ), and connect two partial apps. messagingSystem: type: nats Before installing a Siddhi app you have to check that all prerequisites(Siddhi-operator, nats-operator, and nats-streaming-operator) up and running perfectly like below. $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nats-operator 1/1 1 1 5m nats-streaming-operator 1/1 1 1 5m siddhi-operator 1/1 1 1 5m Now you need to specify a YAML file like below to create stateful Siddhi app deployment. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-consume-app spec: apps: - script: | @App:name(\"PowerConsumptionSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power consumption in 1 minute is greater than or equal to 10000W by printing a message in the log for every 30 seconds.\") /* Input: deviceType string and powerConsuption int(Joules) Output: Alert user from printing a log, if there is a power surge in the dryer within 1 minute period. Notify the user in every 30 seconds when total power consumption is greater than or equal to 10000W in 1 minute time period. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, powerConsumed long); @info(name='power-consumption-window') from DevicePowerStream#window.time(1 min) select deviceType, sum(power) as powerConsumed group by deviceType having powerConsumed 10000 output every 30 sec insert into PowerSurgeAlertStream; container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" - name: BASIC_AUTH_ENABLED value: \"false\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0\" messagingSystem: type: nats persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard volumeMode: Filesystem runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Save this YAML as power-consume-app.yaml as use kubectl to deploy the app. kubectl apply -f power-consume-app.yaml This kubectl execution in the Siddhi operator will do the following tasks. Create a NATS cluster and streaming cluster since the user did not specify it. Parse the given Siddhi app and create two partial Siddhi apps(passthrough and process). Then deploy both apps in separate deployments to distribute I/O time. Check health of the Siddhi runner and make deployments up and running. Create a service for passthrough app. Create an ingress rule that maps to passthrough service. After a successful deployment, your kubernetes cluster should have these artifacts. $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-consume-app Running 2/2 5m $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nats-operator 1/1 1 1 10m nats-streaming-operator 1/1 1 1 10m power-consume-app-0 1/1 1 1 5m power-consume-app-1 1/1 1 1 5m siddhi-operator 1/1 1 1 10m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 2d7h power-consume-app-0 ClusterIP 10.105.67.227 none 8080/TCP 5m siddhi-nats ClusterIP 10.100.205.21 none 4222/TCP 10m siddhi-nats-mgmt ClusterIP None none 6222/TCP,8222/TCP,7777/TCP 10m siddhi-operator ClusterIP 10.103.229.109 none 8383/TCP 10m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE siddhi-pv 1Gi RWO Recycle Bound default/power-consume-app-1-pvc standard 10m $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE power-consume-app-1-pvc Bound siddhi-pv 1Gi RWO standard 5m Here power-consume-app-0 is the passthrough deployment and power-consume-app-1 is the process deployment. Now you can send an HTTP request to the passthrough app. curl -X POST \\ http://siddhi/power-consume-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' The process app logs will show that event. $ kubectl get pods NAME READY STATUS RESTARTS AGE nats-operator-dd7f4945f-x4vf8 1/1 Running 0 10m nats-streaming-operator-6fbb6695ff-9rmlx 1/1 Running 0 10m power-consume-app-0-7486b87979-6tccx 1/1 Running 0 5m power-consume-app-1-588996fcfb-prncj 1/1 Running 0 5m siddhi-nats-1 1/1 Running 0 5m siddhi-operator-6698d8f69d-w2kvj 1/1 Running 0 10m siddhi-stan-1 1/1 Running 1 5m $ kubectl logs power-consume-app-1-588996fcfb-prncj JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-07-12 14:09:16,648] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner ... [2019-07-12 14:12:04,969] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562940716559, data=[dryer, 60000], isExpired=false}","title":"Deploy and run Siddhi App in Distributed Mode"},{"location":"docs/siddhi-as-a-local-microservice/","text":"Siddhi 5.2 as a Local Microservice This section provides information on running Siddhi Apps on Bare Metal or VM. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Local Microservice is as follows. Download the latest Siddhi Runner distribution Unzip the siddhi-runner-x.x.x.zip Start SiddhiApps with the runner config by executing the following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file -Dconfig= config-yaml-file Windows : bin\\runner.bat -Dapps= siddhi-file -Dconfig= config-yaml-file Running Multiple SiddhiApps in one runner. To run multiple SiddhiApps in one runtime, have all SiddhiApps in a directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Local Microservice refer Siddhi Config Guide . Samples Running Siddhi App Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /CountOverTime.siddhi Windows : bin\\runner.bat -Dapps= absolute-siddhi-file-path \\CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false} Running with runner config When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can by configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /ConsumeAndStore.siddhi \\ -Dconfig= absolute-config-yaml-path /TestDb.yaml Windows : bin\\runner.sh -Dapps= absolute-siddhi-file-path \\ConsumeAndStore.siddhi ^ -Dconfig= absolute-config-yaml-path \\TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] } Running with environmental/system variables Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set environment variables by running following in the termial Siddhi is about to run: export THRESHOLD=20 export TO_EMAIL= to email address export EMAIL_ADDRESS= gmail address export EMAIL_USERNAME= gmail username export EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password to the end of the runner startup script. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-file-path /TemplatedFilterAndEmail.siddhi \\ -Dconfig= absolute-config-yaml-path /EmailConfig.yaml Windows : bin\\runner.bat -Dapps= absolute-file-path \\TemplatedFilterAndEmail.siddhi ^ -Dconfig= absolute-config-yaml-path \\EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Siddhi Local Microservice"},{"location":"docs/siddhi-as-a-local-microservice/#siddhi-52-as-a-local-microservice","text":"This section provides information on running Siddhi Apps on Bare Metal or VM. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Local Microservice is as follows. Download the latest Siddhi Runner distribution Unzip the siddhi-runner-x.x.x.zip Start SiddhiApps with the runner config by executing the following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file -Dconfig= config-yaml-file Windows : bin\\runner.bat -Dapps= siddhi-file -Dconfig= config-yaml-file Running Multiple SiddhiApps in one runner. To run multiple SiddhiApps in one runtime, have all SiddhiApps in a directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Local Microservice refer Siddhi Config Guide .","title":"Siddhi 5.2 as a Local Microservice"},{"location":"docs/siddhi-as-a-local-microservice/#samples","text":"","title":"Samples"},{"location":"docs/siddhi-as-a-local-microservice/#running-siddhi-app","text":"Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /CountOverTime.siddhi Windows : bin\\runner.bat -Dapps= absolute-siddhi-file-path \\CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false}","title":"Running Siddhi App"},{"location":"docs/siddhi-as-a-local-microservice/#running-with-runner-config","text":"When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can by configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /ConsumeAndStore.siddhi \\ -Dconfig= absolute-config-yaml-path /TestDb.yaml Windows : bin\\runner.sh -Dapps= absolute-siddhi-file-path \\ConsumeAndStore.siddhi ^ -Dconfig= absolute-config-yaml-path \\TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] }","title":"Running with runner config"},{"location":"docs/siddhi-as-a-local-microservice/#running-with-environmentalsystem-variables","text":"Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set environment variables by running following in the termial Siddhi is about to run: export THRESHOLD=20 export TO_EMAIL= to email address export EMAIL_ADDRESS= gmail address export EMAIL_USERNAME= gmail username export EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password to the end of the runner startup script. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-file-path /TemplatedFilterAndEmail.siddhi \\ -Dconfig= absolute-config-yaml-path /EmailConfig.yaml Windows : bin\\runner.bat -Dapps= absolute-file-path \\TemplatedFilterAndEmail.siddhi ^ -Dconfig= absolute-config-yaml-path \\EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Running with environmental/system variables"},{"location":"docs/tooling/","text":"Siddhi 5.2 Tooling Siddhi Editor Siddhi provides tooling that supports following features to develop and test stream processing applications: Text Query Editor with syntax highlighting and advanced auto completion support. Event Simulator and Debugger to test Siddhi Applications. Graphical Query Editor with drag and drop query building support. Graphical Query Editor Text Query Editor","title":"Tooling"},{"location":"docs/tooling/#siddhi-52-tooling","text":"","title":"Siddhi 5.2 Tooling"},{"location":"docs/tooling/#siddhi-editor","text":"Siddhi provides tooling that supports following features to develop and test stream processing applications: Text Query Editor with syntax highlighting and advanced auto completion support. Event Simulator and Debugger to test Siddhi Applications. Graphical Query Editor with drag and drop query building support. Graphical Query Editor Text Query Editor","title":"Siddhi Editor"},{"location":"docs/api/5.2.0/","text":"API Docs - v5.2.0 Core and (Aggregate Function) Returns the results of AND operation for all the events. Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) Calculates the average for all the events. Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) Returns the count of all the events. Syntax LONG count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) This returns the count of distinct occurrences for a given arg. Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) Returns the maximum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) Returns the minimum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) Returns the results of OR operation for all the events. Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) Returns the calculated standard deviation for all the events. Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) Returns the sum for all the events. Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) Generates a UUID (Universally Unique Identifier). Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) Converts the first input parameter according to the convertedTo parameter. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) Includes the given input parameter in a java.util.HashSet and returns the set. Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) Returns the current timestamp of siddhi application in milliseconds. Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) Returns the timestamp of the processed event. Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) Checks whether the parameter is an instance of Boolean or not. Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) Checks whether the parameter is an instance of Double or not. Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) Checks whether the parameter is an instance of Float or not. Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) Checks whether the parameter is an instance of Integer or not. Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) Checks whether the parameter is an instance of Long or not. Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) Checks whether the parameter is an instance of String or not. Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) Returns the maximum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) Returns the minimum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) Returns the size of an object of type java.util.Set. Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) The logger logs the message on the given priority with or without processed event. Syntax log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log(\"INFO\", \"Sample Event :\", true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log(\"Sample Event :\", true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log(\"Sample Event :\", fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log(\"Sample Event :\") select * insert into barStream; This will log message and fooStream:events. batch (Window) A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Sink inMemory (Sink) In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation. log (Sink) This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. Sinkmapper passThrough (Sink Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. Source inMemory (Source) In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport. Sourcemapper passThrough (Source Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"5.2.0"},{"location":"docs/api/5.2.0/#api-docs-v520","text":"","title":"API Docs - v5.2.0"},{"location":"docs/api/5.2.0/#core","text":"","title":"Core"},{"location":"docs/api/5.2.0/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"docs/api/5.2.0/#avg-aggregate-function","text":"Calculates the average for all the events. Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"docs/api/5.2.0/#count-aggregate-function","text":"Returns the count of all the events. Syntax LONG count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"docs/api/5.2.0/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"docs/api/5.2.0/#max-aggregate-function","text":"Returns the maximum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"docs/api/5.2.0/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"docs/api/5.2.0/#min-aggregate-function","text":"Returns the minimum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"docs/api/5.2.0/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"docs/api/5.2.0/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"docs/api/5.2.0/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"docs/api/5.2.0/#sum-aggregate-function","text":"Returns the sum for all the events. Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"docs/api/5.2.0/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"docs/api/5.2.0/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"docs/api/5.2.0/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"docs/api/5.2.0/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"docs/api/5.2.0/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"docs/api/5.2.0/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"docs/api/5.2.0/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"docs/api/5.2.0/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"docs/api/5.2.0/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"docs/api/5.2.0/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"docs/api/5.2.0/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"docs/api/5.2.0/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"docs/api/5.2.0/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"docs/api/5.2.0/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"docs/api/5.2.0/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"docs/api/5.2.0/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"docs/api/5.2.0/#maximum-function","text":"Returns the maximum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"docs/api/5.2.0/#minimum-function","text":"Returns the minimum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"docs/api/5.2.0/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"docs/api/5.2.0/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"docs/api/5.2.0/#log-stream-processor","text":"The logger logs the message on the given priority with or without processed event. Syntax log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log(\"INFO\", \"Sample Event :\", true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log(\"Sample Event :\", true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log(\"Sample Event :\", fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log(\"Sample Event :\") select * insert into barStream; This will log message and fooStream:events.","title":"log (Stream Processor)"},{"location":"docs/api/5.2.0/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"docs/api/5.2.0/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"docs/api/5.2.0/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"docs/api/5.2.0/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"docs/api/5.2.0/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/5.2.0/#frequent-window","text":"This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"docs/api/5.2.0/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"docs/api/5.2.0/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"docs/api/5.2.0/#lossyfrequent-window","text":"This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"docs/api/5.2.0/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"docs/api/5.2.0/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"docs/api/5.2.0/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"docs/api/5.2.0/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"docs/api/5.2.0/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"docs/api/5.2.0/#sink","text":"","title":"Sink"},{"location":"docs/api/5.2.0/#inmemory-sink","text":"In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation.","title":"inMemory (Sink)"},{"location":"docs/api/5.2.0/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"docs/api/5.2.0/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"docs/api/5.2.0/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"docs/api/5.2.0/#source","text":"","title":"Source"},{"location":"docs/api/5.2.0/#inmemory-source","text":"In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport.","title":"inMemory (Source)"},{"location":"docs/api/5.2.0/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"docs/api/5.2.0/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"docs/api/latest/","text":"API Docs - v5.2.0 Core and (Aggregate Function) Returns the results of AND operation for all the events. Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) Calculates the average for all the events. Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) Returns the count of all the events. Syntax LONG count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) This returns the count of distinct occurrences for a given arg. Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) Returns the maximum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) Returns the minimum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) Returns the results of OR operation for all the events. Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) Returns the calculated standard deviation for all the events. Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) Returns the sum for all the events. Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) Generates a UUID (Universally Unique Identifier). Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) Converts the first input parameter according to the convertedTo parameter. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) Includes the given input parameter in a java.util.HashSet and returns the set. Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) Returns the current timestamp of siddhi application in milliseconds. Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) Returns the timestamp of the processed event. Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) Checks whether the parameter is an instance of Boolean or not. Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) Checks whether the parameter is an instance of Double or not. Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) Checks whether the parameter is an instance of Float or not. Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) Checks whether the parameter is an instance of Integer or not. Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) Checks whether the parameter is an instance of Long or not. Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) Checks whether the parameter is an instance of String or not. Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) Returns the maximum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) Returns the minimum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) Returns the size of an object of type java.util.Set. Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) The logger logs the message on the given priority with or without processed event. Syntax log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log(\"INFO\", \"Sample Event :\", true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log(\"Sample Event :\", true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log(\"Sample Event :\", fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log(\"Sample Event :\") select * insert into barStream; This will log message and fooStream:events. batch (Window) A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Sink inMemory (Sink) In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation. log (Sink) This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. Sinkmapper passThrough (Sink Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. Source inMemory (Source) In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport. Sourcemapper passThrough (Source Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"latest"},{"location":"docs/api/latest/#api-docs-v520","text":"","title":"API Docs - v5.2.0"},{"location":"docs/api/latest/#core","text":"","title":"Core"},{"location":"docs/api/latest/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"docs/api/latest/#avg-aggregate-function","text":"Calculates the average for all the events. Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"docs/api/latest/#count-aggregate-function","text":"Returns the count of all the events. Syntax LONG count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"docs/api/latest/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"docs/api/latest/#max-aggregate-function","text":"Returns the maximum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"docs/api/latest/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"docs/api/latest/#min-aggregate-function","text":"Returns the minimum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"docs/api/latest/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"docs/api/latest/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"docs/api/latest/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"docs/api/latest/#sum-aggregate-function","text":"Returns the sum for all the events. Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"docs/api/latest/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"docs/api/latest/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"docs/api/latest/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"docs/api/latest/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"docs/api/latest/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"docs/api/latest/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"docs/api/latest/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"docs/api/latest/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"docs/api/latest/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"docs/api/latest/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"docs/api/latest/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"docs/api/latest/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"docs/api/latest/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"docs/api/latest/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"docs/api/latest/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"docs/api/latest/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"docs/api/latest/#maximum-function","text":"Returns the maximum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"docs/api/latest/#minimum-function","text":"Returns the minimum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"docs/api/latest/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"docs/api/latest/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"docs/api/latest/#log-stream-processor","text":"The logger logs the message on the given priority with or without processed event. Syntax log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log(\"INFO\", \"Sample Event :\", true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log(\"Sample Event :\", true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log(\"Sample Event :\", fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log(\"Sample Event :\") select * insert into barStream; This will log message and fooStream:events.","title":"log (Stream Processor)"},{"location":"docs/api/latest/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"docs/api/latest/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"docs/api/latest/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"docs/api/latest/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"docs/api/latest/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/latest/#frequent-window","text":"This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"docs/api/latest/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"docs/api/latest/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"docs/api/latest/#lossyfrequent-window","text":"This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"docs/api/latest/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"docs/api/latest/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"docs/api/latest/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"docs/api/latest/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"docs/api/latest/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"docs/api/latest/#sink","text":"","title":"Sink"},{"location":"docs/api/latest/#inmemory-sink","text":"In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation.","title":"inMemory (Sink)"},{"location":"docs/api/latest/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"docs/api/latest/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"docs/api/latest/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"docs/api/latest/#source","text":"","title":"Source"},{"location":"docs/api/latest/#inmemory-source","text":"In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport.","title":"inMemory (Source)"},{"location":"docs/api/latest/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"docs/api/latest/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"docs/examples/basic-types/","text":"var base_url = \"\"; Basic Types Provides introduction to basic Siddhi attribute types which are int , long , float , double , string , and object , and some key functions such as convert() , instanceOf...() , and cast() . In Siddhi, other types such as list, map, etc, should be passed as object into streams. For more information on other types refer other examples under Values and Types section. For information values , and other useful functions , refer the Siddhi query guide . define stream PatientRegistrationInputStream ( seqNo long , name string , age int , height float , weight double , photo object , isEmployee bool , wardNo object ); define stream PatientRegistrationStream ( seqNo long , name string , age int , height double , weight double , photo object , isPhotoString bool , isEmployee bool , wardNo int ); @ info ( name = Type - processor ) from PatientRegistrationInputStream select seqNo , name , age , convert ( height , double ) as height , weight , photo , instanceOfString ( photo ) as isPhotoString , isEmployee , cast ( wardNo , int ) as wardNo insert into PatientRegistrationStream ; define stream PatientRegistrationInputStream ( seqNo long, name string, age int, height float, weight double, photo object, isEmployee bool, wardNo object); Defines PatientRegistrationInputStream having information in all primitive types. define stream PatientRegistrationStream ( seqNo long, name string, age int, height double, weight double, photo object, isPhotoString bool, isEmployee bool, wardNo int); Defines the resulting PatientRegistrationStream after processing. @info(name = Type-processor ) from PatientRegistrationInputStream select seqNo, name, age, convert(height, double ) as height, convert() used to convert float type to double . weight, photo, instanceOfString(photo) as isPhotoString, instanceOfString() checks if the photo is an instance of string . isEmployee, cast(wardNo, int ) as wardNo cast() cast the value of wardNo to int . insert into PatientRegistrationStream; Input Below event is sent to PatientRegistrationInputStream , [ 1200098 , 'Peter Johnson' , 34 , 194.3f , 69.6 , #Fjoiu59%3hkjnknk$#nFT , true , 34 ] Here, assume that the content of the photo ( #Fjoiu59%3hkjnknk$#nFT ) is binary. Output After processing, the event arriving at PatientRegistrationStream will be as follows: [ 1200098 , 'Peter Johnson' , 34 , 194.3 , 69.6 , #Fjoiu59%3hkjnknk$#nFT , false , true , 34 ]","title":"Basic types"},{"location":"docs/examples/batch-length/","text":"var base_url = \"\"; Batch (Tumbling) Event Count Provides examples on aggregating events based on event count in a batch (tumbling) manner. To aggregate events in a sliding manner, based on time, or by session, refer other the examples in Data Summarization section. For more information on windows refer the Siddhi query guide . define stream TemperatureStream ( sensorId string , temperature double ); @ info ( name = Overall - analysis ) from TemperatureStream # window . lengthBatch ( 4 ) select avg ( temperature ) as avgTemperature , max ( temperature ) as maxTemperature , count () as numberOfEvents insert into OverallTemperatureStream ; @ info ( name = SensorId - analysis ) from TemperatureStream # window . lengthBatch ( 5 ) select sensorId , avg ( temperature ) as avgTemperature , min ( temperature ) as maxTemperature group by sensorId having avgTemperature = 20.0 insert into SensorIdTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); @info(name = Overall-analysis ) from TemperatureStream#window.lengthBatch(4) Aggregate every 4 events in a batch (tumbling) manner. select avg(temperature) as avgTemperature, max(temperature) as maxTemperature, count() as numberOfEvents insert into OverallTemperatureStream; Calculate average, maximum, and count for temperature attribute. @info(name = SensorId-analysis ) from TemperatureStream#window.lengthBatch(5) Aggregate every 5 events in a batch (tumbling) manner. select sensorId, avg(temperature) as avgTemperature, min(temperature) as maxTemperature group by sensorId Calculate average, and minimum for temperature , by grouping events by sensorId . having avgTemperature = 20.0 Output events only when avgTemperature is greater than or equal to 20.0 . insert into SensorIdTemperatureStream; Aggregation Behavior When events are sent to TemperatureStream stream, following events will get emitted at OverallTemperatureStream stream via Overall-analysis query, and SensorIdTemperatureStream stream via SensorId-analysis query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 123px; } Input to TemperatureStream Output at OverallTemperatureStream Output at SensorIdTemperatureStream [ '1001' , 19.0 ] - - [ '1002' , 26.0 ] - - [ '1002' , 24.0 ] - - [ '1001' , 20.0 ] [ 22.5 , 26.0 , 4 ] - [ '1001' , 21.0 ] - [ '1002' , 25.5 , 24.0 ], [ '1001' , 20.0 , 19.0 ] [ '1002' , 22.0 ] - - [ '1001' , 21.0 ] - - [ '1002' , 22.0 ] [ 21.5 , 22.0 , 4 ] -","title":"Batch length"},{"location":"docs/examples/batch-time/","text":"var base_url = \"\"; Batch (Tumbling) Time Provides examples on aggregating events over time in a batch (tumbling) manner. To aggregate events in a sliding manner, based on events, or by session, refer other the examples in Data Summarization section. For more information on windows refer the Siddhi query guide . define stream TemperatureStream ( sensorId string , temperature double ); @ info ( name = Overall - analysis ) from TemperatureStream # window . timeBatch ( 1 min ) select avg ( temperature ) as avgTemperature , max ( temperature ) as maxTemperature , count () as numberOfEvents insert into OverallTemperatureStream ; @ info ( name = SensorId - analysis ) from TemperatureStream # window . timeBatch ( 30 sec , 0 ) select sensorId , avg ( temperature ) as avgTemperature , min ( temperature ) as maxTemperature group by sensorId having avgTemperature 20.0 insert into SensorIdTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); @info(name = Overall-analysis ) from TemperatureStream#window.timeBatch(1 min) Aggregate events every 1 minute , from the arrival of the first event. select avg(temperature) as avgTemperature, max(temperature) as maxTemperature, count() as numberOfEvents Calculate average, maximum, and count for temperature attribute. insert into OverallTemperatureStream; @info(name = SensorId-analysis ) from TemperatureStream#window.timeBatch(30 sec, 0) Aggregate events every 30 seconds from epoch timestamp 0 . select sensorId, avg(temperature) as avgTemperature, min(temperature) as maxTemperature group by sensorId Calculate average, and minimum for temperature , by grouping events by sensorId . having avgTemperature 20.0 Output events only when avgTemperature is greater than 20.0 . insert into SensorIdTemperatureStream; Aggregation Behavior When events are sent to TemperatureStream stream, following events will get emitted at OverallTemperatureStream stream via Overall-analysis query, and SensorIdTemperatureStream stream via SensorId-analysis query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 123px; } Time Input to TemperatureStream Output at OverallTemperatureStream Output at SensorIdTemperatureStream 9:00:10 [ '1001' , 21.0 ] - - 9:00:20 [ '1002' , 25.0 ] - - 9:00:30 - - [ '1001' , 21.0 , 21.0 ],[ '1002' , 25.0 , 25.0 ] 9:00:35 [ '1002' , 26.0 ] - - 9:00:40 [ '1002' , 27.0 ] - - 9:00:55 [ '1001' , 19.0 ] - - 9:00:00 - - [ '1002' , 26.5 , 26.0 ] 9:01:10 - [ 23.6 , 27.0 , 5 ] - 9:01:20 [ '1001' , 21.0 ] - - 9:01:30 - - [ '1001' , 21.0 , 21.0 ] 9:02:10 - [ 21.0 , 21.0 , 1 ] -","title":"Batch time"},{"location":"docs/examples/counting-pattern/","text":"var base_url = \"\"; Counting Pattern Counting patterns allow to match multiple events that may have been received for the same matching condition. The number of events matched per condition can be limited via condition postfixes. Refer the Siddhi query guide for more information. define stream TemperatureStream ( sensorID long , roomNo int , temp double ); define stream RegulatorStream ( deviceID long , roomNo int , tempSet double , isOn bool ); @ sink ( type = log ) define stream TemperatureDiffStream ( roomNo int , tempDiff double ); from every ( e1 = RegulatorStream ) - e2 = TemperatureStream [ e1 . roomNo == roomNo ] 1 : - e3 = RegulatorStream [ e1 . roomNo == roomNo ] select e1 . roomNo , e2 [ 0 ]. temp - e2 [ last ]. temp as tempDiff insert into TemperatureDiffStream ; define stream TemperatureStream (sensorID long, roomNo int, temp double); Defines TemperatureStream having information on room temperature such as sensorID , roomNo and temp . define stream RegulatorStream (deviceID long, roomNo int, tempSet double, isOn bool); Defines RegulatorStream which contains the events from regulator with attributes deviceID , roomNo , tempSet , and isOn . @sink(type = log ) define stream TemperatureDiffStream(roomNo int, tempDiff double); Defines TemperatureDiffStream which contains the events related to temperature difference. from every( e1 = RegulatorStream) - e2 = TemperatureStream[e1.roomNo == roomNo] 1: - e3 = RegulatorStream[e1.roomNo == roomNo] Calculates the temperature difference between two regulator events. Here, when at least one TemperatureStream event needs to arrive between two RegulatorStream events. select e1.roomNo, e2[0].temp - e2[last].temp as tempDiff Finds the temperature difference between the first and last temperature event. insert into TemperatureDiffStream; This application calculates the temperature difference between two regulator events. Here, when at least one TemperatureStream event occurs between two RegulatorStream events the pattern is valid and logs can be seen. Input First, below event is sent to RegulatorStream , [ 21 , 2 , 25 , true ] Below events are sent to TemperatureStream , [ 21 , 2 , 29 ] [ 21 , 2 , 26 ] Finally, below event is sent again to RegulatorStream , [ 21 , 2 , 30 , true ] Output After processing the above input events, the event arriving at TemperatureDiffStream will be as follows: [ 2 , 3.0 ]","title":"Counting pattern"},{"location":"docs/examples/deduplicate/","text":"var base_url = \"\"; Remove Duplicate Events Provides examples of removing duplicate events that arrive within a given time duration. define stream TemperatureStream ( sensorId string , seqNo string , temperature double ); @ info ( name = Deduplicate - sensorId ) from TemperatureStream #unique:deduplicate( sensorId , 1 min ) select * insert into UniqueSensorStream ; @ info ( name = Deduplicate - sensorId - and - seqNo ) from TemperatureStream #unique:deduplicate( str : concat ( sensorId , - , seqNo ), 1 min ) select * insert into UniqueSensorSeqNoStream ; define stream TemperatureStream (sensorId string, seqNo string, temperature double); @info(name = Deduplicate-sensorId ) from TemperatureStream#unique:deduplicate(sensorId, 1 min) Remove duplicate events arriving within 1 minute time gap, based on unique sensorId . select * insert into UniqueSensorStream; @info(name = Deduplicate-sensorId-and-seqNo ) from TemperatureStream#unique:deduplicate( str:concat(sensorId, - ,seqNo), 1 min) Remove duplicate events arriving within 1 minute time gap, based on unique sensorId and seqNo combination. select * insert into UniqueSensorSeqNoStream; Behavior When events are sent to TemperatureStream stream, following events will get emitted after deduplication on UniqueSensorStream stream via Deduplicate-sensorId query, and UniqueSensorSeqNoStream stream via Deduplicate-sensorId-and-seqNo query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 155px; } Time Input to TemperatureStream Output at UniqueSensorStream Output at UniqueSensorSeqNoStream 9:00:00 [ 'AD11' , '200' , 18.0 ] [ 'AD11' , '200' , 18.0 ] [ 'AD11' , '200' , 18.0 ] 9:00:10 [ 'AD11' , '201' , 23.0 ] - [ 'AD11' , '201' , 23.0 ] 9:00:20 [ 'FR45' , '500' , 22.0 ] [ 'FR45' , '500' , 22.0 ] [ 'FR45' , '500' , 22.0 ] 9:00:40 [ 'AD11' , '200' , 18.0 ] - - 9:00:50 [ 'AD11' , '202' , 28.0 ] - [ 'AD11' , '202' , 28.0 ] 9:01:05 [ 'FR45' , '501' , 22.0 ] - [ 'FR45' , '501' , 22.0 ] 9:01:10 [ 'AD11' , '203' , 30.0 ] [ 'AD11' , '203' , 30.0 ] [ 'AD11' , '203' , 30.0 ] 9:02:20 [ 'AD11' , '202' , 28.0 ] [ 'AD11' , '202' , 28.0 ] [ 'AD11' , '202' , 28.0 ] 9:03:10 [ 'AD11' , '204' , 30.0 ] - [ 'AD11' , '204' , 30.0 ]","title":"Deduplicate"},{"location":"docs/examples/default/","text":"var base_url = \"\"; Default This application demonstrates how to use default function to process attributes with null values Please see Values in Query guide for more information on format of the various data types. define stream PatientRegistrationInputStream ( seqNo long , name string , age int , height float , weight double , photo object , isEmployee bool , wardNo object ); @ info ( name = SimpleIfElseQuery ) from PatientRegistrationInputStream select default ( name , invalid ) as name , default ( seqNo , 0 l ) as seqNo , default ( weight , 0 d ) as weight , default ( age , 0 ) as age , default ( height , 0 f ) as height insert into PreprocessedPatientRegistrationInputStream ; define stream PatientRegistrationInputStream ( seqNo long, name string, age int, height float, weight double, photo object, isEmployee bool, wardNo object); Defines PatientRegistrationInputStream having information in all primitive types. @info(name = SimpleIfElseQuery ) from PatientRegistrationInputStream select default(name, invalid ) as name, Default value of invalid to be used if name is null default(seqNo, 0l) as seqNo, Default value of 0l to be used if seqNo is null default(weight, 0d) as weight, Default value of 0d to be used if weight is null default(age, 0) as age, Default value of 0 to be used if age is null default(height, 0f) as height Default value of 0f to be used if height is null insert into PreprocessedPatientRegistrationInputStream; Input An event of all null attributes is sent to PatientRegistrationInputStream , Output After processing, the event arriving at PreprocessedPatientRegistrationInputStream will be as follows, [ 'invalid' , 0 0.0 , 0 , 0.0 ] with types, [ string , long , double , int , float ]","title":"Default"},{"location":"docs/examples/error-handling-with-logs/","text":"var base_url = \"\"; Logging Errors in Siddhi can be handled at the Streams and in the Sinks. This example explains how errors are handled at Sink level. There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. Refer the Siddhi query guide for more information. define stream GlucoseReadingStream ( locationRoom string , locationBed string , timeStamp string , sensorID long , patientFirstName string , patientLastName string , sensorValue double ); @ sink ( type = http , on . error = log , publisher . url = http://localhost:8080/logger , method = POST , @ map ( type = json )) define stream AbnormalGlucoseReadingStream ( timeStampInLong long , locationRoom string , locationBed string , sensorID long , patientFullName string , sensorReadingValue double ); @ info ( name = abnormal - reading - identifier ) from GlucoseReadingStream [ sensorValue 220 ] select math : parseLong ( timeStamp ) as timeStampInLong , locationRoom , locationBed , sensorID , str : concat ( patientFirstName , , patientLastName ) as patientFullName , sensorValue as sensorReadingValue insert into AbnormalGlucoseReadingStream ; define stream GlucoseReadingStream (locationRoom string, locationBed string, timeStamp string, sensorID long, patientFirstName string, patientLastName string, sensorValue double); Defines GlucoseReadingStream stream which contains events related to Glucose readings. @sink(type = http , on.error= log , publisher.url = http://localhost:8080/logger , method = POST , @map(type = json )) If HTTP endpoint which defined in sink annotation is unavailable then it logs the event with the error and drops the event. Errors can be gracefully handled by configuring on.error parameter. define stream AbnormalGlucoseReadingStream (timeStampInLong long, locationRoom string, locationBed string, sensorID long, patientFullName string, sensorReadingValue double); @info(name= abnormal-reading-identifier ) from GlucoseReadingStream[sensorValue 220] select math:parseLong(timeStamp) as timeStampInLong, locationRoom, locationBed, sensorID, Identifies the abnormal Glucose reading if sensorValue 220 str:concat(patientFirstName, , patientLastName) as patientFullName, Concatenate string attributes patientFirstName and patientLastName sensorValue as sensorReadingValue insert into AbnormalGlucoseReadingStream; Above is a simple example to publish abnormal Glucose reading events to an unavailable HTTP endpoint and error is handled by logging the events to the logs. Input Below event is sent to GlucoseReadingStream stream, [ 'Get-1024' , 'Level2' , '1576829362' , 10348 , 'Alex' , 'John' , 250 ] Output After processing, the following log gets printed in the console: ERROR {io.siddhi.core.stream.output.sink.Sink} - Error on 'ErrorHandling'. Dropping event at Sink 'http' at 'AbnormalGlucoseReadingStream' as its still trying to reconnect!, events dropped '{ event :{ timeStampInLong :1576829362, locationRoom : 1024 , locationBed : Level2 , sensorID :10348, patientFullName : Alex John , sensorReadingValue :250.0}}'","title":"Error handling with logs"},{"location":"docs/examples/error-handling-with-wait-retry/","text":"var base_url = \"\"; Wait & Retry This example explains how errors are handled at Sink level by wait and retry mode. In this mode, publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publish to it. Refer the Siddhi query guide for more information. define stream GlucoseReadingStream ( locationRoom string , locationBed string , timeStamp string , sensorID long , patientFirstName string , patientLastName string , sensorValue double ); @ sink ( type = http , on . error = wait , publisher . url = http://localhost:8080/logger , method = POST , @ map ( type = json )) define stream AbnormalGlucoseReadingStream ( timeStampInLong long , locationRoom string , locationBed string , sensorID long , patientFullName string , sensorReadingValue double ); @ info ( name = abnormal - reading - identifier ) from GlucoseReadingStream [ sensorValue 220 ] select math : parseLong ( timeStamp ) as timeStampInLong , locationRoom , locationBed , sensorID , str : concat ( patientFirstName , , patientLastName ) as patientFullName , sensorValue as sensorReadingValue insert into AbnormalGlucoseReadingStream ; define stream GlucoseReadingStream (locationRoom string, locationBed string, timeStamp string, sensorID long, patientFirstName string, patientLastName string, sensorValue double); Defines GlucoseReadingStream stream which contains events related to Glucose readings. @sink(type = http , on.error= wait , publisher.url = http://localhost:8080/logger , method = POST , @map(type = json )) If HTTP endpoint is unavailable then threads who bring events via AbnormalGlucoseReadingStream wait in back-off and re-trying mode. Errors can be gracefully handled by configuring on.error parameter. define stream AbnormalGlucoseReadingStream (timeStampInLong long, locationRoom string, locationBed string, sensorID long, patientFullName string, sensorReadingValue double); @info(name= abnormal-reading-identifier ) from GlucoseReadingStream[sensorValue 220] select math:parseLong(timeStamp) as timeStampInLong, locationRoom, locationBed, sensorID, Identifies the abnormal Glucose reading if sensorValue 220 str:concat(patientFirstName, , patientLastName) as patientFullName, Concatenate string attributes patientFirstName and patientLastName sensorValue as sensorReadingValue insert into AbnormalGlucoseReadingStream; Above is a simple example to publish abnormal Glucose reading events to an unavailable HTTP endpoint and error is handled by wait and retry mode. prerequisites Download the mock logger service from here . Input Output Below event is sent to GlucoseReadingStream stream, [ 'Get-1024' , 'Level2' , '1576829362' , 10348 , 'Alex' , 'John' , 250 ] You could see ConnectException is get printed since logger service is unavailable. Then, execute the below command to start the mock logger service. java -jar logservice-1.0.0.jar Now, you could see the event sent in step #1 is get logged in the logger service console as given below. LoggerService:42 - {event={timeStampInLong=1.576829362E9, locationRoom=Get-1024, locationBed=Level2, sensorID=10348.0, patientFullName=Alex John, sensorReadingValue=250.0}}","title":"Error handling with wait retry"},{"location":"docs/examples/grpc-service-integration/","text":"var base_url = \"\"; gRPC Service Integration This application demonstrates how to achieve gRPC service integration (request-response) while processing events in the realtime. There could be use cases which need to integrate with an external gRPC service to make some decision when processing events. The below example demonstrates such a requirement. define stream TicketBookingStream ( name string , phoneNo string , movie string , ticketClass string , qty int , bookingTime long ); @ sink ( type = grpc - call , publisher . url = grpc : //localhost:5003/org.wso2.grpc.EventService/process , sink . id = ticket - price , @ map ( type = json )) define stream TicketPriceFinderStream ( name string , phoneNo string , movie string , ticketClass string , qty int , bookingTime long ); @ source ( type = grpc - call - response , receiver . url = grpc : //localhost:9763/org.wso2.grpc.EventService/process , sink . id = ticket - price , @ map ( type = json , @ attributes ( customerName = trp : name , phoneNo = trp : phoneNo , movie = trp : movie , qty = trp : qty , bookingTime = trp : bookingTime , ticketPrice = price ))) define stream TicketPriceResponseStream ( customerName string , phoneNo string , movie string , qty int , ticketPrice double , bookingTime long ); @ sink ( type = log ) define stream TotalTicketPaymentStream ( customerName string , phoneNo string , movie string , totalAmount double , bookingTime long ); @ info ( name = filter - basic - ticket - bookings ) from TicketBookingStream [ ticketClass == BASIC ] select name as customerName , phoneNo , movie , qty * 20.0 as totalAmount , bookingTime insert into TotalTicketPaymentStream ; @ info ( name = filter - non - basic - tickets ) from TicketBookingStream [ ticketClass != BASIC ] select * insert into TicketPriceFinderStream ; @ info ( name = total - price - calculator ) from TicketPriceResponseStream select customerName , phoneNo , movie , ( qty * ticketPrice ) as totalAmount , bookingTime insert into TotalTicketPaymentStream ; define stream TicketBookingStream (name string, phoneNo string, movie string, ticketClass string, qty int, bookingTime long); Defines TicketBookingStream which is the input stream that contains the ticket booking events. @sink(type= grpc-call , publisher.url = grpc://localhost:5003/org.wso2.grpc.EventService/process , sink.id= ticket-price , @map(type= json )) The grpc-call sink is used for scenarios where we send a request out and expect a response back. In default mode this will use EventService process method. grpc-call-response source is used to receive the responses. A unique sink.id is used to correlate between the sink and its corresponding source. define stream TicketPriceFinderStream (name string, phoneNo string, movie string, ticketClass string, qty int, bookingTime long); Defines TicketPriceFinderStream to forward events to the gRPC endpoint. @source(type= grpc-call-response , receiver.url = grpc://localhost:9763/org.wso2.grpc.EventService/process , sink.id= ticket-price , This grpc-call-response source receives responses received from gRPC server for requests sent from a grpc-call sink. The source will receive responses for sink with the same sink.id . @map(type= json , @attributes(customerName= trp:name , phoneNo= trp:phoneNo , movie= trp:movie , qty= trp:qty , bookingTime= trp:bookingTime , ticketPrice= price ))) Attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. define stream TicketPriceResponseStream (customerName string, phoneNo string, movie string, qty int, ticketPrice double, bookingTime long); Defines TicketPriceResponseStream which contains the response events for the ticket price requests. @sink(type= log ) define stream TotalTicketPaymentStream (customerName string, phoneNo string, movie string, totalAmount double, bookingTime long); Defines TotalTicketPaymentStream which contains the events with the total payment amount for tickets. @info(name = filter-basic-ticket-bookings ) from TicketBookingStream[ticketClass == BASIC ] select name as customerName, phoneNo, movie, qty * 20.0 as totalAmount, bookingTime insert into TotalTicketPaymentStream; Filter the ticket bookings of class Basic and apply the default ticket price as 20 USD. @info(name = filter-non-basic-tickets ) from TicketBookingStream[ticketClass != BASIC ] select * insert into TicketPriceFinderStream; Filter the ticket bookings other than the class Basic and route them to stream called TicketPriceFinderStream . @info(name = total-price-calculator ) from TicketPriceResponseStream select customerName, phoneNo, movie, (qty * ticketPrice) as totalAmount, bookingTime Calculate the total ticket payment amount based on the price amount received from the gRPC service. insert into TotalTicketPaymentStream; Input and Output Let s assume there is an external gRPC service that responds with the ticket price based on the gRPC request. When an event with values [ Mohan , +181234579212 , Iron Man , Gold , 4, 0130] is sent to TicketBookingStream stream then a gRPC request is sent to the loan gRPC service to find the ticket price if ticket class is not basic . Then, gRPC server responds with the ticket price as shown below. { price : 25 } There is a grpc-call-response source configured to consume the response from the gRPC server and those responses will be mapped to the stream called TicketPriceResponseStream . Then Siddhi calculates the total ticket payment amount accordingly and pushes it to a stream called TotalTicketPaymentStream . In this example, those events are logged to the console. Sample console log is give below, TotalPurchaseCalculator : TotalTicketPaymentStream : Event{timestamp=1575449841536, data=[Mohan, +181234579212, Iron Man, 100.0, 0130], isExpired=false}","title":"Grpc service integration"},{"location":"docs/examples/http-service-integration/","text":"var base_url = \"\"; HTTP Service Integration This application demonstrates how to achieve HTTP service integration (request-response) while processing events in the realtime. There could be use cases which need to integrate with an external service to make some decision when processing events. The below example demonstrates such a requirement. @ sink ( type = http - call , publisher . url = http : //localhost:8005/validate-loan , method = POST , sink . id = loan - validation , @ map ( type = json )) define stream LoanValidationStream ( clientId long , name string , amount double ); @ source ( type = http - call - response , sink . id = loan - validation , http . status . code = 2 \\ d + , @ map ( type = json , @ attributes ( customerName = trp : name , clientId = trp : clientId , loanAmount = trp : amount , interestRate = validation - response . rate , totalYears = validation - response . years - approved ))) define stream SuccessLoanRequestStream ( clientId long , customerName string , loanAmount double , interestRate double , totalYears int ); @ source ( type = http - call - response , sink . id = loan - validation , http . status . code = 400 , @ map ( type = json , @ attributes ( customerName = trp : name , clientId = trp : clientId , failureReason = validation - response . reason ))) define stream FailureLoanRequestStream ( clientId long , customerName string , failureReason string ); define stream LoanRequestStream ( clientId long , name string , amount double ); @ sink ( type = log ) define stream LoanResponseStream ( clientId long , customerName string , message string ); @ info ( name = attribute - projection ) from LoanRequestStream select clientId , name , amount insert into LoanValidationStream ; @ info ( name = successful - message - generator ) from SuccessLoanRequestStream select clientId , customerName , Loan Request is accepted for processing as message insert into LoanResponseStream ; @ info ( name = failure - message - generator ) from FailureLoanRequestStream select clientId , customerName , str : concat ( Loan Request is rejected due to , failureReason ) as message insert into LoanResponseStream ; @sink(type= http-call , publisher.url= http://localhost:8005/validate-loan , method= POST , sink.id= loan-validation , @map(type= json )) http-call sink publishes messages to endpoints via HTTP or HTTPS protocols. and consume responses through its corresponding http-call-response source define stream LoanValidationStream (clientId long, name string, amount double); Defines LoanValidationStream to forward events to the loan validation purposes. @source(type= http-call-response , sink.id= loan-validation , The http-call-response source receives the responses for the calls made by its corresponding http-call sink. sink.id used to map the corresponding http-call sink. http.status.code= 2\\d+ , To handle messages with different HTTP status codes having different formats, multiple http-call-response sources are allowed to associate with a single http-call sink. In this case, it only handles HTTP responses which come with 2xx response codes. @map(type= json , @attributes(customerName= trp:name , clientId= trp:clientId , loanAmount= trp:amount , interestRate= validation-response.rate , totalYears= validation-response.years-approved ))) Attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. define stream SuccessLoanRequestStream(clientId long, customerName string, loanAmount double, interestRate double, totalYears int); Defines SuccessLoanRequestStream to process loan requests which are validated successfully. @source(type= http-call-response , sink.id= loan-validation , http.status.code= 400 , @map(type= json , @attributes(customerName= trp:name , clientId= trp:clientId , failureReason= validation-response.reason ))) The http-call-response source which handles the responses which come with 400 HTTP response code. define stream FailureLoanRequestStream(clientId long, customerName string, failureReason string); Defines FailureLoanRequestStream to process loan requests which are rejected. define stream LoanRequestStream (clientId long, name string, amount double); Defines LoanRequestStream which is the input stream that contains the initial loan request events. @sink(type= log ) define stream LoanResponseStream(clientId long, customerName string, message string); Defines LoanResponseStream which contains the events with the loan request responses. @info(name = attribute-projection ) from LoanRequestStream select clientId, name, amount insert into LoanValidationStream; Project attributes clientId , name and amount to perform the http request to validate-loan service. @info(name = successful-message-generator ) from SuccessLoanRequestStream select clientId, customerName, Loan Request is accepted for processing as message insert into LoanResponseStream; Process the successful loan requests and generate a response message. @info(name = failure-message-generator ) from FailureLoanRequestStream select clientId, customerName, str:concat( Loan Request is rejected due to , failureReason) as message insert into LoanResponseStream; Process the rejected loan requests and generate a response message. Input and Output Let s assume there is an external HTTP service that performs the loan request validation. When an event with values [002345, David Warner , 200000] is sent to LoanRequestStream stream then the respective event is sent to the loan validation service. If the loan request is valid then Siddhi gets a response as shown below from the loan validation service. In this situation, the response code would be 200 . { validation-response : { isValid : true, rate : 12.5, years-approved : 5 } } If the loan request is rejected then the loan validation service sent a response with response code 400 with below payload. { validation-response : { isValid : false, reason : Not a registered user } } Then Siddhi logs the response message accordingly. If the loan request is accepted then log message would be something similar as below. LoanRequestProcessor : LoanResponseStream : Event{timestamp=1575293895348, data=[002345, David Warner, Loan Request is accepted for processing], isExpired=false","title":"Http service integration"},{"location":"docs/examples/if-then-else/","text":"var base_url = \"\"; If-Then-Else This application demonstrates how to enrich events based on a simple if-then-else conditions. define stream TemperatureStream ( sensorId string , temperature double ); @ info ( name = SimpleIfElseQuery ) from TemperatureStream select sensorId , ifThenElse ( temperature - 2 , Valid , InValid ) as isValid insert into ValidTemperatureStream ; @ info ( name = ComplexIfElseQuery ) from TemperatureStream select sensorId , ifThenElse ( temperature - 2 , ifThenElse ( temperature 40 , High , Normal ), InValid ) as tempStatus insert into ProcessedTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); Defines TemperatureStream stream to process events having sensorId and temperature (F). @info(name = SimpleIfElseQuery ) from TemperatureStream select sensorId, ifThenElse(temperature -2, Valid , InValid ) as isValid if temperature -2, isValid will be true else false insert into ValidTemperatureStream; @info(name = ComplexIfElseQuery ) from TemperatureStream select sensorId, ifThenElse(temperature -2, ifThenElse(temperature 40, High , Normal ), InValid ) as tempStatus If the temperature 40 the status is set to High , between -2 and 40 as Normal less than -2 as InValid insert into ProcessedTemperatureStream; Events at each stream When an event with values [ 'sensor1' , 35.4 ] is sent to TemperatureStream stream it will get converted and travel through the streams as below. ValidTemperatureStream : [ 'sensor1' , 'Valid' ] ProcessedTemperatureStream : [ 'sensor1' , 'Normal' ]","title":"If then else"},{"location":"docs/examples/list/","text":"var base_url = \"\"; List Provides examples on basic list functions provided via siddhi-execution-list extension. For information of performing scatter and gather using list:tokenize() , and list:collect() refer the examples in Data Pipelining section. For information on all list functions , refer the Siddhi APIs . define stream ProductComboStream ( product1 string , product2 string , product3 string ); @ info ( name = Create - list ) from ProductComboStream select list : create ( product1 , product2 , product3 ) as productList insert into NewListStream ; @ info ( name = Check - list ) from NewListStream select list : isList ( productList ) as isList , list : contains ( productList , Cake ) as isCakePresent , list : isEmpty ( productList ) as isEmpty , list : get ( productList , 1 ) as valueAt1 , list : size ( productList ) as size insert into ListAnalysisStream ; @ info ( name = Clone - and - update ) from NewListStream select list : remove ( list : add ( list : clone ( productList ), Toffee ), Cake ) as productList insert into UpdatedListStream ; define stream ProductComboStream ( product1 string, product2 string, product3 string); Defines ProductComboStream having string type attributes product1 , product2 , and product3 . @info(name = Create-list ) from ProductComboStream select list:create(product1, product2, product3) as productList Create a list with values of product1 , product2 , and product3 . insert into NewListStream; @info(name = Check-list ) from NewListStream select list:isList(productList) as isList, Check if productList is a List. list:contains(productList, Cake ) as isCakePresent, Check if productList contains 'Cake' . list:isEmpty(productList) as isEmpty, Check if productList is empty. list:get(productList, 1) as valueAt1, Get the value at index 1 from productList . list:size(productList) as size Get size of productList . insert into ListAnalysisStream; @info(name = Clone-and-update ) from NewListStream select list:remove( list:add(list:clone(productList), Toffee ), Cake ) as productList Clone productList , add Toffee to the end of the list, and remove Cake from the list. insert into UpdatedListStream; Input Below event is sent to ProductComboStream , [ 'Ice Cream' , 'Chocolate' , 'Cake' ] Output After processing, the following events will be arriving at each stream: NewListStream: [ [Ice Cream, Chocolate, Cake] ] ListAnalysisStream: [ true , true , false , Chocolate , 3 ] UpdatedListStream: [ [Ice Cream, Chocolate, Toffee] ]","title":"List"},{"location":"docs/examples/logical-pattern/","text":"var base_url = \"\"; Logical Pattern Logical patterns match events that arrive in temporal order and correlate them with logical relationships such as and , or and not . Refer the Siddhi query guide for more information. define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream RoomKeyStream ( deviceID long , roomNo int , action string ); @ sink ( type = log ) define stream RegulatorActionStream ( roomNo int , action string ); from every e1 = RegulatorStateChangeStream [ action == on ] - e2 = RoomKeyStream [ e1 . roomNo == roomNo and action == removed ] or e3 = RegulatorStateChangeStream [ e1 . roomNo == roomNo and action == off ] select e1 . roomNo , ifThenElse ( e2 is null , none , stop ) as action having action != none insert into RegulatorActionStream ; define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); Defines RegulatorStateChangeStream having information of regulator state change such as deviceID , roomNo , tempSet and action . define stream RoomKeyStream(deviceID long, roomNo int, action string); Defines RoomKeyStream which contains the events related to room key usage. @sink(type= log ) define stream RegulatorActionStream(roomNo int, action string); Defines RegulatorActionStream which contains the events related to regulator state changes. from every e1=RegulatorStateChangeStream[ action == on ] - e2=RoomKeyStream [ e1.roomNo == roomNo and action == removed ] or e3=RegulatorStateChangeStream [ e1.roomNo == roomNo and action == off ] Sends a stop action on RegulatorActionStream stream, if a removed action is triggered in the RoomKeyStream stream before the regulator state changing to off which is notified RegulatorStateChangeStream stream select e1.roomNo, ifThenElse( e2 is null, none , stop ) as action having action != none Checks whether pattern triggered due to removal of room key. insert into RegulatorActionStream; This application sends a stop action on the regulator if a removed action is triggered in the RoomKeyStream stream. Input First, below event is sent to RegulatorStateChangeStream , [ 10 , 5 , 30 , on ] Then, send below events are sent to RoomKeyStream , [ 10 , 5 , removed ] Output After processing the above input events, the event arriving at RegulatorActionStream will be as follows: [ 5 , stop ]","title":"Logical pattern"},{"location":"docs/examples/logical-sequence/","text":"var base_url = \"\"; Logical Sequence The sequence can repetitively match event sequences and use logical event ordering (using and, or, and not). Refer the Siddhi query guide for more information. define stream TempSensorStream ( deviceID long , isActive bool ); define stream HumidSensorStream ( deviceID long , isActive bool ); define stream RegulatorStream ( deviceID long , isOn bool ); @ sink ( type = log ) define stream StateNotificationStream ( deviceID long , tempSensorActive bool , humidSensorActive bool ); from every e1 = RegulatorStream [ isOn == true ], e2 = TempSensorStream and e3 = HumidSensorStream select e1 . deviceID , e2 . isActive as tempSensorActive , e3 . isActive as humidSensorActive insert into StateNotificationStream ; define stream TempSensorStream(deviceID long, isActive bool); Defines TempSensorStream having information of temperature sensor device. define stream HumidSensorStream(deviceID long, isActive bool); Defines HumidSensorStream having information of humidity sensor device. define stream RegulatorStream(deviceID long, isOn bool); Defines RegulatorStream which contains the events from regulator with attributes deviceID and isOn . @sink(type= log ) define stream StateNotificationStream (deviceID long, tempSensorActive bool, humidSensorActive bool); Defines StateNotificationStream which tells the state of temperature and humidity sensors. from every e1=RegulatorStream[isOn == true], e2=TempSensorStream and e3=HumidSensorStream select e1.deviceID, e2.isActive as tempSensorActive, e3.isActive as humidSensorActive insert into StateNotificationStream; Identifies a regulator activation event immediately followed by both temperature sensor and humidity sensor activation events in either order. This application can be used identify a regulator activation event immediately followed by both temperature sensor and humidity sensor activation events in either order. Input First, below event is sent to RegulatorStream , [ 2134 , true ] Then, below event is sent to HumidSensorStream , [ 124 , true ] Then, below event is sent to TempSensorStream , [ 242 , false ] Output After processing the above input events, the event arriving at StateNotificationStream will be as follows: [ 2134 , false , true ]","title":"Logical sequence"},{"location":"docs/examples/map/","text":"var base_url = \"\"; Map Provides examples on basic map functions provided via siddhi-execution-map extension. For information of performing scatter and gather using map:tokenize() , and map:collect() refer the examples in Data Pipelining section. For information on all map functions , refer the Siddhi APIs . define stream CoupleDealInfoStream ( item1 string , price1 double , item2 string , price2 double ); @ info ( name = Create - map ) from CoupleDealInfoStream select map : create ( item1 , price1 , item2 , price2 ) as itemPriceMap insert into NewMapStream ; @ info ( name = Check - map ) from NewMapStream select map : isMap ( itemPriceMap ) as isMap , map : containsKey ( itemPriceMap , Cookie ) as isCookiePresent , map : containsValue ( itemPriceMap , 24.0 ) as isThereItemWithPrice24 , map : isEmpty ( itemPriceMap ) as isEmpty , map : keys ( itemPriceMap ) as keys , map : size ( itemPriceMap ) as size insert into MapAnalysisStream ; @ info ( name = Clone - and - update ) from NewMapStream select map : replace ( map : put ( map : clone ( itemPriceMap ), Gift , 1.0 ), Cake , 12.0 ) as itemPriceMap insert into ItemInsertedMapStream ; define stream CoupleDealInfoStream ( item1 string, price1 double, item2 string, price2 double); Defines CoupleDealInfoStream having attributes item1 , price1 , item2 , and price2 with string and double types. @info(name = Create-map ) from CoupleDealInfoStream select map:create(item1, price1, item2, price2) as itemPriceMap Create a map with values of item1 and item2 as keys, and price1 and price2 as values. insert into NewMapStream; @info(name = Check-map ) from NewMapStream select map:isMap(itemPriceMap) as isMap, Check if itemPriceMap is a Map. map:containsKey(itemPriceMap, Cookie ) as isCookiePresent, Check if itemPriceMap contains a key 'Cookie' . map:containsValue(itemPriceMap, 24.0) as isThereItemWithPrice24, Check if itemPriceMap contains a value 24.0 . map:isEmpty(itemPriceMap) as isEmpty, Check if itemPriceMap is empty. map:keys(itemPriceMap) as keys, Get all keys of itemPriceMap as a List. map:size(itemPriceMap) as size Get size of itemPriceMap . insert into MapAnalysisStream; @info(name = Clone-and-update ) from NewMapStream select map:replace( map:put(map:clone(itemPriceMap), Gift , 1.0), Cake , 12.0) as itemPriceMap Clone itemPriceMap , put Gift key with value 1.0 to it, and replace Cake key with value 12.0 . insert into ItemInsertedMapStream; Input Below event is sent to CoupleDealInfoStream , [ 'Chocolate' , 18.0 , 'Ice Cream' , 24.0 ] Output After processing, the following events will be arriving at each stream: NewMapStream: [ {Ice Cream=24.0, Chocolate =18.0} ] MapAnalysisStream: [ true , false , true , false , [Ice Cream, Chocolate] , 2 ] ItemInsertedMapStream: [ {Ice Cream=24.0, Gift=1.0, Chocolate =18.0} ]","title":"Map"},{"location":"docs/examples/math-logical-operations/","text":"var base_url = \"\"; Math & Logical Operation Provides examples on performing math or logical operations on events. To see all complex math operations that can be performed please see Siddhi Execution Math Docs define stream TemperatureStream ( sensorId string , temperature double ); @ infor ( name = celciusTemperature ) from TemperatureStream select sensorId , ( temperature * 9 / 5 ) + 32 as temperature insert into FahrenheitTemperatureStream ; @ info ( name = Overall - analysis ) from FahrenheitTemperatureStream select sensorId , math : floor ( temperature ) as approximateTemp insert all events into OverallTemperatureStream ; @ info ( name = RangeFilter ) from OverallTemperatureStream [ approximateTemp - 2 and approximateTemp 40 ] select * insert into NormalTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); @infor(name = celciusTemperature ) from TemperatureStream select sensorId, (temperature * 9 / 5) + 32 as temperature Converts Celsius value into Fahrenheit. insert into FahrenheitTemperatureStream; @info(name = Overall-analysis ) from FahrenheitTemperatureStream select sensorId, math:floor(temperature) as approximateTemp Calculate approximated temperature to the first digit insert all events into OverallTemperatureStream; @info(name = RangeFilter ) from OverallTemperatureStream [ approximateTemp -2 and approximateTemp 40] Filter out events where -2 approximateTemp 40 select * insert into NormalTemperatureStream; Input Below event is sent to TemperatureStream , [ 'SensorId' , -17 ] Output After processing, the following events will be arriving at each stream: FahrenheitTemperatureStream: [ 'SensorId' , 1.4 ] OverallTemperatureStream: [ 'SensorId' , 1.0 ] NormalTemperatureStream: [ 'SensorId' , 1.0 ]","title":"Math logical operations"},{"location":"docs/examples/named-window/","text":"var base_url = \"\"; Named Window Provides examples on defining a named window, and summarizing data based on that. This example uses time window as the named window, but any window can be defined and used as a name window. For more information on named windows refer the Siddhi query guide . define stream TemperatureStream ( sensorId string , temperature double ); define window OneMinTimeWindow ( sensorId string , temperature double ) time ( 1 min ) ; @ info ( name = Insert - to - window ) from TemperatureStream insert into OneMinTimeWindow ; @ info ( name = Min - max - analysis ) from OneMinTimeWindow select min ( temperature ) as minTemperature , max ( temperature ) as maxTemperature insert into MinMaxTemperatureOver1MinStream ; @ info ( name = Per - sensor - analysis ) from OneMinTimeWindow select sensorId , avg ( temperature ) as avgTemperature group by sensorId insert into AvgTemperaturePerSensorStream ; define stream TemperatureStream (sensorId string, temperature double); define window OneMinTimeWindow (sensorId string, temperature double) time(1 min) ; Define a named window with name OneMinTimeWindow to retain events over 1 minute in a sliding manner. @info(name = Insert-to-window ) from TemperatureStream insert into OneMinTimeWindow; Insert events in to the named time window. @info(name = Min-max-analysis ) from OneMinTimeWindow select min(temperature) as minTemperature, max(temperature) as maxTemperature Calculate minimum and maximum of temperature on events in OneMinTimeWindow window. insert into MinMaxTemperatureOver1MinStream; @info(name = Per-sensor-analysis ) from OneMinTimeWindow select sensorId, avg(temperature) as avgTemperature group by sensorId Calculate average of temperature , by grouping events by sensorId , on the OneMinTimeWindow window. insert into AvgTemperaturePerSensorStream; Aggregation Behavior When events are sent to TemperatureStream stream, following events will get emitted at MinMaxTemperatureOver1MinStream stream via Min-max-analysis query, and AvgTemperaturePerSensorStream stream via Per-sensor-analysis query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 125px; } Time Input to TemperatureStream Output at MinMaxTemperatureOver1MinStream Output at AvgTemperaturePerSensorStream 9:00:10 [ '1001' , 21.0 ] [ 21.0 , 21.0 ] [ '1001' , 21.0 ] 9:00:20 [ '1002' , 25.0 ] [ 21.0 , 25.0 ] [ '1002' , 25.0 ] 9:00:35 [ '1002' , 26.0 ] [ 21.0 , 26.0 ] [ '1002' , 25.5 ] 9:00:40 [ '1002' , 27.0 ] [ 21.0 , 27.0 ] [ '1002' , 26.0 ] 9:00:55 [ '1001' , 19.0 ] [ 19.0 , 27.0 ] [ '1001' , 20.0 ] 9:01:30 [ '1002' , 22.0 ] [ 19.0 , 27.0 ] [ '1002' , 25.0 ] 9:02:10 [ '1001' , 18.0 ] [ 18.0 , 22.0 ] [ '1001' , 18.0 ]","title":"Named window"},{"location":"docs/examples/non-occurrence-pattern/","text":"var base_url = \"\"; Non Occurrence Pattern Non occurrence patterns identifies the absence of events when detecting a pattern. Siddhi detects non-occurrence of events using the not keyword, and its effective non-occurrence checking period is bounded either by fulfillment of a condition associated by and or via an expiry time using time period . Refer the Siddhi query guide for more information. define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream TemperatureStream ( roomNo int , temp double ); @ sink ( type = log ) define stream RoomTemperatureAlertStream ( roomNo int ); from e1 = RegulatorStateChangeStream [ action == on ] - not TemperatureStream [ e1 . roomNo == roomNo and temp = e1 . tempSet ] for 30 sec select e1 . roomNo as roomNo insert into RoomTemperatureAlertStream ; define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); Defines RegulatorStateChangeStream having information of regulator state change such as deviceID , roomNo , tempSet and action . define stream TemperatureStream (roomNo int, temp double); Defines TemperatureStream having information of room temperature such as roomNo and temp . @sink(type= log ) define stream RoomTemperatureAlertStream(roomNo int); Defines RoomTemperatureAlertStream which contains the temperature alerts. from e1=RegulatorStateChangeStream[action == on ] - not TemperatureStream[e1.roomNo == roomNo and temp = e1.tempSet] for 30 sec Alerts if no temperature event having a temperature less than what is set in regulator arrives within 5 minutes after switching on the regulator. select e1.roomNo as roomNo insert into RoomTemperatureAlertStream; This application sends a notification alert if the room temperature is not reduced to the expected level after the regulator is started. Input First, below event is sent to RegulatorStateChangeStream , [ 10 , 5 , 30 , on ] Output After processing the above input event, there will be an alert event arriving at RoomTemperatureAlertStream after the 30 seconds (from the first event): [ 5 ]","title":"Non occurrence pattern"},{"location":"docs/examples/null/","text":"var base_url = \"\"; Null Provides examples on using nulls in Siddhi Apps. For more information refer Siddhi query guide . define stream ProductInputStream ( item string , price double ); define Table ProductInfoTable ( item string , discount double ); @ info ( name = Check - for - null ) from ProductInputStream [ not ( item is null )] select item , price is null as isPriceNull insert into ProductValidationStream ; @ info ( name = Outer - join - with - table ) from ProductInputStream as s left outer join ProductInfoTable as t on s . item == t . item select s . item , s . price , t . discount , math : power ( t . discount , 2 ) is null as isFunctionReturnsNull , t is null as isTNull , s is null as isSNull , t . discount is null as isTDiscountNull , s . item is null as isSItemNull insert into DiscountValidationStream ; define stream ProductInputStream (item string, price double); define Table ProductInfoTable (item string, discount double); Empty ProductInfoTable with attributes item and discount . @info(name = Check-for-null ) from ProductInputStream [not(item is null)] Filter events with item not having null value. select item, price is null as isPriceNull Checks if price contains null value. insert into ProductValidationStream; @info(name = Outer-join-with-table ) from ProductInputStream as s left outer join ProductInfoTable as t on s.item == t.item select s.item, s.price, t.discount, math:power(t.discount, 2) is null as isFunctionReturnsNull, Check if math:power() returns null . t is null as isTNull, s is null as isSNull, Check if streams t and s are null . t.discount is null as isTDiscountNull, s.item is null as isSItemNull Check if streams attributes t.discount and s.item are null . insert into DiscountValidationStream; Input Below event is sent to ProductInputStream , [ 'Cake' , 12.0 ] Output After processing, the following events will be arriving at each stream: ProductValidationStream: [ Cake , false ] DiscountValidationStream: [ Cake , 12.0 , null , true , true , false , true , false ]","title":"Null"},{"location":"docs/examples/partition-by-value/","text":"var base_url = \"\"; Partition Events by Value Provides example on partitioning events by attribute values. For more information on partitioning events based on value ranges, refer other examples under data pipelining section. For more information on partition refer the Siddhi query guide . define stream LoginStream ( userID string , loginSuccessful bool ); @ purge ( enable = true , interval = 10 sec , idle . period = 1 hour ) partition with ( userID of LoginStream ) begin @ info ( name = Aggregation - query ) from LoginStream # window . length ( 3 ) select userID , loginSuccessful , count () as attempts group by loginSuccessful insert into # LoginAttempts ; @ info ( name = Alert - query ) from # LoginAttempts [ loginSuccessful == false and attempts == 3 ] select userID , 3 consecutive login failures! as message insert into UserSuspensionStream ; end ; define stream LoginStream ( userID string, loginSuccessful bool); @purge(enable= true , interval= 10 sec , idle.period= 1 hour ) Optional purging configuration, to remove partition instances that haven t received events for 1 hour by checking every 10 sec . partition with ( userID of LoginStream ) Partitions the events based on userID . begin @info(name= Aggregation-query ) from LoginStream#window.length(3) select userID, loginSuccessful, count() as attempts group by loginSuccessful Calculates success and failure login attempts from last 3 events of each userID . insert into #LoginAttempts; Inserts results to #LoginAttempts inner stream that is only accessible within the partition instance. @info(name= Alert-query ) from #LoginAttempts[loginSuccessful==false and attempts==3] select userID, 3 consecutive login failures! as message insert into UserSuspensionStream; Consumes events from the inner stream, and suspends userID s that have 3 consecutive login failures. end; Partition Behavior When events are sent to LoginStream stream, following events will be generated at #LoginAttempts inner stream via Aggregation-query query, and UserSuspensionStream stream via Alert-query query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 150px; } Input to TemperatureStream At #LoginAttempts Output at UserSuspensionStream [ '1001' , false ] [ '1001' , false , 1 ] - [ '1002' , true ] [ '1002' , true , 1 ] - [ '1002' , false ] [ '1002' , false , 1 ] - [ '1002' , false ] [ '1002' , false , 2 ] - [ '1001' , false ] [ '1001' , false , 2 ] - [ '1001' , true ] [ '1001' , true , 1 ] - [ '1001' , false ] [ '1001' , false , 2 ] - [ '1002' , false ] [ '1002' , false , 2 ] [ '1002' , '3 consecutive login failures!' ]","title":"Partition by value"},{"location":"docs/examples/regex-matching/","text":"var base_url = \"\"; Regex Matching This demonstrates event cleansing using regex expressions. Regex operations can be performed using Siddhi Execution Regex extension. define stream SweetProductionStream ( name string , amount int ); @ info ( name = ProcessSweetProductionStream ) from SweetProductionStream select name , regex : matches ( chocolate (. * ) , name ) as isAChocolateProduct , regex : group ( . * \\ s (. * ) , name , 1 ) as sweetType insert into ChocolateProductStream ; define stream SweetProductionStream (name string, amount int); Defines SweetProductionStream having information of name and amount @info(name= ProcessSweetProductionStream ) from SweetProductionStream select name, regex:matches( chocolate(.*) , name) as isAChocolateProduct, Matches if name begins with the word chocolate regex:group( .*\\s(.*) , name, 1) as sweetType Captures the sweetType of the sweet following the flavour in name insert into ChocolateProductStream; Input Below event is sent to SweetProductionStream , [ 'chocolate cake' , 34 ] Output After processing, the event arriving at ChocolateProductStream will be as follows: [ 'chocolate cake' , true , 'cake' ]","title":"Regex matching"},{"location":"docs/examples/scatter-and-gather-json/","text":"var base_url = \"\"; Scatter and Gather (JSON) Provides example on performing scatter and gather on JSON values. For more information on performing scatter and gather on string, refer other examples under data pipelining section. define stream PurchaseStream ( order string , store string ); @ info ( name = Scatter - query ) from PurchaseStream #json:tokenize( order , $ . order . items ) select json : getString ( order , $ . order . id ) as orderId , jsonElement as item , store insert into TokenizedItemStream ; @ info ( name = Transform - query ) from TokenizedItemStream select orderId , ifThenElse ( json : getString ( item , name ) == cake , json : toString ( json : setElement ( item , price , json : getDouble ( item , price ) - 5 ) ), item ) as item , store insert into DiscountedItemStream ; @ info ( name = Gather - query ) from DiscountedItemStream # window . batch () select orderId , json : group ( item ) as items , store insert into GroupedItemStream ; @ info ( name = Format - query ) from GroupedItemStream select str : fillTemplate ( { discountedOrder : { id : {{ 1 }} , store : {{ 3 }} , items :{{2}} } } , orderId , items , store ) as discountedOrder insert into DiscountedOrderStream ; define stream PurchaseStream (order string, store string); @info(name = Scatter-query ) from PurchaseStream#json:tokenize(order, $.order.items ) select json:getString(order, $.order.id ) as orderId, jsonElement as item, store Scatter elements under $.order.items in to separate events. insert into TokenizedItemStream; @info(name = Transform-query ) from TokenizedItemStream select orderId, ifThenElse(json:getString(item, name ) == cake , json:toString( json:setElement(item, price , json:getDouble(item, price ) - 5 ) ), item) as item, store Provide $5 discount to cakes. insert into DiscountedItemStream; @info(name = Gather-query ) from DiscountedItemStream#window.batch() Collect events traveling as a batch via batch() window. select orderId, json:group(item) as items, store Combine item from all events in a batch as a single JSON Array. insert into GroupedItemStream; @info(name = Format-query ) from GroupedItemStream select str:fillTemplate( { discountedOrder : { id : {{1}} , store : {{3}} , items :{{2}} } } , orderId, items, store) as discountedOrder Format the final JSON by combining orderId , items , and store . insert into DiscountedOrderStream; Input Below event is sent to PurchaseStream , [{ order :{ id : 501 , items :[{ name : cake , price :25.0}, { name : cookie , price :15.0}, { name : bun , price :20.0} ] } }, 'CA'] Output After processing, following events will be arriving at TokenizedItemStream : [ '501' , '{ name : cake , price :25.0}' , 'CA' ], [ '501' , '{ name : cookie , price :15.0}' , 'CA' ], [ '501' , '{ name : bun , price :20.0}' , 'CA' ] The events arriving at DiscountedItemStream will be as follows: [ '501' , '{ name : cake , price :20.0}' , 'CA' ], [ '501' , '{ name : cookie , price :15.0}' , 'CA' ], [ '501' , '{ name : bun , price :20.0}' , 'CA' ] The event arriving at GroupedItemStream will be as follows: [ '501' , '[{ price :20.0, name : cake },{ price :15.0, name : cookie },{ price :20.0, name : bun }]' , 'CA' ] The event arriving at DiscountedOrderStream will be as follows: [ '{ discountedOrder : { id : 501 , store : CA , items :[{ price :20.0, name : cake }, { price :15.0, name : cookie }, { price :20.0, name : bun }] } }' ]","title":"Scatter and gather json"},{"location":"docs/examples/scatter-and-gather-string/","text":"var base_url = \"\"; Scatter and Gather (String) Provides example on performing scatter and gather on string values. For more information on performing scatter and gather on json, refer other examples under data pipelining section. define stream PurchaseStream ( userId string , items string , store string ); @ info ( name = Scatter - query ) from PurchaseStream #str:tokenize( items , , , true ) select userId , token as item , store insert into TokenizedItemStream ; @ info ( name = Transform - query ) from TokenizedItemStream select userId , str : concat ( store , - , item ) as itemKey insert into TransformedItemStream ; @ info ( name = Gather - query ) from TransformedItemStream # window . batch () select userId , str : groupConcat ( itemKey , , ) as itemKeys insert into GroupedPurchaseItemStream ; define stream PurchaseStream (userId string, items string, store string); @info(name = Scatter-query ) from PurchaseStream#str:tokenize(items, , , true) select userId, token as item, store Scatter value of items in to separate events by , . insert into TokenizedItemStream; @info(name = Transform-query ) from TokenizedItemStream select userId, str:concat(store, - , item) as itemKey Concat tokenized item with store . insert into TransformedItemStream; @info(name = Gather-query ) from TransformedItemStream#window.batch() Collect events traveling as a batch via batch() window. select userId, str:groupConcat(itemKey, , ) as itemKeys Concat all events in a batch separating them by , . insert into GroupedPurchaseItemStream; Input Below event containing a JSON string is sent to PurchaseStream , [ '501' , 'cake,cookie,bun,cookie' , 'CA' ] Output After processing, the events arriving at TokenizedItemStream will be as follows: [ '501' , 'cake' , 'CA' ], [ '501' , 'cookie' , 'CA' ], [ '501' , 'bun' , 'CA' ] The events arriving at TransformedItemStream will be as follows: [ '501' , 'CA-cake' ], [ '501' , 'CA-cookie' ], [ '501' , 'CA-bun' ] The event arriving at GroupedPurchaseItemStream will be as follows: [ '501' , 'CA-cake,CA-cookie,CA-bun' ]","title":"Scatter and gather string"},{"location":"docs/examples/sequence-with-count/","text":"var base_url = \"\"; Sequence with Count Sequence query does expect the matching events to occur immediately after each other, and it can successfully correlate the events who do not have other events in between. Here, sequence can count event occurrences. Refer the Siddhi query guide for more information. define stream TemperatureStream ( roomNo int , temp double ); @ sink ( type = log ) define stream PeekTemperatureStream ( roomNo int , initialTemp double , peekTemp double , firstDropTemp double ); partition with ( roomNo of TemperatureStream ) begin @ info ( name = temperature - trend - analyzer ) from every e1 = TemperatureStream , e2 = TemperatureStream [ ifThenElse ( e2 [ last ]. temp is null , e1 . temp = temp , e2 [ last ]. temp = temp )] + , e3 = TemperatureStream [ e2 [ last ]. temp temp ] select e1 . roomNo , e1 . temp as initialTemp , e2 [ last ]. temp as peekTemp , e3 . temp as firstDropTemp insert into PeekTemperatureStream ; end ; define stream TemperatureStream(roomNo int, temp double); Defines TemperatureStream having information on room temperatures such as roomNo and temp . @sink(type= log ) define stream PeekTemperatureStream(roomNo int, initialTemp double, peekTemp double, firstDropTemp double); Defines PeekTemperatureStream which contains events related to peak temperature trends. partition with (roomNo of TemperatureStream) begin Partition the TemperatureStream events by roomNo @info(name = temperature-trend-analyzer ) from every e1=TemperatureStream, e2=TemperatureStream[ifThenElse(e2[last].temp is null, e1.temp = temp, e2[last].temp = temp)]+, e3=TemperatureStream[e2[last].temp temp] Identifies the trend of the temperature in a room select e1.roomNo, e1.temp as initialTemp, e2[last].temp as peekTemp, e3.temp as firstDropTemp insert into PeekTemperatureStream ; Projects the lowest, highest and the first drop in the temperature trend end; This application identifies temperature peeks by monitoring continuous increases in temp attribute and alerts upon the first drop. Input Below events are sent to TemperatureStream , [ 20 , 29 ] [ 10 , 28 ] [ 20 , 30 ] [ 20 , 32 ] [ 20 , 35 ] [ 20 , 33 ] Output After processing the above input events, the event arriving at PeekTemperatureStream will be as follows: [ 20 , 29.0 , 35.0 , 33.0 ]","title":"Sequence with count"},{"location":"docs/examples/session/","text":"var base_url = \"\"; Session Provides examples on aggregating events over continuous activity sessions in a sliding manner. To aggregate events in batches, or based on events, refer other the examples in Data Summarization section. For more information on windows refer the Siddhi query guide . define stream PurchaseStream ( userId string , item string , price double ); @ info ( name = Session - analysis ) from PurchaseStream # window . session ( 1 min , userId ) select userId , count () as totalItems , sum ( price ) as totalPrice group by userId insert into UserIdPurchaseStream ; @ info ( name = Session - analysis - with - late - event - arrivals ) from PurchaseStream # window . session ( 1 min , userId , 20 sec ) select userId , count () as totalItems , sum ( price ) as totalPrice group by userId insert into OutOfOrderUserIdPurchaseStream ; define stream PurchaseStream (userId string, item string, price double); @info(name = Session-analysis ) from PurchaseStream#window.session(1 min, userId) Aggregate events over a userId based session window with 1 minute session gap. select userId, count() as totalItems, sum(price) as totalPrice group by userId Calculate count and sum of price per userId during the session. insert into UserIdPurchaseStream; Output when events are added to the session. @info(name = Session-analysis-with-late-event-arrivals ) from PurchaseStream#window.session(1 min, userId, 20 sec) Aggregate events over a userId based session window with 1 minute session gap, and 20 seconds of allowed latency to capture late event arrivals. select userId, count() as totalItems, sum(price) as totalPrice group by userId Calculate count and sum of price per userId during the session. insert into OutOfOrderUserIdPurchaseStream; Output when events are added to the session. Aggregation Behavior When events are sent to PurchaseStream stream, following events will get emitted at UserIdPurchaseStream stream via Session-analysis query, and OutOfOrderUserIdPurchaseStream stream via Session-analysis-with-late-event-arrivals query. .md-typeset table:not([class]) th:nth-child(3) { min-width: 120px; } .md-typeset table:not([class]) th { min-width: 0; } Time Event Timestamp Input to PurchaseStream Output at UserIdPurchaseStream Output at OutOfOrderUserIdPurchaseStream 9:00:00 9:00:00 [ '1001' , 'cake' , 18.0 ] [ '1001' , 1 , 18.0 ] [ '1001' , 1 , 18.0 ] 9:00:20 9:00:20 [ '1002' , 'croissant' , 23.0 ] [ '1002' , 1 , 23.0 ] [ '1002' , 1 , 23.0 ] 9:00:40 9:00:40 [ '1002' , 'cake' , 22.0 ] [ '1002' , 2 , 45.0 ] [ '1002' , 2 , 45.0 ] 9:01:05 9:00:50 [ '1001' , 'pie' , 22.0 ] No events, as event arrived late, and did not fall into a session. [ '1001' , 2 , 40.0 ] 9:01:10 9:01:10 [ '1001' , 'cake' , 10.0 ] [ '1001' , 1 , 10.0 ] [ '1001' , 3 , 50.0 ] 9:01:50 9:01:50 [ '1002' , 'cake' , 20.0 ] [ '1002' , 1 , 20.0 ] [ '1002' , 1 , 23.0 ] 9:02:40 9:02:40 [ '1001' , 'croissant' , 23.0 ] [ '1001' , 1 , 23.0 ] [ '1001' , 1 , 23.0 ]","title":"Session"},{"location":"docs/examples/siddhiapp/","text":"var base_url = \"\"; Siddhi Application Provides introduction to the concept of Siddhi Application . Siddhi App provides an isolated execution environment for processing the execution logic. It can be deployed and processed independently of other SiddhiApps in the system. Siddhi Apps can use inMemory sources and sinks to communicate between each other. @app:name( Temperature - Processor ) @app:description( App for processing temperature data . ) @ source ( type = inMemory , topic = SensorDetail ) define stream TemperatureStream ( sensorId string , temperature double ); @ sink ( type = inMemory , topic = Temperature ) define stream TemperatureOnlyStream ( temperature double ); @ info ( name = Simple - selection ) from TemperatureStream select temperature insert into TemperatureOnlyStream ; @app:name( Temperature-Processor ) Name of the Siddhi Application @app:description( App for processing temperature data. ) Optional description for Siddhi Application @source(type= inMemory , topic= SensorDetail ) define stream TemperatureStream ( sensorId string, temperature double); InMemory source to consume events from other Siddhi Apps. @sink(type= inMemory , topic= Temperature ) define stream TemperatureOnlyStream (temperature double); InMemory sink to publish events from other Siddhi Apps. @info(name = Simple-selection ) from TemperatureStream select temperature insert into TemperatureOnlyStream; Input When an event [ 'aq-14' , 35.4 ] is pushed via the SensorDetail topic of the inMemory transport from another Siddhi App, the event will be consumed and mapped to the TemperatureStream stream. Output After processing, the event [ 35.4 ] arriving at TemperatureOnlyStream will be emitted via Temperature topic of the inMemory transport to other subscribed Siddhi Apps.","title":"Siddhiapp"},{"location":"docs/examples/simple-pattern/","text":"var base_url = \"\"; Simple Pattern The pattern is a state machine implementation that detects event occurrences from events arrived via one or more event streams over time. This application demonstrates a simple pattern use case of detecting high-temperature event occurrence of a continuous event stream. define stream TemperatureStream ( roomNo int , temp double ); @ sink ( type = log ) define Stream HighTempAlertStream ( roomNo int , initialTemp double , finalTemp double ); @ info ( name = temperature - increase - identifier ) from every ( e1 = TemperatureStream ) - e2 = TemperatureStream [ e1 . roomNo == roomNo and ( e1 . temp + 5 ) = temp ] within 10 min select e1 . roomNo , e1 . temp as initialTemp , e2 . temp as finalTemp insert into HighTempAlertStream ; define stream TemperatureStream(roomNo int, temp double); Defines TemperatureStream having information of room temperature such as roomNo and temp . @sink(type = log ) define Stream HighTempAlertStream(roomNo int, initialTemp double, finalTemp double); Defines HighTempAlertStream which contains the alerts for high temperature. @info(name= temperature-increase-identifier ) from every( e1 = TemperatureStream ) - e2 = TemperatureStream[ e1.roomNo == roomNo and (e1.temp + 5) = temp ] within 10 min Identify if the temperature of a room increases by 5 degrees within 10 min. select e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp insert into HighTempAlertStream; This application sends an alert if the temperature of a room increases by 5 degrees within 10 min. Input Below events are sent to TemperatureStream within 10 minutes, [ 2 , 35 ] [ 2 , 37 ] [ 2 , 40 ] Output After processing the above input events, the event arriving at HighTempAlertStream will be as follows: [ 2 , 35.0 , 40.0 ]","title":"Simple pattern"},{"location":"docs/examples/simple-sequence/","text":"var base_url = \"\"; Simple Sequence Sequence is a state machine implementation that detects consecutive event occurrences from events arrived via one or more event streams over time. Here all matching events need to arrive consecutively, and there should not be any non-matching events in between the matching sequence of events. Refer the Siddhi query guide for more information. define stream StockRateStream ( symbol string , price float , volume int ); @ sink ( type = log ) define stream PeakStockRateStream ( symbol string , rateAtPeak float ); partition with ( symbol of StockRateStream ) begin from every e1 = StockRateStream , e2 = StockRateStream [ e1 . price price ], e3 = StockRateStream [ e2 . price price ] within 10 min select e1 . symbol , e2 . price as rateAtPeak insert into PeakStockRateStream ; end ; define stream StockRateStream (symbol string, price float, volume int); Defines StockRateStream having information on stock rate such as symbol , price and volume . @sink(type= log ) define stream PeakStockRateStream (symbol string, rateAtPeak float); Defines PeakStockRateStream which contains the peak stock rate. partition with (symbol of StockRateStream) begin Partition the StockRateStream events by symbol from every e1=StockRateStream, e2=StockRateStream[e1.price price], e3=StockRateStream[e2.price price] Identifies the peak stock price (top rate of the stock price trend) within 10 min select e1.symbol, e2.price as rateAtPeak insert into PeakStockRateStream ; end; This application can be used to detect trends from a stock trades stream; in the above example, peak stock rate identified. Input Below events are sent to StockRateStream within 10 minutes, [ mint-leaves , 35 , 20 ] [ mint-leaves , 40 , 15 ] [ mint-leaves , 38 , 20 ] Output After processing the above input events, the event arriving at PeakStockRateStream will be as follows: [ mint-leaves , 40 ]","title":"Simple sequence"},{"location":"docs/examples/sliding-length/","text":"var base_url = \"\"; Sliding Event Count Provides examples on aggregating events based on event count in a sliding manner. To aggregate events in batches, based on time, or by session, refer other the examples in Data Summarization section. For more information on windows refer the Siddhi query guide . define stream TemperatureStream ( sensorId string , temperature double ); @ info ( name = Overall - analysis ) from TemperatureStream # window . length ( 4 ) select avg ( temperature ) as avgTemperature , max ( temperature ) as maxTemperature , count () as numberOfEvents insert into OverallTemperatureStream ; @ info ( name = SensorId - analysis ) from TemperatureStream # window . length ( 5 ) select sensorId , avg ( temperature ) as avgTemperature , min ( temperature ) as maxTemperature group by sensorId having avgTemperature = 20.0 insert into SensorIdTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); @info(name = Overall-analysis ) from TemperatureStream#window.length(4) Aggregate last 4 events in a sliding manner. select avg(temperature) as avgTemperature, max(temperature) as maxTemperature, count() as numberOfEvents insert into OverallTemperatureStream; Calculate average, maximum, and count for temperature attribute. @info(name = SensorId-analysis ) from TemperatureStream#window.length(5) Aggregate last 5 events in a sliding manner. select sensorId, avg(temperature) as avgTemperature, min(temperature) as maxTemperature group by sensorId Calculate average, and minimum for temperature , by grouping events by sensorId . having avgTemperature = 20.0 Output events only when avgTemperature is greater than or equal to 20.0 . insert into SensorIdTemperatureStream; Aggregation Behavior When events are sent to TemperatureStream stream, following events will get emitted at OverallTemperatureStream stream via Overall-analysis query, and SensorIdTemperatureStream stream via SensorId-analysis query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 123px; } Input to TemperatureStream Output at OverallTemperatureStream Output at SensorIdTemperatureStream [ '1001' , 19.0 ] [ 19.0 , 19.0 , 1 ] No events, as having condition not satisfied for '1001' . [ '1002' , 26.0 ] [ 22.5 , 26.0 , 2 ] [ '1002' , 26.0 , 26.0 ] [ '1002' , 24.0 ] [ 23.0 , 26.0 , 3 ] [ '1002' , 25.5 , 24.0 ] [ '1001' , 20.0 ] [ 22.5 , 26.0 , 4 ] No events, as having condition not satisfied for '1001' . [ '1001' , 21.0 ] [ 22.75 , 26.0 , 4 ] [ '1001' , 20.0 , 19.0 ] [ '1001' , 22.0 ] [ 21.75 , 24.0 , 4 ] [ '1001' , 21.0 , 20.0 ]","title":"Sliding length"},{"location":"docs/examples/sliding-time/","text":"var base_url = \"\"; Sliding Time Provides examples on aggregating events over time in a sliding manner. To aggregate events in batches, based on events, or by session, refer other the examples in Data Summarization section. For more information on windows refer the Siddhi query guide . define stream TemperatureStream ( sensorId string , temperature double ); @ info ( name = Overall - analysis ) from TemperatureStream # window . time ( 1 min ) select avg ( temperature ) as avgTemperature , max ( temperature ) as maxTemperature , count () as numberOfEvents insert all events into OverallTemperatureStream ; @ info ( name = SensorId - analysis ) from TemperatureStream # window . time ( 30 sec ) select sensorId , avg ( temperature ) as avgTemperature , min ( temperature ) as maxTemperature group by sensorId having avgTemperature 20.0 insert into SensorIdTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); @info(name = Overall-analysis ) from TemperatureStream#window.time(1 min) Aggregate events over 1 minute sliding window select avg(temperature) as avgTemperature, max(temperature) as maxTemperature, count() as numberOfEvents Calculate average, maximum, and count for temperature attribute. insert all events into OverallTemperatureStream; Output when events are added, and removed (expired) from window.time() . @info(name = SensorId-analysis ) from TemperatureStream#window.time(30 sec) Aggregate events over 30 seconds sliding window select sensorId, avg(temperature) as avgTemperature, min(temperature) as maxTemperature group by sensorId Calculate average, and minimum for temperature , by grouping events by sensorId . having avgTemperature 20.0 Output events only when avgTemperature is greater than 20.0 . insert into SensorIdTemperatureStream; Output only when events are added to window.time() . Aggregation Behavior When events are sent to TemperatureStream stream, following events will get emitted at OverallTemperatureStream stream via Overall-analysis query, and SensorIdTemperatureStream stream via SensorId-analysis query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 123px; } Time Input to TemperatureStream Output at OverallTemperatureStream Output at SensorIdTemperatureStream 9:00:00 [ '1001' , 18.0 ] [ 18.0 , 18.0 , 1 ] No events, as having condition not satisfied. 9:00:10 [ '1002' , 23.0 ] [ 20.5 , 23.0 , 2 ] [ '1002' , 23.0 , 23.0 ] 9:00:20 [ '1002' , 22.0 ] [ 21.0 , 23.0 , 3 ] [ '1002' , 22.5 , 22.0 ] 9:00:40 - - No events, as expired events are not emitted. 9:00:50 - - No events, as expired events are not emitted. 9:00:00 - [ 22.5 , 23.0 , 2 ] - 9:01:10 [ '1001' , 17.0 ] [ 19.5 , 22.0 , 2 ] - 9:01:20 - [ 17.0 , 17.0 , 1 ] - 9:02:10 - [ null , null , 0 ] -","title":"Sliding time"},{"location":"docs/examples/source-and-sink/","text":"var base_url = \"\"; Source and Sink Provides introduction to sources and sink that are used to consume and publish events to external systems. There are multiple source and sink types, but this example only explains http source, log sink, and kafka sink. For more info refer the Siddhi query guide . @ source ( type = http , receiver . url = http : //0.0.0.0:8006/temp , @ map ( type = json )) define stream TemperatureStream ( sensorId string , temperature double ); @ sink ( type = log ) @ sink ( type = kafka , topic = temperature , bootstrap . servers = localhost : 9092 , @ map ( type = json , @payload( { temp : {{ temperature }} } ))) define stream TemperatureOnlyStream ( temperature double ); @ info ( name = Simple - selection ) from TemperatureStream select temperature insert into TemperatureOnlyStream ; @source(type= http , receiver.url= http://0.0.0.0:8006/temp , @map(type= json )) HTTP source to consume JSON messages with default mapping via url http://0.0.0.0:8006/temp . define stream TemperatureStream ( sensorId string, temperature double); Defines TemperatureStream stream having sensorId and temperature attributes of types string and double . @sink(type= log ) Log sink to log Siddhi events arriving via TemperatureOnlyStream stream. @sink(type= kafka , topic= temperature , bootstrap.servers= localhost:9092 , @map(type= json , @payload( { temp : {{temperature}} } ))) Kafka sink to map events arriving via TemperatureOnlyStream stream as custom JSON events, and publish to temperature topic. define stream TemperatureOnlyStream (temperature double); Defines TemperatureOnlyStream stream having temperature attribute of type double . @info(name = Simple-selection ) from TemperatureStream select temperature insert into TemperatureOnlyStream; Input When a JSON message in the following default message format is sent to url http://0.0.0.0:8006/temp with content type application/json . It will automatically get mapped to an event in the TemperatureStream stream. { event :{ sensorId : aq-14 , temperature :35.4 } } To process custom input messages, please refer the examples related to Input Data Mapping. Output After processing, the event arriving at TemperatureOnlyStream will be emitted via log and kafka sinks. As log sink uses passThrough mapper by default, it directly logs the Siddhi Events to the console as following; Event{timestamp=1574515771712, data=[35.4], isExpired=false} The kafka sink maps the event to a custom JSON message as below and publishes it to the temperature topic. { temp : 35.4 } To output messages using other message formats, pleases refer the examples related to Output Data Mapping.","title":"Source and sink"},{"location":"docs/examples/stream-and-query/","text":"var base_url = \"\"; Stream and Query Provides introduction to streams , queries , and how queries can be chained to one another. There are multiple type of queries such as window query, join query, pattern query, etc. But this example only explains how pass-through and selection queries work. For more info refer the Siddhi query guide . define stream InputTemperatureStream ( sensorId string , temperature double ); @ info ( name = Pass - through ) from InputTemperatureStream select * insert into TemperatureAndSensorStream ; @ info ( name = Simple - selection ) from TemperatureAndSensorStream select temperature insert into TemperatureOnlyStream ; define stream InputTemperatureStream ( sensorId string, temperature double); Defines InputTemperatureStream stream to pass events having sensorId and temperature attributes of types string and double . @info(name = Pass-through ) Optional @info annotation to name the query. from InputTemperatureStream select * insert into TemperatureAndSensorStream; Query to consume events from InputTemperatureStream , produce new events by selecting all the attributes from the incoming events, and outputs them to TemperatureStream . @info(name = Simple-selection ) from TemperatureAndSensorStream Consumes events from TemperatureAndSensorStream . The schema of the stream is inferred from the previous query, hence no need to be defined. select temperature insert into TemperatureOnlyStream; Selects only the temperature attribute from events, and outputs to TemperatureOnlyStream . Events at each stream When an event with values [ 'aq-14' , 35.4 ] is sent to InputTemperatureStream stream it will get converted and travel through the streams as below. InputTemperatureStream : [ 'aq-14' , 35.4 ] TemperatureAndSensorStream : [ 'aq-14' , 35.4 ] TemperatureOnlyStream : [ 35.4 ]","title":"Stream and query"},{"location":"docs/examples/stream-join/","text":"var base_url = \"\"; Stream Join Provides examples on joining two stream based on a condition. For more information on other join operations refer the Siddhi query guide . define stream TemperatureStream ( roomNo string , temperature double ); define stream HumidityStream ( roomNo string , humidity double ); @ info ( name = Equi - join ) from TemperatureStream #window.unique:time( roomNo , 1 min ) as t join HumidityStream #window.unique:time( roomNo , 1 min ) as h on t . roomNo == h . roomNo select t . roomNo , t . temperature , h . humidity insert into TemperatureHumidityStream ; @ info ( name = Join - on - temperature ) from TemperatureStream as t left outer join HumidityStream # window . time ( 1 min ) as h on t . roomNo == h . roomNo select t . roomNo , t . temperature , h . humidity insert into EnrichedTemperatureStream ; define stream TemperatureStream (roomNo string, temperature double); define stream HumidityStream (roomNo string, humidity double); @info(name = Equi-join ) from TemperatureStream#window.unique:time(roomNo, 1 min) as t join HumidityStream#window.unique:time(roomNo, 1 min) as h on t.roomNo == h.roomNo Join latest temperature and humidity events arriving within 1 minute for each roomNo . select t.roomNo, t.temperature, h.humidity insert into TemperatureHumidityStream; @info(name = Join-on-temperature ) from TemperatureStream as t Join when events arrive in TemperatureStream . left outer join HumidityStream#window.time(1 min) as h on t.roomNo == h.roomNo When events get matched in time() window, all matched events are emitted, else null is emitted. select t.roomNo, t.temperature, h.humidity insert into EnrichedTemperatureStream; Join Behavior When events are sent to TemperatureStream stream and HumidityStream stream, following events will get emitted at TemperatureHumidityStream stream via Equi-join query, and EnrichedTemperatureStream stream via Join-on-temperature query. .md-typeset table:not([class]) th:nth-child(3) { min-width: 123px; } .md-typeset table:not([class]) th:nth-child(2) { min-width: 123px; } .md-typeset table:not([class]) th { min-width: 0; } Time Input to TemperatureStream Input to HumidityStream Output at TemperatureHumidityStream Output at EnrichedTemperatureStream 9:00:00 [ '1001' , 18.0 ] - - [ '1001' , 18.0 , null ] 9:00:10 - [ '1002' , 72.0 ] - - 9:00:15 - [ '1002' , 73.0 ] - - 9:00:30 [ '1002' , 22.0 ] - [ '1002' , 22.0 , 73.0 ] [ '1002' , 22.0 , 72.0 ], [ '1002' , 22.0 , 73.0 ] 9:00:50 - [ '1001' , 60.0 ] [ '1001' , 18.0 , 60.0 ] - 9:01:10 - [ '1001' , 62.0 ] - - 9:01:20 [ '1001' , 17.0 ] - [ '1001' , 17.0 , 62.0 ] [ '1001' , 17.0 , 60.0 ], [ '1001' , 17.0 , 62.0 ] 9:02:10 [ '1002' , 23.5 ] - - [ '1002' , 23.5 , null ]","title":"Stream join"},{"location":"docs/examples/table-and-store/","text":"var base_url = \"\"; Table and Store Provides introduction to in-memory tables and database backed stores that can be used to store events. For information on various types of stores, primary keys, indexes, and caching, refer examples related to Event Store Integration and Siddhi query guide . define stream TemperatureStream ( sensorId string , temperature double ); define table TemperatureLogTable ( sensorId string , roomNo string , temperature double ); @ store ( type = rdbms , jdbc . url = jdbc:mysql://localhost:3306/sid , username = root , password = root , jdbc . driver . name = com.mysql.jdbc.Driver ) define table SensorIdInfoTable ( sensorId string , roomNo string ); @ info ( name = Join - query ) from TemperatureStream as t join SensorIdInfoTable as s on t . sensorId == s . sensorId select t . sensorId as sensorId , s . roomNo as roomNo t . temperature as temperature insert into TemperatureLogTable ; define stream TemperatureStream ( sensorId string, temperature double); Defines TemperatureStream stream having sensorId and temperature attributes of types string and double . define table TemperatureLogTable ( sensorId string, roomNo string, temperature double); Defines in-memory TemperatureLogTable having sensorId , roomNo , and temperature attributes of types string , string , and double . @store(type= rdbms , jdbc.url= jdbc:mysql://localhost:3306/sid , username= root , password= root , jdbc.driver.name= com.mysql.jdbc.Driver ) Store annotation to back SensorIdInfoTable by a MySQL RDBMS with sid DB and SensorIdInfoTable table. define table SensorIdInfoTable ( sensorId string, roomNo string); Defines SensorIdInfoTable table. @info(name = Join-query ) from TemperatureStream as t join SensorIdInfoTable as s on t.sensorId == s.sensorId TemperatureStream with alias t joins with SensorIdInfoTable with alias s based on sensorId . select t.sensorId as sensorId, s.roomNo as roomNo t.temperature as temperature insert into TemperatureLogTable; Selects sensorId , roomNo , and temperature attributes from stream and table, and adds events to TemperatureLogTable . Event at table and store When SensorIdInfoTable table contains a recode [ 'aq-14' , '789' ], and when an event with values [ 'aq-14' , 35.4 ] is sent to TemperatureStream stream. The event will get converted and added to the TemperatureLogTable table as below. [ 'aq-14' , '789' , 35.4 ] Retrieving values from tables and stores The stored values can be retrieved by join tables and stores with the streams as in the Join-query depicted in the example, or using on-demand queries. The data in TemperatureDetailsTable can be retrieved via on-demand queries as below, using the On Demand Query REST API or by calling query() method of SiddhiAppRuntime . from TemperatureDetailsTable select *","title":"Table and store"},{"location":"docs/examples/time-rate-limit/","text":"var base_url = \"\"; Rate Limit Based on Time Output rate-limiting limits the number of events emitted by the queries based on a specified criterion such as time, and the number of events. This example provides some basic understanding of how rate limiting can be done based on time. Refer the Siddhi query guide for more information. define stream APIRequestStream ( apiName string , version string , tier string , user string , userEmail string ); define stream UserNotificationStream ( user string , apiName string , version string , tier string , userEmail string , throttledCount long ); @ info ( name = api - throttler ) from APIRequestStream # window . timeBatch ( 1 min , 0 , true ) select apiName , version , user , tier , userEmail , count () as totalRequestCount group by apiName , version , user having totalRequestCount == 3 or totalRequestCount == 0 insert all events into ThrottledStream ; @ info ( name = throttle - flag - generator ) from ThrottledStream select apiName , version , user , tier , userEmail , ifThenElse ( totalRequestCount == 0 , false , true ) as isThrottled insert into ThrottleOutputStream ; @ info ( name = notification - generator ) from ThrottleOutputStream [ isThrottled ] # window . time ( 1 hour ) select user , apiName , version , tier , userEmail , count () as throttledCount group by user , apiName , version , tier having throttledCount 2 output first every 15 min insert into UserNotificationStream ; define stream APIRequestStream (apiName string, version string, tier string, user string, userEmail string); Defines APIRequestStream stream which contains the events regarding the API request. define stream UserNotificationStream (user string, apiName string, version string, tier string, userEmail string, throttledCount long); Defines UserNotificationStream stream which contains the notification events. @info(name= api-throttler ) from APIRequestStream#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 3 or totalRequestCount == 0 insert all events into ThrottledStream; This query generates events when API is throttled and released from throttling. It generates a throttling event if an API is called more than 3 times in a minute. @info(name= throttle-flag-generator ) from ThrottledStream select apiName, version, user, tier, userEmail, ifThenElse(totalRequestCount == 0, false, true) as isThrottled Create isThrottled flag based on the API request count insert into ThrottleOutputStream; @info(name= notification-generator ) from ThrottleOutputStream[isThrottled]#window.time(1 hour) select user, apiName, version, tier, userEmail, count() as throttledCount group by user, apiName, version, tier This query helps to generate notifications to the user based on the API usage. User is notified to upgrade the subscription tier if he is frequently throttled. having throttledCount 2 Find the users who are throttled more than 2 times in an hour output first every 15 min Notify the first occurrence when throttledCount 2 for every 15 minutes. insert into UserNotificationStream; API throttling use case is considered in this example to explain the time-based rate limiting. API request events arrive through APIRequestStream stream. If there are more than 3 API requests sent by a user within 1 minute then the subsequence requests are getting throttled. And, if a user gets throttled more than 2 times within 1 hour then there is a notification event generated to notify the user to consider upgrading the subscription tier. In this case, the notification events are get rate limited based on time. For example, within 15 minutes there will be only one notification for an API, tier and user combination. Input Sent below three events to APIRequestStream stream for three minutes (three events for every minute), [ 'Get-Weather' , '1.0.0' , 'Gold' , 'George' , 'george@gmail.com' ] [ 'Get-Weather' , '1.0.0' , 'Gold' , 'George' , 'george@gmail.com' ] [ 'Get-Weather' , '1.0.0' , 'Gold' , 'George' , 'george@gmail.com' ] Output After processing, the event arriving at UserNotificationStream will be as follows: [ 'Get-Weather' , '1.0.0' , 'Gold' , 'george@gmail.com' , 3 ]","title":"Time rate limit"},{"location":"docs/examples/transform-json/","text":"var base_url = \"\"; Transform JSON Provides examples on transforming JSON object within Siddhi. For all functions available to transform JSON see Siddhi Execution JSON . define stream InputStream ( jsonString string ); from InputStream select json : toObject ( jsonString ) as jsonObj insert into PersonalDetails ; from PersonalDetails select jsonObj , json : getString ( jsonObj , $ . name ) as name , json : isExists ( jsonObj , $ . salary ) as isSalaryAvailable , json : toString ( jsonObj ) as jsonString insert into OutputStream ; from OutputStream [ isSalaryAvailable == false ] select json : setElement ( jsonObj , $ , 0 f , salary ) as jsonObj insert into PreprocessedStream ; define stream InputStream(jsonString string); from InputStream select json:toObject(jsonString) as jsonObj Transforms JSON string to JSON object which can then be manipulated insert into PersonalDetails; from PersonalDetails select jsonObj, json:getString(jsonObj, $.name ) as name, Get the name element(string) form the JSON json:isExists(jsonObj, $.salary ) as isSalaryAvailable, Validate if salary element is available json:toString(jsonObj) as jsonString Stringify the JSON object insert into OutputStream; from OutputStream[isSalaryAvailable == false] select json:setElement(jsonObj, $ , 0f, salary ) as jsonObj Set salary element to 0 is not available insert into PreprocessedStream; Input Below event is sent to InputStream , [ { name : siddhi.user , address : { country : Sri Lanka , }, contact : +9xxxxxxxx } ] Output After processing, the following events will be arriving: OutputStream: [ { address :{ country : Sri Lanka }, contact : +9xxxxxxxx , name : siddhi.user } , siddhi.user , false , {\\ name\\ : \\ siddhi.user\\ , \\ address\\ : { \\ country\\ : \\ Sri Lanka\\ , }, \\ contact\\ : \\ +9xxxxxxxx\\ } ] PreprocessedStream: [ { name : siddhi.user , salary : 0.0 address : { country : Sri Lanka , }, contact : +9xxxxxxxx } ]","title":"Transform json"},{"location":"docs/examples/type-based-filtering/","text":"var base_url = \"\"; Type based Filtering This application demonstrates filter out events based on data type of the attribute define stream SweetProductionStream ( name string , amount int ); @ info ( name = ProcessSweetProductionStream ) from SweetProductionStream select instanceOfInteger ( amount ) as isAIntInstance , name , amount insert into ProcessedSweetProductionStream ; define stream SweetProductionStream (name string, amount int); Defines SweetProductionStream having information of name and amount @info(name= ProcessSweetProductionStream ) from SweetProductionStream select instanceOfInteger(amount) as isAIntInstance, true if amount is of int type name, amount insert into ProcessedSweetProductionStream; Input Below event is sent to SweetProductionStream , [ 'chocolate cake' , 'invalid' ] Output After processing, the event arriving at ProcessedSweetProductionStream will be as follows: [ false , 'chocolate cake' , 'invalid' ]","title":"Type based filtering"},{"location":"docs/examples/value-based-filtering/","text":"var base_url = \"\"; Value based Filtering This application demonstrates filter out events based on simple conditions such as number value, range or null type. define stream TemperatureStream ( sensorId string , temperature double ); @ info ( name = EqualsFilter ) from TemperatureStream [ sensorId == A1234 ] select * insert into SenorA1234TemperatureStream ; @ info ( name = RangeFilter ) from TemperatureStream [ temperature - 2 and temperature 40 ] select * insert into NormalTemperatureStream ; @ info ( name = NullFilter ) from TemperatureStream [ sensorId is null ] select * insert into InValidTemperatureStream ; define stream TemperatureStream ( sensorId string, temperature double); Defines TemperatureStream stream to process events having sensorId and temperature (F). @info(name = EqualsFilter ) from TemperatureStream[ sensorId == A1234 ] Filter out events with sensorId equalling A1234 select * insert into SenorA1234TemperatureStream; @info(name = RangeFilter ) from TemperatureStream[ temperature -2 and temperature 40] Filter out events where -2 temperature 40 select * insert into NormalTemperatureStream; @info(name = NullFilter ) from TemperatureStream[ sensorId is null ] Filter out events with SensorId being null select * insert into InValidTemperatureStream; Input Below events are sent to TemperatureStream , [ 'A1234' , 39] [ 'sensor1' , 35] [ null , 43] Output After processing, the following events will be arriving at each stream: SenorA1234TemperatureStream: [ 'A1234' , 39] only NormalTemperatureStream: [ 'sensor1' , 35] only InValidTemperatureStream: [ null , 43] only","title":"Value based filtering"},{"location":"docs/quick-start/","text":"Siddhi 5.2 Quick Start Guide Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Siddhi is used by many companies including Uber, eBay, PayPal (via Apache Eagle), here Uber processed more than 20 billion events per day using Siddhi for their fraud analytics use cases. Siddhi is also used in various analytics and integration platforms such as Apache Eagle as a policy enforcement engine, WSO2 API Manager as analytics and throttling engine, WSO2 Identity Server as an adaptive authorization engine. This quick start guide contains the following six sections: Domain of Siddhi Overview of Siddhi architecture Using Siddhi for the first time Writing first Siddhi Application Testing Siddhi Application A bit of Stream Processing 1. Domain of Siddhi Siddhi is an event driven system where all the data it consumes, processes and sends are modeled as events. Therefore, Siddhi can play a vital part in any event-driven architecture. As Siddhi works with events, first let's understand what an event is through an example. If we consider transactions carried out via an ATM as a data stream, one withdrawal from it can be considered as an event . This event contains data such as amount, time, account number, etc. Many such transactions form a stream. Siddhi provides following functionalities, Streaming Data Analytics Forrester defines Streaming Analytics as: Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . Complex Event Processing (CEP) Gartner\u2019s IT Glossary defines CEP as follows: \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201d event data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" Streaming Data Integration Streaming data integration is a way of integrating several systems by processing, correlating, and analyzing the data in memory, while continuously moving data in real-time from one system to another. Alerts Notifications The system to continuously monitor event streams, and send alerts and notifications, based on defined KPIs and other analytics. Adaptive Decision Making A way to dynamically making real-time decisions based on predefined rules, the current state of the connected systems, and machine learning techniques. Basically, Siddhi receives data event-by-event and processes them in real-time to produce meaningful information. Using the above Siddhi can be used to solve may use-cases as follows: Fraud Analytics Monitoring System Integration Anomaly Detection Sentiment Analysis Processing Customer Behavior .. etc 2. Overview of Siddhi architecture As indicated above, Siddhi can: Accept event inputs from many different types of sources. Process them to transform, enrich, and generate insights. Publish them to multiple types of sinks. To use Siddhi, you need to write the processing logic as a Siddhi Application in the Siddhi Streaming SQL language which is discussed in the section 4 . Here a Siddhi Application is a script file that contains business logic for a scenario. When the Siddhi application is started, it: Consumes data one-by-one as events. Pipe the events to queries through various streams for processing. Generates new events based on the processing done at the queries. Finally, Sends newly generated events through output to streams. 3. Using Siddhi for the first time In this section, we will be using the Siddhi tooling distribution\u200a\u2014\u200aa server version of Siddhi that has a sophisticated web based editor with a GUI (referred to as \u201cSiddhi Editor\u201d ) where you can write Siddhi Apps and simulate events to test your scenario. Step 1 \u200a\u2014\u200aInstall Oracle Java SE Development Kit (JDK) version 1.8. Step 2 \u200a\u2014\u200a Set the JAVA_HOME environment variable. Step 3 \u200a\u2014\u200aDownload the latest tooling distribution from here . Step 4 \u200a\u2014\u200aExtract the downloaded zip and navigate to TOOLING_HOME /bin . ( TOOLING_HOME refers to the extracted folder) Step 5 \u200a\u2014\u200aIssue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh After successfully starting the Siddhi Editor, the terminal should look like as shown below: After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser (Google Chrome is the Recommended). http://localhost:9390/editor This takes you to the Siddhi Editor landing page. 4. Writing first Siddhi Application Siddhi Streaming SQL is a rich, compact, easy-to-use SQL-like language. As the first Siddhi Application, let's learn how to find the total of values from the incoming events and output the current running total value for each event. Siddhi has lot of in-built functions and extensions available for complex analysis, and you can find more information about the Siddhi grammar and its functions from the Siddhi Query Guide . Let's consider sample scenario where we are loading cargo boxes into a ship . Here, we need to keep track of the total weight of the cargo added, and the weight of each loaded cargo box is considered an event . We can write a Siddhi Application for the above scenario using the following 4 parts . Part 1\u200a\u2014\u200aGiving our Siddhi application a suitable name. This allows us to uniquely identity a Siddhi Application. In this example, let's name our application as \u201cHelloWorldApp\u201d @App:name(\"HelloWorldApp\") Part 2\u200a\u2014\u200aDefining the input stream. The stream needs to have a name and a schema defining the data that each incoming event should contain. The event data attributes are expressed as name and type pairs. We can also attach a \"source\" to the created stream, so that we can consume events from outside and send them to the stream. ( Source is the Siddhi way to consume streams from external systems ). For this scenario we will use an http source to consume Cargo Events. When added the http source will spin up a HTTP endpoint and keep on listening for messages. To learn more about sources, refer source ) In this scenario: The name of the input stream\u200a\u2014\u200a \u201cCargoStream\u201d This contains only one data attribute: The name of the data in each event\u200a\u2014\u200a \u201cweight\u201d Type of the data \u201cweight\u201d \u200a\u2014\u200aint Type of source - HTTP HTTP endpoint address - http://0.0.0.0:8006/cargo Accepted input data format - JSON @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); Part 3 - Defining the output stream. This has the same info as the input \u201cCargoStream\u201d stream\u200adefinition with an additional totalWeight attribute containing the total weight calculated so far. In addition we also need to add a log \"sink\" to log the OutputStream so that we can observe the output produced by the stream. ( Sink is the Siddhi way to publish streams to external systems ). This particular log type sink simply logs the stream events. To learn more about sinks, refer sink ) @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); Part 4\u200a\u2014\u200aWriting the Siddhi query. As part of the query we need to specify the following: A name for the query\u200a\u2014\u200a \u201cHelloWorldQuery\u201d The input stream from which the query consumes events \u2014\u200a \u201cCargoStream\u201d How the output to be calculated - by calculating the sum of the *weight**s The data outputted to the output stream\u200a\u2014\u200a \u201cweight\u201d , \u201ctotalWeight\u201d The output stream to which the event should be outputted\u200a\u2014\u200a \u201cOutputStream\u201d @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; This query will calculate the sum of weights from the start of the Siddhi application. For more complex use cases refer Siddhi Query Guild ) Final Siddhi application in the editor will look like following. You can copy the final Siddhi app from below. @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; 5. Testing Siddhi Application In this section first we will test the logical accuracy of Siddhi query using in-built functions of Siddhi Editor. In a later section we will invoke the HTTP endpoint and perform an end to end test. The Siddhi Editor has in-built support to simulate events. You can do it via the \u201cEvent Simulator\u201d panel at the left of the Siddhi Editor. Before running the event simulation, you should save your HelloWorldApp by browsing to File menu - and clicking Save . To simulate events, click Event Simulator and configure Single Simulation as shown below. Step 1\u200a\u2014\u200aConfigurations: Siddhi App Name\u200a\u2014\u200a \u201cHelloWorldApp\u201d Stream Name\u200a\u2014\u200a \u201cCargoStream\u201d Timestamp\u200a\u2014\u200a(Leave it blank) weight\u200a\u2014\u200a2 (or some integer) Step 2\u200a\u2014\u200aClick \u201cRun\u201d mode and then click \u201cStart and Send\u201d . This starts the Siddhi Application and send the event. If the Siddhi application is successfully started, the following message is printed in the Stream Processor Studio console: HelloWorldApp.siddhi Started Successfully! Step 3\u200a\u2014\u200aClick \u201cSend\u201d and observe the terminal . This will send a new event for each click. You can see a logs containing outputData=[2, 2] and outputData=[2, 4] , etc. You can change the value of the weight and send it to see how the sum of the weight is updated. Bravo! You have successfully completed building and testing your first Siddhi Application! 6. A bit of Stream Processing This section will improve our Siddhi app to demonstrates how to carry out temporal window processing with Siddhi. Up to this point, we are calculating the sum of weights from the start of the Siddhi app, and now let's improve it to consider only the last three events for the calculation. For this scenario, let's imagine that when we are loading cargo boxes into the ship and we need to keep track of the average weight of the last three loaded boxes so that we can balance the weight across the ship. For this purpose, let's try to find the average weight of last three boxes of each event. For window processing, we need to modify our query as follows: @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; from CargoStream#window.length(3) - Specifies that we need to consider the last three events in a sliding manner. avg(weight) as averageWeight - Specifies calculating the average of events stored in the window and producing the results as \"averageWeight\" (Note: Similarly the sum also calculates the totalWeight based on the last three events). We also need to modify the \"OutputStream\" definition to accommodate the new \"averageWeight\" . define stream OutputStream(weight int, totalWeight long, averageWeight double); The updated Siddhi Application is given below: @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\",@map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long, averageWeight double); @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; Now you can send events using the Event Simulator and observe the log to see the sum and average of the weights based on the last three cargo events. In the earlier scenario when the window is not used, the system only stored the running sum in its memory, and it did not store any events. But for length based window processing the system will retain the events that fall into the window to perform aggregation operations such as average, maximum, etc. In this case when the 4 th event arrives, the first event in the window is removed ensuring the memory usage does not grow beyond a specific limit. Note: some window types in Siddhi are even more optimized to perform the operations with minimal or no event retention. 7. Running Siddhi Application as a Docker microservice In this step we will run above developed Siddhi application as a microservice utilizing Docker. For other available options please refer here . Here we will use siddhi-runner docker distribution. Follow the below steps to obtain the docker. Install docker in your machine and start the daemon ( https://docs.docker.com/install/ ). Pull the latest siddhi-runner image by executing below command. docker pull siddhiio/siddhi-runner-alpine:latest * Navigate to Siddhi Editor and choose File - Export File for download above Siddhi application as a file. * Move downloaded Siddhi file( HelloWorldApp.siddhi ) to a desired location (e.g. /home/me/siddhi-apps ) * Execute below command to start the Siddhi Application as a microservice. docker run -it -p 8006:8006 -v /home/me/siddhi-apps:/apps siddhiio/siddhi-runner-alpine -Dapps=/apps/HelloWorldApp.siddhi Note: Make sure to update the /home/me/siddhi-apps with the folder path you have stored the HelloWorldApp.siddhi app. * Once container is started use below curl command to send events into \"CargoStream\" curl -X POST http://localhost:8006/cargo \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"weight\":2}}' * You will be able to observe outputs via logs as shown below. [2019-04-24 08:54:51,755] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096091751, data=[2, 2, 2.0], isExpired=false} [2019-04-24 08:56:25,307] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096185307, data=[2, 4, 2.0], isExpired=false} To learn more about the Siddhi functionality, see Siddhi Documentation . If you have questions please post them on Stackoverflow with \"Siddhi\" tag.","title":"Quick Start"},{"location":"docs/quick-start/#siddhi-52-quick-start-guide","text":"Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Siddhi is used by many companies including Uber, eBay, PayPal (via Apache Eagle), here Uber processed more than 20 billion events per day using Siddhi for their fraud analytics use cases. Siddhi is also used in various analytics and integration platforms such as Apache Eagle as a policy enforcement engine, WSO2 API Manager as analytics and throttling engine, WSO2 Identity Server as an adaptive authorization engine. This quick start guide contains the following six sections: Domain of Siddhi Overview of Siddhi architecture Using Siddhi for the first time Writing first Siddhi Application Testing Siddhi Application A bit of Stream Processing","title":"Siddhi 5.2 Quick Start Guide"},{"location":"docs/quick-start/#1-domain-of-siddhi","text":"Siddhi is an event driven system where all the data it consumes, processes and sends are modeled as events. Therefore, Siddhi can play a vital part in any event-driven architecture. As Siddhi works with events, first let's understand what an event is through an example. If we consider transactions carried out via an ATM as a data stream, one withdrawal from it can be considered as an event . This event contains data such as amount, time, account number, etc. Many such transactions form a stream. Siddhi provides following functionalities, Streaming Data Analytics Forrester defines Streaming Analytics as: Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . Complex Event Processing (CEP) Gartner\u2019s IT Glossary defines CEP as follows: \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201d event data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" Streaming Data Integration Streaming data integration is a way of integrating several systems by processing, correlating, and analyzing the data in memory, while continuously moving data in real-time from one system to another. Alerts Notifications The system to continuously monitor event streams, and send alerts and notifications, based on defined KPIs and other analytics. Adaptive Decision Making A way to dynamically making real-time decisions based on predefined rules, the current state of the connected systems, and machine learning techniques. Basically, Siddhi receives data event-by-event and processes them in real-time to produce meaningful information. Using the above Siddhi can be used to solve may use-cases as follows: Fraud Analytics Monitoring System Integration Anomaly Detection Sentiment Analysis Processing Customer Behavior .. etc","title":"1. Domain of Siddhi"},{"location":"docs/quick-start/#2-overview-of-siddhi-architecture","text":"As indicated above, Siddhi can: Accept event inputs from many different types of sources. Process them to transform, enrich, and generate insights. Publish them to multiple types of sinks. To use Siddhi, you need to write the processing logic as a Siddhi Application in the Siddhi Streaming SQL language which is discussed in the section 4 . Here a Siddhi Application is a script file that contains business logic for a scenario. When the Siddhi application is started, it: Consumes data one-by-one as events. Pipe the events to queries through various streams for processing. Generates new events based on the processing done at the queries. Finally, Sends newly generated events through output to streams.","title":"2. Overview of Siddhi architecture"},{"location":"docs/quick-start/#3-using-siddhi-for-the-first-time","text":"In this section, we will be using the Siddhi tooling distribution\u200a\u2014\u200aa server version of Siddhi that has a sophisticated web based editor with a GUI (referred to as \u201cSiddhi Editor\u201d ) where you can write Siddhi Apps and simulate events to test your scenario. Step 1 \u200a\u2014\u200aInstall Oracle Java SE Development Kit (JDK) version 1.8. Step 2 \u200a\u2014\u200a Set the JAVA_HOME environment variable. Step 3 \u200a\u2014\u200aDownload the latest tooling distribution from here . Step 4 \u200a\u2014\u200aExtract the downloaded zip and navigate to TOOLING_HOME /bin . ( TOOLING_HOME refers to the extracted folder) Step 5 \u200a\u2014\u200aIssue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh After successfully starting the Siddhi Editor, the terminal should look like as shown below: After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser (Google Chrome is the Recommended). http://localhost:9390/editor This takes you to the Siddhi Editor landing page.","title":"3. Using Siddhi for the first time"},{"location":"docs/quick-start/#4-writing-first-siddhi-application","text":"Siddhi Streaming SQL is a rich, compact, easy-to-use SQL-like language. As the first Siddhi Application, let's learn how to find the total of values from the incoming events and output the current running total value for each event. Siddhi has lot of in-built functions and extensions available for complex analysis, and you can find more information about the Siddhi grammar and its functions from the Siddhi Query Guide . Let's consider sample scenario where we are loading cargo boxes into a ship . Here, we need to keep track of the total weight of the cargo added, and the weight of each loaded cargo box is considered an event . We can write a Siddhi Application for the above scenario using the following 4 parts . Part 1\u200a\u2014\u200aGiving our Siddhi application a suitable name. This allows us to uniquely identity a Siddhi Application. In this example, let's name our application as \u201cHelloWorldApp\u201d @App:name(\"HelloWorldApp\") Part 2\u200a\u2014\u200aDefining the input stream. The stream needs to have a name and a schema defining the data that each incoming event should contain. The event data attributes are expressed as name and type pairs. We can also attach a \"source\" to the created stream, so that we can consume events from outside and send them to the stream. ( Source is the Siddhi way to consume streams from external systems ). For this scenario we will use an http source to consume Cargo Events. When added the http source will spin up a HTTP endpoint and keep on listening for messages. To learn more about sources, refer source ) In this scenario: The name of the input stream\u200a\u2014\u200a \u201cCargoStream\u201d This contains only one data attribute: The name of the data in each event\u200a\u2014\u200a \u201cweight\u201d Type of the data \u201cweight\u201d \u200a\u2014\u200aint Type of source - HTTP HTTP endpoint address - http://0.0.0.0:8006/cargo Accepted input data format - JSON @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); Part 3 - Defining the output stream. This has the same info as the input \u201cCargoStream\u201d stream\u200adefinition with an additional totalWeight attribute containing the total weight calculated so far. In addition we also need to add a log \"sink\" to log the OutputStream so that we can observe the output produced by the stream. ( Sink is the Siddhi way to publish streams to external systems ). This particular log type sink simply logs the stream events. To learn more about sinks, refer sink ) @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); Part 4\u200a\u2014\u200aWriting the Siddhi query. As part of the query we need to specify the following: A name for the query\u200a\u2014\u200a \u201cHelloWorldQuery\u201d The input stream from which the query consumes events \u2014\u200a \u201cCargoStream\u201d How the output to be calculated - by calculating the sum of the *weight**s The data outputted to the output stream\u200a\u2014\u200a \u201cweight\u201d , \u201ctotalWeight\u201d The output stream to which the event should be outputted\u200a\u2014\u200a \u201cOutputStream\u201d @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; This query will calculate the sum of weights from the start of the Siddhi application. For more complex use cases refer Siddhi Query Guild ) Final Siddhi application in the editor will look like following. You can copy the final Siddhi app from below. @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream;","title":"4. Writing first Siddhi Application"},{"location":"docs/quick-start/#5-testing-siddhi-application","text":"In this section first we will test the logical accuracy of Siddhi query using in-built functions of Siddhi Editor. In a later section we will invoke the HTTP endpoint and perform an end to end test. The Siddhi Editor has in-built support to simulate events. You can do it via the \u201cEvent Simulator\u201d panel at the left of the Siddhi Editor. Before running the event simulation, you should save your HelloWorldApp by browsing to File menu - and clicking Save . To simulate events, click Event Simulator and configure Single Simulation as shown below. Step 1\u200a\u2014\u200aConfigurations: Siddhi App Name\u200a\u2014\u200a \u201cHelloWorldApp\u201d Stream Name\u200a\u2014\u200a \u201cCargoStream\u201d Timestamp\u200a\u2014\u200a(Leave it blank) weight\u200a\u2014\u200a2 (or some integer) Step 2\u200a\u2014\u200aClick \u201cRun\u201d mode and then click \u201cStart and Send\u201d . This starts the Siddhi Application and send the event. If the Siddhi application is successfully started, the following message is printed in the Stream Processor Studio console: HelloWorldApp.siddhi Started Successfully! Step 3\u200a\u2014\u200aClick \u201cSend\u201d and observe the terminal . This will send a new event for each click. You can see a logs containing outputData=[2, 2] and outputData=[2, 4] , etc. You can change the value of the weight and send it to see how the sum of the weight is updated. Bravo! You have successfully completed building and testing your first Siddhi Application!","title":"5. Testing Siddhi Application"},{"location":"docs/quick-start/#6-a-bit-of-stream-processing","text":"This section will improve our Siddhi app to demonstrates how to carry out temporal window processing with Siddhi. Up to this point, we are calculating the sum of weights from the start of the Siddhi app, and now let's improve it to consider only the last three events for the calculation. For this scenario, let's imagine that when we are loading cargo boxes into the ship and we need to keep track of the average weight of the last three loaded boxes so that we can balance the weight across the ship. For this purpose, let's try to find the average weight of last three boxes of each event. For window processing, we need to modify our query as follows: @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; from CargoStream#window.length(3) - Specifies that we need to consider the last three events in a sliding manner. avg(weight) as averageWeight - Specifies calculating the average of events stored in the window and producing the results as \"averageWeight\" (Note: Similarly the sum also calculates the totalWeight based on the last three events). We also need to modify the \"OutputStream\" definition to accommodate the new \"averageWeight\" . define stream OutputStream(weight int, totalWeight long, averageWeight double); The updated Siddhi Application is given below: @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\",@map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long, averageWeight double); @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; Now you can send events using the Event Simulator and observe the log to see the sum and average of the weights based on the last three cargo events. In the earlier scenario when the window is not used, the system only stored the running sum in its memory, and it did not store any events. But for length based window processing the system will retain the events that fall into the window to perform aggregation operations such as average, maximum, etc. In this case when the 4 th event arrives, the first event in the window is removed ensuring the memory usage does not grow beyond a specific limit. Note: some window types in Siddhi are even more optimized to perform the operations with minimal or no event retention.","title":"6. A bit of Stream Processing"},{"location":"docs/quick-start/#7-running-siddhi-application-as-a-docker-microservice","text":"In this step we will run above developed Siddhi application as a microservice utilizing Docker. For other available options please refer here . Here we will use siddhi-runner docker distribution. Follow the below steps to obtain the docker. Install docker in your machine and start the daemon ( https://docs.docker.com/install/ ). Pull the latest siddhi-runner image by executing below command. docker pull siddhiio/siddhi-runner-alpine:latest * Navigate to Siddhi Editor and choose File - Export File for download above Siddhi application as a file. * Move downloaded Siddhi file( HelloWorldApp.siddhi ) to a desired location (e.g. /home/me/siddhi-apps ) * Execute below command to start the Siddhi Application as a microservice. docker run -it -p 8006:8006 -v /home/me/siddhi-apps:/apps siddhiio/siddhi-runner-alpine -Dapps=/apps/HelloWorldApp.siddhi Note: Make sure to update the /home/me/siddhi-apps with the folder path you have stored the HelloWorldApp.siddhi app. * Once container is started use below curl command to send events into \"CargoStream\" curl -X POST http://localhost:8006/cargo \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"weight\":2}}' * You will be able to observe outputs via logs as shown below. [2019-04-24 08:54:51,755] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096091751, data=[2, 2, 2.0], isExpired=false} [2019-04-24 08:56:25,307] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096185307, data=[2, 4, 2.0], isExpired=false} To learn more about the Siddhi functionality, see Siddhi Documentation . If you have questions please post them on Stackoverflow with \"Siddhi\" tag.","title":"7. Running Siddhi Application as a Docker microservice"}]}